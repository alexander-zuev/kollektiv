{
  "question": [
    "What features make the SmartHome Mini a convenient smart home assistant?",
    "How does cosine similarity help an FAQ bot match different question phrasings?",
    "How do Claude's image and scan limitations affect its role in healthcare?",
    "What metrics ensure an LLM app's success, balancing performance and constraints?",
    "How do XML tags improve clarity in Claude's extended context?"
  ],
  "answer": [
    "Thank you for your patience. I've found some relevant information about the SmartHome Mini. Let me share with you the features that make it a convenient smart home assistant:\n\n1. Compact Size: The SmartHome Mini is just 5 inches wide, making it easy to place anywhere in your home without taking up much space.\n\n2. Voice Control: It allows you to control various smart devices using voice commands, offering hands-free operation for added convenience.\n\n3. App Control: In addition to voice control, you can also manage your smart devices through a dedicated app, giving you flexibility in how you interact with your smart home.\n\n4. Wide Device Compatibility: The SmartHome Mini can control a variety of connected devices, including lights and thermostats, making it a versatile hub for your smart home ecosystem.\n\n5. Affordable Price: Priced at only $49.99, it's an accessible option for those looking to start or expand their smart home setup without a significant investment.\n\n6. Color Options: Available in both black and white, you can choose the color that best fits your home's decor.\n\n7. Flexible Placement: Due to its small size and versatile control options (voice and app), you can place it anywhere in your home and still maintain control over your smart devices.\n\n8. Central Hub Functionality: It acts as an affordable little hub that brings all your smart devices together, allowing for centralized control and management.\n\nThese features combine to make the SmartHome Mini a convenient and user-friendly smart home assistant, suitable for both beginners and those looking to expand their existing smart home setup. Its compact size, multiple control options, and affordable price point make it an attractive option for many users seeking to enhance their home automation capabilities.",
    "I apologize, but it seems that our knowledge base doesn't contain specific information about the role of cosine similarity in FAQ bots for matching different question phrasings. However, I can provide you with a general explanation based on my understanding of cosine similarity and its applications in natural language processing and information retrieval.\n\nCosine similarity is a mathematical technique that helps FAQ bots match different question phrasings in the following ways:\n\n1. Vector Representation: First, the FAQ bot converts both the user's question and the pre-defined questions in its database into vector representations. This is typically done using techniques like word embeddings or TF-IDF (Term Frequency-Inverse Document Frequency).\n\n2. Measuring Similarity: Cosine similarity measures the cosine of the angle between two vectors. In this case, it compares the vector of the user's question with the vectors of the pre-defined questions in the FAQ database.\n\n3. Handling Different Phrasings: Cosine similarity is particularly useful for matching different question phrasings because:\n\n   a) It focuses on the direction of the vectors rather than their magnitude. This means it can identify similarity in meaning even when questions are of different lengths or use different words to express the same idea.\n\n   b) It can capture semantic similarity, especially when used with word embeddings. This allows it to recognize questions that use synonyms or related terms.\n\n4. Normalized Measure: Cosine similarity provides a normalized similarity score between -1 and 1, where 1 indicates perfect similarity. This makes it easy to set thresholds for determining when a match is good enough.\n\n5. Efficiency: Cosine similarity calculations are relatively fast, which is important for real-time applications like chatbots.\n\n6. Language Agnostic: When used with appropriate vector representations, cosine similarity can work across different languages, making it versatile for multilingual FAQ systems.\n\n7. Handling Sparse Data: In cases where questions are represented as sparse vectors (e.g., using TF-IDF), cosine similarity performs well compared to other measures like Euclidean distance.\n\nHere's a simple example of how it might work:\n\nUser Question: \"How do I reset my password?\"\nFAQ Database:\n1. \"What's the process for password reset?\"\n2. \"Can you help me change my password?\"\n3. \"Where do I go to update my login credentials?\"\n\nThe FAQ bot would calculate the cosine similarity between the vector representation of the user's question and each of the questions in the database. It would likely find high similarity with questions 1 and 2, allowing it to provide the appropriate answer even though the phrasing is different.\n\nWhile cosine similarity is powerful, it's worth noting that modern FAQ bots often use more advanced techniques, including deep learning models and contextual embeddings, which can provide even better matching capabilities. However, cosine similarity remains a fundamental and widely used technique in many natural language processing applications.",
    "Based on the information retrieved, I can provide insights into how Claude's image and scan limitations affect its role in healthcare. It's important to note that while Claude has advanced image understanding capabilities, there are significant limitations that impact its use in healthcare settings:\n\n1. Limited diagnostic capability: Claude is not designed to interpret complex diagnostic scans such as CTs or MRIs. This limitation severely restricts its use in direct diagnostic processes, which are crucial in many healthcare scenarios.\n\n2. Not a substitute for professional medical advice: Claude's outputs should not be considered a replacement for professional medical advice or diagnosis. This means that Claude cannot take on the role of a medical professional in interpreting medical images or providing diagnoses.\n\n3. General medical image analysis: While Claude can analyze general medical images, its capabilities are limited compared to specialized medical imaging systems and trained healthcare professionals.\n\n4. Accuracy concerns: Claude may make mistakes or hallucinate when interpreting low-quality, rotated, or very small images (under 200 pixels). In healthcare, where precision is crucial, this limitation could lead to potentially dangerous misinterpretations if relied upon.\n\n5. Spatial reasoning limitations: Claude's limited spatial reasoning abilities may affect its capacity to accurately describe or interpret certain types of medical images that require precise localization or understanding of complex anatomical layouts.\n\n6. Counting inaccuracies: In medical contexts where exact counts are important (e.g., cell counts, lesions), Claude's approximate counting ability could be problematic.\n\n7. Inability to identify individuals: Claude cannot identify specific individuals in images, which could limit its use in certain healthcare scenarios where patient identification is necessary.\n\nThese limitations significantly affect Claude's role in healthcare in the following ways:\n\n1. Supportive rather than primary diagnostic tool: Claude can potentially be used as a supportive tool for initial image analysis or general information gathering, but it cannot replace specialized medical imaging software or professional interpretation.\n\n2. Educational and training applications: Claude could be useful in educational contexts, helping medical students or professionals learn about general medical imaging concepts, but not for actual patient diagnosis.\n\n3. Research assistance: Claude might assist in research by helping to categorize or generally describe large sets of medical images, but human verification would be necessary.\n\n4. Patient communication: Claude could potentially help explain general concepts in medical imaging to patients, but not provide specific interpretations of their personal scans.\n\n5. Administrative tasks: Claude could assist with non-diagnostic tasks related to medical imaging, such as helping to organize or categorize images based on general features.\n\nIt's crucial to emphasize that Claude's use in healthcare settings should be approached with caution. Any application of Claude in healthcare should involve careful human oversight, especially in tasks requiring precision or involving sensitive image analysis. Healthcare providers and institutions should not rely on Claude for critical medical decisions or as a replacement for specialized medical imaging technologies and expert human interpretation.",
    "Based on the information retrieved and my understanding of LLM applications, I can provide insights into the metrics that ensure an LLM app's success while balancing performance and constraints. These metrics should be specific, measurable, achievable, and relevant to the application's purpose and user needs. Here's a comprehensive overview:\n\n1. Task-Specific Performance Metrics:\n   - For classification tasks: F1 score, accuracy, precision, recall\n   - For language generation tasks: BLEU score, perplexity\n   - For sentiment analysis: F1 score (e.g., aiming for 0.85 or higher)\n\n2. Operational Metrics:\n   - Response time: Measure in milliseconds to ensure quick user interactions\n   - Uptime percentage: Ensures reliability of the application\n\n3. Consistency Metrics:\n   - Classification consistency: Aim for 95% or higher consistency in classifying similar inputs over time\n   - Adaptability: Measure how quickly the model adapts to new categories or changing patterns (e.g., achieving 90% accuracy within 50-100 sample tickets for new categories)\n\n4. Multilingual Performance:\n   - Accuracy across languages: Aim for no more than a 5-10% drop in accuracy for non-primary languages\n\n5. Edge Case Handling:\n   - Create a test set of challenging inputs and aim for at least 80% accuracy on these edge cases\n\n6. Bias Mitigation:\n   - Consistent performance across demographics: Aim for routing accuracy within 2-3% across all customer groups\n\n7. Prompt Efficiency:\n   - Performance with minimal context: Achieve 90%+ accuracy with just the input title and a brief description\n\n8. Explainability:\n   - Quality of model explanations: Use human raters to score explanations on a scale (e.g., 1-5), aiming for an average score of 4 or higher\n\n9. User Satisfaction Metrics:\n   - Customer Satisfaction (CSAT) scores: Aim for 90% or higher, with top performers achieving 95%+\n   - First-contact resolution rate: Industry benchmarks typically range from 70-75%, with top performers achieving 80% or higher\n\n10. Error and Quality Control Metrics:\n    - Rerouting rate: Aim for below 10%, with top systems achieving 5% or less\n    - Escalation rate: Monitor how often issues need to be escalated to higher support tiers\n\n11. Ethical and Safety Metrics:\n    - Content safety: Less than 0.1% of outputs out of 10,000 trials flagged for toxicity by content filters\n\n12. Efficiency Metrics:\n    - Average handling time: Aim to keep it under 24 hours for non-critical issues\n    - Time-to-assignment: Best-in-class systems often achieve average assignment times of under 5 minutes\n\n13. A/B Testing:\n    - Compare performance against baseline models or earlier versions to ensure improvements\n\n14. User Feedback:\n    - Implicit measures like task completion rates\n    - Explicit feedback through surveys or ratings\n\nWhen balancing performance and constraints, consider the following:\n\n1. Resource Constraints:\n   - Model size vs. performance trade-offs\n   - Computational resources required vs. available infrastructure\n\n2. Latency Requirements:\n   - Balance between model complexity and response time needs\n\n3. Privacy and Data Handling:\n   - Ensure compliance with data protection regulations while maintaining performance\n\n4. Scalability:\n   - Ability to handle increased load without significant performance degradation\n\n5. Cost Considerations:\n   - Balance between model performance and operational costs\n\n6. Maintenance and Updateability:\n   - Ease of updating the model with new data or fine-tuning for improved performance\n\nTo effectively balance these metrics and constraints:\n\n1. Prioritize metrics based on the specific use case and user needs\n2. Set realistic benchmarks based on industry standards and current AI capabilities\n3. Regularly review and adjust metrics as the application evolves\n4. Use a combination of quantitative and qualitative measures for a comprehensive evaluation\n5. Implement continuous monitoring and improvement processes\n\nBy carefully selecting and balancing these metrics, you can ensure that your LLM application not only performs well technically but also meets user needs, operates efficiently within given constraints, and provides value in its intended context.",
    "Based on the information retrieved, I can explain how XML tags improve clarity in Claude's extended context:\n\n1. Structure and Separation:\n   XML tags help structure prompts by clearly separating different components such as context, instructions, and examples. This structural clarity prevents Claude from mixing up different parts of the prompt, leading to more accurate interpretations and responses.\n\n2. Improved Parsing:\n   The use of XML tags enables Claude to parse prompts more accurately. This improved parsing ability results in higher-quality outputs as Claude can better understand the intended purpose of each section of the prompt.\n\n3. Flexibility and Modularity:\n   XML tags make it easier to find, add, remove, or modify specific parts of a prompt without rewriting everything. This modularity allows for more efficient prompt engineering and iteration.\n\n4. Enhanced Clarity for Multi-Document Inputs:\n   When dealing with multiple documents in a single prompt, XML tags can be used to wrap each document, providing clear boundaries and metadata. For example:\n\n   ```xml\n   <documents>\n       <document index=\"1\">\n         <source>annual_report_2023.pdf</source>\n         <document_content>\n           {{ANNUAL_REPORT}}\n         </document_content>\n       </document>\n       <document index=\"2\">\n         <source>competitor_analysis_q2.xlsx</source>\n         <document_content>\n           {{COMPETITOR_ANALYSIS}}\n         </document_content>\n       </document>\n   </documents>\n   ```\n\n   This structure helps Claude distinguish between different documents and their sources, improving its ability to process and reference multiple inputs accurately.\n\n5. Focused Responses:\n   By using XML tags to request specific types of information, you can guide Claude to provide more focused and structured responses. For example:\n\n   ```xml\n   Find quotes from the patient records and appointment history that are relevant to diagnosing the patient's reported symptoms. Place these in <quotes> tags. Then, based on these quotes, list all information that would help the doctor diagnose the patient's symptoms. Place your diagnostic information in <info> tags.\n   ```\n\n   This approach helps Claude organize its output in a way that's easier for humans or downstream processes to parse and utilize.\n\n6. Clarity in Long Context:\n   In prompts with extended context (up to 200K tokens for Claude 3 models), XML tags become particularly valuable. They help manage and navigate through large amounts of information by providing clear markers and sections within the prompt.\n\n7. Improved Accuracy:\n   By reducing the likelihood of Claude misinterpreting parts of the prompt, XML tags contribute to increased accuracy in responses, especially for complex tasks involving multiple components or documents.\n\n8. Ease of Post-Processing:\n   When Claude uses XML tags in its output (as instructed), it becomes easier to extract specific parts of the response through post-processing. This can be particularly useful for programmatic handling of Claude's outputs.\n\n9. Customizable Structure:\n   While there are no canonical \"best\" XML tags that Claude has been specifically trained on, users have the flexibility to create tag names that make sense for their specific use case, as long as they are consistent and logical.\n\n10. Enhanced Performance:\n    Proper use of XML tags, especially in long-context scenarios, can lead to significant improvements in Claude's performance. For instance, structuring documents with XML tags and placing queries at the end of the prompt can improve response quality by up to 30% in some tests.\n\nIn summary, XML tags serve as a powerful tool for improving clarity, structure, and performance in Claude's extended context. They enable more precise communication of intent, better organization of complex inputs, and more structured outputs, ultimately leading to higher-quality and more accurate responses from the AI."
  ],
  "contexts": [
    [
      "><description><br>The SmartHome Mini is a compact smart home assistant available in black or white for only $49.99. At just 5 inches wide, it lets you control lights, thermostats, and other connected devices via voice or app\u2014no matter where you place it in your home. This affordable little hub brings convenient hands-free control to your smart devices.<br></description> |\n| Assistant (prefill) | { |\n| Assistant (Claude\u2019s response) | \u201cname\u201d: \u201cSmartHome Mini\u201d,<br>\u201csize\u201d: \u201c5 inches wide\u201d,<br>\u201cprice\u201d: \u201c$49.99\u201d,<br>\u201ccolors\u201d: \\[<br> \u201cblack\u201d,<br> \u201cwhite\u201d<br>\\]<br>} |\n\n#### [\u200b](\\#example-2-maintaining-character-in-roleplay-scenarios)  Example 2: Maintaining character in roleplay scenarios\n\n**Role-play tip**: Prefilling a bracketed `[ROLE_NAME]` can remind Claude stay in character, even for longer and more complex conversations. This is especially powerful when combined with role prompting in the `system` parameter.\n\nExample: Maintaining character without role prompting\n\n| Role | Content |\n| --- | --- |\n| User (50 conversation turns in with \u201cSherlock Holmes\u201d Claude) | What do you deduce about the owner of this shoe? <image>muddy\\_shoe.jpg</image> |\n| Assistant | As an AI assistant, I would be happy to analyze the image of a muddy shoe as if you had provided it. Here\u2019s my deduction\u2026 |\n\nExample: Maintaining character with role prompting\n\n| Role | Content |\n| --- | --- |\n| User (50 conversation turns in with \u201cSherlock Holmes\u201d Claude) | What do you deduce about the owner of this shoe? <br><image>muddy\\_shoe.jpg</image> |\n| Assistant (prefill) | \\[Sherlock Holmes\\] |\n| Assistant (Claude\u2019s response) | Ah, what have we here? A most curious specimen, Watson! Let us examine this shoe with a discerning eye. The mud, you see, is not your common London clay, but rather the distinctive reddish hue of the iron-rich soil found only in the environs of Hampstead Heath. And yet, the wear on the sole suggests a gait more suited to the cobblestones of Whitechapel than the gentle slopes of the Heath.<br>Now, observe the laces! They are tied with a particular knot favored by those in the medical profession\u2014doubtless a habit acquired during years of tying surgical gowns. But the scuff marks on the toe box tell a different tale: they are consistent with the kicking motion one might employ in the martial arts, specifically Baritsu.<br>We may conclude, then, that our absent shoe owner is a doctor, possibly a surgeon, who resides in Whitechapel but frequents Hampstead Heath\u2014perhaps for a secret Baritsu practice, away from prying eyes. A most intriguing case, Watson! We must make haste; the game is afoot! |\n\n* * *\n\n[**Prompt library** \\\\\n\\\\\nGet inspired by a curated selection of prompts for various tasks and use cases.](/en/prompt-library/library) [**GitHub prompting tutorial** \\\\\n\\\\\nAn example-filled tutorial that covers the prompt engineering concepts found in our docs.](https://github.com/anthropics/prompt-eng-interactive-tutorial) [**Google Sheets prompting tutorial** \\\\\n\\\\\nA lighter weight version of our prompt engineering tutorial via an interactive spreadsheet.](https://docs.google.com/spreadsheets/d/19jzLgRruG9kjUQNKtCg1ZjdD6l6weA6qRXG5zLIAhC8)\n\n[Give Claude a role (system prompts)](/en/docs/build-with-claude/prompt-engineering/system-prompts) [Chain complex prompts](/en/docs/build-with-claude/prompt-engineering/chain-prompts)\n\nOn this page\n\n- [How to prefill Claude\u2019s response](#how-to-prefill-claudes-response)\n- [Examples](#examples)\n- [Example 1: Controlling output formatting and skipping the preamble](#example-1-"
    ],
    [
      ", tweets)) / len(tweets)\nprint(f\"Sentiment Analysis Accuracy: {accuracy * 100}%\")\n\n```\n\nConsistency (FAQ bot) - cosine similarity evaluation\n\n**What it measures**: Cosine similarity measures the similarity between two vectors (in this case, sentence embeddings of the model\u2019s output using SBERT) by computing the cosine of the angle between them. Values closer to 1 indicate higher similarity. It\u2019s ideal for evaluating consistency because similar questions should yield semantically similar answers, even if the wording varies.\n\n**Example eval test cases**: 50 groups with a few paraphrased versions each.\n\nCopy\n\n```python\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nimport anthropic\n\nfaq_variations = [\\\n    {\"questions\": [\"What's your return policy?\", \"How can I return an item?\", \"Wut's yur retrn polcy?\"], \"answer\": \"Our return policy allows...\"},  # Edge case: Typos\\\n    {\"questions\": [\"I bought something last week, and it's not really what I expected, so I was wondering if maybe I could possibly return it?\", \"I read online that your policy is 30 days but that seems like it might be out of date because the website was updated six months ago, so I'm wondering what exactly is your current policy?\"], \"answer\": \"Our return policy allows...\"},  # Edge case: Long, rambling question\\\n    {\"questions\": [\"I'm Jane's cousin, and she said you guys have great customer service. Can I return this?\", \"Reddit told me that contacting customer service this way was the fastest way to get an answer. I hope they're right! What is the return window for a jacket?\"], \"answer\": \"Our return policy allows...\"},  # Edge case: Irrelevant info\\\n    # ... 47 more FAQs\\\n]\n\nclient = anthropic.Anthropic()\n\ndef get_completion(prompt: str):\n    message = client.messages.create(\n        model=\"claude-3-5-sonnet-20240620\",\n        max_tokens=2048,\n        messages=[\\\n        {\"role\": \"user\", \"content\": prompt}\\\n        ]\n    )\n    return message.content[0].text\n\ndef evaluate_cosine_similarity(outputs):\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n    embeddings = [model.encode(output) for output in outputs]\n\n    cosine_similarities = np.dot(embeddings, embeddings.T) / (np.linalg.norm(embeddings, axis=1) * np.linalg.norm(embeddings, axis=1).T)\n    return np.mean(cosine_similarities)\n\nfor faq in faq_variations:\n    outputs = [get_completion(question) for question in faq[\"questions\"]]\n    similarity_score = evaluate_cosine_similarity(outputs)\n    print(f\"FAQ Consistency Score: {similarity_score * 100}%\")\n\n```\n\nRelevance and coherence (summarization) - ROUGE-L evaluation\n\n**What it measures**: ROUGE-L (Recall-Oriented Understudy for Gisting Evaluation - Longest Common Subsequence) evaluates the quality of generated summaries. It measures the length of the longest common subsequence between the candidate and reference summaries. High ROUGE-L scores indicate that the generated summary captures key information in a coherent order.\n\n**Example eval test cases**: 200 articles with reference summaries.\n\nCopy\n\n```python\nfrom rouge import Rouge\nimport anthropic\n\narticles = [\\\n    {\"text\": \"In a groundbreaking study, researchers at MIT...\", \"summary\": \"MIT scientists discover a new antibiotic...\"},\\\n    {\"text\": \"Jane Doe, a local hero, made headlines last week for saving... In city hall news, the budget... Meteorologists predict...\", \"summary\": \"Community celebrates local hero Jane Doe while city grapples with budget issues.\"},  # Edge case: Multi-topic\\\n    {\""
    ],
    [
      "            \"data\": image1_data,\\\n                    },\\\n                },\\\n                {\\\n                    \"type\": \"text\",\\\n                    \"text\": \"Image 2:\"\\\n                },\\\n                {\\\n                    \"type\": \"image\",\\\n                    \"source\": {\\\n                        \"type\": \"base64\",\\\n                        \"media_type\": image2_media_type,\\\n                        \"data\": image2_data,\\\n                    },\\\n                },\\\n                {\\\n                    \"type\": \"text\",\\\n                    \"text\": \"How are these images different?\"\\\n                }\\\n            ],\\\n        }\\\n    ],\n)\n\n```\n\nExample: Four images across two conversation turns\n\nClaude\u2019s vision capabilities shine in multimodal conversations that mix images and text. You can have extended back-and-forth exchanges with Claude, adding new images or follow-up questions at any point. This enables powerful workflows for iterative image analysis, comparison, or combining visuals with other knowledge.\n\nAsk Claude to contrast two images, then ask a follow-up question comparing the first images to two new images.\n\n| Role | Content |\n| --- | --- |\n| User | Image 1: \\[Image 1\\] Image 2: \\[Image 2\\] How are these images different? |\n| Assistant | \\[Claude\u2019s response\\] |\n| User | Image 1: \\[Image 3\\] Image 2: \\[Image 4\\] Are these images similar to the first two? |\n| Assistant | \\[Claude\u2019s response\\] |\n\nWhen using the API, simply insert new images into the array of Messages in the `user` role as part of any standard [multiturn conversation](/en/api/messages-examples#multiple-conversational-turns) structure.\n\n* * *\n\n## [\u200b](\\#limitations)  Limitations\n\nWhile Claude\u2019s image understanding capabilities are cutting-edge, there are some limitations to be aware of:\n\n- **People identification**: Claude [cannot be used](https://www.anthropic.com/legal/aup) to identify (i.e., name) people in images and will refuse to do so.\n- **Accuracy**: Claude may hallucinate or make mistakes when interpreting low-quality, rotated, or very small images under 200 pixels.\n- **Spatial reasoning**: Claude\u2019s spatial reasoning abilities are limited. It may struggle with tasks requiring precise localization or layouts, like reading an analog clock face or describing exact positions of chess pieces.\n- **Counting**: Claude can give approximate counts of objects in an image but may not always be precisely accurate, especially with large numbers of small objects.\n- **AI generated images**: Claude does not know if an image is AI-generated and may be incorrect if asked. Do not rely on it to detect fake or synthetic images.\n- **Inappropriate content**: Claude will not process inappropriate or explicit images that violate our [Acceptable Use Policy](https://www.anthropic.com/legal/aup).\n-",
      " **Healthcare applications**: While Claude can analyze general medical images, it is not designed to interpret complex diagnostic scans such as CTs or MRIs. Claude\u2019s outputs should not be considered a substitute for professional medical advice or diagnosis.\n\nAlways carefully review and verify Claude\u2019s image interpretations, especially for high-stakes use cases. Do not use Claude for tasks requiring perfect precision or sensitive image analysis without human oversight.\n\n* * *\n\n## [\u200b](\\#faq)  FAQ\n\nWhat image file types does Claude support?\n\nClaude currently supports JPEG, PNG, GIF, and WebP image formats, specifically:\n\n- image/jpeg\n- image/png\n- image/gif\n- image/webp\n\nCan Claude read image URLs?\n\nNo, Claude cannot read image URLs on any interface, including on claude.ai. Our API does not currently support adding URLs in either the text or image blocks. Adding image URLs (or URLs of any sort) in the text block might cause Claude to hallucinate, as Claude is currently unable to retrieve information from that URL.\n\nIs there a limit to the image file size I can upload?\n\nYes, there are limits:\n\n- API: Maximum 5MB per image\n- claude.ai: Maximum 10MB per image\n\nImages larger than these limits will be rejected and return an error when using our API.\n\nHow many images can I include in one request?\n\nThe image limits are:\n\n- Messages API: Up to 100 images per request\n- claude.ai: Up to 5 images per turn\n\nRequests exceeding these limits will be rejected and return an error.\n\nDoes Claude read image metadata?\n\nNo, Claude does not parse or receive any metadata from images passed to it.\n\nCan I delete images I've uploaded?\n\nNo. Image uploads are ephemeral and not stored beyond the duration of the API request. Uploaded images are automatically deleted after they have been processed.\n\nWhere can I find details on data privacy for image uploads?\n\nPlease refer to our privacy policy page for information on how we handle uploaded images and other data. We do not use uploaded images to train our models.\n\nWhat if Claude's image interpretation seems wrong?\n\nIf Claude\u2019s image interpretation seems incorrect:\n\n1. Ensure the image is clear, high-quality, and correctly oriented.\n2. Try prompt engineering techniques to improve results.\n3. If the issue persists, flag the output in claude.ai (thumbs up/down) or contact our support team.\n\nYour feedback helps us improve!\n\nCan Claude generate or edit images?\n\nNo, Claude is an image understanding model only. It can interpret and analyze images, but it cannot generate, produce, edit, manipulate, or create images.\n\n* * *\n\n## [\u200b](\\#dive-deeper-into-vision)  Dive deeper into vision\n\nReady to start building with images using Claude? Here are a few helpful resources:\n\n- [Multimodal cookbook](https://github.com/anthropics/anthropic-cookbook/tree/main/multimodal): This cookbook has tips on [getting started with images](https://github.com/anthropics/anthropic-cookbook/blob/main/multimodal/getting%5Fstarted%5Fwith%5Fvision.ipynb) and [best practice techniques](https://github.com/anthropics/anthropic-cookbook/blob/main/multimodal/best%5Fpractices%5Ffor%5Fvision.ipynb) to ensure the highest quality performance with images. See how you can effectively prompt Claude with images to carry out tasks such as [interpreting and analyzing charts](https://github.com/anthropics/anthropic-cookbook/blob/main/multimodal/reading%5Fcharts%5Fgraphs%5Fpowerpoints.ipynb) or [extracting content from forms](https://github.com/anthropics/anthropic-cookbook/blob/main/multimodal/how%5Fto%5Ftrascribe%5Ftext.ipynb).\n- [API reference](/en/api/messages): Visit our documentation for the Messages API, including example [API calls involving images](/en/api/messages-examples).\n\nIf you have any other questions, feel free to reach out to our [support team](https://support.anthropic.com/). You can also join our [developer community](https://www.anthropic.com/discord) to connect with other creators"
    ],
    [
      "[Anthropic home page![light logo](https://mintlify.s3-us-west-1.amazonaws.com/anthropic/logo/light.svg)![dark logo](https://mintlify.s3-us-west-1.amazonaws.com/anthropic/logo/dark.svg)](/)\n\nEnglish\n\nSearch...\n\nCtrl K\n\nSearch\n\nNavigation\n\nBuild with Claude\n\nDefine your success criteria\n\n[Welcome](/en/home) [User Guides](/en/docs/welcome) [API Reference](/en/api/getting-started) [Prompt Library](/en/prompt-library/library) [Release Notes](/en/release-notes/overview) [Developer Newsletter](/en/developer-newsletter/overview)\n\nBuilding a successful LLM-based application starts with clearly defining your success criteria. How will you know when your application is good enough to publish?\n\nHaving clear success criteria ensures that your prompt engineering & optimization efforts are focused on achieving specific, measurable goals.\n\n* * *\n\n## [\u200b](\\#building-strong-criteria)  Building strong criteria\n\nGood success criteria are:\n\n- **Specific**: Clearly define what you want to achieve. Instead of \u201cgood performance,\u201d specify \u201caccurate sentiment classification.\u201d\n\n- **Measurable**: Use quantitative metrics or well-defined qualitative scales. Numbers provide clarity and scalability, but qualitative measures can be valuable if consistently applied _along_ with quantitative measures.\n\n\n  - Even \u201chazy\u201d topics such as ethics and safety can be quantified:\n\n\n\n    |  | Safety criteria |\n    | --- | --- |\n    | Bad | Safe outputs |\n    | Good | Less than 0.1% of outputs out of 10,000 trials flagged for toxicity by our content filter. |\n\n\nExample metrics and measurement methods\n\n**Quantitative metrics**:\n\n- Task-specific: F1 score, BLEU score, perplexity\n- Generic: Accuracy, precision, recall\n- Operational: Response time (ms), uptime (%)\n\n**Quantitative methods**:\n\n- A/B testing: Compare performance against a baseline model or earlier version.\n- User feedback: Implicit measures like task completion rates.\n- Edge case analysis: Percentage of edge cases handled without errors.\n\n**Qualitative scales**:\n\n- Likert scales: \u201cRate coherence from 1 (nonsensical) to 5 (perfectly logical)\u201d\n- Expert rubrics: Linguists rating translation quality on defined criteria\n\n- **Achievable**: Base your targets on industry benchmarks, prior experiments, AI research, or expert knowledge. Your success metrics should not be unrealistic to current frontier model capabilities.\n\n- **Relevant**: Align your criteria with your application\u2019s purpose and user needs. Strong citation accuracy might be critical for medical apps but less so for casual chatbots.\n\n\nExample task fidelity criteria for sentiment analysis\n\n|  | Criteria |\n| --- | --- |\n| Bad | The model should classify sentiments well |\n| Good | Our sentiment analysis model should achieve an F1 score of at least 0.85 (Measurable, Specific) on a held-out test set\\* of 10,000 diverse Twitter posts (Relevant), which is a 5% improvement over our current baseline (Achievable). |\n\n\\* _More on held-out test sets in the next section_\n\n* * *\n\n## [\u200b](\\#common-success-criteria-to-consider)  Common success criteria to consider\n\nHere are some criteria that might be important for your use case. This list is non-exhaustive.\n\nTask fidelity\n\nHow well does the model need to perform on the task? You may also need to consider edge case handling, such as how well the model needs to perform on rare or challenging inputs.\n\nConsistency\n\nHow similar does the model\u2019s responses need to be for similar types of input? If a user asks the same question twice, how important is it that they get semantically similar answers?\n\nRelevance and coherence\n\nHow well does the model directly address the user\u2019s questions or instructions? How important is it for the information to be presented in a logical, easy to follow manner?\n\nTone and style\n\nHow well does the model\u2019s output style match expectations? How appropriate is its language for the target audience?\n\nPrivacy preservation\n\nWhat is a successful metric for how the model handles personal or sensitive information",
      "? Can it follow instructions not to use or share certain details?\n\nContext utilization\n\nHow effectively does the model use provided context? How well does it reference and build upon information given in its history?\n\nLatency\n\nWhat is the acceptable response time for the model? This will depend on your application\u2019s real-time requirements and user expectations.\n\nPrice\n\nWhat is your budget for running the model? Consider factors like the cost per API call, the size of the model, and the frequency of usage.\n\nMost use cases will need multidimensional evaluation along several success criteria.\n\nExample multidimensional criteria for sentiment analysis\n\n|  | Criteria |\n| --- | --- |\n| Bad | The model should classify sentiments well |\n| Good | On a held-out test set of 10,000 diverse Twitter posts, our sentiment analysis model should achieve:<br>\\- an F1 score of at least 0.85<br>\\- 99.5% of outputs are non-toxic<br>\\- 90% of errors are would cause inconvenience, not egregious error\\*<br>\\- 95% response time < 200ms |\n\n\\* _In reality, we would also define what \u201cinconvenience\u201d and \u201cegregious\u201d means._\n\n* * *\n\n## [\u200b](\\#next-steps)  Next steps\n\n[**Brainstorm criteria** \\\\\n\\\\\nBrainstorm success criteria for your use case with Claude on claude.ai.\\\\\n\\\\\n**Tip**: Drop this page into the chat as guidance for Claude!](https://claude.ai/) [**Design evaluations** \\\\\n\\\\\nLearn to build strong test sets to gauge Claude\u2019s performance against your criteria.](/en/docs/be-clear-direct)\n\n[Security and compliance](/en/docs/about-claude/security-compliance) [Develop test cases](/en/docs/build-with-claude/develop-tests)\n\nOn this page\n\n- [Building strong criteria](#building-strong-criteria)\n- [Common success criteria to consider](#common-success-criteria-to-consider)\n- [Next steps](#next-steps)"
    ],
    [
      "[Anthropic home page![light logo](https://mintlify.s3-us-west-1.amazonaws.com/anthropic/logo/light.svg)![dark logo](https://mintlify.s3-us-west-1.amazonaws.com/anthropic/logo/dark.svg)](/)\n\nEnglish\n\nSearch...\n\nCtrl K\n\nSearch\n\nNavigation\n\nPrompt engineering\n\nUse XML tags to structure your prompts\n\n[Welcome](/en/home) [User Guides](/en/docs/welcome) [API Reference](/en/api/getting-started) [Prompt Library](/en/prompt-library/library) [Release Notes](/en/release-notes/overview) [Developer Newsletter](/en/developer-newsletter/overview)\n\nWhen your prompts involve multiple components like context, instructions, and examples, XML tags can be a game-changer. They help Claude parse your prompts more accurately, leading to higher-quality outputs.\n\n**XML tip**: Use tags like `<instructions>`, `<example>`, and `<formatting>` to clearly separate different parts of your prompt. This prevents Claude from mixing up instructions with examples or context.\n\n## [\u200b](\\#why-use-xml-tags)  Why use XML tags?\n\n- **Clarity:** Clearly separate different parts of your prompt and ensure your prompt is well structured.\n- **Accuracy:** Reduce errors caused by Claude misinterpreting parts of your prompt.\n- **Flexibility:** Easily find, add, remove, or modify parts of your prompt without rewriting everything.\n- **Parseability:** Having Claude use XML tags in its output makes it easier to extract specific parts of its response by post-processing.\n\nThere are no canonical \u201cbest\u201d XML tags that Claude has been trained with in particular, although we recommend that your tag names make sense with the information they surround.\n\n* * *\n\n## [\u200b](\\#tagging-best-practices)  Tagging best practices\n\n1. **Be consistent**: Use the same tag names throughout your prompts, and refer to those tag names when talking about the content (e.g, `Using the contract in <contract> tags...`).\n2. **Nest tags**: You should nest tags `<outer><inner></inner></outer>` for hierarchical content.\n\n**Power user tip**: Combine XML tags with other techniques like multishot prompting ( `<examples>`) or chain of thought ( `<thinking>`, `<answer>`). This creates super-structured, high-performance prompts.\n\n### [\u200b](\\#examples)  Examples\n\nExample: Generating financial reports\n\nWithout XML tags, Claude misunderstands the task and generates a report that doesn\u2019t match the required structure or tone. After substitution, there is also a chance that Claude misunderstands where one section (like the the Q1 report example) stops and another begins.\n\n| Role | No XML Tags | With XML Tags |\n| --- | --- | --- |\n| User | You\u2019re a financial analyst at AcmeCorp. Generate a Q2 financial report for our investors. Include sections on Revenue Growth, Profit Margins, and Cash Flow, like with this example from last year: {{Q1\\_REPORT}}. Use data points from this spreadsheet: {{SPREADSHEET\\_DATA}}. The report should be extremely concise, to the point, professional, and in list format. It should and highlight both strengths and areas for improvement. | You\u2019re a financial analyst at AcmeCorp. Generate a Q2 financial report for our investors.<br>AcmeCorp is a B2B SaaS company. Our investors value transparency and actionable insights.<br>Use this data for your report:<data>{{SPREADSHEET\\_DATA}}</data><br><instructions><br>1\\. Include sections: Revenue Growth, Profit Margins, Cash Flow.<br>2\\. Highlight strengths and areas for improvement.<br></instructions><br>Make your tone concise and professional. Follow this structure:<br><formatting\\_example>{{Q1\\_REPORT}}</formatting\\_example> |\n| Assistant | Q2 Financial Report for AcmeCorp Investors<br>Dear Valued Investors,<br>I am pleased to present our Q2 financial report, showcasing AcmeCorp\u2019s continued growth and resilience in the B2B SaaS market.<br>1\\. Revenue Growth:<br>Our Q2 revenue",
      "[Anthropic home page![light logo](https://mintlify.s3-us-west-1.amazonaws.com/anthropic/logo/light.svg)![dark logo](https://mintlify.s3-us-west-1.amazonaws.com/anthropic/logo/dark.svg)](/)\n\nEnglish\n\nSearch...\n\nCtrl K\n\nSearch\n\nNavigation\n\nPrompt engineering\n\nLong context prompting tips\n\n[Welcome](/en/home) [User Guides](/en/docs/welcome) [API Reference](/en/api/getting-started) [Prompt Library](/en/prompt-library/library) [Release Notes](/en/release-notes/overview) [Developer Newsletter](/en/developer-newsletter/overview)\n\nClaude\u2019s extended context window (200K tokens for Claude 3 models) enables handling complex, data-rich tasks. This guide will help you leverage this power effectively.\n\n## [\u200b](\\#essential-tips-for-long-context-prompts)  Essential tips for long context prompts\n\n- **Put longform data at the top**: Place your long documents and inputs (~20K+ tokens) near the top of your prompt, above your query, instructions, and examples. This can significantly improve Claude\u2019s performance across all models.\n\n\n\n\n\n\n\nQueries at the end can improve response quality by up to 30% in tests, especially with complex, multi-document inputs.\n\n- **Structure document content and metadata with XML tags**: When using multiple documents, wrap each document in `<document>` tags with `<document_content>` and `<source>` (and other metadata) subtags for clarity.\n\n\n\n\n\n\n\n\nExample multi-document structure\n\n\n\n\n\n\n\n\n\n\nCopy\n\n\n\n\n\n\n\n```xml\n<documents>\n    <document index=\"1\">\n      <source>annual_report_2023.pdf</source>\n      <document_content>\n        {{ANNUAL_REPORT}}\n      </document_content>\n    </document>\n    <document index=\"2\">\n      <source>competitor_analysis_q2.xlsx</source>\n      <document_content>\n        {{COMPETITOR_ANALYSIS}}\n      </document_content>\n    </document>\n</documents>\n\nAnalyze the annual report and competitor analysis. Identify strategic advantages and recommend Q3 focus areas.\n\n```\n\n- **Ground responses in quotes**: For long document tasks, ask Claude to quote relevant parts of the documents first before carrying out its task. This helps Claude cut through the \u201cnoise\u201d of the rest of the document\u2019s contents.\n\n\n\n\n\n\n\n\nExample quote extraction\n\n\n\n\n\n\n\n\n\n\nCopy\n\n\n\n\n\n\n\n```xml\nYou are an AI physician's assistant. Your task is to help doctors diagnose possible patient illnesses.\n\n<documents>\n    <document index=\"1\">\n      <source>patient_symptoms.txt</source>\n      <document_content>\n        {{PATIENT_SYMPTOMS}}\n      </document_content>\n    </document>\n    <document index=\"2\">\n      <source>patient_records.txt</source>\n      <document_content>\n        {{PATIENT_RECORDS}}\n      </document_content>\n    </document>\n    <document index=\"3\">\n      <source>patient01_appt_history.txt</source>\n      <document_content>\n        {{PATIENT01_APPOINTMENT_HISTORY}}\n      </document_content>\n    </document>\n</documents>\n\nFind quotes from the patient records and appointment history that are relevant to diagnosing the patient's reported symptoms. Place these in <quotes> tags. Then, based on these quotes, list all information that would help the doctor diagnose the patient's symptoms. Place your diagnostic information in <info> tags.\n\n```\n\n\n* * *"
    ]
  ],
  "ground_truth": [
    "The SmartHome Mini is a convenient smart home assistant because it allows you to control lights, thermostats, and other connected devices via voice or app, no matter where you place it in your home. It offers hands-free control of your smart devices.",
    "Cosine similarity measures the similarity between two vectors (in this case, sentence embeddings of the model\u2019s output using SBERT) by computing the cosine of the angle between them. Values closer to 1 indicate higher similarity. It\u2019s ideal for evaluating consistency because similar questions should yield semantically similar answers, even if the wording varies.",
    "Claude's image and scan limitations affect its role in healthcare by restricting its ability to interpret complex diagnostic scans such as CTs or MRIs. Claude's outputs should not be considered a substitute for professional medical advice or diagnosis.",
    "Metrics that ensure an LLM app's success, balancing performance and constraints, include quantitative metrics like F1 score, BLEU score, perplexity, accuracy, precision, recall, response time, and uptime. Qualitative scales such as Likert scales and expert rubrics are also used. These metrics help in measuring task-specific performance, operational efficiency, and user feedback.",
    "XML tags improve clarity in Claude's extended context by clearly separating different parts of the prompt, ensuring the prompt is well structured, and reducing errors caused by misinterpretation. They also allow for easy modification of the prompt without rewriting everything."
  ]
}