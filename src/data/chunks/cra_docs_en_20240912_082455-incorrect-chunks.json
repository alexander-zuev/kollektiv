{
  "too_small": [
    {
      "id": "b1d30c7e-8970-41cb-8037-097f0e4d1ae9",
      "size": 41,
      "headers": {
        "h1": "!/bin/bash",
        "h2": "​  Next Steps",
        "h3": ""
      },
      "text": "Generally, use Claude 3 Opus for complex tools and ambiguous queries; it handles multiple tools better and seeks clarification when needed.\n\nUse Haiku for straightforward tools, but note it may infer missing parameters.\n"
    },
    {
      "id": "180f2264-a550-458f-9096-1ed13e8930d9",
      "size": 73,
      "headers": {
        "h1": "!/bin/bash",
        "h2": "​  Next Steps",
        "h3": ""
      },
      "text": "Tools do not necessarily need to be client-side functions — you can use tools anytime you want the model to return JSON output that follows a provided schema. For example, you might use a `record_summary` tool with a particular schema. See [tool use examples](/en/docs/build-with-claude/tool-use#json-mode) for a full working example.\n"
    },
    {
      "id": "b095ec20-8a57-4760-8207-ba2e19d0bebb",
      "size": 82,
      "headers": {
        "h1": "",
        "h2": "​  Examples",
        "h3": "​  Example 2: Financial analysis"
      },
      "text": "- **Enhanced accuracy:** In complex scenarios like legal analysis or financial modeling, role prompting can significantly boost Claude’s performance.\n- **Tailored tone:** Whether you need a CFO’s brevity or a copywriter’s flair, role prompting adjusts Claude’s communication style.\n- **Improved focus:** By setting the role context, Claude stays more within the bounds of your task’s specific requirements.\n\n* * *\n"
    },
    {
      "id": "b6902306-5265-471b-ab87-adbdf605baba",
      "size": 93,
      "headers": {
        "h1": "",
        "h2": "​  Tokens",
        "h3": ""
      },
      "text": "Latency, in the context of generative AI and large language models, refers to the time it takes for the model to respond to a given prompt. It is the delay between submitting a prompt and receiving the generated output. Lower latency indicates faster response times, which is crucial for real-time applications, chatbots, and interactive experiences. Factors that can affect latency include model size, hardware capabilities, network conditions, and the complexity of the prompt and the generated response.\n"
    },
    {
      "id": "bcaabeb7-f72f-4f6a-ae75-44dfe2016527",
      "size": 85,
      "headers": {
        "h1": "",
        "h2": "​  Tokens",
        "h3": ""
      },
      "text": "Large language models (LLMs) are AI language models with many parameters that are capable of performing a variety of surprisingly useful tasks. These models are trained on vast amounts of text data and can generate human-like text, answer questions, summarize information, and more. Claude is a conversational assistant based on a large language model that has been fine-tuned and trained using RLHF to be more helpful, honest, and harmless.\n"
    },
    {
      "id": "a5b598f0-f9bb-467b-8330-1912752b9f86",
      "size": 65,
      "headers": {
        "h1": "is the same as dot-product.",
        "h2": "​  Pricing",
        "h3": ""
      },
      "text": "The `voyageai` package can be installed using the following command:\n\nPython\n\nCopy\n\n```Python\npip install -U voyageai\n\n```\n\nThen, you can create a client object and start using it to embed your texts:\n\nPython\n\nCopy\n\n```Python\nimport voyageai\n\nvo = voyageai.Client()\n"
    },
    {
      "id": "dc927928-497e-4b69-9c31-421d0f043636",
      "size": 29,
      "headers": {
        "h1": "is the same as dot-product.",
        "h2": "​  Pricing",
        "h3": ""
      },
      "text": "query_embd = vo.embed(\n    [query], model=\"voyage-2\", input_type=\"query\"\n).embeddings[0]\n"
    },
    {
      "id": "deab589b-1fc5-4070-bf14-648066a0dbf4",
      "size": 92,
      "headers": {
        "h1": "is the same as dot-product.",
        "h2": "​  Pricing",
        "h3": ""
      },
      "text": "How do I calculate the distance between two embedding vectors?\n\nCosine similarity is a popular choice, but most distance functions will do fine. Voyage embeddings are normalized to length 1, therefore cosine similarity is essentially the same as the dot-product between two vectors. Here is a code snippet you can use for calculating cosine similarity between two embedding vectors.\n\nCopy\n\n```python\nimport numpy as np\n\nsimilarity = np.dot(embd1, embd2)\n"
    },
    {
      "id": "f266d080-b8a5-49c8-bf30-1cb6734de010",
      "size": 83,
      "headers": {
        "h1": "is the same as dot-product.",
        "h2": "​  Pricing",
        "h3": ""
      },
      "text": "```\n\nIf you want to find the K nearest embedding vectors over a large corpus, we recommend using the capabilities built into most vector databases.\n\nCan I count the number of tokens in a string before embedding it?\n\nYes! You can do so with the following code.\n\nCopy\n\n```python\nimport voyageai\n\nvo = voyageai.Client()\ntotal_tokens = vo.count_tokens([\"Sample text\"])\n\n```\n\n* * *\n"
    },
    {
      "id": "c4e321ab-59d5-4df4-b887-ab45a26bfc89",
      "size": 13,
      "headers": {
        "h1": "",
        "h2": "​  Raw HTTP Stream response",
        "h3": "​  Streaming request with tool use"
      },
      "text": "Event streams may also include any number of `ping` events.\n"
    },
    {
      "id": "c6b5ea4a-ed66-4020-bc75-64943b7043a2",
      "size": 92,
      "headers": {
        "h1": "",
        "h2": "​  Raw HTTP Stream response",
        "h3": "​  Streaming request with tool use"
      },
      "text": "We may occasionally send [errors](/en/api/errors) in the event stream. For example, during periods of high usage, you may receive an `overloaded_error`, which would normally correspond to an HTTP 529 in a non-streaming context:\n\nExample error\n\nCopy\n\n```json\nevent: error\ndata: {\"type\": \"error\", \"error\": {\"type\": \"overloaded_error\", \"message\": \"Overloaded\"}}\n\n```\n"
    },
    {
      "id": "f05895a9-675d-440e-a93f-06d39c223786",
      "size": 32,
      "headers": {
        "h1": "",
        "h2": "​  Raw HTTP Stream response",
        "h3": "​  Streaming request with tool use"
      },
      "text": "In accordance with our [versioning policy](/en/api/versioning), we may add new event types, and your code should handle unknown event types gracefully.\n"
    },
    {
      "id": "d56a2d50-8196-42d7-8a1f-90735828485c",
      "size": 28,
      "headers": {
        "h1": "",
        "h2": "​  Raw HTTP Stream response",
        "h3": "​  Streaming request with tool use"
      },
      "text": "Each `content_block_delta` event contains a `delta` of a type that updates the `content` block at a given `index`.\n"
    },
    {
      "id": "3e668f86-a9c8-42e8-92e4-2bdbc81b6efe",
      "size": 59,
      "headers": {
        "h1": "",
        "h2": "​  Raw HTTP Stream response",
        "h3": "​  Streaming request with tool use"
      },
      "text": "A `text` content block delta looks like:\n\nText delta\n\nCopy\n\n```JSON\nevent: content_block_delta\ndata: {\"type\": \"content_block_delta\",\"index\": 0,\"delta\": {\"type\": \"text_delta\", \"text\": \"ello frien\"}}\n\n```\n"
    },
    {
      "id": "745c59f5-15d1-4d61-84d8-b519af4ea5f8",
      "size": 74,
      "headers": {
        "h1": "",
        "h2": "",
        "h3": "​  Streaming format"
      },
      "text": "The Messages API requires that you specify the full model version (e.g. `claude-3-opus-20240229`).\n\nWe previously supported specifying only the major version number (e.g. `claude-2`), which resulted in automatic upgrades to minor versions. However, we no longer recommend this integration pattern, and Messages do not support it.\n"
    },
    {
      "id": "3358c01e-c278-4c21-83a6-44fa29cb6fa3",
      "size": 47,
      "headers": {
        "h1": "",
        "h2": "",
        "h3": "​  Streaming format"
      },
      "text": "- Text Completions: `max_tokens_to_sample` parameter. No validation, but capped values per-model.\n- Messages: `max_tokens` parameter. If passing a value higher than the model supports, returns a validation error.\n"
    },
    {
      "id": "fe20150e-1110-480b-9ed3-e905b1d5c929",
      "size": 84,
      "headers": {
        "h1": "",
        "h2": "​  Next steps",
        "h3": ""
      },
      "text": "You will need:\n\n- An Anthropic [Console account](console.anthropic.com)\n- An [API key](https://console.anthropic.com/settings/keys)\n- Python 3.7+ or TypeScript 4.5+\n\nAnthropic provides [Python and TypeScript SDKs](https://docs.anthropic.com/en/api/client-sdks), although you can make direct HTTP requests to the API.\n"
    },
    {
      "id": "525509fb-ed6d-4374-8841-41b60798e8ce",
      "size": 75,
      "headers": {
        "h1": "",
        "h2": "​  Next steps",
        "h3": ""
      },
      "text": "Every API call requires a valid API key. The SDKs are designed to pull the API key from an environmental variable `ANTHROPIC_API_KEY`. You can also supply the key to the Anthropic client when initializing it.\n\n- macOS and Linux\n- Windows\n\nCopy\n\n```bash\nexport ANTHROPIC_API_KEY='your-api-key-here'\n\n```\n"
    },
    {
      "id": "2c679e70-04af-4e26-a95d-d544daa007e6",
      "size": 29,
      "headers": {
        "h1": "",
        "h2": "​  Advanced: Chain safeguards",
        "h3": "Prompt within `harmlessness_screen` tool"
      },
      "text": "Combine strategies for robust protection. Here’s an enterprise-grade example with tool use:\n\nExample: Multi-layered protection for a financial advisor chatbot\n"
    },
    {
      "id": "8bf2d37a-2cbe-4ebc-9f1d-0683c9f37192",
      "size": 76,
      "headers": {
        "h1": "ERROR!, ⚠ DEFERRED ⚠ or ⚠ THROTTLED ⚠",
        "h2": "​  Further information",
        "h3": ""
      },
      "text": "Our [Claude for Sheets prompting examples workbench](https://docs.google.com/spreadsheets/d/1sUrBWO0u1-ZuQ8m5gt3-1N5PLR6r%5F%5FUsRsB7WeySDQA/copy) is a Claude-powered spreadsheet that houses example prompts and prompt engineering structures.\n"
    },
    {
      "id": "5b4760e8-b600-45db-ab5f-4873372c2841",
      "size": 73,
      "headers": {
        "h1": "ERROR!, ⚠ DEFERRED ⚠ or ⚠ THROTTLED ⚠",
        "h2": "​  Further information",
        "h3": ""
      },
      "text": "Make a copy of our [Claude for Sheets workbook template](https://docs.google.com/spreadsheets/d/1UwFS-ZQWvRqa6GkbL4sy0ITHK2AhXKe-jpMLzS0kTgk/copy) to get started with your own Claude for Sheets work!\n\n* * *\n"
    },
    {
      "id": "e754c1ca-17b5-4a0e-9e38-97744d75ec22",
      "size": 56,
      "headers": {
        "h1": "",
        "h2": "​  Start building with Claude",
        "h3": ""
      },
      "text": "Enterprise use cases often mean complex needs and edge cases. Anthropic offers a range of models across the Claude 3 and Claude 3.5 families to allow you to choose the right balance of intelligence, speed, and [cost](https://www.anthropic.com/api).\n"
    },
    {
      "id": "4a380a5f-f5a3-4d53-a8fa-f29651b13152",
      "size": 31,
      "headers": {
        "h1": "",
        "h2": "​  FAQ",
        "h3": ""
      },
      "text": "Prompt Caching is currently supported on:\n\n- Claude 3.5 Sonnet\n- Claude 3 Haiku\n- Claude 3 Opus\n"
    },
    {
      "id": "b7c207b5-0bd8-4fba-9f3f-138a40424a98",
      "size": 84,
      "headers": {
        "h1": "",
        "h2": "​  FAQ",
        "h3": ""
      },
      "text": "Place static content (tool definitions, system instructions, context, examples) at the beginning of your prompt. Mark the end of the reusable content for caching using the `cache_control` parameter.\n\nCache prefixes are created in the following order: `tools`, `system`, then `messages`.\n\nUsing the `cache_control` parameter, you can define up to 4 cache breakpoints, allowing you to cache different reusable sections separately.\n"
    },
    {
      "id": "d31c1509-7a60-4e59-8c86-be3260d379bb",
      "size": 80,
      "headers": {
        "h1": "",
        "h2": "​  FAQ",
        "h3": ""
      },
      "text": "Monitor cache performance using these API response fields, within `usage` in the response (or `message_start` event if [streaming](https://docs.anthropic.com/en/api/messages-streaming)):\n\n- `cache_creation_input_tokens`: Number of tokens written to the cache when creating a new entry.\n- `cache_read_input_tokens`: Number of tokens retrieved from the cache for this request.\n"
    },
    {
      "id": "9ba34cc4-0833-417f-98fd-a29065113c98",
      "size": 68,
      "headers": {
        "h1": "",
        "h2": "​  FAQ",
        "h3": ""
      },
      "text": "To optimize Prompt Caching performance:\n\n- Cache stable, reusable content like system instructions, background information, large contexts, or frequent tool definitions.\n- Place cached content at the prompt’s beginning for best performance.\n- Use cache breakpoints strategically to separate different cacheable prefix sections.\n- Regularly analyze cache hit rates and adjust your strategy as needed.\n"
    },
    {
      "id": "3640a46a-def1-4e12-bd1f-448816aa1998",
      "size": 12,
      "headers": {
        "h1": "Print the results for each detected violation",
        "h2": "",
        "h3": ""
      },
      "text": "user_comments = allowed_user_comments + disallowed_user_comments\n"
    },
    {
      "id": "f9f706fc-6e82-4a9c-b883-4e2f58ece591",
      "size": 47,
      "headers": {
        "h1": "Print the results for each detected violation",
        "h2": "",
        "h3": ""
      },
      "text": "In order to use Claude for content moderation, Claude must understand the moderation requirements of your application. Let’s start by writing a prompt that allows you to define your moderation needs:\n\nCopy\n\n```python\nimport anthropic\nimport json\n"
    },
    {
      "id": "6bd6737e-b5c4-45b7-bc41-b56f1f816d50",
      "size": 49,
      "headers": {
        "h1": "Print the results for each detected violation",
        "h2": "",
        "h3": ""
      },
      "text": "In complex scenarios, it may be helpful to consider additional strategies to improve performance beyond standard [prompt engineering techniques](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview). Here are some advanced strategies:\n"
    },
    {
      "id": "070d5b38-14e4-4bf8-8e13-7a685c1121b3",
      "size": 38,
      "headers": {
        "h1": "Print the results for each detected violation",
        "h2": "",
        "h3": ""
      },
      "text": "In addition to listing the unsafe categories in the prompt, further improvements can be made by providing definitions and phrases related to each category.\n\nCopy\n\n```python\nimport anthropic\nimport json\n"
    },
    {
      "id": "fb1b2e6b-d806-47fa-bca3-abeefd8cea0c",
      "size": 8,
      "headers": {
        "h1": "Print the results for each detected violation",
        "h2": "",
        "h3": ""
      },
      "text": "client = anthropic.Anthropic()\n"
    },
    {
      "id": "4c0603c0-9488-40ca-8039-0e7e5a71fcd5",
      "size": 52,
      "headers": {
        "h1": "Print the results for each detected violation",
        "h2": "",
        "h3": ""
      },
      "text": "To reduce costs in situations where real-time moderation isn’t necessary, consider moderating messages in batches. Include multiple messages within the prompt’s context, and ask Claude to assess which messages should be moderated.\n\nCopy\n\n```python\nimport anthropic\nimport json\n"
    },
    {
      "id": "31204841-4716-4565-b1fd-15dc30b9401b",
      "size": 14,
      "headers": {
        "h1": "Print the results for each detected violation",
        "h2": "",
        "h3": ""
      },
      "text": "response_obj = batch_moderate_messages(user_comments, unsafe_categories)\n"
    },
    {
      "id": "63770157-88c9-416c-8f03-b4bb1c3d9056",
      "size": 73,
      "headers": {
        "h1": "",
        "h2": "​  Support",
        "h3": ""
      },
      "text": "Claude consists of a family of large language models that enable you to balance intelligence, speed, and cost.\n\n![](https://mintlify.s3-us-west-1.amazonaws.com/anthropic/images/3-5-sonnet-curve.png)\n\n[Compare our state-of-the-art models.](/en/docs/about-claude/models)\n\n* * *\n"
    },
    {
      "id": "97cb0a2b-680d-4a63-ab1f-1384f7f2f769",
      "size": 93,
      "headers": {
        "h1": "",
        "h2": "​  Support",
        "h3": ""
      },
      "text": "Claude can assist with many tasks that involve text, code, and images.\n\n[**Text and code generation** \\\\\n\\\\\nSummarize text, answer questions, extract data, translate text, and explain and generate code.](/en/docs/build-with-claude/text-generation) [**Vision** \\\\\n\\\\\nProcess and analyze visual input and generate text and code from images.](/en/docs/build-with-claude/vision)\n\n* * *\n"
    },
    {
      "id": "a15c070f-7cfa-428a-a0b8-bc36eeba2957",
      "size": 71,
      "headers": {
        "h1": "",
        "h2": "​  Examples",
        "h3": ""
      },
      "text": "All requests to the Anthropic API must include an `x-api-key` header with your API key. If you are using the Client SDKs, you will set the API when constructing a client, and then the SDK will send the header on your behalf with every request. If integrating directly with the API, you’ll need to send this header yourself.\n"
    },
    {
      "id": "c1aadbce-e8a6-4dff-aebb-6141660c584f",
      "size": 51,
      "headers": {
        "h1": "",
        "h2": "​  Examples",
        "h3": ""
      },
      "text": "The Anthropic API always accepts JSON in request bodies and returns JSON in response bodies. You will need to send the `content-type: application/json` header in requests. If you are using the Client SDKs, this will be taken care of automatically.\n"
    },
    {
      "id": "fb3d217f-4d60-4352-b14b-fb1ebc534826",
      "size": 67,
      "headers": {
        "h1": "",
        "h2": "",
        "h3": ""
      },
      "text": "- We’ve added Workspaces to the [Developer Console](https://console.anthropic.com). Workspaces allow you to set custom spend or rate limits, group API keys, track usage by project, and control access with user roles. Read more in our [blog post](https://www.anthropic.com/news/workspaces).\n"
    },
    {
      "id": "9ff25c9a-05e7-4ae9-84c4-3fcdf5202c17",
      "size": 29,
      "headers": {
        "h1": "",
        "h2": "",
        "h3": ""
      },
      "text": "- We announced the deprecation of the Claude 1 models. Read more in [our documentation](/en/docs/resources/model-deprecations).\n"
    },
    {
      "id": "00978364-6e5d-4cbb-9b26-40c26c5710de",
      "size": 39,
      "headers": {
        "h1": "",
        "h2": "",
        "h3": ""
      },
      "text": "- We’ve added support for usage of the SDK in browsers by returning CORS headers in the API responses. Set `dangerouslyAllowBrowser: true` in the SDK instantiation to enable this feature.\n"
    },
    {
      "id": "de8e87af-e345-442d-a5cc-3a66b8880cf2",
      "size": 24,
      "headers": {
        "h1": "",
        "h2": "",
        "h3": ""
      },
      "text": "- We’ve moved 8,192 token outputs from beta to general availability for Claude 3.5 Sonnet.\n"
    },
    {
      "id": "ff4b30d8-b173-49e0-9ad5-6c0f04a2805f",
      "size": 54,
      "headers": {
        "h1": "",
        "h2": "",
        "h3": ""
      },
      "text": "- [Prompt caching](/en/docs/build-with-claude/prompt-caching) is now available as a beta feature in the Anthropic API. Cache and re-use prompts to reduce latency by up to 80% and costs by up to 90%.\n"
    },
    {
      "id": "e91fb426-8983-483c-89bc-261664359965",
      "size": 72,
      "headers": {
        "h1": "",
        "h2": "",
        "h3": ""
      },
      "text": "- Generate outputs up to 8,192 tokens in length from Claude 3.5 Sonnet with the new `anthropic-beta: max-tokens-3-5-sonnet-2024-07-15` header. More details [here](https://x.com/alexalbert__/status/1812921642143900036).\n"
    },
    {
      "id": "37106c18-b783-4c1f-9ba3-940a395e1aba",
      "size": 74,
      "headers": {
        "h1": "",
        "h2": "",
        "h3": ""
      },
      "text": "- Automatically generate test cases for your prompts using Claude in the [Developer Console](https://console.anthropic.com). Read more in our [blog post](https://www.anthropic.com/news/test-case-generation).\n- Compare the outputs from different prompts side by side in the new output comparison mode in the [Developer Console](https://console.anthropic.com).\n"
    },
    {
      "id": "d8573998-d891-474a-b661-8fc3b0a622f9",
      "size": 53,
      "headers": {
        "h1": "",
        "h2": "",
        "h3": ""
      },
      "text": "- [Claude 3.5 Sonnet](http://anthropic.com/news/claude-3-5-sonnet), our most intelligent model yet, is now generally available across the Anthropic API, Amazon Bedrock, and Google Vertex AI.\n"
    },
    {
      "id": "a58620da-b6e0-43bb-b96d-be2636877234",
      "size": 35,
      "headers": {
        "h1": "",
        "h2": "",
        "h3": ""
      },
      "text": "- [Tool use](/en/docs/build-with-claude/tool-use) is now generally available across the Anthropic API, Amazon Bedrock, and Google Vertex AI.\n"
    },
    {
      "id": "3f407f5d-f2e3-45fa-9b96-e325ee4b8b3e",
      "size": 65,
      "headers": {
        "h1": "",
        "h2": "​  Accessing Bedrock",
        "h3": "​  Making requests"
      },
      "text": "Anthropic’s [client SDKs](/en/api/client-sdks) support Bedrock. You can also use an AWS SDK like `boto3` directly.\n\nPython\n\nTypescript\n\nBoto3 (Python)\n\nCopy\n\n```Python\npip install -U \"anthropic[bedrock]\"\n\n```\n"
    },
    {
      "id": "3b08217c-674c-4ca9-9a2c-13955cf2d88f",
      "size": 78,
      "headers": {
        "h1": "",
        "h2": "​  Accessing Bedrock",
        "h3": "​  Making requests"
      },
      "text": "Go to the [AWS Console > Bedrock > Model Access](https://console.aws.amazon.com/bedrock/home?region=us-west-2#/modelaccess) and request access to Anthropic models. Note that Anthropic model availability varies by region. See [AWS documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/models-regions.html) for latest information.\n"
    },
    {
      "id": "e99a49eb-2c7b-478e-9aec-5f01dc27ed46",
      "size": 66,
      "headers": {
        "h1": "",
        "h2": "​  Accessing Bedrock",
        "h3": "​  Making requests"
      },
      "text": "The following examples show how to print a list of all the Claude models available through Bedrock:\n\nAWS CLI\n\nBoto3 (Python)\n\nCopy\n\n```bash\naws bedrock list-foundation-models --region=us-west-2 --by-provider anthropic --query \"modelSummaries[*].modelId\"\n\n```\n"
    },
    {
      "id": "851c999d-ae03-491e-8eea-adfd92ce61fb",
      "size": 61,
      "headers": {
        "h1": "",
        "h2": "​  How to prompt for thinking",
        "h3": "​  Examples"
      },
      "text": "- **Accuracy:** Stepping through problems reduces errors, especially in math, logic, analysis, or generally complex tasks.\n- **Coherence:** Structured thinking leads to more cohesive, well-organized responses.\n- **Debugging:** Seeing Claude’s thought process helps you pinpoint where prompts may be unclear.\n"
    },
    {
      "id": "cfc91580-6606-494c-b042-a85acb12b1c2",
      "size": 68,
      "headers": {
        "h1": "",
        "h2": "​  How to prompt for thinking",
        "h3": "​  Examples"
      },
      "text": "- Increased output length may impact latency.\n- Not all tasks require in-depth thinking. Use CoT judiciously to ensure the right balance of performance and latency.\n\nUse CoT for tasks that a human would need to think through, like complex math, multi-step analysis, writing complex documents, or decisions with many factors.\n\n* * *\n"
    },
    {
      "id": "b1f30f46-31cd-49b0-9107-7ae9d1fccd8d",
      "size": 97,
      "headers": {
        "h1": "",
        "h2": "​  How to implement Claude as a customer service agent",
        "h3": "​  Integrate Claude into your support workflow"
      },
      "text": "The choice of model depends on the trade-offs between cost, accuracy, and response time.\n\nFor customer support chat, `claude-3-5-sonnet-20240620` is well suited to balance intelligence, latency, and cost. However, for instances where you have conversation flow with multiple prompts including RAG, tool use, and/or long-context prompts, `claude-3-haiku-20240307` may be more suitable to optimize for latency.\n"
    },
    {
      "id": "1585edaf-4882-46a1-a0dd-d7a065c0f956",
      "size": 81,
      "headers": {
        "h1": "",
        "h2": "​  How to implement Claude as a customer service agent",
        "h3": "​  Integrate Claude into your support workflow"
      },
      "text": "In complex scenarios, it may be helpful to consider additional strategies to improve performance beyond standard [prompt engineering techniques](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview) & [guardrail implementation strategies](https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/reduce-hallucinations). Here are some common scenarios:\n"
    },
    {
      "id": "ef21c92d-0050-4686-bf04-827ebf05ca20",
      "size": 88,
      "headers": {
        "h1": "",
        "h2": "​  How to reduce latency",
        "h3": "​  3\\. Leverage streaming"
      },
      "text": "One of the most straightforward ways to reduce latency is to select the appropriate model for your use case. Anthropic offers a [range of models](/en/docs/about-claude/models) with different capabilities and performance characteristics. Consider your specific requirements and choose the model that best fits your needs in terms of speed and output quality. For more details about model metrics, see our [models overview](/en/docs/models-overview) page.\n"
    },
    {
      "id": "e0615ad6-e675-4697-be10-66c4efb75c76",
      "size": 12,
      "headers": {
        "h1": "!/bin/sh",
        "h2": "​  Tool use and JSON mode",
        "h3": ""
      },
      "text": "Shell\n\nPython\n\nTypeScript\n\nCopy\n\n```bash\n"
    },
    {
      "id": "b7fcd496-6b3d-48b6-b2ad-25a6a763da54",
      "size": 65,
      "headers": {
        "h1": "!/bin/sh",
        "h2": "​  Tool use and JSON mode",
        "h3": ""
      },
      "text": "The Messages API is stateless, which means that you always send the full conversational history to the API. You can use this pattern to build up a conversation over time. Earlier conversational turns don’t necessarily need to actually originate from Claude — you can use synthetic `assistant` messages.\n\nShell\n\nCopy\n\n```bash\n"
    },
    {
      "id": "f27b4008-1f2a-437d-b826-558134a2cc1e",
      "size": 62,
      "headers": {
        "h1": "!/bin/sh",
        "h2": "​  Tool use and JSON mode",
        "h3": ""
      },
      "text": "You can pre-fill part of Claude’s response in the last position of the input messages list. This can be used to shape Claude’s response. The example below uses `\"max_tokens\": 1` to get a single multiple choice answer from Claude.\n\nShell\n\nPython\n\nTypeScript\n\nCopy\n\n```bash\n"
    },
    {
      "id": "b11a927c-1a71-4336-b5c2-2633bfee3d83",
      "size": 76,
      "headers": {
        "h1": "!/bin/sh",
        "h2": "​  Tool use and JSON mode",
        "h3": ""
      },
      "text": "Claude can read both text and images in requests. Currently, we support the `base64` source type for images, and the `image/jpeg`, `image/png`, `image/gif`, and `image/webp` media types. See our [vision guide](/en/docs/vision) for more details.\n\nShell\n\nPython\n\nTypeScript\n\nCopy\n\n```bash\n"
    },
    {
      "id": "e74cb382-f8e5-4344-93e9-d94038e786e5",
      "size": 58,
      "headers": {
        "h1": "",
        "h2": "​  Best Practices",
        "h3": ""
      },
      "text": "Once a model is deprecated, please migrate all usage to a suitable replacement before the retirement date. Requests to models past the retirement date will fail.\n\nTo help measure the performance of replacement models on your tasks, we recommend thorough testing of your applications with the new models well before the retirement date.\n"
    },
    {
      "id": "082098bf-868e-43c1-a992-bb836eee5c97",
      "size": 67,
      "headers": {
        "h1": "",
        "h2": "​  Best Practices",
        "h3": ""
      },
      "text": "Anthropic notifies customers with active deployments for models with upcoming retirements. We notify customers of upcoming retirements as follows:\n\n1. At model launch, we designate a “Guaranteed Available Until” date (at least one year out).\n2. We provide at least 6 months† notice before model retirement for publicly released models.\n"
    },
    {
      "id": "d377e803-8d16-4d59-9f28-a6436ea062ef",
      "size": 17,
      "headers": {
        "h1": "",
        "h2": "​  Best Practices",
        "h3": ""
      },
      "text": "All deprecations are listed below, with the most recent announcements at the top.\n"
    },
    {
      "id": "e2b5c4b8-7f85-4b1e-8d82-1583891a75ec",
      "size": 42,
      "headers": {
        "h1": "Initialize the Anthropic client",
        "h2": "",
        "h3": "​  Fine-tune Claude to learn from your dataset"
      },
      "text": "url = \"https://raw.githubusercontent.com/anthropics/anthropic-cookbook/main/skills/summarization/data/Sample Sublease Agreement.pdf\"\nurl = url.replace(\" \", \"%20\")\n"
    },
    {
      "id": "11418d95-0d6b-4b27-a551-33825e40e42f",
      "size": 6,
      "headers": {
        "h1": "Initialize the Anthropic client",
        "h2": "",
        "h3": "​  Fine-tune Claude to learn from your dataset"
      },
      "text": "response = requests.get(url)\n"
    },
    {
      "id": "3baffe05-1843-4f1b-bc9d-d233de2a51cc",
      "size": 83,
      "headers": {
        "h1": "Initialize the Anthropic client",
        "h2": "",
        "h3": "​  Fine-tune Claude to learn from your dataset"
      },
      "text": "Claude can adapt to various summarization styles. You can change the details of the prompt to guide Claude to be more or less verbose, include more or less technical terminology, or provide a higher or lower level summary of the context at hand.\n\nHere’s an example of how to create a prompt that ensures the generated summaries follow a consistent structure when analyzing sublease agreements:\n\nCopy\n\n```python\nimport anthropic\n"
    },
    {
      "id": "b74ca359-9cdf-4227-ac85-27994d99df33",
      "size": 52,
      "headers": {
        "h1": "",
        "h2": "​  Examples",
        "h3": ""
      },
      "text": "1. **Accuracy**: Each subtask gets Claude’s full attention, reducing errors.\n2. **Clarity**: Simpler subtasks mean clearer instructions and outputs.\n3. **Traceability**: Easily pinpoint and fix issues in your prompt chain.\n\n* * *\n"
    },
    {
      "id": "1b0c87fa-7c78-4848-bd68-874a7d731c11",
      "size": 96,
      "headers": {
        "h1": "",
        "h2": "​  Examples",
        "h3": ""
      },
      "text": "Use prompt chaining for multi-step tasks like research synthesis, document analysis, or iterative content creation. When a task involves multiple transformations, citations, or instructions, chaining prevents Claude from dropping or mishandling steps.\n\n**Remember:** Each link in the chain gets Claude’s full attention!\n\n**Debugging tip**: If Claude misses a step or performs poorly, isolate that step in its own prompt. This lets you fine-tune problematic steps without redoing the entire task.\n\n* * *\n"
    },
    {
      "id": "cc49cbb2-2a88-4fdb-81da-558e8da851ce",
      "size": 73,
      "headers": {
        "h1": "",
        "h2": "​  Examples",
        "h3": ""
      },
      "text": "1. **Identify subtasks**: Break your task into distinct, sequential steps.\n2. **Structure with XML for clear handoffs**: Use XML tags to pass outputs between prompts.\n3. **Have a single-task goal**: Each subtask should have a single, clear objective.\n4. **Iterate**: Refine subtasks based on Claude’s performance.\n"
    },
    {
      "id": "165e7b7c-225b-4af7-96f0-08effefd789f",
      "size": 36,
      "headers": {
        "h1": "",
        "h2": "​  Examples",
        "h3": ""
      },
      "text": "You can chain prompts to have Claude review its own work! This catches errors and refines outputs, especially for high-stakes tasks.\n\nExample: Self-correcting research summary\n"
    },
    {
      "id": "1ccaa54d-016b-43c2-a41b-892108bc58a7",
      "size": 41,
      "headers": {
        "h1": "",
        "h2": "​  Crafting effective examples",
        "h3": ""
      },
      "text": "- **Accuracy**: Examples reduce misinterpretation of instructions.\n- **Consistency**: Examples enforce uniform structure and style.\n- **Performance**: Well-chosen examples boost Claude’s ability to handle complex tasks.\n"
    },
    {
      "id": "4f51b167-e3c6-4872-9adb-d9e63011d132",
      "size": 11,
      "headers": {
        "h1": "",
        "h2": "",
        "h3": ""
      },
      "text": "`160.79.104.0/23`\n"
    },
    {
      "id": "b9567db7-4036-4260-939e-5d136c663352",
      "size": 54,
      "headers": {
        "h1": "",
        "h2": "",
        "h3": ""
      },
      "text": "`2607:6bc0::/48`\n\n[Getting started](/en/api/getting-started) [Versions](/en/api/versioning)\n\nOn this page\n\n- [IPv4](#ipv4)\n- [IPv6](#ipv6)\n"
    },
    {
      "id": "9d833ab0-a47a-4e60-9002-1c9030a0eac3",
      "size": 51,
      "headers": {
        "h1": "Where the model is running. e.g. us-central1 or europe-west4 for haiku",
        "h2": "",
        "h3": ""
      },
      "text": "First, install Anthropic’s [client SDK](/en/api/client-sdks) for your language of choice.\n\nPython\n\nTypescript\n\nCopy\n\n```Python\npip install -U google-cloud-aiplatform \"anthropic[vertex]\"\n\n```\n"
    },
    {
      "id": "fa95e9c5-34d9-4662-9796-cc3cc03e4cfb",
      "size": 75,
      "headers": {
        "h1": "Where the model is running. e.g. us-central1 or europe-west4 for haiku",
        "h2": "",
        "h3": ""
      },
      "text": "Note that Anthropic model availability varies by region. Search for “Claude” in the [Vertex AI Model Garden](https://console.cloud.google.com/vertex-ai/model-garden) or go to [Use Claude 3](https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude) for the latest information.\n"
    },
    {
      "id": "620bca99-eeaf-441d-a7a8-f8fd6c05cf23",
      "size": 97,
      "headers": {
        "h1": "Where the model is running. e.g. us-central1 or europe-west4 for haiku",
        "h2": "",
        "h3": ""
      },
      "text": "| Model | Vertex AI API model name |\n| --- | --- |\n| Claude 3 Haiku | claude-3-haiku@20240307 |\n| Claude 3 Sonnet | claude-3-sonnet@20240229 |\n| Claude 3 Opus (Public Preview) | claude-3-opus@20240229 |\n| Claude 3.5 Sonnet | claude-3-5-sonnet@20240620 |\n"
    },
    {
      "id": "01b3e971-51cf-4287-9ef1-4b921edbd500",
      "size": 69,
      "headers": {
        "h1": "Where the model is running. e.g. us-central1 or europe-west4 for haiku",
        "h2": "",
        "h3": ""
      },
      "text": "Before running requests you may need to run `gcloud auth application-default login` to authenticate with GCP.\n\nThe following examples shows how to generate text from Claude 3 Haiku on Vertex AI:\n\nPython\n\nTypescript\n\ncURL\n\nCopy\n\n```Python\nfrom anthropic import AnthropicVertex\n\nproject_id = \"MY_PROJECT_ID\"\n"
    },
    {
      "id": "715635b1-d3bd-4897-8245-e4ee710c854e",
      "size": 59,
      "headers": {
        "h1": "",
        "h2": "​  Get started with Claude",
        "h3": ""
      },
      "text": "Later this year\n\n[**Claude 3.5 Sonnet** \\\\\n\\\\\nOur most intelligent model\\\\\n\\\\\nText and image input\\\\\n\\\\\nText output\\\\\n\\\\\n200k context window](/en/docs/about-claude/models#model-comparison-table)\n"
    },
    {
      "id": "7f031a7a-7d0a-4afb-b801-419472ac9aa8",
      "size": 60,
      "headers": {
        "h1": "",
        "h2": "​  Get started with Claude",
        "h3": ""
      },
      "text": "Here is a visualization comparing cost vs. speed across Claude 3 and 3.5 models, showcasing the range in tradeoffs between cost and intelligence:\n\n![](https://mintlify.s3-us-west-1.amazonaws.com/anthropic/images/3-5-sonnet-curve.png)\n"
    }
  ],
  "too_large": []
}