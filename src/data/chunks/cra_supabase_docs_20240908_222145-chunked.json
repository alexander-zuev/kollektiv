[
  {
    "chunk_id": "f6fd8cb3-df94-4e59-97d6-23da471d71fe",
    "metadata": {
      "token_count": 674,
      "source_url": "https://supabase.com/docs/guides/ai/quickstarts/text-deduplication",
      "page_title": "Semantic Text Deduplication | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "create vector store client",
        "h2": "Connecting to your database \\#",
        "h3": ""
      },
      "text": "AI & Vectors\nThis guide will walk you through a [\"Semantic Text Deduplication\"](https://github.com/supabase/supabase/blob/master/examples/ai/semantic_text_deduplication.ipynb) example using Colab and Supabase Vecs. You'll learn how to find similar movie reviews using embeddings, and remove any that seem like duplicates. You will:\n\n1. Launch a Postgres database that uses pgvector to store embeddings\n2. Launch a notebook that connects to your database\n3. Load the IMDB dataset\n4. Use the <code>sentence-transformers/all-MiniLM-L6-v2</code> model to create an embedding representing the semantic meaning of each review.\n5. Search for all duplicates.\nLet's create a new Postgres database. This is as simple as starting a new Project in Supabase:\n\n1. [Create a new project](https://database.new/) in the Supabase dashboard.\n2. Enter your project details. Remember to store your password somewhere safe.\n\nYour database will be available in less than a minute.\n\n**Finding your credentials:**\n\nYou can find your project credentials inside the project [settings](https://supabase.com/dashboard/project/_/settings/), including:\n\n- [Database credentials](https://supabase.com/dashboard/project/_/settings/database): connection strings and connection pooler details.\n- [API credentials](https://supabase.com/dashboard/project/_/settings/database): your serverless API URL and <code>anon</code> / <code>service_role</code> keys.\nLaunch our [<code>semantic_text_deduplication</code>](https://github.com/supabase/supabase/blob/master/examples/ai/semantic_text_deduplication.ipynb) notebook in Colab:\n\n[![](https://supabase.com/docs/img/ai/colab-badge.svg)](https://colab.research.google.com/github/supabase/supabase/blob/master/examples/ai/semantic_text_deduplication.ipynb)\n\nAt the top of the notebook, you'll see a button <code>Copy to Drive</code>. Click this button to copy the notebook to your Google Drive.\nInside the Notebook, find the cell which specifies the <code>DB_CONNECTION</code>. It will contain some code like this:\n\n`\n_10\nimport vecs\n_10\n_10\nDB_CONNECTION = \"postgresql://<user>:<password>@<host>:<port>/<db_name>\"\n_10\n_10\n_10\nvx = vecs.create_client(DB_CONNECTION)\n`\n\nReplace the <code>DB_CONNECTION</code> with your own connection string for your database. You can find the Postgres connection string in the [Database Settings](https://supabase.com/dashboard/project/_/settings/database) of your Supabase project.\n\nSQLAlchemy requires the connection string to start with <code>postgresql://</code> (instead of <code>postgres://</code>). Don't forget to rename this after copying the string from the dashboard.\n\nYou must use the \"connection pooling\" string (domain ending in <code>*.pooler.supabase.com</code>) with Google Colab since Colab does not support IPv6.\n"
    }
  },
  {
    "chunk_id": "fbe2e723-d389-434d-b2fd-c5303d87e27c",
    "metadata": {
      "token_count": 233,
      "source_url": "https://supabase.com/docs/guides/ai/quickstarts/text-deduplication",
      "page_title": "Semantic Text Deduplication | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "create vector store client",
        "h2": "Next steps \\#",
        "h3": ""
      },
      "text": "Now all that's left is to step through the notebook. You can do this by clicking the \"execute\" button ( <code>ctrl+enter</code>) at the top left of each code cell. The notebook guides you through the process of creating a collection, adding data to it, and querying it.\n\nYou can view the inserted items in the [Table Editor](https://supabase.com/dashboard/project/_/editor/), by selecting the <code>vecs</code> schema from the schema dropdown.\n\n![Colab documents](https://supabase.com/docs/img/ai/google-colab/colab-documents.png)\nIf you have your own infrastructure for deploying Python apps, you can continue to use <code>vecs</code> as described in this guide.\n\nAlternatively if you would like to quickly deploy using Supabase, check out our guide on using the [Hugging Face Inference API](/docs/guides/ai/hugging-face) in Edge Functions using TypeScript.\nYou can now start building your own applications with Vecs. Check our [examples](/docs/guides/ai#examples) for ideas.\n",
      "overlap_text": {
        "previous_chunk_id": "f6fd8cb3-df94-4e59-97d6-23da471d71fe",
        "text": " to rename this after copying the string from the dashboard.\n\nYou must use the \"connection pooling\" string (domain ending in <code>*.pooler.supabase.com</code>) with Google Colab since Colab does not support IPv6.\n"
      }
    }
  },
  {
    "chunk_id": "d671365b-ca1d-444a-bec2-cd39cdc09e21",
    "metadata": {
      "token_count": 590,
      "source_url": "https://supabase.com/docs/guides/ai/examples/semantic-image-search-amazon-titan",
      "page_title": "Semantic Image Search with Amazon Titan | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Semantic Image Search with Amazon Titan",
        "h2": "Install the dependencies \\#",
        "h3": ""
      },
      "text": "AI & Vectors\n[Amazon Bedrock](https://aws.amazon.com/bedrock) is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon. Each model is accessible through a common API which implements a broad set of features to help build generative AI applications with security, privacy, and responsible AI in mind.\n\n[Amazon Titan](https://aws.amazon.com/bedrock/titan/) is a family of foundation models (FMs) for text and image generation, summarization, classification, open-ended Q&A, information extraction, and text or image search.\n\nIn this guide we'll look at how we can get started with Amazon Bedrock and Supabase Vector in Python using the Amazon Titan multimodal model and the [vecs client](/docs/guides/ai/vecs-python-client).\n\nYou can find the full application code as a Python Poetry project on [GitHub](https://github.com/supabase/supabase/tree/master/examples/ai/aws_bedrock_image_search).\n[Poetry](https://python-poetry.org/) provides packaging and dependency management for Python. If you haven't already, install poetry via pip:\n\n`\n_10\npip install poetry\n`\n\nThen initialize a new project:\n\n`\n_10\npoetry new aws_bedrock_image_search\n`\nIf you haven't already, head over to [database.new](https://database.new) and create a new project. Every Supabase project comes with a full Postgres database and the [pgvector extension](/docs/guides/database/extensions/pgvector) preconfigured.\n\nWhen creating your project, make sure to note down your database password as you will need it to construct the <code>DB_URL</code> in the next step.\n\nYou can find the database connection string in your Supabase Dashboard [database settings](https://supabase.com/dashboard/project/_/settings/database). Select \"Use connection pooling\" with <code>Mode: Session</code> for a direct connection to your Postgres database. It will look something like this:\n\n`\n_10\npostgresql://postgres.[PROJECT-REF]:[YOUR-PASSWORD]@aws-0-[REGION].pooler.supabase.com:5432/postgres\n`\nWe will need to add the following dependencies to our project:\n\n- [<code>vecs</code>](https://github.com/supabase/vecs#vecs): Supabase Vector Python Client.\n- [<code>boto3</code>](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html): AWS SDK for Python.\n- [<code>matplotlib</code>](https://matplotlib.org/): for displaying our image result.\n\n`\n_10\npoetry add vecs boto3 matplotlib\n`\n"
    }
  },
  {
    "chunk_id": "caef1c4e-c190-4655-ad8b-17541044dd8b",
    "metadata": {
      "token_count": 262,
      "source_url": "https://supabase.com/docs/guides/ai/examples/semantic-image-search-amazon-titan",
      "page_title": "Semantic Image Search with Amazon Titan | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Credentials from your AWS account",
        "h2": "Import the necessary dependencies \\#",
        "h3": ""
      },
      "text": "At the top of your main python script, import the dependencies and store your <code>DB URL</code> from above in a variable:\n\n`\n_10\nimport sys\n_10\nimport boto3\n_10\nimport vecs\n_10\nimport json\n_10\nimport base64\n_10\nfrom matplotlib import pyplot as plt\n_10\nfrom matplotlib import image as mpimg\n_10\nfrom typing import Optional\n_10\n_10\nDB_CONNECTION = \"postgresql://postgres.[PROJECT-REF]:[YOUR-PASSWORD]@aws-0-[REGION].pooler.supabase.com:5432/postgres\"\n`\n\nNext, get the [credentials to your AWS account](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html) and instantiate the <code>boto3</code> client:\n\n`\n_10\nbedrock_client = boto3.client(\n_10\n    'bedrock-runtime',\n_10\n    region_name='us-west-2',\n_10\n_10\n    aws_access_key_id='<replace_your_own_credentials>',\n_10\n    aws_secret_access_key='<replace_your_own_credentials>',\n_10\n    aws_session_token='<replace_your_own_credentials>',\n_10\n)\n`\n",
      "overlap_text": {
        "previous_chunk_id": "d671365b-ca1d-444a-bec2-cd39cdc09e21",
        "text": ".amazonaws.com/v1/documentation/api/latest/index.html): AWS SDK for Python.\n- [<code>matplotlib</code>](https://matplotlib.org/): for displaying our image result.\n\n`\n_10\npoetry add vecs boto3 matplotlib\n`\n"
      }
    }
  },
  {
    "chunk_id": "52f22679-129b-4243-9da7-5bb80449bfae",
    "metadata": {
      "token_count": 706,
      "source_url": "https://supabase.com/docs/guides/ai/examples/semantic-image-search-amazon-titan",
      "page_title": "Semantic Image Search with Amazon Titan | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Generate image embeddings with Amazon Titan Model",
        "h2": "Create embeddings for your images \\#",
        "h3": ""
      },
      "text": "In the root of your project, create a new folder called <code>images</code> and add some images. You can use the images from the example project on [GitHub](https://github.com/supabase/supabase/tree/master/examples/ai/aws_bedrock_image_search/images) or you can find license free images on [unsplash](https://unsplash.com).\n\nTo send images to the Amazon Bedrock API we need to need to encode them as <code>base64</code> strings. Create the following helper methods:\n\n`\n_44\ndef readFileAsBase64(file_path):\n_44\n    \"\"\"Encode image as base64 string.\"\"\"\n_44\n    try:\n_44\n        with open(file_path, \"rb\") as image_file:\n_44\n            input_image = base64.b64encode(image_file.read()).decode(\"utf8\")\n_44\n        return input_image\n_44\n    except:\n_44\n        print(\"bad file name\")\n_44\n        sys.exit(0)\n_44\n_44\n_44\ndef construct_bedrock_image_body(base64_string):\n_44\n    \"\"\"Construct the request body.\n_44\n_44\n    https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-embed-mm.html\n_44\n    \"\"\"\n_44\n    return json.dumps(\n_44\n        {\n_44\n            \"inputImage\": base64_string,\n_44\n            \"embeddingConfig\": {\"outputEmbeddingLength\": 1024},\n_44\n        }\n_44\n    )\n_44\n_44\n_44\ndef get_embedding_from_titan_multimodal(body):\n_44\n    \"\"\"Invoke the Amazon Titan Model via API request.\"\"\"\n_44\n    response = bedrock_client.invoke_model(\n_44\n        body=body,\n_44\n        modelId=\"amazon.titan-embed-image-v1\",\n_44\n        accept=\"application/json\",\n_44\n        contentType=\"application/json\",\n_44\n    )\n_44\n_44\n    response_body = json.loads(response.get(\"body\").read())\n_44\n    print(response_body)\n_44\n    return response_body[\"embedding\"]\n_44\n_44\n_44\ndef encode_image(file_path):\n_44\n    \"\"\"Generate embedding for the image at file_path.\"\"\"\n_44\n    base64_string = readFileAsBase64(file_path)\n_44\n    body = construct_bedrock_image_body(base64_string)\n_44\n    emb = get_embedding_from_titan_multimodal(body)\n_44\n    return emb\n`\n\nNext, create a <code>seed</code> method, which will create a new Supabase Vector Collection, generate embeddings for your images, and upsert the embeddings into your database:\n\n`\n_40\ndef seed():\n_40\n_40\n    vx = vecs.create_client(DB_CONNECTION)\n_40\n_40\n_40\n    images = vx.get_or_create_collection(name=\"image_vectors\", dimension=1024)\n_40\n_40\n_40\n    img_emb1 = encode_image('./images/one.jpg')\n_40\n    img_emb2 = encode_image('./images/two.jpg')\n_40\n    img_emb3 = encode_image('./images/three.jpg')\n_40\n    img_emb4 = encode_image('./images/four.jpg')\n_40\n_40\n",
      "overlap_text": {
        "previous_chunk_id": "caef1c4e-c190-4655-ad8b-17541044dd8b",
        "text": "\n_10\n    aws_access_key_id='<replace_your_own_credentials>',\n_10\n    aws_secret_access_key='<replace_your_own_credentials>',\n_10\n    aws_session_token='<replace_your_own_credentials>',\n_10\n)\n`\n"
      }
    }
  },
  {
    "chunk_id": "3dfef0a4-de24-42da-a59e-750eac356346",
    "metadata": {
      "token_count": 795,
      "source_url": "https://supabase.com/docs/guides/ai/examples/semantic-image-search-amazon-titan",
      "page_title": "Semantic Image Search with Amazon Titan | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "query the collection filtering metadata for \"type\" = \"jpg\"",
        "h2": "Conclusion \\#",
        "h3": ""
      },
      "text": "_40\n    images.upsert(\n_40\n        records=[\\\n_40\\\n            (\\\n_40\\\n                \"one.jpg\",       # the vector's identifier\\\n_40\\\n                img_emb1,        # the vector. list or np.array\\\n_40\\\n                {\"type\": \"jpg\"}  # associated  metadata\\\n_40\\\n            ), (\\\n_40\\\n                \"two.jpg\",\\\n_40\\\n                img_emb2,\\\n_40\\\n                {\"type\": \"jpg\"}\\\n_40\\\n            ), (\\\n_40\\\n                \"three.jpg\",\\\n_40\\\n                img_emb3,\\\n_40\\\n                {\"type\": \"jpg\"}\\\n_40\\\n            ), (\\\n_40\\\n                \"four.jpg\",\\\n_40\\\n                img_emb4,\\\n_40\\\n                {\"type\": \"jpg\"}\\\n_40\\\n            )\\\n_40\\\n        ]\n_40\n    )\n_40\n    print(\"Inserted images\")\n_40\n_40\n_40\n    images.create_index()\n_40\n    print(\"Created index\")\n`\n\nAdd this method as a script in your <code>pyproject.toml</code> file:\n\n`\n_10\n[tool.poetry.scripts]\n_10\nseed = \"image_search.main:seed\"\n_10\nsearch = \"image_search.main:search\"\n`\n\nAfter activating the virtual environtment with <code>poetry shell</code> you can now run your seed script via <code>poetry run seed</code>. You can inspect the generated embeddings in your Supabase Dashboard by visiting the [Table Editor](https://supabase.com/dashboard/project/_/editor), selecting the <code>vecs</code> schema, and the <code>image_vectors</code> table.\nWith Supabase Vector we can easily query our embeddings. We can use either an image as the search input or alternatively we can generate an embedding from a string input and use that as the query input:\n\n`\n_28\ndef search(query_term: Optional[str] = None):\n_28\n    if query_term is None:\n_28\n        query_term = sys.argv[1]\n_28\n_28\n_28\n    vx = vecs.create_client(DB_CONNECTION)\n_28\n    images = vx.get_or_create_collection(name=\"image_vectors\", dimension=1024)\n_28\n_28\n_28\n    text_emb = get_embedding_from_titan_multimodal(json.dumps(\n_28\n        {\n_28\n            \"inputText\": query_term,\n_28\n            \"embeddingConfig\": {\"outputEmbeddingLength\": 1024},\n_28\n        }\n_28\n    ))\n_28\n_28\n_28\n    results = images.query(\n_28\n        data=text_emb,                      # required\n_28\n        limit=1,                            # number of records to return\n_28\n        filters={\"type\": {\"$eq\": \"jpg\"}},   # metadata filters\n_28\n    )\n_28\n    result = results[0]\n_28\n    print(result)\n_28\n    plt.title(result)\n_28\n    image = mpimg.imread('./images/' + result)\n_28\n    plt.imshow(image)\n_28\n    plt.show()\n`\n\nBy limiting the query to one result, we can show the most relevant image to the user. Finally we use <code>matplotlib</code> to show the image result to the user.\n\nThat's it, go ahead and test it out by running <code>poetry run search</code> and you will be presented with an image of a \"bike in front of a red brick wall\".\nWith just a couple of lines of Python you are able to implement image search as well as reverse image search using the Amazon Titan multimodal model and Supabase Vector.\n",
      "overlap_text": {
        "previous_chunk_id": "52f22679-129b-4243-9da7-5bb80449bfae",
        "text": " img_emb2 = encode_image('./images/two.jpg')\n_40\n    img_emb3 = encode_image('./images/three.jpg')\n_40\n    img_emb4 = encode_image('./images/four.jpg')\n_40\n_40\n"
      }
    }
  },
  {
    "chunk_id": "2de96bd3-d726-46a4-885f-677696e7503b",
    "metadata": {
      "token_count": 825,
      "source_url": "https://supabase.com/docs/guides/ai/quickstarts/face-similarity",
      "page_title": "Face similarity search | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "create vector store client",
        "h2": "Next steps \\#",
        "h3": ""
      },
      "text": "AI & Vectors\nThis guide will walk you through a [\"Face Similarity Search\"](https://github.com/supabase/supabase/blob/master/examples/ai/face_similarity.ipynb) example using Colab and Supabase Vecs. You will be able to identify the celebrities who look most similar to you (or any other person). You will:\n\n1. Launch a Postgres database that uses pgvector to store embeddings\n2. Launch a notebook that connects to your database\n3. Load the \" <code>ashraq/tmdb-people-image</code>\" celebrity dataset\n4. Use the <code>face_recognition</code> model to create an embedding for every celebrity photo.\n5. Search for similar faces inside the dataset.\nLet's create a new Postgres database. This is as simple as starting a new Project in Supabase:\n\n1. [Create a new project](https://database.new/) in the Supabase dashboard.\n2. Enter your project details. Remember to store your password somewhere safe.\n\nYour database will be available in less than a minute.\n\n**Finding your credentials:**\n\nYou can find your project credentials inside the project [settings](https://supabase.com/dashboard/project/_/settings/), including:\n\n- [Database credentials](https://supabase.com/dashboard/project/_/settings/database): connection strings and connection pooler details.\n- [API credentials](https://supabase.com/dashboard/project/_/settings/database): your serverless API URL and <code>anon</code> / <code>service_role</code> keys.\nLaunch our [<code>semantic_text_deduplication</code>](https://github.com/supabase/supabase/blob/master/examples/ai/face_similarity.ipynb) notebook in Colab:\n\n[![](https://supabase.com/docs/img/ai/colab-badge.svg)](https://colab.research.google.com/github/supabase/supabase/blob/master/examples/ai/face_similarity.ipynb)\n\nAt the top of the notebook, you'll see a button <code>Copy to Drive</code>. Click this button to copy the notebook to your Google Drive.\nInside the Notebook, find the cell which specifies the <code>DB_CONNECTION</code>. It will contain some code like this:\n\n`\n_10\nimport vecs\n_10\n_10\nDB_CONNECTION = \"postgresql://<user>:<password>@<host>:<port>/<db_name>\"\n_10\n_10\n_10\nvx = vecs.create_client(DB_CONNECTION)\n`\n\nReplace the <code>DB_CONNECTION</code> with your own connection string for your database. You can find the Postgres connection string in the [Database Settings](https://supabase.com/dashboard/project/_/settings/database) of your Supabase project.\n\nSQLAlchemy requires the connection string to start with <code>postgresql://</code> (instead of <code>postgres://</code>). Don't forget to rename this after copying the string from the dashboard.\n\nYou must use the \"connection pooling\" string (domain ending in <code>*.pooler.supabase.com</code>) with Google Colab since Colab does not support IPv6.\nNow all that's left is to step through the notebook. You can do this by clicking the \"execute\" button ( <code>ctrl+enter</code>) at the top left of each code cell. The notebook guides you through the process of creating a collection, adding data to it, and querying it.\n\nYou can view the inserted items in the [Table Editor](https://supabase.com/dashboard/project/_/editor/), by selecting the <code>vecs</code> schema from the schema dropdown.\n\n![Colab documents](https://supabase.com/docs/img/ai/google-colab/colab-documents.png)\nYou can now start building your own applications with Vecs. Check our [examples](/docs/guides/ai#examples) for ideas.\n"
    }
  },
  {
    "chunk_id": "1563fbd2-bb05-445b-979b-fc65e0ffc001",
    "metadata": {
      "token_count": 512,
      "source_url": "https://supabase.com/docs/guides/ai/integrations/roboflow",
      "page_title": "Roboflow | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Roboflow",
        "h2": "Save computer vision predictions \\#",
        "h3": "Step 1: Install and start Roboflow Inference \\#"
      },
      "text": "AI & Vectors\nIn this guide, we will walk through two examples of using [Roboflow Inference](https://inference.roboflow.com) to run fine-tuned and foundation models. We will run inference and save predictions using an object detection model and [CLIP](https://github.com/openai/CLIP).\nLet's create a new Postgres database. This is as simple as starting a new Project in Supabase:\n\n1. [Create a new project](https://database.new/) in the Supabase dashboard.\n2. Enter your project details. Remember to store your password somewhere safe.\n\nYour database will be available in less than a minute.\n\n**Finding your credentials:**\n\nYou can find your project credentials inside the project [settings](https://supabase.com/dashboard/project/_/settings/), including:\n\n- [Database credentials](https://supabase.com/dashboard/project/_/settings/database): connection strings and connection pooler details.\n- [API credentials](https://supabase.com/dashboard/project/_/settings/database): your serverless API URL and <code>anon</code> / <code>service_role</code> keys.\nOnce you have a trained vision model, you need to create business logic for your application. In many cases, you want to save inference results to a file.\n\nThe steps below show you how to run a vision model locally and save predictions to Supabase.\nBefore you begin, you will need an object detection model trained on your data.\n\nYou can [train a model on Roboflow](https://blog.roboflow.com/getting-started-with-roboflow/), leveraging end-to-end tools from data management and annotation to deployment, or [upload custom model weights](https://docs.roboflow.com/deploy/upload-custom-weights) for deployment.\n\nAll models have an infinitely scalable API through which you can query your model, and can be run locally.\n\nFor this guide, we will use a demo [rock, paper, scissors](https://universe.roboflow.com/roboflow-58fyf/rock-paper-scissors-sxsw) model.\nYou will deploy our model locally using Roboflow Inference, a computer vision inference server.\n\nTo install and start Roboflow Inference, first install Docker on your machine.\n\nThen, run:\n\n`\n_10\npip install inference inference-cli inference-sdk && inference server start\n`\n\nAn inference server will be available at <code>http://localhost:9001</code>.\n"
    }
  },
  {
    "chunk_id": "8f14ec96-f462-4e40-8844-56779fdd4e29",
    "metadata": {
      "token_count": 851,
      "source_url": "https://supabase.com/docs/guides/ai/integrations/roboflow",
      "page_title": "Roboflow | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Roboflow",
        "h2": "Calculate and save CLIP embeddings \\#",
        "h3": "Step 1: Install and start Roboflow Inference \\#"
      },
      "text": "You can run inference on images and videos. Let's run inference on an image.\n\nCreate a new Python file and add the following code:\n\n`\n_13\nfrom inference_sdk import InferenceHTTPClient\n_13\n_13\nimage = \"example.jpg\"\n_13\nMODEL_ID = \"rock-paper-scissors-sxsw/11\"\n_13\n_13\nclient = InferenceHTTPClient(\n_13\n    api_url=\"http://localhost:9001\",\n_13\n    api_key=\"ROBOFLOW_API_KEY\"\n_13\n)\n_13\nwith client.use_model(MODEL_ID):\n_13\n    predictions = client.infer(image)\n_13\n_13\nprint(predictions)\n`\n\nAbove, replace:\n\n1. The image URL with the name of the image on which you want to run inference.\n2. <code>ROBOFLOW_API_KEY</code> with your Roboflow API key. [Learn how to retrieve your Roboflow API key](https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key).\n3. <code>MODEL_ID</code> with your Roboflow model ID. [Learn how to retrieve your model ID](https://docs.roboflow.com/api-reference/workspace-and-project-ids).\n\nWhen you run the code above, a list of predictions will be printed to the console:\n\n`\n_10\n{'time': 0.05402109300121083, 'image': {'width': 640, 'height': 480}, 'predictions': [{'x': 312.5, 'y': 392.0, 'width': 255.0, 'height': 110.0, 'confidence': 0.8620790839195251, 'class': 'Paper', 'class_id': 0}]}\n`\nTo save results in Supabase, add the following code to your script:\n\n`\n_10\nimport os\n_10\nfrom supabase import create_client, Client\n_10\n_10\nurl: str = os.environ.get(\"SUPABASE_URL\")\n_10\nkey: str = os.environ.get(\"SUPABASE_KEY\")\n_10\nsupabase: Client = create_client(url, key)\n_10\n_10\nresult = supabase.table('predictions') \\\n_10\n    .insert({\"filename\": image, \"predictions\": predictions}) \\\n_10\n    .execute()\n`\n\nYou can then query your predictions using the following code:\n\n`\n_10\nresult = supabase.table('predictions') \\\n_10\n    .select(\"predictions\") \\\n_10\n    .filter(\"filename\", \"eq\", image) \\\n_10\n    .execute()\n_10\n_10\nprint(result)\n`\n\nHere is an example result:\n\n`\n_10\ndata=[{'predictions': {'time': 0.08492901099998562, 'image': {'width': 640, 'height': 480}, 'predictions': [{'x': 312.5, 'y': 392.0, 'width': 255.0, 'height': 110.0, 'confidence': 0.8620790839195251, 'class': 'Paper', 'class_id': 0}]}}, {'predictions': {'time': 0.08818970100037404, 'image': {'width': 640, 'height': 480}, 'predictions': [{'x': 312.5, 'y': 392.0, 'width': 255.0, 'height': 110.0, 'confidence': 0.8620790839195251, 'class': 'Paper', 'class_id': 0}]}}] count=None\n`\nYou can use the Supabase vector database functionality to store and query CLIP embeddings.\n\nRoboflow Inference provides a HTTP interface through which you can calculate image and text embeddings using CLIP.\nSee [Step #1: Install and Start Roboflow Inference](#step-1-install-and-start-roboflow-inference) above to install and start Roboflow Inference.\n",
      "overlap_text": {
        "previous_chunk_id": "1563fbd2-bb05-445b-979b-fc65e0ffc001",
        "text": " Inference, first install Docker on your machine.\n\nThen, run:\n\n`\n_10\npip install inference inference-cli inference-sdk && inference server start\n`\n\nAn inference server will be available at <code>http://localhost:9001</code>.\n"
      }
    }
  },
  {
    "chunk_id": "8d5d733b-7735-4bb9-b1a7-dc5308faaa3d",
    "metadata": {
      "token_count": 592,
      "source_url": "https://supabase.com/docs/guides/ai/integrations/roboflow",
      "page_title": "Roboflow | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "create a collection of vectors with 3 dimensions",
        "h2": "Calculate and save CLIP embeddings \\#",
        "h3": "Step 3: Save embeddings in Supabase \\#"
      },
      "text": "Create a new Python file and add the following code:\n\n`\n_32\nimport cv2\n_32\nimport supervision as sv\n_32\nimport requests\n_32\nimport base64\n_32\nimport os\n_32\n_32\nIMAGE_DIR = \"images/train/images/\"\n_32\nAPI_KEY = \"\"\n_32\nSERVER_URL = \"http://localhost:9001\"\n_32\n_32\nresults = []\n_32\n_32\nfor i, image in enumerate(os.listdir(IMAGE_DIR)):\n_32\n    print(f\"Processing image {image}\")\n_32\n    infer_clip_payload = {\n_32\n        \"image\": {\n_32\n            \"type\": \"base64\",\n_32\n            \"value\": base64.b64encode(open(IMAGE_DIR + image, \"rb\").read()).decode(\"utf-8\"),\n_32\n        },\n_32\n    }\n_32\n_32\n    res = requests.post(\n_32\n        f\"{SERVER_URL}/clip/embed_image?api_key={API_KEY}\",\n_32\n        json=infer_clip_payload,\n_32\n    )\n_32\n_32\n    embeddings = res.json()['embeddings']\n_32\n_32\n    results.append({\n_32\n        \"filename\": image,\n_32\n        \"embeddings\": embeddings\n_32\n    })\n`\n\nThis code will calculate CLIP embeddings for each image in the directory and print the results to the console.\n\nAbove, replace:\n\n1. <code>IMAGE_DIR</code> with the directory containing the images on which you want to run inference.\n2. <code>ROBOFLOW_API_KEY</code> with your Roboflow API key. [Learn how to retrieve your Roboflow API key](https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key).\n\nYou can also calculate CLIP embeddings in the cloud by setting <code>SERVER_URL</code> to <code>https://infer.roboflow.com</code>.\nYou can store your image embeddings in Supabase using the Supabase <code>vecs</code> Python package:\n\nFirst, install <code>vecs</code>:\n\n`\n_10\npip install vecs\n`\n\nNext, add the following code to your script to create an index:\n\n`\n_26\n_26\nimport vecs\n_26\n_26\nDB_CONNECTION = \"postgresql://postgres:[password]@[host]:[port]/[database]\"\n_26\n_26\nvx = vecs.create_client(DB_CONNECTION)\n_26\n_26\n_26\nimages = vx.get_or_create_collection(name=\"image_vectors\", dimension=512)\n_26\n_26\nfor result in results:\n_26\n    image = result[\"filename\"]\n_26\n    embeddings = result[\"embeddings\"][0]\n_26\n_26\n",
      "overlap_text": {
        "previous_chunk_id": "8f14ec96-f462-4e40-8844-56779fdd4e29",
        "text": " you can calculate image and text embeddings using CLIP.\nSee [Step #1: Install and Start Roboflow Inference](#step-1-install-and-start-roboflow-inference) above to install and start Roboflow Inference.\n"
      }
    }
  },
  {
    "chunk_id": "69864209-1a36-4fde-887b-d2a362e59ebe",
    "metadata": {
      "token_count": 330,
      "source_url": "https://supabase.com/docs/guides/ai/integrations/roboflow",
      "page_title": "Roboflow | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "insert a vector into the collection",
        "h2": "Resources \\#",
        "h3": ""
      },
      "text": "_26\n    images.upsert(\n_26\n        records=[\\\n_26\\\n            (\\\n_26\\\n                image,\\\n_26\\\n                embeddings,\\\n_26\\\n                {} # metadata\\\n_26\\\n            )\\\n_26\\\n        ]\n_26\n    )\n_26\n_26\nimages.create_index()\n`\n\nReplace <code>DB_CONNECTION</code> with the authentication information for your database. You can retrieve this from the Supabase dashboard in <code>Project Settings > Database Settings</code>.\n\nYou can then query your embeddings using the following code:\n\n`\n_17\ninfer_clip_payload = {\n_17\n    \"text\": \"cat\",\n_17\n}\n_17\n_17\nres = requests.post(\n_17\n    f\"{SERVER_URL}/clip/embed_text?api_key={API_KEY}\",\n_17\n    json=infer_clip_payload,\n_17\n)\n_17\n_17\nembeddings = res.json()['embeddings']\n_17\n_17\nresult = images.query(\n_17\n    data=embeddings[0],\n_17\n    limit=1\n_17\n)\n_17\n_17\nprint(result[0])\n`\n- [Roboflow Inference documentation](https://inference.roboflow.com)\n- [Roboflow Getting Started guide](https://blog.roboflow.com/getting-started-with-roboflow/)\n- [How to Build a Semantic Image Search Engine with Supabase and OpenAI CLIP](https://blog.roboflow.com/how-to-use-semantic-search-supabase-openai-clip/)\n",
      "overlap_text": {
        "previous_chunk_id": "8d5d733b-7735-4bb9-b1a7-dc5308faaa3d",
        "text": "_collection(name=\"image_vectors\", dimension=512)\n_26\n_26\nfor result in results:\n_26\n    image = result[\"filename\"]\n_26\n    embeddings = result[\"embeddings\"][0]\n_26\n_26\n"
      }
    }
  },
  {
    "chunk_id": "227a89a9-1a76-43c9-9741-071f6776522c",
    "metadata": {
      "token_count": 808,
      "source_url": "https://supabase.com/docs/guides/ai/integrations/llamaindex",
      "page_title": "Learn how to integrate Supabase with LlamaIndex, a data framework for your LLM applications. | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "create vector store client",
        "h2": "Resources \\#",
        "h3": ""
      },
      "text": "AI & Vectors\nThis guide will walk you through a basic example using the LlamaIndex [SupabaseVectorStore](https://github.com/supabase/supabase/blob/master/examples/ai/llamaindex/llamaindex.ipynb).\nLet's create a new Postgres database. This is as simple as starting a new Project in Supabase:\n\n1. [Create a new project](https://database.new/) in the Supabase dashboard.\n2. Enter your project details. Remember to store your password somewhere safe.\n\nYour database will be available in less than a minute.\n\n**Finding your credentials:**\n\nYou can find your project credentials inside the project [settings](https://supabase.com/dashboard/project/_/settings/), including:\n\n- [Database credentials](https://supabase.com/dashboard/project/_/settings/database): connection strings and connection pooler details.\n- [API credentials](https://supabase.com/dashboard/project/_/settings/database): your serverless API URL and <code>anon</code> / <code>service_role</code> keys.\nLaunch our [LlamaIndex](https://github.com/supabase/supabase/blob/master/examples/ai/llamaindex/llamaindex.ipynb) notebook in Colab:\n\n[![](https://supabase.com/docs/img/ai/colab-badge.svg)](https://colab.research.google.com/github/supabase/supabase/blob/master/examples/ai/llamaindex/llamaindex.ipynb)\n\nAt the top of the notebook, you'll see a button <code>Copy to Drive</code>. Click this button to copy the notebook to your Google Drive.\nInside the Notebook, add your <code>OPENAI_API_KEY</code> key. Find the cell which contains this code:\n\n`\n_10\nimport os\n_10\nos.environ['OPENAI_API_KEY'] = \"[your_openai_api_key]\"\n`\nInside the Notebook, find the cell which specifies the <code>DB_CONNECTION</code>. It will contain some code like this:\n\n`\n_10\nDB_CONNECTION = \"postgresql://<user>:<password>@<host>:<port>/<db_name>\"\n_10\n_10\n_10\nvx = vecs.create_client(DB_CONNECTION)\n`\n\nReplace the <code>DB_CONNECTION</code> with your own connection string for your database. You can find the Postgres connection string in the [Database Settings](https://supabase.com/dashboard/project/_/settings/database) of your Supabase project.\n\nSQLAlchemy requires the connection string to start with <code>postgresql://</code> (instead of <code>postgres://</code>). Don't forget to rename this after copying the string from the dashboard.\n\nYou must use the \"connection pooling\" string (domain ending in <code>*.pooler.supabase.com</code>) with Google Colab since Colab does not support IPv6.\nNow all that's left is to step through the notebook. You can do this by clicking the \"execute\" button ( <code>ctrl+enter</code>) at the top left of each code cell. The notebook guides you through the process of creating a collection, adding data to it, and querying it.\n\nYou can view the inserted items in the [Table Editor](https://supabase.com/dashboard/project/_/editor/), by selecting the <code>vecs</code> schema from the schema dropdown.\n\n![Colab documents](https://supabase.com/docs/img/ai/google-colab/colab-documents.png)\n- Visit the LlamaIndex + SupabaseVectorStore [docs](https://gpt-index.readthedocs.io/en/latest/examples/vector_stores/SupabaseVectorIndexDemo.html)\n- Visit the official LlamaIndex [repo](https://github.com/jerryjliu/llama_index/)\n"
    }
  },
  {
    "chunk_id": "7e02e5c8-665a-4fa1-bfbb-e21bd98ff392",
    "metadata": {
      "token_count": 730,
      "source_url": "https://supabase.com/docs/guides/ai/examples/nextjs-vector-search",
      "page_title": "Vector search with Next.js and OpenAI | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Get your key at https://platform.openai.com/account/api-keys",
        "h2": "Pre-process the knowledge base at build time \\#",
        "h3": "Run script at build time"
      },
      "text": "AI & Vectors\nWhile our [Headless Vector search](/docs/guides/ai/examples/headless-vector-search) provides a toolkit for generative Q&A, in this tutorial we'll go more in-depth, build a custom ChatGPT-like search experience from the ground-up using Next.js. You will:\n\n1. Convert your markdown into embeddings using OpenAI.\n2. Store you embeddings in Postgres using pgvector.\n3. Deploy a function for answering your users' questions.\n\nYou can read our [Supabase Clippy](https://supabase.com/blog/chatgpt-supabase-docs) blog post for a full example.\n\nWe assume that you have a Next.js project with a collection of <code>.mdx</code> files nested inside your <code>pages</code> directory. We will start developing locally with the Supabase CLI and then push our local database changes to our hosted Supabase project. You can find the [full Next.js example on GitHub](https://github.com/supabase-community/nextjs-openai-doc-search).\n1. [Create a new project](https://supabase.com/dashboard) in the Supabase Dashboard.\n2. Enter your project details.\n3. Wait for the new database to launch.\nLet's prepare the database schema. We can use the \"OpenAI Vector Search\" quickstart in the [SQL Editor](https://supabase.com/dashboard/project/_/sql), or you can copy/paste the SQL below and run it yourself.\n\nDashboardSQL\n\n1. Go to the [SQL Editor](https://supabase.com/dashboard/project/_/sql) page in the Dashboard.\n2. Click **OpenAI Vector Search**.\n3. Click **Run**.\nWith our database set up, we need to process and store all <code>.mdx</code> files in the <code>pages</code> directory. You can find the full script [here](https://github.com/supabase-community/nextjs-openai-doc-search/blob/main/lib/generate-embeddings.ts), or follow the steps below:\n\n1\nCreate a new file <code>lib/generate-embeddings.ts</code> and copy the code over from [GitHub](https://github.com/supabase-community/nextjs-openai-doc-search/blob/main/lib/generate-embeddings.ts).\n\n`\n1\ncurl \\\n2\nhttps://raw.githubusercontent.com/supabase-community/nextjs-openai-doc-search/main/lib/generate-embeddings.ts \\\n3\n-o \"lib/generate-embeddings.ts\"\n`\n\n2\nWe need some environment variables to run the script. Add them to your <code>.env</code> file and make sure your <code>.env</code> file is not committed to source control!\nYou can get your local Supabase credentials by running <code>supabase status</code>.\n\n`\n1\nNEXT_PUBLIC_SUPABASE_URL=\n2\nNEXT_PUBLIC_SUPABASE_ANON_KEY=\n3\nSUPABASE_SERVICE_ROLE_KEY=\n4\n5\n6\nOPENAI_API_KEY=\n`\n\n3\nInclude the script in your <code>package.json</code> script commands to enable Vercel to automaticall run it at build time.\n\n`\n1\n\"scripts\": {\n2\n\"dev\": \"next dev\",\n3\n\"build\": \"pnpm run embeddings && next build\",\n4\n\"start\": \"next start\",\n5\n\"embeddings\": \"tsx lib/generate-embeddings.ts\"\n6\n},\n`\n"
    }
  },
  {
    "chunk_id": "ac155220-f59d-41f1-a359-453c4c3a6d2c",
    "metadata": {
      "token_count": 404,
      "source_url": "https://supabase.com/docs/guides/ai/examples/nextjs-vector-search",
      "page_title": "Vector search with Next.js and OpenAI | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Get your key at https://platform.openai.com/account/api-keys",
        "h2": "Create text completion with OpenAI API \\#",
        "h3": "Perform similarity search"
      },
      "text": "Anytime a user asks a question, we need to create an embedding for their question, perform a similarity search, and then send a text completion request to the OpenAI API with the query and then context content merged together into a prompt.\n\nAll of this is glued together in a [Vercel Edge Function](https://vercel.com/docs/concepts/functions/edge-functions), the code for which can be found on [GitHub](https://github.com/supabase-community/nextjs-openai-doc-search/blob/main/pages/api/vector-search.ts).\n\n1\nIn order to perform similarity search we need to turn the question into an embedding.\n\n``\n1\nconst embeddingResponse = await fetch('https://api.openai.com/v1/embeddings', {\n2\nmethod: 'POST',\n3\nheaders: {\n4\n    Authorization: <code>Bearer ${openAiKey}</code>,\n5\n    'Content-Type': 'application/json',\n6\n},\n7\nbody: JSON.stringify({\n8\n    model: 'text-embedding-ada-002',\n9\n    input: sanitizedQuery.replaceAll('\\n', ' '),\n10\n}),\n11\n})\n12\n13\nif (embeddingResponse.status !== 200) {\n14\nthrow new ApplicationError('Failed to create embedding for question', embeddingResponse)\n15\n}\n16\n17\nconst {\n18\ndata: [{ embedding }],\n19\n} = await embeddingResponse.json()\n``\n\n2\nUsing the <code>embeddingResponse</code> we can now perform similarity search by performing an remote procedure call (RPC) to the database function we created earlier.\n\n`\n1\nconst { error: matchError, data: pageSections } = await supabaseClient.rpc(\n2\n'match_page_sections',\n3\n{\n4\n    embedding,\n5\n    match_threshold: 0.78,\n6\n    match_count: 10,\n7\n    min_content_length: 50,\n8\n}\n9\n)\n`\n\n3\n",
      "overlap_text": {
        "previous_chunk_id": "7e02e5c8-665a-4fa1-bfbb-e21bd98ff392",
        "text": "\"dev\": \"next dev\",\n3\n\"build\": \"pnpm run embeddings && next build\",\n4\n\"start\": \"next start\",\n5\n\"embeddings\": \"tsx lib/generate-embeddings.ts\"\n6\n},\n`\n"
      }
    }
  },
  {
    "chunk_id": "8a4f8340-bc69-4fb0-8139-c4f5011d2550",
    "metadata": {
      "token_count": 449,
      "source_url": "https://supabase.com/docs/guides/ai/examples/nextjs-vector-search",
      "page_title": "Vector search with Next.js and OpenAI | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Get your key at https://platform.openai.com/account/api-keys",
        "h2": "Create text completion with OpenAI API \\#",
        "h3": "Perform text completion request"
      },
      "text": "With the relevant content for the user's question identified, we can now build the prompt and make a text completion request via the OpenAI API.\n\nIf successful, the OpenAI API will respond with a <code>text/event-stream</code> response that we can simply forward to the client where we'll process the event stream to smoothly print the answer to the user.\n\n``\n1\nconst prompt = codeBlock`\n2\n${oneLine`\n3\n    You are a very enthusiastic Supabase representative who loves\n4\n    to help people! Given the following sections from the Supabase\n5\n    documentation, answer the question using only that information,\n6\n    outputted in markdown format. If you are unsure and the answer\n7\n    is not explicitly written in the documentation, say\n8\n    \"Sorry, I don't know how to help with that.\"\n9\n`}\n10\n11\nContext sections:\n12\n${contextText}\n13\n14\nQuestion: \"\"\"\n15\n${sanitizedQuery}\n16\n\"\"\"\n17\n18\nAnswer as markdown (including related code snippets if available):\n19\n`\n20\n21\nconst completionOptions: CreateCompletionRequest = {\n22\nmodel: 'gpt-3.5-turbo-instruct',\n23\nprompt,\n24\nmax_tokens: 512,\n25\ntemperature: 0,\n26\nstream: true,\n27\n}\n28\n29\nconst response = await fetch('https://api.openai.com/v1/completions', {\n30\nmethod: 'POST',\n31\nheaders: {\n32\n    Authorization: <code>Bearer ${openAiKey}</code>,\n33\n    'Content-Type': 'application/json',\n34\n},\n35\nbody: JSON.stringify(completionOptions),\n36\n})\n37\n38\nif (!response.ok) {\n39\nconst error = await response.json()\n40\nthrow new ApplicationError('Failed to generate completion', error)\n41\n}\n42\n43\n// Proxy the streamed SSE response from OpenAI\n44\nreturn new Response(response.body, {\n45\nheaders: {\n46\n    'Content-Type': 'text/event-stream',\n47\n},\n48\n})\n``\n",
      "overlap_text": {
        "previous_chunk_id": "ac155220-f59d-41f1-a359-453c4c3a6d2c",
        "text": "_page_sections',\n3\n{\n4\n    embedding,\n5\n    match_threshold: 0.78,\n6\n    match_count: 10,\n7\n    min_content_length: 50,\n8\n}\n9\n)\n`\n\n3\n"
      }
    }
  },
  {
    "chunk_id": "0cf03f6f-6150-4117-943c-b4f8e9d0a4ff",
    "metadata": {
      "token_count": 703,
      "source_url": "https://supabase.com/docs/guides/ai/examples/nextjs-vector-search",
      "page_title": "Vector search with Next.js and OpenAI | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Get your key at https://platform.openai.com/account/api-keys",
        "h2": "Learn more \\#",
        "h3": ""
      },
      "text": "In a last step, we need to process the event stream from the OpenAI API and print the answer to the user. The full code for this can be found on [GitHub](https://github.com/supabase-community/nextjs-openai-doc-search/blob/main/components/SearchDialog.tsx).\n\n``\n1\nconst handleConfirm = React.useCallback(\n2\nasync (query: string) => {\n3\n    setAnswer(undefined)\n4\n    setQuestion(query)\n5\n    setSearch('')\n6\n    dispatchPromptData({ index: promptIndex, answer: undefined, query })\n7\n    setHasError(false)\n8\n    setIsLoading(true)\n9\n10\n    const eventSource = new SSE(<code>api/vector-search</code>, {\n11\n      headers: {\n12\n        apikey: process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY ?? '',\n13\n        Authorization: <code>Bearer ${process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY}</code>,\n14\n        'Content-Type': 'application/json',\n15\n      },\n16\n      payload: JSON.stringify({ query }),\n17\n    })\n18\n19\n    function handleError<T>(err: T) {\n20\n      setIsLoading(false)\n21\n      setHasError(true)\n22\n      console.error(err)\n23\n    }\n24\n25\n    eventSource.addEventListener('error', handleError)\n26\n    eventSource.addEventListener('message', (e: any) => {\n27\n      try {\n28\n        setIsLoading(false)\n29\n30\n        if (e.data === '[DONE]') {\n31\n          setPromptIndex((x) => {\n32\n            return x + 1\n33\n          })\n34\n          return\n35\n        }\n36\n37\n        const completionResponse: CreateCompletionResponse = JSON.parse(e.data)\n38\n        const text = completionResponse.choices[0].text\n39\n40\n        setAnswer((answer) => {\n41\n          const currentAnswer = answer ?? ''\n42\n43\n          dispatchPromptData({\n44\n            index: promptIndex,\n45\n            answer: currentAnswer + text,\n46\n          })\n47\n48\n          return (answer ?? '') + text\n49\n        })\n50\n      } catch (err) {\n51\n        handleError(err)\n52\n      }\n53\n    })\n54\n55\n    eventSource.stream()\n56\n57\n    eventSourceRef.current = eventSource\n58\n59\n    setIsLoading(true)\n60\n},\n61\n[promptIndex, promptData]\n62\n)\n``\nWant to learn more about the awesome tech that is powering this?\n\n- Read about how we built [ChatGPT for the Supabase Docs](https://supabase.com/blog/chatgpt-supabase-docs).\n- Read the pgvector Docs for [Embeddings and vector similarity](https://supabase.com/docs/guides/database/extensions/pgvector)\n- Watch Greg's video for a full breakdown:\n\nWatch video guide\n\n![Video guide preview](https://supabase.com/docs/_next/image?url=http%3A%2F%2Fimg.youtube.com%2Fvi%2FxmfNUCjszh4%2F0.jpg&w=3840&q=75&dpl=dpl_GiCDf4oknfdUcgmXNidH7itZWLva)\n",
      "overlap_text": {
        "previous_chunk_id": "8a4f8340-bc69-4fb0-8139-c4f5011d2550",
        "text": "41\n}\n42\n43\n// Proxy the streamed SSE response from OpenAI\n44\nreturn new Response(response.body, {\n45\nheaders: {\n46\n    'Content-Type': 'text/event-stream',\n47\n},\n48\n})\n``\n"
      }
    }
  },
  {
    "chunk_id": "523fe5a5-7c91-4565-929d-66c7353373ac",
    "metadata": {
      "token_count": 145,
      "source_url": "https://supabase.com/docs/guides/ai/examples/nextjs-vector-search",
      "page_title": "Vector search with Next.js and OpenAI | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Get your key at https://platform.openai.com/account/api-keys",
        "h2": "Learn more \\#",
        "h3": "Is this helpful?"
      },
      "text": "YesNo\n\nThanks for your feedback!\n\n- [Create a project](#create-a-project)\n- [Prepare the database](#prepare-the-database)\n- [Pre-process the knowledge base at build time](#pre-process-the-knowledge-base-at-build-time)\n- [Create text completion with OpenAI API](#create-text-completion-with-openai-api)\n- [Display the answer on the frontend](#display-the-answer-on-the-frontend)\n- [Learn more](#learn-more)\n\n1. We only collect analytics essential to ensuring smooth operation of our services. [Learn more](https://supabase.com/privacy)\n\n   AcceptOpt out[Learn more](https://supabase.com/privacy)\n",
      "overlap_text": {
        "previous_chunk_id": "0cf03f6f-6150-4117-943c-b4f8e9d0a4ff",
        "text": "2Fvi%2FxmfNUCjszh4%2F0.jpg&w=3840&q=75&dpl=dpl_GiCDf4oknfdUcgmXNidH7itZWLva)\n"
      }
    }
  },
  {
    "chunk_id": "dc503b9a-f12a-4700-ab2f-8f2345463798",
    "metadata": {
      "token_count": 611,
      "source_url": "https://supabase.com/docs/guides/ai/vecs-python-client",
      "page_title": "Python client | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "query the collection filtering metadata for \"year\" = 2012",
        "h2": "Resources \\#",
        "h3": "Query the collection \\#"
      },
      "text": "AI & Vectors\nSupabase provides a Python client called [<code>vecs</code>](https://github.com/supabase/vecs) for managing unstructured vector stores. This client provides a set of useful tools for creating and querying collections in PostgreSQL using the [pgvector](/docs/guides/database/extensions/pgvector) extension.\nLet's see how Vecs works using a local database. Make sure you have the Supabase CLI [installed](/docs/guides/cli#installation) on your machine.\nStart a local Postgres instance in any folder using the <code>init</code> and <code>start</code> commands. Make sure you have Docker running!\n\n`\n_10\n_10\nsupabase init\n_10\n_10\n_10\nsupabase start\n`\nInside a Python shell, run the following commands to create a new collection called \"docs\", with 3 dimensions.\n\n`\n_10\nimport vecs\n_10\n_10\n_10\nvx = vecs.create_client(\"postgresql://postgres:postgres@localhost:54322/postgres\")\n_10\n_10\n_10\ndocs = vx.get_or_create_collection(name=\"docs\", dimension=3)\n`\nNow we can insert some embeddings into our \"docs\" collection using the <code>upsert()</code> command:\n\n`\n_13\nimport vecs\n_13\n_13\n_13\ndocs = vecs.get_or_create_collection(name=\"docs\", dimension=3)\n_13\n_13\n_13\nvectors=[\\\n_13\\\n(\"vec0\", [0.1, 0.2, 0.3], {\"year\": 1973}),\\\n_13\\\n(\"vec1\", [0.7, 0.8, 0.9], {\"year\": 2012})\\\n_13\\\n]\n_13\n_13\n_13\ndocs.upsert(vectors=vectors)\n`\nYou can now query the collection to retrieve a relevant match:\n\n`\n_10\nimport vecs\n_10\n_10\ndocs = vecs.get_or_create_collection(name=\"docs\", dimension=3)\n_10\n_10\n_10\ndocs.query(\n_10\n    data=[0.4,0.5,0.6],      # required\n_10\n    limit=1,                         # number of records to return\n_10\n    filters={\"year\": {\"$eq\": 2012}}, # metadata filters\n_10\n)\n`\nFor a more in-depth guide on <code>vecs</code> collections, see [API](/docs/guides/ai/python/api).\n- Official Vecs Documentation: [https://supabase.github.io/vecs/api](https://supabase.github.io/vecs/api)\n- Source Code: [https://github.com/supabase/vecs](https://github.com/supabase/vecs)\n"
    }
  },
  {
    "chunk_id": "c04a06a5-7e4c-4588-9598-840434b57db7",
    "metadata": {
      "token_count": 734,
      "source_url": "https://supabase.com/docs/guides/ai",
      "page_title": "AI & Vectors | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "AI & Vectors",
        "h2": "Examples \\#",
        "h3": ""
      },
      "text": "AI & Vectors\nSupabase provides an open source toolkit for developing AI applications using Postgres and pgvector. Use the Supabase client libraries to store, index, and query your vector embeddings at scale.\n\nThe toolkit includes:\n\n- A [vector store](/docs/guides/ai/vector-columns) and embeddings support using Postgres and pgvector.\n- A [Python client](/docs/guides/ai/vecs-python-client) for managing unstructured embeddings.\n- An [embedding generation](/docs/guides/ai/quickstarts/generate-text-embeddings) process using open source models directly in Edge Functions.\n- [Database migrations](/docs/guides/ai/examples/headless-vector-search#prepare-your-database) for managing structured embeddings.\n- Integrations with all popular AI providers, such as [OpenAI](/docs/guides/ai/examples/openai), [Hugging Face](/docs/guides/ai/hugging-face), [LangChain](/docs/guides/ai/langchain), and more.\n\nYou can use Supabase to build different types of search features for your app, including:\n\n- [Semantic search](/docs/guides/ai/semantic-search): search by meaning rather than exact keywords\n- [Keyword search](/docs/guides/ai/keyword-search): search by words or phrases\n- [Hybrid search](/docs/guides/ai/hybrid-search): combine semantic search with keyword search\nCheck out all of the AI [templates and examples](https://github.com/supabase/supabase/tree/master/examples/ai) in our GitHub repository.\n\n[![Headless Vector Search](https://supabase.com/docs/img/icons/github-icon-light.svg)\\\\\n\\\\\nHeadless Vector Search\\\\\n\\\\\nA toolkit to perform vector similarity search on your knowledge base embeddings.](/docs/guides/ai/examples/headless-vector-search)\n\n[![Image Search with OpenAI CLIP](https://supabase.com/docs/img/icons/github-icon-light.svg)\\\\\n\\\\\nImage Search with OpenAI CLIP\\\\\n\\\\\nImplement image search with the OpenAI CLIP Model and Supabase Vector.](/docs/guides/ai/examples/image-search-openai-clip)\n\n[![Hugging Face inference](https://supabase.com/docs/img/icons/github-icon-light.svg)\\\\\n\\\\\nHugging Face inference\\\\\n\\\\\nGenerate image captions using Hugging Face.](/docs/guides/ai/examples/huggingface-image-captioning)\n\n[![OpenAI completions](https://supabase.com/docs/img/icons/github-icon-light.svg)\\\\\n\\\\\nOpenAI completions\\\\\n\\\\\nGenerate GPT text completions using OpenAI in Edge Functions.](/docs/guides/ai/examples/openai)\n\n[![Building ChatGPT Plugins](https://supabase.com/docs/img/icons/github-icon-light.svg)\\\\\n\\\\\nBuilding ChatGPT Plugins\\\\\n\\\\\nUse Supabase as a Retrieval Store for your ChatGPT plugin.](/docs/guides/ai/examples/building-chatgpt-plugins)\n\n[![Vector search with Next.js and OpenAI](https://supabase.com/docs/img/icons/github-icon-light.svg)\\\\\n\\\\\nVector search with Next.js and OpenAI\\\\\n\\\\\nLearn how to build a ChatGPT-style doc search powered by Next.js, OpenAI, and Supabase.](/docs/guides/ai/examples/nextjs-vector-search)\n"
    }
  },
  {
    "chunk_id": "85ddd66a-f50f-49a5-bf27-f9ba7adec3cd",
    "metadata": {
      "token_count": 416,
      "source_url": "https://supabase.com/docs/guides/ai",
      "page_title": "AI & Vectors | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "AI & Vectors",
        "h2": "Case studies \\#",
        "h3": ""
      },
      "text": "[OpenAI\\\\\n\\\\\nOpenAI is an AI research and deployment company. Supabase provides a simple way to use OpenAI in your applications.](/docs/guides/ai/examples/building-chatgpt-plugins)\n\n[Amazon Bedrock\\\\\n\\\\\nA fully managed service that offers a choice of high-performing foundation models from leading AI companies.](/docs/guides/ai/integrations/amazon-bedrock)\n\n[Hugging Face\\\\\n\\\\\nHugging Face is an open-source provider of NLP technologies. Supabase provides a simple way to use Hugging Face's models in your applications.](/docs/guides/ai/hugging-face)\n\n[LangChain\\\\\n\\\\\nLangChain is a language-agnostic, open-source, and self-hosted API for text translation, summarization, and sentiment analysis.](/docs/guides/ai/langchain)\n\n[LlamaIndex\\\\\n\\\\\nLlamaIndex is a data framework for your LLM applications.](/docs/guides/ai/integrations/llamaindex)\n[Berri AI Boosts Productivity by Migrating from AWS RDS to Supabase with pgvector\\\\\n\\\\\nLearn how Berri AI overcame challenges with self-hosting their vector database on AWS RDS and successfully migrated to Supabase.](https://supabase.com/customers/berriai)\n\n[Mendable switches from Pinecone to Supabase for PostgreSQL vector embeddings\\\\\n\\\\\nHow Mendable boosts efficiency and accuracy of chat powered search for documentation using Supabase with pgvector](https://supabase.com/customers/mendableai)\n\n[Markprompt: GDPR-Compliant AI Chatbots for Docs and Websites\\\\\n\\\\\nAI-powered chatbot platform, Markprompt, empowers developers to deliver efficient and GDPR-compliant prompt experiences on top of their content, by leveraging Supabase's secure and privacy-focused database and authentication solutions](https://supabase.com/customers/markprompt)\n",
      "overlap_text": {
        "previous_chunk_id": "c04a06a5-7e4c-4588-9598-840434b57db7",
        "text": " search with Next.js and OpenAI\\\\\n\\\\\nLearn how to build a ChatGPT-style doc search powered by Next.js, OpenAI, and Supabase.](/docs/guides/ai/examples/nextjs-vector-search)\n"
      }
    }
  },
  {
    "chunk_id": "e2b94533-0fd6-4999-9e66-083500d4b0f4",
    "metadata": {
      "token_count": 702,
      "source_url": "https://supabase.com/docs/guides/ai/structured-unstructured",
      "page_title": "Structured and Unstructured | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Structured and Unstructured",
        "h2": "Unstructured \\#",
        "h3": ""
      },
      "text": "AI & Vectors\nMost vector stores treat metadata associated with embeddings like NoSQL, unstructured data. Supabase is flexible enough to store unstructured and structured metadata.\n`\n_11\ncreate table docs (\n_11\nid uuid primary key,\n_11\nembedding vector(3),\n_11\ncontent text,\n_11\nurl string\n_11\n);\n_11\n_11\ninsert into docs\n_11\n(id, embedding, content, url)\n_11\nvalues\n_11\n('79409372-7556-4ccc-ab8f-5786a6cfa4f7', array[0.1, 0.2, 0.3], 'Hello world', '/hello-world');\n`\n\nNotice that we've associated two pieces of metadata, <code>content</code> and <code>url</code>, with the embedding. Those fields can be filtered, constrained, indexed, and generally operated on using the full power of SQL. Structured metadata fits naturally with a traditional Supabase application, and can be managed via database [migrations](/docs/guides/getting-started/local-development#database-migrations).\n`\n_14\ncreate table docs (\n_14\nid uuid primary key,\n_14\nembedding vector(3),\n_14\nmeta jsonb\n_14\n);\n_14\n_14\ninsert into docs\n_14\n(id, embedding, meta)\n_14\nvalues\n_14\n(\n_14\n    '79409372-7556-4ccc-ab8f-5786a6cfa4f7',\n_14\n    array[0.1, 0.2, 0.3],\n_14\n    '{\"content\": \"Hello world\", \"url\": \"/hello-world\"}'\n_14\n);\n`\n\nAn unstructured approach does not specify the metadata fields that are expected. It stores all metadata in a flexible <code>json</code>/ <code>jsonb</code> column. The tradeoff is that the querying/filtering capabilities of a schemaless data type are less flexible than when each field has a dedicated column. It also pushes the burden of metadata data integrity onto application code, which is more error prone than enforcing constraints in the database.\n\nThe unstructured approach is recommended:\n\n- for ephemeral/interactive workloads e.g. data science or scientific research\n- when metadata fields are user-defined or unknown\n- during rapid prototyping\n\nClient libraries like python's [vecs](https://github.com/supabase/vecs) use this structure. For example, running:\n\n`\n_10\n_10\nimport vecs\n_10\n_10\ndocs = vx.get_or_create_collection(name=\"docs\", dimension=1536)\n_10\n_10\ndocs.upsert(vectors=[\\\n_10\\\n('79409372-7556-4ccc-ab8f-5786a6cfa4f7', [100, 200, 300], { url: '/hello-world' })\\\n_10\\\n])\n`\n\nautomatically creates the unstructured SQL table during the call to <code>get_or_create_collection</code>.\n\nNote that when working with client libraries that emit SQL DDL, like <code>create table ...</code>, you should add that SQL to your migrations when moving to production to maintain a single source of truth for your database's schema.\n"
    }
  },
  {
    "chunk_id": "8057bd4c-2cba-41d1-97bb-84c3f18df9d5",
    "metadata": {
      "token_count": 334,
      "source_url": "https://supabase.com/docs/guides/ai/structured-unstructured",
      "page_title": "Structured and Unstructured | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Structured and Unstructured | Supabase Docs",
        "h2": "Choosing the right model \\#",
        "h3": ""
      },
      "text": "The structured metadata style is recommended when the fields being tracked are known in advance. If you have a combination of known and unknown metadata fields, you can accommodate the unknown fields by adding a <code>json</code>/ <code>jsonb</code> column to the table. In that situation, known fields should continue to use dedicated columns for best query performance and throughput.\n\n`\n_18\ncreate table docs (\n_18\nid uuid primary key,\n_18\nembedding vector(3),\n_18\ncontent text,\n_18\nurl string,\n_18\nmeta jsonb\n_18\n);\n_18\n_18\ninsert into docs\n_18\n(id, embedding, content, url, meta)\n_18\nvalues\n_18\n(\n_18\n    '79409372-7556-4ccc-ab8f-5786a6cfa4f7',\n_18\n    array[0.1, 0.2, 0.3],\n_18\n    'Hello world',\n_18\n    '/hello-world',\n_18\n    '{\"key\": \"value\"}'\n_18\n);\n`\nBoth approaches create a table where you can store your embeddings and some metadata. You should choose the best approach for your use-case. In summary:\n\n- Structured metadata is best when fields are known in advance or query patterns are predictable e.g. a production Supabase application\n- Unstructured metadata is best when fields are unknown/user-defined or when working with data interactively e.g. exploratory research\n\nBoth approaches are valid, and the one you should choose depends on your use-case.\n",
      "overlap_text": {
        "previous_chunk_id": "e2b94533-0fd6-4999-9e66-083500d4b0f4",
        "text": "code>.\n\nNote that when working with client libraries that emit SQL DDL, like <code>create table ...</code>, you should add that SQL to your migrations when moving to production to maintain a single source of truth for your database's schema.\n"
      }
    }
  },
  {
    "chunk_id": "402163ce-a078-4423-809a-d738fd3b81ce",
    "metadata": {
      "token_count": 791,
      "source_url": "https://supabase.com/docs/guides/ai/examples/openai",
      "page_title": "Generating OpenAI GPT3 completions | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Generating OpenAI GPT3 completions",
        "h2": "Deploy \\#",
        "h3": ""
      },
      "text": "AI & Vectors\nOpenAI provides a [completions API](https://platform.openai.com/docs/api-reference/completions) that allows you to use their generative GPT models in your own applications.\n\nOpenAI's API is intended to be used from the server-side. Supabase offers Edge Functions to make it easy to interact with third party APIs like OpenAI.\nIf you haven't already, [install the Supabase CLI](/docs/guides/cli) and initialize your project:\n\n`\n1\nsupabase init\n`\nScaffold a new edge function called <code>openai</code> by running:\n\n`\n1\nsupabase functions new openai\n`\n\nA new edge function will now exist under <code>./supabase/functions/openai/index.ts</code>.\n\nWe'll design the function to take your user's query (via POST request) and forward it to OpenAI's API.\n\nindex.ts\n\n`\n1\nimport OpenAI from 'https://deno.land/x/openai@v4.24.0/mod.ts'\n2\n3\nDeno.serve(async (req) => {\n4\nconst { query } = await req.json()\n5\nconst apiKey = Deno.env.get('OPENAI_API_KEY')\n6\nconst openai = new OpenAI({\n7\n    apiKey: apiKey,\n8\n})\n9\n10\n// Documentation here: https://github.com/openai/openai-node\n11\nconst chatCompletion = await openai.chat.completions.create({\n12\n    messages: [{ role: 'user', content: query }],\n13\n    // Choose model from here: https://platform.openai.com/docs/models\n14\n    model: 'gpt-3.5-turbo',\n15\n    stream: false,\n16\n})\n17\n18\nconst reply = chatCompletion.choices[0].message.content\n19\n20\nreturn new Response(reply, {\n21\n    headers: { 'Content-Type': 'text/plain' },\n22\n})\n23\n})\n`\n\nNote that we are setting <code>stream</code> to <code>false</code> which will wait until the entire response is complete before returning. If you wish to stream GPT's response word-by-word back to your client, set <code>stream</code> to <code>true</code>.\nYou may have noticed we were passing <code>OPENAI_API_KEY</code> in the Authorization header to OpenAI. To generate this key, go to [https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys) and create a new secret key.\n\nAfter getting the key, copy it into a new file called <code>.env.local</code> in your <code>./supabase</code> folder:\n\n`\n1\nOPENAI_API_KEY=your-key-here\n`\nServe the edge function locally by running:\n\n`\n1\nsupabase functions serve --env-file ./supabase/.env.local --no-verify-jwt\n`\n\nNotice how we are passing in the <code>.env.local</code> file.\n\nUse cURL or Postman to make a POST request to [http://localhost:54321/functions/v1/openai](http://localhost:54321/functions/v1/openai).\n\n`\n1\ncurl -i --location --request POST http://localhost:54321/functions/v1/openai \\\n2\n  --header 'Content-Type: application/json' \\\n3\n  --data '{\"query\":\"What is Supabase?\"}'\n`\n\nYou should see a GPT response come back from OpenAI!\nDeploy your function to the cloud by runnning:\n\n`\n1\nsupabase functions deploy --no-verify-jwt openai\n2\nsupabase secrets set --env-file ./supabase/.env.local\n`\n"
    }
  },
  {
    "chunk_id": "5a7299a8-e457-4efd-a54f-ee786a7b4369",
    "metadata": {
      "token_count": 248,
      "source_url": "https://supabase.com/docs/guides/ai/examples/openai",
      "page_title": "Generating OpenAI GPT3 completions | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Generating OpenAI GPT3 completions",
        "h2": "Go deeper \\#",
        "h3": "Is this helpful?"
      },
      "text": "If you're interesting in learning how to use this to build your own ChatGPT, read [the blog post](/blog/chatgpt-supabase-docs) and check out the video:\n\nWatch video guide\n\n![Video guide preview](https://supabase.com/docs/_next/image?url=http%3A%2F%2Fimg.youtube.com%2Fvi%2F29p8kIqyU_Y%2F0.jpg&w=3840&q=75&dpl=dpl_GiCDf4oknfdUcgmXNidH7itZWLva)\nYesNo\n\nThanks for your feedback!\n\n- [Setup Supabase project](#setup-supabase-project)\n- [Create edge function](#create-edge-function)\n- [Create OpenAI key](#create-openai-key)\n- [Run locally](#run-locally)\n- [Deploy](#deploy)\n- [Go deeper](#go-deeper)\n\n1. We only collect analytics essential to ensuring smooth operation of our services. [Learn more](https://supabase.com/privacy)\n\n   AcceptOpt out[Learn more](https://supabase.com/privacy)\n",
      "overlap_text": {
        "previous_chunk_id": "402163ce-a078-4423-809a-d738fd3b81ce",
        "text": " OpenAI!\nDeploy your function to the cloud by runnning:\n\n`\n1\nsupabase functions deploy --no-verify-jwt openai\n2\nsupabase secrets set --env-file ./supabase/.env.local\n`\n"
      }
    }
  },
  {
    "chunk_id": "5d5f1611-3962-455f-8618-a494898a6de1",
    "metadata": {
      "token_count": 629,
      "source_url": "https://supabase.com/docs/guides/ai/examples/building-chatgpt-plugins",
      "page_title": "Building ChatGPT plugins | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Building ChatGPT plugins",
        "h2": "Example: Chat with Postgres docs \\#",
        "h3": "Step 3: Create a Supabase project \\#"
      },
      "text": "AI & Vectors\nChatGPT recently released [Plugins](https://openai.com/blog/chatgpt-plugins) which help ChatGPT access up-to-date information, run computations, or use third-party services.\nIf you're building a plugin for ChatGPT, you'll probably want to answer questions from a specific source. We can solve this with \u201cretrieval plugins\u201d, which allow ChatGPT to access information from a database.\nA [Retrieval Plugin](https://github.com/openai/chatgpt-retrieval-plugin) is a Python project designed to inject external data into a ChatGPT conversation. It does a few things:\n\n1. Turn documents into smaller chunks.\n2. Converts chunks into embeddings using OpenAI's <code>text-embedding-ada-002</code> model.\n3. Stores the embeddings into a vector database.\n4. Queries the vector database for relevant documents when a question is asked.\n\nIt allows ChatGPT to dynamically pull relevant information into conversations from your data sources. This could be PDF documents, Confluence, or Notion knowledge bases.\nLet\u2019s build an example where we can \u201cask ChatGPT questions\u201d about the Postgres documentation. Although ChatGPT already knows about the Postgres documentation because it is publicly available, this is a simple example which demonstrates how to work with PDF files.\n\nThis plugin requires several steps:\n\n1. Download all the [Postgres docs as a PDF](https://www.postgresql.org/files/documentation/pdf/15/postgresql-15-US.pdf)\n2. Convert the docs into chunks of embedded text and store them in Supabase\n3. Run our plugin locally so that we can ask questions about the Postgres docs.\n\nWe'll be saving the Postgres documentation in Postgres, and ChatGPT will be retrieving the documentation whenever a user asks a question:\nFork the ChatGPT Retrieval Plugin repository to your GitHub account and clone it to your local machine. Read through the <code>README.md</code> file to understand the project structure.\nChoose your desired datastore provider and remove unused dependencies from <code>pyproject.toml</code>. For this example, we'll use Supabase. And install dependencies with Poetry:\n\n`\n_10\npoetry install\n`\nCreate a [Supabase project](https://supabase.com/dashboard) and database by following the instructions [here](https://supabase.com/docs/guides/platform). Export the environment variables required for the retrieval plugin to work:\n\n`\n_10\nexport OPENAI_API_KEY=<open_ai_api_key>\n_10\nexport DATASTORE=supabase\n_10\nexport SUPABASE_URL=<supabase_url>\n_10\nexport SUPABASE_SERVICE_ROLE_KEY=<supabase_key>\n`\n\nFor Postgres datastore, you'll need to export these environment variables instead:\n\n`\n_10\nexport OPENAI_API_KEY=<open_ai_api_key>\n_10\nexport DATASTORE=postgres\n_10\nexport PG_HOST=<postgres_host_url>\n_10\nexport PG_PASSWORD=<postgres_password>\n`\n"
    }
  },
  {
    "chunk_id": "cb8c67fd-049d-467f-8189-939a41675c67",
    "metadata": {
      "token_count": 784,
      "source_url": "https://supabase.com/docs/guides/ai/examples/building-chatgpt-plugins",
      "page_title": "Building ChatGPT plugins | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "output",
        "h2": "Example: Chat with Postgres docs \\#",
        "h3": "Step 6: Populating data in the datastore \\#"
      },
      "text": "To start quicker you may use Supabase CLI to spin everything up locally as it already includes pgvector from the start. Install <code>supabase-cli</code>, go to the <code>examples/providers</code> folder in the repo and run:\n\n`\n_10\nsupabase start\n`\n\nThis will pull all docker images and run supabase stack in docker on your local machine. It will also apply all the necessary migrations to set the whole thing up. You can then use your local setup the same way, just export the environment variables and follow to the next steps.\n\nUsing <code>supabase-cli</code> is not required and you can use any other docker image or hosted version of PostgresDB that includes <code>pgvector</code>. Just make sure you run migrations from <code>examples/providers/supabase/migrations/20230414142107_init_pg_vector.sql</code>.\nTo create embeddings Plugin uses OpenAI API and <code>text-embedding-ada-002</code> model. Each time we add some data to our datastore, or try to query relevant information from it, embedding will be created either for inserted data chunk, or for the query itself. To make it work we need to export <code>OPENAI_API_KEY</code>. If you already have an account in OpenAI, you just need to go to [User Settings - API keys](https://platform.openai.com/account/api-keys) and Create new secret key.\n\n![OpenAI Secret Keys](https://supabase.com/docs/img/ai/chatgpt-plugins/openai-secret-keys.png)\nExecute the following command to run the plugin:\n\n`\n_10\npoetry run dev\n_10\n_10\nINFO:     Will watch for changes in these directories: ['./chatgpt-retrieval-plugin']\n_10\nINFO:     Uvicorn running on http://localhost:3333 (Press CTRL+C to quit)\n_10\nINFO:     Started reloader process [87843] using WatchFiles\n_10\nINFO:     Started server process [87849]\n_10\nINFO:     Waiting for application startup.\n_10\nINFO:     Application startup complete.\n`\n\nThe plugin will start on your localhost - port <code>:3333</code> by default.\nFor this example, we'll upload Postgres documentation to the datastore. Download the [Postgres documentation](https://www.postgresql.org/files/documentation/pdf/15/postgresql-15-US.pdf) and use the <code>/upsert-file</code> endpoint to upload it:\n\n`\n_10\ncurl -X POST -F \\\\\"file=@./postgresql-15-US.pdf\\\\\" <http://localhost:3333/upsert-file>\n`\n\nThe plugin will split your data and documents into smaller chunks automatically. You can view the chunks using the Supabase dashboard or any other SQL client you prefer. For the whole Postgres Documentation I got 7,904 records in my documents table, which is not a lot, but we can try to add index for <code>embedding</code> column to speed things up by a little. To do so, you should run the following SQL command:\n\n`\n_10\ncreate index on documents\n_10\nusing hnsw (embedding vector_ip_ops)\n_10\nwith (lists = 10);\n`\n\nThis will create an index for the inner product distance function. Important to note that it is an approximate index. It will change the logic from performing the exact nearest neighbor search to the approximate nearest neighbor search.\n\nWe are using <code>lists = 10</code>, because as a general guideline, you should start looking for optimal lists constant value with the formula: <code>rows / 1000</code> when you have less than 1 million records in your table.\n",
      "overlap_text": {
        "previous_chunk_id": "5d5f1611-3962-455f-8618-a494898a6de1",
        "text": " environment variables instead:\n\n`\n_10\nexport OPENAI_API_KEY=<open_ai_api_key>\n_10\nexport DATASTORE=postgres\n_10\nexport PG_HOST=<postgres_host_url>\n_10\nexport PG_PASSWORD=<postgres_password>\n`\n"
      }
    }
  },
  {
    "chunk_id": "f66c96bd-e105-4137-b7b5-f3b2524cbf36",
    "metadata": {
      "token_count": 337,
      "source_url": "https://supabase.com/docs/guides/ai/examples/building-chatgpt-plugins",
      "page_title": "Building ChatGPT plugins | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "output",
        "h2": "Resources \\#",
        "h3": "Step 7: Using our plugin within ChatGPT \\#"
      },
      "text": "To integrate our plugin with ChatGPT, register it in the ChatGPT dashboard. Assuming you have access to ChatGPT Plugins and plugin development, select the Plugins model in a new chat, then choose \"Plugin store\" and \"Develop your own plugin.\" Enter <code>localhost:3333</code> into the domain input, and your plugin is now part of ChatGPT.\n\n![ChatGPT Plugin Store](https://supabase.com/docs/img/ai/chatgpt-plugins/chatgpt-plugin-store.png)\n\n![ChatGPT Local Plugin](https://supabase.com/docs/img/ai/chatgpt-plugins/chatgpt-local-plugin.png)\n\nYou can now ask questions about Postgres and receive answers derived from the documentation.\n\nLet's try it out: ask ChatGPT to find out when to use <code>check</code> and when to use <code>using</code>. You will be able to see what queries were sent to our plugin and what it responded to.\n\n![Ask ChatGPT](https://supabase.com/docs/img/ai/chatgpt-plugins/ask-chatgpt.png)\n\nAnd after ChatGPT receives a response from the plugin it will answer your question with the data from the documentation.\n\n![ChatGPT Reply](https://supabase.com/docs/img/ai/chatgpt-plugins/chatgpt-reply.png)\n- ChatGPT Retrieval Plugin: [github.com/openai/chatgpt-retrieval-plugin](https://github.com/openai/chatgpt-retrieval-plugin)\n- ChatGPT Plugins: [official documentation](https://platform.openai.com/docs/plugins/introduction)\n",
      "overlap_text": {
        "previous_chunk_id": "cb8c67fd-049d-467f-8189-939a41675c67",
        "text": "lists = 10</code>, because as a general guideline, you should start looking for optimal lists constant value with the formula: <code>rows / 1000</code> when you have less than 1 million records in your table.\n"
      }
    }
  },
  {
    "chunk_id": "d7c6f268-c69e-4133-ba5e-70f5adcb43ac",
    "metadata": {
      "token_count": 854,
      "source_url": "https://supabase.com/docs/guides/ai/integrations/amazon-bedrock",
      "page_title": "Amazon Bedrock | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "create vector store client",
        "h2": "Create Embeddings \\#",
        "h3": "Querying for Most Similar Sentences \\#"
      },
      "text": "AI & Vectors\n[Amazon Bedrock](https://aws.amazon.com/bedrock) is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon. Each model is accessible through a common API which implements a broad set of features to help build generative AI applications with security, privacy, and responsible AI in mind.\n\nThis guide will walk you through an example using Amazon Bedrock SDK with <code>vecs</code>. We will create embeddings using the Amazon Titan Embeddings G1 \u2013 Text v1.2 (amazon.titan-embed-text-v1) model, insert these embeddings into a PostgreSQL database using vecs, and then query the collection to find the most similar sentences to a given query sentence.\nFirst, you need to set up your environment. You will need Python 3.7+ with the <code>vecs</code> and <code>boto3</code> libraries installed.\n\nYou can install the necessary Python libraries using pip:\n\n`\n_10\npip install vecs boto3\n`\n\nYou'll also need:\n\n- [Credentials to your AWS account](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html)\n- [A Postgres Database with the pgvector extension](hosting.md)\nNext, we will use Amazon\u2019s Titan Embedding G1 - Text v1.2 model to create embeddings for a set of sentences.\n\n`\n_34\nimport boto3\n_34\nimport vecs\n_34\nimport json\n_34\n_34\nclient = boto3.client(\n_34\n    'bedrock-runtime',\n_34\n    region_name='us-east-1',\n_34\n_34\n    aws_access_key_id='<replace_your_own_credentials>',\n_34\n    aws_secret_access_key='<replace_your_own_credentials>',\n_34\n    aws_session_token='<replace_your_own_credentials>',\n_34\n)\n_34\n_34\ndataset = [\\\n_34\\\n    \"The cat sat on the mat.\",\\\n_34\\\n    \"The quick brown fox jumps over the lazy dog.\",\\\n_34\\\n    \"Friends, Romans, countrymen, lend me your ears\",\\\n_34\\\n    \"To be or not to be, that is the question.\",\\\n_34\\\n]\n_34\n_34\nembeddings = []\n_34\n_34\nfor sentence in dataset:\n_34\n_34\n    response = client.invoke_model(\n_34\n        body= json.dumps({\"inputText\": sentence}),\n_34\n        modelId= \"amazon.titan-embed-text-v1\",\n_34\n        accept = \"application/json\",\n_34\n        contentType = \"application/json\"\n_34\n    )\n_34\n_34\n    response_body = json.loads(response[\"body\"].read())\n_34\n_34\n    embeddings.append((sentence, response_body.get(\"embedding\"), {}))\n`\nNow that we have our embeddings, we can insert them into a PostgreSQL database using vecs.\n\n`\n_16\nimport vecs\n_16\n_16\nDB_CONNECTION = \"postgresql://<user>:<password>@<host>:<port>/<db_name>\"\n_16\n_16\n_16\nvx = vecs.Client(DB_CONNECTION)\n_16\n_16\n_16\n_16\nsentences = vx.get_or_create_collection(name=\"sentences\", dimension=1536)\n_16\n_16\n_16\nsentences.upsert(records=embeddings)\n_16\n_16\n_16\nsentences.create_index()\n`\nNow, we query the <code>sentences</code> collection to find the most similar sentences to a sample query sentence. First need to create an embedding for the query sentence. Next, we query the collection we created earlier to find the most similar sentences.\n\n`\n_27\nquery_sentence = \"A quick animal jumps over a lazy one.\"\n_27\n_27\n_27\nvx = vecs.Client(DB_CONNECTION)\n_27\n_27\n"
    }
  },
  {
    "chunk_id": "710c1bbd-0774-478e-b5e3-88ba6984999f",
    "metadata": {
      "token_count": 306,
      "source_url": "https://supabase.com/docs/guides/ai/integrations/amazon-bedrock",
      "page_title": "Amazon Bedrock | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "print the results",
        "h2": "Resources \\#",
        "h3": ""
      },
      "text": "_27\nresponse = client.invoke_model(\n_27\n        body= json.dumps({\"inputText\": query_sentence}),\n_27\n        modelId= \"amazon.titan-embed-text-v1\",\n_27\n        accept = \"application/json\",\n_27\n        contentType = \"application/json\"\n_27\n    )\n_27\n_27\nresponse_body = json.loads(response[\"body\"].read())\n_27\n_27\nquery_embedding = response_body.get(\"embedding\")\n_27\n_27\n_27\nresults = sentences.query(\n_27\n    data=query_embedding,\n_27\n    limit=3,\n_27\n    include_value = True\n_27\n)\n_27\n_27\n_27\nfor result in results:\n_27\n    print(result)\n`\n\nThis returns the most similar 3 records and their distance to the query vector.\n\n`\n_10\n('The quick brown fox jumps over the lazy dog.', 0.27600620558852)\n_10\n('The cat sat on the mat.', 0.609986272479202)\n_10\n('To be or not to be, that is the question.', 0.744849503688346)\n`\n- [Amazon Bedrock](https://aws.amazon.com/bedrock)\n- [Amazon Titan](https://aws.amazon.com/bedrock/titan)\n- [Semantic Image Search with Amazon Titan](/docs/guides/ai/examples/semantic-image-search-amazon-titan)\n",
      "overlap_text": {
        "previous_chunk_id": "d7c6f268-c69e-4133-ba5e-70f5adcb43ac",
        "text": " we created earlier to find the most similar sentences.\n\n`\n_27\nquery_sentence = \"A quick animal jumps over a lazy one.\"\n_27\n_27\n_27\nvx = vecs.Client(DB_CONNECTION)\n_27\n_27\n"
      }
    }
  },
  {
    "chunk_id": "403473d9-e950-4e39-863e-d8b26c4a1446",
    "metadata": {
      "token_count": 682,
      "source_url": "https://supabase.com/docs/guides/ai/quickstarts/generate-text-embeddings",
      "page_title": "Generate Embeddings | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Generate Embeddings",
        "h2": "Build the Edge Function \\#",
        "h3": "Implement request handler"
      },
      "text": "AI & Vectors\nThis guide will walk you through how to generate high quality text embeddings in [Edge Functions](/docs/guides/functions) using its built-in AI inference API, so no external API is required.\nLet's build an Edge Function that will accept an input string and generate an embedding for it. Edge Functions are server-side TypeScript HTTP endpoints that run on-demand closest to your users.\n\n1\nMake sure you have the latest version of the [Supabase CLI installed](/docs/guides/cli/getting-started).\n\nInitialize Supabase in the root directory of your app and start your local stack.\n\n`\n_10\nsupabase init\n_10\nsupabase start\n`\n\n2\nCreate an Edge Function that we will use to generate embeddings. We'll call this <code>embed</code> (you can name this anything you like).\n\nThis will create a new TypeScript file called <code>index.ts</code> under <code>./supabase/functions/embed</code>.\n\n`\n_10\nsupabase functions new embed\n`\n\n3\nLet's create a new inference session to be used in the lifetime of this function. Multiple requests can use the same inference session.\n\nCurrently, only the <code>gte-small</code> ( [https://huggingface.co/Supabase/gte-small](https://huggingface.co/Supabase/gte-small)) text embedding model is supported in Supabase's Edge Runtime.\n\n./supabase/functions/embed/index.ts\n\n`\n_10\nconst session = new Supabase.ai.Session('gte-small');\n`\n\n4\nModify our request handler to accept an <code>input</code> string from the POST request JSON body.\n\nThen generate the embedding by calling <code>session.run(input)</code>.\n\n./supabase/functions/embed/index.ts\n\n`\n_16\nDeno.serve(async (req) => {\n_16\n// Extract input string from JSON body\n_16\nconst { input } = await req.json();\n_16\n_16\n// Generate the embedding from the user input\n_16\nconst embedding = await session.run(input, {\n_16\n    mean_pool: true,\n_16\n    normalize: true,\n_16\n});\n_16\n_16\n// Return the embedding\n_16\nreturn new Response(\n_16\n    JSON.stringify({ embedding }),\n_16\n    { headers: { 'Content-Type': 'application/json' } }\n_16\n);\n_16\n});\n`\n\nNote the two options we pass to <code>session.run()</code>:\n\n- <code>mean_pool</code>: The first option sets <code>pooling</code> to <code>mean</code>. Pooling referes to how token-level embedding representations are compressed into a single sentence embedding that reflects the meaning of the entire sentence. Average pooling is the most common type of pooling for sentence embeddings.\n- <code>normalize</code>: The second option tells to normalize the embedding vector so that it can be used with distance measures like dot product. A normalized vector means its length (magnitude) is 1 - also referred to as a unit vector. A vector is normalized by dividing each element by the vector's length (magnitude), which maintains its direction but changes its length to 1.\n\n5\n"
    }
  },
  {
    "chunk_id": "cd46608a-d0ce-48b8-a586-9094f845e7c2",
    "metadata": {
      "token_count": 190,
      "source_url": "https://supabase.com/docs/guides/ai/quickstarts/generate-text-embeddings",
      "page_title": "Generate Embeddings | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Generate Embeddings",
        "h2": "Next steps \\#",
        "h3": "Test it!"
      },
      "text": "To test the Edge Function, first start a local functions server.\n\n`\n_10\nsupabase functions serve\n`\n\nThen in a new shell, create an HTTP request using cURL and pass in your input in the JSON body.\n\n`\n_10\ncurl --request POST 'http://localhost:54321/functions/v1/embed' \\\n_10\n  --header 'Authorization: Bearer ANON_KEY' \\\n_10\n  --header 'Content-Type: application/json' \\\n_10\n  --data '{ \"input\": \"hello world\" }'\n`\n\nBe sure to replace <code>ANON_KEY</code> with your project's anonymous key. You can get this key by running <code>supabase status</code>.\n- Learn more about [embedding concepts](/docs/guides/ai/concepts)\n- [Store your embeddings](/docs/guides/ai/vector-columns) in a database\n",
      "overlap_text": {
        "previous_chunk_id": "403473d9-e950-4e39-863e-d8b26c4a1446",
        "text": " normalized vector means its length (magnitude) is 1 - also referred to as a unit vector. A vector is normalized by dividing each element by the vector's length (magnitude), which maintains its direction but changes its length to 1.\n\n5\n"
      }
    }
  },
  {
    "chunk_id": "e00cac98-fa96-4825-8492-d05e8b1f41a1",
    "metadata": {
      "token_count": 681,
      "source_url": "https://supabase.com/docs/guides/ai/semantic-search",
      "page_title": "Semantic search | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Semantic search",
        "h2": "Embedding models \\#",
        "h3": ""
      },
      "text": "AI & Vectors\nSemantic search interprets the meaning behind user queries rather than exact [keywords](/docs/guides/ai/keyword-search). It uses machine learning to capture the intent and context behind the query, handling language nuances like synonyms, phrasing variations, and word relationships.\nSemantic search is useful in applications where the depth of understanding and context is important for delivering relevant results. A good example is in customer support or knowledge base search engines. Users often phrase their problems or questions in various ways, and a traditional keyword-based search might not always retrieve the most helpful documents. With semantic search, the system can understand the meaning behind the queries and match them with relevant solutions or articles, even if the exact wording differs.\n\nFor instance, a user searching for \"increase text size on display\" might miss articles titled \"How to adjust font size in settings\" in a keyword-based search system. However, a semantic search engine would understand the intent behind the query and correctly match it to relevant articles, regardless of the specific terminology used.\n\nIt's also possible to combine semantic search with keyword search to get the best of both worlds. See [Hybrid search](/docs/guides/ai/hybrid-search) for more details.\nSemantic search uses an intermediate representation called an \u201cembedding vector\u201d to link database records with search queries. A vector, in the context of semantic search, is a list of numerical values. They represent various features of the text and allow for the semantic comparison between different pieces of text.\n\nThe best way to think of embeddings is by plotting them on a graph, where each embedding is a single point whose coordinates are the numerical values within its vector. Importantly, embeddings are plotted such that similar concepts are positioned close together while dissimilar concepts are far apart. For more details, see [What are embeddings?](/docs/guides/ai/concepts#what-are-embeddings)\n\nEmbeddings are generated using a language model, and embeddings are compared to each other using a similarity metric. The language model is trained to understand the semantics of language, including syntax, context, and the relationships between words. It generates embeddings for both the content in the database and the search queries. Then the similarity metric, often a function like cosine similarity or dot product, is used to compare the query embeddings with the document embeddings (in other words, to measure how close they are to each other on the graph). The documents with embeddings most similar to the query's are deemed the most relevant and are returned as search results.\nThere are many embedding models available today. Supabase Edge Functions has [built in support](/docs/guides/functions/examples/semantic-search) for the <code>gte-small</code> model. Others can be accessed through third-party APIs like [OpenAI](https://platform.openai.com/docs/guides/embeddings), where you send your text in the request and receive an embedding vector in the response. Others can run locally on your own compute, such as through Transformers.js for JavaScript implementations. For more information on local implementation, see [Generate embeddings](/docs/guides/ai/quickstarts/generate-text-embeddings).\n\nIt's crucial to remember that when using embedding models with semantic search, you must use the same model for all embedding comparisons. Comparing embeddings created by different models will yield meaningless results.\n"
    }
  },
  {
    "chunk_id": "116f2271-8e99-4668-bca8-fb770bb40edf",
    "metadata": {
      "token_count": 276,
      "source_url": "https://supabase.com/docs/guides/ai/semantic-search",
      "page_title": "Semantic search | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Semantic search",
        "h2": "Semantic search in Postgres \\#",
        "h3": ""
      },
      "text": "To implement semantic search in Postgres we use <code>pgvector</code> \\- an extension that allows for efficient storage and retrieval of high-dimensional vectors. These vectors are numerical representations of text (or other types of data) generated by embedding models.\n\n1. Enable the <code>pgvector</code> extension by running:\n\n`\n_10\ncreate extension vector\n_10\nwith\n_10\nschema extensions;\n`\n\n2. Create a table to store the embeddings:\n\n`\n_10\ncreate table documents (\n_10\nid bigint primary key generated always as identity,\n_10\ncontent text,\n_10\nembedding vector(512)\n_10\n);\n`\n\nOr if you have an existing table, you can add a vector column like so:\n\n`\n_10\nalter table documents\n_10\nadd column embedding vector(512);\n`\n\nIn this example, we create a column named <code>embedding</code> which uses the newly enabled <code>vector</code> data type. The size of the vector (as indicated in parentheses) represents the number of dimensions in the embedding. Here we use 512, but adjust this to match the number of dimensions produced by your embedding model.\n\nFor more details on vector columns, including how to generate embeddings and store them, see [Vector columns](/docs/guides/ai/vector-columns).\n",
      "overlap_text": {
        "previous_chunk_id": "e00cac98-fa96-4825-8492-d05e8b1f41a1",
        "text": "es/ai/quickstarts/generate-text-embeddings).\n\nIt's crucial to remember that when using embedding models with semantic search, you must use the same model for all embedding comparisons. Comparing embeddings created by different models will yield meaningless results.\n"
      }
    }
  },
  {
    "chunk_id": "8db42d60-7303-43e2-8928-993177d493fc",
    "metadata": {
      "token_count": 748,
      "source_url": "https://supabase.com/docs/guides/ai/semantic-search",
      "page_title": "Semantic search | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Semantic search",
        "h2": "Semantic search in Postgres \\#",
        "h3": "Similarity metric \\#"
      },
      "text": "<code>pgvector</code> support 3 operators for computing distance between embeddings:\n\n| **Operator** | **Description** |\n| --- | --- |\n| <code><-></code> | Euclidean distance |\n| <code><#></code> | negative inner product |\n| <code><=></code> | cosine distance |\n\nThese operators are used directly in your SQL query to retrieve records that are most similar to the user's search query. Choosing the right operator depends on your needs. Inner product (also known as dot product) tends to be the fastest if your vectors are normalized.\n\nThe easiest way to perform semantic search in Postgres in by creating a function:\n\n`\n_15\n-- Match documents using cosine distance (<=>)\n_15\ncreate or replace function match_documents (\n_15\nquery_embedding vector(512),\n_15\nmatch_threshold float,\n_15\nmatch_count int\n_15\n)\n_15\nreturns setof documents\n_15\nlanguage sql\n_15\nas $$\n_15\nselect *\n_15\nfrom documents\n_15\nwhere documents.embedding <=> query_embedding < 1 - match_threshold\n_15\norder by documents.embedding <=> query_embedding asc\n_15\nlimit least(match_count, 200);\n_15\n$$;\n`\n\nHere we create a function <code>match_documents</code> that accepts three parameters:\n\n1. <code>query_embedding</code>: a one-time embedding generated for the user's search query. Here we set the size to 512, but adjust this to match the number of dimensions produced by your embedding model.\n2. <code>match_threshold</code>: the minimum similarity between embeddings. This is a value between 1 and -1, where 1 is most similar and -1 is most dissimilar.\n3. <code>match_count</code>: the maximum number of results to return. Note the query may return less than this number if <code>match_threshold</code> resulted in a small shortlist. Limited to 200 records to avoid unintentionally overloading your database.\n\nIn this example, we return a <code>setof documents</code> and refer to <code>documents</code> throughout the query. Adjust this to use the relevant tables in your application.\n\nYou'll notice we are using the cosine distance ( <code><=></code>) operator in our query. Cosine distance is a safe default when you don't know whether or not your embeddings are normalized. If you know for a fact that they are normalized (for example, your embedding is returned from OpenAI), you can use negative inner product ( <code><#></code>) for better performance:\n\n`\n_15\n-- Match documents using negative inner product (<#>)\n_15\ncreate or replace function match_documents (\n_15\nquery_embedding vector(512),\n_15\nmatch_threshold float,\n_15\nmatch_count int\n_15\n)\n_15\nreturns setof documents\n_15\nlanguage sql\n_15\nas $$\n_15\nselect *\n_15\nfrom documents\n_15\nwhere documents.embedding <#> query_embedding < -match_threshold\n_15\norder by documents.embedding <#> query_embedding asc\n_15\nlimit least(match_count, 200);\n_15\n$$;\n`\n\nNote that since <code><#></code> is negative, we negate <code>match_threshold</code> accordingly in the <code>where</code> clause. For more information on the different operators, see the [pgvector docs](https://github.com/pgvector/pgvector?tab=readme-ov-file#vector-operators).\n",
      "overlap_text": {
        "previous_chunk_id": "116f2271-8e99-4668-bca8-fb770bb40edf",
        "text": " use 512, but adjust this to match the number of dimensions produced by your embedding model.\n\nFor more details on vector columns, including how to generate embeddings and store them, see [Vector columns](/docs/guides/ai/vector-columns).\n"
      }
    }
  },
  {
    "chunk_id": "f5d54613-f39d-45d0-a55d-645cba5b2d0e",
    "metadata": {
      "token_count": 381,
      "source_url": "https://supabase.com/docs/guides/ai/semantic-search",
      "page_title": "Semantic search | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Semantic search",
        "h2": "See also \\#",
        "h3": "Calling from your application \\#"
      },
      "text": "Finally you can execute this function from your application. If you are using a Supabase client library such as [<code>supabase-js</code>](https://github.com/supabase/supabase-js), you can invoke it using the <code>rpc()</code> method:\n\n`\n_10\nconst { data: documents } = await supabase.rpc('match_documents', {\n_10\nquery_embedding: embedding, // pass the query embedding\n_10\nmatch_threshold: 0.78, // choose an appropriate threshold for your data\n_10\nmatch_count: 10, // choose the number of matches\n_10\n})\n`\n\nYou can also call this method directly from SQL:\n\n`\n_10\nselect *\n_10\nfrom match_documents(\n_10\n'[...]'::vector(512), -- pass the query embedding\n_10\n0.78, -- chose an appropriate threshold for your data\n_10\n10 -- choose the number of matches\n_10\n);\n`\n\nIn this scenario, you'll likely use a Postgres client library to establish a direct connection from your application to the database. It's best practice to parameterize your arguments before executing the query.\nAs your database scales, you will need an index on your vector columns to maintain fast query speeds. See [Vector indexes](/docs/guides/ai/vector-indexes) for an in-depth guide on the different types of indexes and how they work.\n- [Embedding concepts](/docs/guides/ai/concepts)\n- [Vector columns](/docs/guides/ai/vector-columns)\n- [Vector indexes](/docs/guides/ai/vector-indexes)\n- [Hybrid search](/docs/guides/ai/hybrid-search)\n- [Keyword search](/docs/guides/ai/keyword-search)\n",
      "overlap_text": {
        "previous_chunk_id": "8db42d60-7303-43e2-8928-993177d493fc",
        "text": "</code> accordingly in the <code>where</code> clause. For more information on the different operators, see the [pgvector docs](https://github.com/pgvector/pgvector?tab=readme-ov-file#vector-operators).\n"
      }
    }
  },
  {
    "chunk_id": "9fec6393-63c0-4711-a5f7-e156265b075d",
    "metadata": {
      "token_count": 672,
      "source_url": "https://supabase.com/docs/guides/ai/hybrid-search",
      "page_title": "Hybrid search | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Hybrid search",
        "h2": "How to combine search methods \\#",
        "h3": ""
      },
      "text": "AI & Vectors\nHybrid search combines [full text search](/docs/guides/ai/keyword-search) (searching by keyword) with [semantic search](/docs/guides/ai/semantic-search) (searching by meaning) to identify results that are both directly and contextually relevant to the user's query.\nSometimes a single search method doesn't quite capture what a user is really looking for. For example, if a user searches for \"Italian recipes with tomato sauce\" on a cooking app, a keyword search would pull up recipes that specifically mention \"Italian,\" \"recipes,\" and \"tomato sauce\" in the text. However, it might miss out on dishes that are quintessentially Italian and use tomato sauce but don't explicitly label themselves with these words, or use variations like \"pasta sauce\" or \"marinara.\" On the other hand, a semantic search might understand the culinary context and find recipes that match the intent, such as a traditional \"Spaghetti Marinara,\" even if they don't match the exact keyword phrase. However, it could also suggest recipes that are contextually related but not what the user is looking for, like a \"Mexican salsa\" recipe, because it understands the context to be broadly about tomato-based sauces.\n\nHybrid search combines the strengths of both these methods. It would ensure that recipes explicitly mentioning the keywords are prioritized, thus capturing direct hits that satisfy the keyword criteria. At the same time, it would include recipes identified through semantic understanding as being related in meaning or context, like different Italian dishes that traditionally use tomato sauce but might not have been tagged explicitly with the user's search terms. It identifies results that are both directly and contextually relevant to the user's query while ideally minimizing misses and irrelevant suggestions.\nThe decision to use hybrid search depends on what your users are looking for in your app. For a code repository where developers need to find exact lines of code or error messages, keyword search is likely ideal because it matches specific terms. In a mental health forum where users search for advice or experiences related to their feelings, semantic search may be better because it finds results based on the meaning of a query, not just specific words. For a shopping app where customers might search for specific product names yet also be open to related suggestions, hybrid search combines the best of both worlds - finding exact matches while also uncovering similar products based on the shopping context.\nHybrid search merges keyword search and semantic search, but how does this process work?\n\nFirst, each search method is executed separately. Keyword search, which involves searching by specific words or phrases present in the content, will yield its own set of results. Similarly, semantic search, which involves understanding the context or meaning behind the search query rather than the specific words used, will generate its own unique results.\n\nNow with these separate result lists available, the next step is to combine them into a single, unified list. This is achieved through a process known as \u201cfusion\u201d. Fusion takes the results from both search methods and merges them together based on a certain ranking or scoring system. This system may prioritize certain results based on factors like their relevance to the search query, their ranking in the individual lists, or other criteria. The result is a final list that integrates the strengths of both keyword and semantic search methods.\n"
    }
  },
  {
    "chunk_id": "17fdd117-ab34-4299-b849-a8f7f62bc58b",
    "metadata": {
      "token_count": 444,
      "source_url": "https://supabase.com/docs/guides/ai/hybrid-search",
      "page_title": "Hybrid search | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Hybrid search",
        "h2": "Reciprocal Ranked Fusion (RRF) \\#",
        "h3": "Smoothing constant <code>k</code> \\#"
      },
      "text": "One of the most common fusion methods is Reciprocal Ranked Fusion (RRF). The key idea behind RRF is to give more weight to the top-ranked items in each individual result list when building the final combined list.\n\nIn RRF, we iterate over each record and assign a score (noting that each record could exist in one or both lists). The score is calculated as 1 divided by that record's rank in each list, summed together between both lists. For example, if a record with an ID of <code>123</code> was ranked third in the keyword search and ninth in semantic search, it would receive a score of 13+19=0.444\\\\dfrac{1}{3} + \\\\dfrac{1}{9} = 0.44431\u200b+91\u200b=0.444. If the record was found in only one list and not the other, it would receive a score of 0 for the other list. The records are then sorted by this score to create the final list. The items with the highest scores are ranked first, and lowest scores ranked last.\n\nThis method ensures that items that are ranked high in multiple lists are given a high rank in the final list. It also ensures that items that are ranked high in only a few lists but low in others are not given a high rank in the final list. Placing the rank in the denominator when calculating score helps penalize the low ranking records.\nTo prevent extremely high scores for items that are ranked first (since we're dividing by the rank), a <code>k</code> constant is often added to the denominator to smooth the score:\n\n1k+rank\\\\dfrac{1}{k+rank}k+rank1\u200b\n\nThis constant can be any positive number, but is typically small. A constant of 1 would mean that a record ranked first would have a score of 11+1=0.5\\\\dfrac{1}{1+1} = 0.51+11\u200b=0.5 instead of 111. This adjustment can help balance the influence of items that are ranked very high in individual lists when creating the final combined list.\n",
      "overlap_text": {
        "previous_chunk_id": "9fec6393-63c0-4711-a5f7-e156265b075d",
        "text": " or scoring system. This system may prioritize certain results based on factors like their relevance to the search query, their ranking in the individual lists, or other criteria. The result is a final list that integrates the strengths of both keyword and semantic search methods.\n"
      }
    }
  },
  {
    "chunk_id": "3a7c37c2-d175-462c-962b-c38782178977",
    "metadata": {
      "token_count": 779,
      "source_url": "https://supabase.com/docs/guides/ai/hybrid-search",
      "page_title": "Hybrid search | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Hybrid search",
        "h2": "Hybrid search in Postgres \\#",
        "h3": ""
      },
      "text": "Let's implement hybrid search in Postgres using <code>tsvector</code> (keyword search) and <code>pgvector</code> (semantic search).\n\nFirst we'll create a <code>documents</code> table to store the documents that we will search over. This is just an example - adjust this to match the structure of your application.\n\n`\n_10\ncreate table documents (\n_10\nid bigint primary key generated always as identity,\n_10\ncontent text,\n_10\nfts tsvector generated always as (to_tsvector('english', content)) stored,\n_10\nembedding vector(512)\n_10\n);\n`\n\nThe table contains 4 columns:\n\n- <code>id</code> is an auto-generated unique ID for the record. We'll use this later to match records when performing RRF.\n- <code>content</code> contains the actual text we will be searching over.\n- <code>fts</code> is an auto-generated <code>tsvector</code> column that is generated using the text in <code>content</code>. We will use this for [full text search](/docs/guides/database/full-text-search) (search by keyword).\n- <code>embedding</code> is a [vector column](/docs/guides/ai/vector-columns) that stores the vector generated from our embedding model. We will use this for [semantic search](/docs/guides/ai/semantic-search) (search by meaning). We chose 512 dimensions for this example, but adjust this to match the size of the embedding vectors generated from your preferred model.\n\nNext we'll create indexes on the <code>fts</code> and <code>embedding</code> columns so that their individual queries will remain fast at scale:\n\n`\n_10\n-- Create an index for the full-text search\n_10\ncreate index on documents using gin(fts);\n_10\n_10\n-- Create an index for the semantic vector search\n_10\ncreate index on documents using hnsw (embedding vector_ip_ops);\n`\n\nFor full text search we use a [generalized inverted (GIN) index](https://www.postgresql.org/docs/current/gin-intro.html) which is designed for handling composite values like those stored in a <code>tsvector</code>.\n\nFor semantic vector search we use an [HNSW index](/docs/guides/ai/vector-indexes/hnsw-indexes), which is a high performing approximate nearest neighbor (ANN) search algorithm. Note that we are using the <code>vector_ip_ops</code> (inner product) operator with this index because we plan on using the inner product ( <code><#></code>) operator later in our query. If you plan to use a different operator like cosine distance ( <code><=></code>), be sure to update the index accordingly. For more information, see [distance operators](/docs/guides/ai/vector-indexes#distance-operators).\n\nFinally we'll create our <code>hybrid_search</code> function:\n\n`\n_48\ncreate or replace function hybrid_search(\n_48\nquery_text text,\n_48\nquery_embedding vector(512),\n_48\nmatch_count int,\n_48\nfull_text_weight float = 1,\n_48\nsemantic_weight float = 1,\n_48\nrrf_k int = 50\n_48\n)\n_48\nreturns setof documents\n_48\nlanguage sql\n_48\nas $$\n_48\nwith full_text as (\n_48\nselect\n_48\n    id,\n_48\n    -- Note: ts_rank_cd is not indexable but will only rank matches of the where clause\n_48\n    -- which shouldn't be too big\n_48\n",
      "overlap_text": {
        "previous_chunk_id": "17fdd117-ab34-4299-b849-a8f7f62bc58b",
        "text": "dfrac{1}{1+1} = 0.51+11\u200b=0.5 instead of 111. This adjustment can help balance the influence of items that are ranked very high in individual lists when creating the final combined list.\n"
      }
    }
  },
  {
    "chunk_id": "cd2135de-b353-429d-a00f-c50148e037cb",
    "metadata": {
      "token_count": 760,
      "source_url": "https://supabase.com/docs/guides/ai/hybrid-search",
      "page_title": "Hybrid search | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Hybrid search",
        "h2": "Hybrid search in Postgres \\#",
        "h3": ""
      },
      "text": "    row_number() over(order by ts_rank_cd(fts, websearch_to_tsquery(query_text)) desc) as rank_ix\n_48\nfrom\n_48\n    documents\n_48\nwhere\n_48\n    fts @@ websearch_to_tsquery(query_text)\n_48\norder by rank_ix\n_48\nlimit least(match_count, 30) * 2\n_48\n),\n_48\nsemantic as (\n_48\nselect\n_48\n    id,\n_48\n    row_number() over (order by embedding <#> query_embedding) as rank_ix\n_48\nfrom\n_48\n    documents\n_48\norder by rank_ix\n_48\nlimit least(match_count, 30) * 2\n_48\n)\n_48\nselect\n_48\ndocuments.*\n_48\nfrom\n_48\nfull_text\n_48\nfull outer join semantic\n_48\n    on full_text.id = semantic.id\n_48\njoin documents\n_48\n    on coalesce(full_text.id, semantic.id) = documents.id\n_48\norder by\n_48\ncoalesce(1.0 / (rrf_k + full_text.rank_ix), 0.0) * full_text_weight +\n_48\ncoalesce(1.0 / (rrf_k + semantic.rank_ix), 0.0) * semantic_weight\n_48\ndesc\n_48\nlimit\n_48\nleast(match_count, 30)\n_48\n$$;\n`\n\nLet's break this down:\n\n- **Parameters:** The function accepts quite a few parameters, but the main (required) ones are <code>query_text</code>, <code>query_embedding</code>, and <code>match_count</code>.\n\n  - <code>query_text</code> is the user's query text (more on this shortly)\n  - <code>query_embedding</code> is the vector representation of the user's query produced by the embedding model. We chose 512 dimensions for this example, but adjust this to match the size of the embedding vectors generated from your preferred model. This must match the size of the <code>embedding</code> vector on the <code>documents</code> table (and use the same model).\n  - <code>match_count</code> is the number of records returned in the <code>limit</code> clause.\n\nThe other parameters are optional, but give more control over the fusion process.\n  - <code>full_text_weight</code> and <code>semantic_weight</code> decide how much weight each search method gets in the final score. These are both 1 by default which means they both equally contribute towards the final rank. A <code>full_text_weight</code> of 2 and <code>semantic_weight</code> of 1 would give full-text search twice as much weight as semantic search.\n  - <code>rrf_k</code> is the <code>k</code> [smoothing constant](#smoothing-constant-k) added to the reciprocal rank. The default is 50.\n- **Return type:** The function returns a set of records from our <code>documents</code> table.\n\n- **CTE:** We create two [common table expressions (CTE)](https://www.postgresql.org/docs/current/queries-with.html), one for full-text search and one for semantic search. These perform each query individually prior to joining them.\n\n- **RRF:** The final query combines the results from the two CTEs using [reciprocal rank fusion (RRF)](#reciprocal-ranked-fusion-rrf).\n",
      "overlap_text": {
        "previous_chunk_id": "3a7c37c2-d175-462c-962b-c38782178977",
        "text": "_48\nselect\n_48\n    id,\n_48\n    -- Note: ts_rank_cd is not indexable but will only rank matches of the where clause\n_48\n    -- which shouldn't be too big\n_48\n"
      }
    }
  },
  {
    "chunk_id": "2bb45726-bf25-4feb-b058-c3e9d8682ea2",
    "metadata": {
      "token_count": 850,
      "source_url": "https://supabase.com/docs/guides/ai/hybrid-search",
      "page_title": "Hybrid search | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Hybrid search",
        "h2": "See also \\#",
        "h3": ""
      },
      "text": "To use this function in SQL, we can run:\n\n`\n_10\nselect\n_10\n*\n_10\nfrom\n_10\nhybrid_search(\n_10\n    'Italian recipes with tomato sauce', -- user query\n_10\n    '[...]'::vector(512), -- embedding generated from user query\n_10\n    10\n_10\n);\n`\n\nIn practice, you will likely be calling this from the [Supabase client](/docs/reference/javascript/introduction) or through a custom backend layer. Here is a quick example of how you might call this from an [Edge Function](/docs/guides/functions) using JavaScript:\n\n`\n_38\nimport { createClient } from 'jsr:@supabase/supabase-js@2'\n_38\nimport OpenAI from 'npm:openai'\n_38\n_38\nconst supabaseUrl = Deno.env.get('SUPABASE_URL')!\n_38\nconst supabaseServiceRoleKey = Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!\n_38\nconst openaiApiKey = Deno.env.get('OPENAI_API_KEY')!\n_38\n_38\nDeno.serve(async (req) => {\n_38\n// Grab the user's query from the JSON payload\n_38\nconst { query } = await req.json()\n_38\n_38\n// Instantiate OpenAI client\n_38\nconst openai = new OpenAI({ apiKey: openaiApiKey })\n_38\n_38\n// Generate a one-time embedding for the user's query\n_38\nconst embeddingResponse = await openai.embeddings.create({\n_38\n    model: 'text-embedding-3-large',\n_38\n    input: query,\n_38\n    dimensions: 512,\n_38\n})\n_38\n_38\nconst [{ embedding }] = embeddingResponse.data\n_38\n_38\n// Instantiate the Supabase client\n_38\n// (replace service role key with user's JWT if using Supabase auth and RLS)\n_38\nconst supabase = createClient(supabaseUrl, supabaseServiceRoleKey)\n_38\n_38\n// Call hybrid_search Postgres function via RPC\n_38\nconst { data: documents } = await supabase.rpc('hybrid_search', {\n_38\n    query_text: query,\n_38\n    query_embedding: embedding,\n_38\n    match_count: 10,\n_38\n})\n_38\n_38\nreturn new Response(JSON.stringify(documents), {\n_38\n    headers: { 'Content-Type': 'application/json' },\n_38\n})\n_38\n})\n`\n\nThis uses OpenAI's <code>text-embedding-3-large</code> model to generate embeddings (shortened to 512 dimensions for faster retrieval). Swap in your preferred embedding model (and dimension size) accordingly.\n\nTo test this, make a <code>POST</code> request to the function's endpoint while passing in a JSON payload containing the user's query. Here is an example <code>POST</code> request using cURL:\n\n`\n_10\ncurl -i --location --request POST \\\n_10\n'http://127.0.0.1:54321/functions/v1/hybrid-search' \\\n_10\n  --header 'Authorization: Bearer <anonymous key>' \\\n_10\n  --header 'Content-Type: application/json' \\\n_10\n  --data '{\"query\":\"Italian recipes with tomato sauce\"}'\n`\n\nFor more information on how to create, test, and deploy edge functions, see [Getting started](/docs/guides/functions/quickstart).\n- [Embedding concepts](/docs/guides/ai/concepts)\n- [Vector columns](/docs/guides/ai/vector-columns)\n- [Vector indexes](/docs/guides/ai/vector-indexes)\n- [Semantic search](/docs/guides/ai/semantic-search)\n- [Full text (keyword) search](/docs/guides/database/full-text-search)\n",
      "overlap_text": {
        "previous_chunk_id": "cd2135de-b353-429d-a00f-c50148e037cb",
        "text": ". These perform each query individually prior to joining them.\n\n- **RRF:** The final query combines the results from the two CTEs using [reciprocal rank fusion (RRF)](#reciprocal-ranked-fusion-rrf).\n"
      }
    }
  },
  {
    "chunk_id": "84565a8d-732e-41ff-9d37-4ba6b7af7573",
    "metadata": {
      "token_count": 899,
      "source_url": "https://supabase.com/docs/guides/ai/vector-indexes/ivf-indexes",
      "page_title": "IVFFlat indexes | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "IVFFlat indexes",
        "h2": "Resources \\#",
        "h3": "Approximate nearest neighbor \\#"
      },
      "text": "AI & Vectors\nIVFFlat is a type of vector index for approximate nearest neighbor search. It is a frequently used index type that can improve performance when querying highly-dimensional vectors, like those representing embeddings.\nToday <code>pgvector</code> supports two types of indexes:\n\n- [HNSW](/docs/guides/ai/vector-indexes/hnsw-indexes)\n- [IVFFlat](/docs/guides/ai/vector-indexes/ivf-indexes)\n\nIn general we recommend using [HNSW](/docs/guides/ai/vector-indexes/hnsw-indexes) because of its [performance](https://supabase.com/blog/increase-performance-pgvector-hnsw#hnsw-performance-1536-dimensions) and [robustness against changing data](/docs/guides/ai/vector-indexes/hnsw-indexes#when-should-you-create-hnsw-indexes). If you have a special use case that requires IVFFlat instead, keep reading.\nThe way you create an IVFFlat index depends on the distance operator you are using. <code>pgvector</code> includes 3 distance operators:\n\n| Operator | Description | [**Operator class**](https://www.postgresql.org/docs/current/sql-createopclass.html) |\n| --- | --- | --- |\n| <code><-></code> | Euclidean distance | <code>vector_l2_ops</code> |\n| <code><#></code> | negative inner product | <code>vector_ip_ops</code> |\n| <code><=></code> | cosine distance | <code>vector_cosine_ops</code> |\n\nUse the following SQL commands to create an IVFFlat index for the operator(s) used in your queries.\n`\n_10\ncreate index on items using ivfflat (column_name vector_l2_ops) with (lists = 100);\n`\n`\n_10\ncreate index on items using ivfflat (column_name vector_ip_ops) with (lists = 100);\n`\n`\n_10\ncreate index on items using ivfflat (column_name vector_cosine_ops) with (lists = 100);\n`\n\nCurrently vectors with up to 2,000 dimensions can be indexed.\nIVF stands for 'inverted file indexes'. It works by clustering your vectors in order to reduce the similarity search scope. Rather than comparing a vector to every other vector, the vector is only compared against vectors within the same cell cluster (or nearby clusters, depending on your configuration).\nWhen you create the index, you choose the number of inverted lists (cell clusters). Increase this number to speed up queries, but at the expense of recall.\n\nFor example, to create an index with 100 lists on a column that uses the cosine operator:\n\n`\n_10\ncreate index on items using ivfflat (column_name vector_cosine_ops) with (lists = 100);\n`\n\nFor more info on the different operators, see [Distance operations](#distance-operators).\n\nFor every query, you can set the number of probes (1 by default). The number of probes corresponds to the number of nearby cells to probe for a match. Increase this for better recall at the expense of speed.\n\nTo set the number of probes for the duration of the session run:\n\n`\n_10\nset ivfflat.probes = 10;\n`\n\nTo set the number of probes only for the current transaction run:\n\n`\n_10\nbegin;\n_10\nset local ivfflat.probes = 10;\n_10\nselect ...\n_10\ncommit;\n`\n\nIf the number of probes is the same as the number of lists, exact nearest neighbor search will be performed and the planner won't use the index.\nOne important note with IVF indexes is that nearest neighbor search is approximate, since exact search on high dimensional data can't be indexed efficiently. This means that similarity results will change (slightly) after you add an index (trading recall for speed).\n<code>pgvector</code> recommends building IVFFlat indexes only after the table has sufficient data, so that the internal IVFFlat cell clusters are based on your data's distribution. Anytime the distribution changes significantly, consider rebuilding indexes.\nRead more about indexing on <code>pgvector</code>'s [GitHub page](https://github.com/pgvector/pgvector#indexing).\n"
    }
  },
  {
    "chunk_id": "45b08aeb-c3bf-4fd6-a018-f915397b5f01",
    "metadata": {
      "token_count": 842,
      "source_url": "https://supabase.com/docs/guides/ai/examples/image-search-openai-clip",
      "page_title": "Image Search with OpenAI CLIP | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Encode an image:",
        "h2": "Create embeddings for your images \\#",
        "h3": ""
      },
      "text": "AI & Vectors\nThe [OpenAI CLIP Model](https://github.com/openai/CLIP) was trained on a variety of (image, text)-pairs. You can use the CLIP model for:\n\n- Text-to-Image / Image-To-Text / Image-to-Image / Text-to-Text Search\n- You can fine-tune it on your own image and text data with the regular SentenceTransformers training code.\n\n[SentenceTransformers](https://www.sbert.net/examples/applications/image-search/README.html) provides models that allow you to embed images and text into the same vector space. You can use this to find similar images as well as to implement image search.\n\nYou can find the full application code as a Python Poetry project on [GitHub](https://github.com/supabase/supabase/tree/master/examples/ai/image_search#image-search-with-supabase-vector).\n[Poetry](https://python-poetry.org/) provides packaging and dependency management for Python. If you haven't already, install poetry via pip:\n\n`\n_10\npip install poetry\n`\n\nThen initialize a new project:\n\n`\n_10\npoetry new image-search\n`\nIf you haven't already, [install the Supabase CLI](/docs/guides/cli), then initialize Supabase in the root of your newly created poetry project:\n\n`\n_10\nsupabase init\n`\n\nNext, start your local Supabase stack:\n\n`\n_10\nsupabase start\n`\n\nThis will start up the Supabase stack locally and print out a bunch of environment details, including your local <code>DB URL</code>. Make a note of that for later user.\nWe will need to add the following dependencies to our project:\n\n- [<code>vecs</code>](https://github.com/supabase/vecs#vecs): Supabase Vector Python Client.\n- [<code>sentence-transformers</code>](https://huggingface.co/sentence-transformers/clip-ViT-B-32): a framework for sentence, text and image embeddings (used with OpenAI CLIP model)\n- [<code>matplotlib</code>](https://matplotlib.org/): for displaying our image result\n\n`\n_10\npoetry add vecs sentence-transformers matplotlib\n`\nAt the top of your main python script, import the dependencies and store your <code>DB URL</code> from above in a variable:\n\n`\n_10\nfrom PIL import Image\n_10\nfrom sentence_transformers import SentenceTransformer\n_10\nimport vecs\n_10\nfrom matplotlib import pyplot as plt\n_10\nfrom matplotlib import image as mpimg\n_10\n_10\nDB_CONNECTION = \"postgresql://postgres:postgres@localhost:54322/postgres\"\n`\nIn the root of your project, create a new folder called <code>images</code> and add some images. You can use the images from the example project on [GitHub](https://github.com/supabase/supabase/tree/master/examples/ai/image_search/images) or you can find license free images on [unsplash](https://unsplash.com).\n\nNext, create a <code>seed</code> method, which will create a new Supabase Vector Collection, generate embeddings for your images, and upsert the embeddings into your database:\n\n`\n_43\ndef seed():\n_43\n_43\n    vx = vecs.create_client(DB_CONNECTION)\n_43\n_43\n_43\n    images = vx.get_or_create_collection(name=\"image_vectors\", dimension=512)\n_43\n_43\n_43\n    model = SentenceTransformer('clip-ViT-B-32')\n_43\n_43\n_43\n    img_emb1 = model.encode(Image.open('./images/one.jpg'))\n_43\n    img_emb2 = model.encode(Image.open('./images/two.jpg'))\n_43\n    img_emb3 = model.encode(Image.open('./images/three.jpg'))\n_43\n    img_emb4 = model.encode(Image.open('./images/four.jpg'))\n_43\n_43\n"
    }
  },
  {
    "chunk_id": "5156d25e-7284-4f3b-bf4c-682f19feabcc",
    "metadata": {
      "token_count": 751,
      "source_url": "https://supabase.com/docs/guides/ai/examples/image-search-openai-clip",
      "page_title": "Image Search with OpenAI CLIP | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "query the collection filtering metadata for \"type\" = \"jpg\"",
        "h2": "Conclusion \\#",
        "h3": ""
      },
      "text": "_43\n    images.upsert(\n_43\n        records=[\\\n_43\\\n            (\\\n_43\\\n                \"one.jpg\",        # the vector's identifier\\\n_43\\\n                img_emb1,          # the vector. list or np.array\\\n_43\\\n                {\"type\": \"jpg\"}   # associated  metadata\\\n_43\\\n            ), (\\\n_43\\\n                \"two.jpg\",\\\n_43\\\n                img_emb2,\\\n_43\\\n                {\"type\": \"jpg\"}\\\n_43\\\n            ), (\\\n_43\\\n                \"three.jpg\",\\\n_43\\\n                img_emb3,\\\n_43\\\n                {\"type\": \"jpg\"}\\\n_43\\\n            ), (\\\n_43\\\n                \"four.jpg\",\\\n_43\\\n                img_emb4,\\\n_43\\\n                {\"type\": \"jpg\"}\\\n_43\\\n            )\\\n_43\\\n        ]\n_43\n    )\n_43\n    print(\"Inserted images\")\n_43\n_43\n_43\n    images.create_index()\n_43\n    print(\"Created index\")\n`\n\nAdd this method as a script in your <code>pyproject.toml</code> file:\n\n`\n_10\n[tool.poetry.scripts]\n_10\nseed = \"image_search.main:seed\"\n_10\nsearch = \"image_search.main:search\"\n`\n\nAfter activating the virtual environtment with <code>poetry shell</code> you can now run your seed script via <code>poetry run seed</code>. You can inspect the generated embeddings in your local database by visiting the local Supabase dashboard at [localhost:54323](http://localhost:54323/project/default/editor), selecting the <code>vecs</code> schema, and the <code>image_vectors</code> database.\nWith Supabase Vector we can easily query our embeddings. We can use either an image as search input or alternative we can generate an embedding from a string input and use that as the query input:\n\n`\n_23\ndef search():\n_23\n_23\n    vx = vecs.create_client(DB_CONNECTION)\n_23\n    images = vx.get_or_create_collection(name=\"image_vectors\", dimension=512)\n_23\n_23\n_23\n    model = SentenceTransformer('clip-ViT-B-32')\n_23\n_23\n    query_string = \"a bike in front of a red brick wall\"\n_23\n    text_emb = model.encode(query_string)\n_23\n_23\n_23\n    results = images.query(\n_23\n        data=text_emb,                      # required\n_23\n        limit=1,                            # number of records to return\n_23\n        filters={\"type\": {\"$eq\": \"jpg\"}},   # metadata filters\n_23\n    )\n_23\n    result = results[0]\n_23\n    print(result)\n_23\n    plt.title(result)\n_23\n    image = mpimg.imread('./images/' + result)\n_23\n    plt.imshow(image)\n_23\n    plt.show()\n`\n\nBy limiting the query to one result, we can show the most relevant image to the user. Finally we use <code>matplotlib</code> to show the image result to the user.\n\nThat's it, go ahead and test it out by running <code>poetry run search</code> and you will be presented with an image of a \"bike in front of a red brick wall\".\nWith just a couple of lines of Python you are able to implement image search as well as reverse image search using OpenAI's CLIP model and Supabase Vector.\n",
      "overlap_text": {
        "previous_chunk_id": "45b08aeb-c3bf-4fd6-a018-f915397b5f01",
        "text": "(Image.open('./images/two.jpg'))\n_43\n    img_emb3 = model.encode(Image.open('./images/three.jpg'))\n_43\n    img_emb4 = model.encode(Image.open('./images/four.jpg'))\n_43\n_43\n"
      }
    }
  },
  {
    "chunk_id": "60d1cdef-6fd4-4184-addf-d3496ca541ab",
    "metadata": {
      "token_count": 757,
      "source_url": "https://supabase.com/docs/guides/ai/choosing-compute-addon",
      "page_title": "Choosing your Compute Add-on | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Choosing your Compute Add-on",
        "h2": "HNSW \\#",
        "h3": "384 dimensions \\#"
      },
      "text": "AI & Vectors\nYou have two options for scaling your vector workload:\n\n1. Increase the size of your database. This guide will help you choose the right size for your workload.\n2. Spread your workload across multiple databases. You can find more details about this approach in [Engineering for Scale](engineering-for-scale).\nThe number of dimensions in your embeddings is the most important factor in choosing the right Compute Add-on. In general, the lower the dimensionality the better the performance. We've provided guidance for some of the more common embedding dimensions below. For each benchmark, we used [Vecs](https://github.com/supabase/vecs) to create a collection, upload the embeddings to a single table, and create both the <code>IVFFlat</code> and <code>HNSW</code> indexes for <code>inner-product</code> distance measure for the embedding column. We then ran a series of queries to measure the performance of different compute add-ons:\nThis benchmark uses the dbpedia-entities-openai-1M dataset containing 1,000,000 embeddings of text, regenerated for 384 dimension embeddings. Each embedding is generated using [gte-small](https://huggingface.co/Supabase/gte-small).\n\ngte-small-384\n\n| Compute Size | Vectors | m | ef\\_construction | ef\\_search | QPS | Latency Mean | Latency p95 | RAM Usage | RAM |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| Micro | 100,000 | 16 | 64 | 60 | 580 | 0.017 sec | 0.024 sec | 1.2 (Swap) | 1 GB |\n| Small | 250,000 | 24 | 64 | 60 | 440 | 0.022 sec | 0.033 sec | 2 GB | 2 GB |\n| Medium | 500,000 | 24 | 64 | 80 | 350 | 0.028 sec | 0.045 sec | 4 GB | 4 GB |\n| Large | 1,000,000 | 32 | 80 | 100 | 270 | 0.073 sec | 0.108 sec | 7 GB | 8 GB |\n| XL | 1,000,000 | 32 | 80 | 100 | 525 | 0.038 sec | 0.059 sec | 9 GB | 16 GB |\n| 2XL | 1,000,000 | 32 | 80 | 100 | 790 | 0.025 sec | 0.037 sec | 9 GB | 32 GB |\n| 4XL | 1,000,000 | 32 | 80 | 100 | 1650 | 0.015 sec | 0.018 sec | 11 GB | 64 GB |\n| 8XL | 1,000,000 | 32 | 80 | 100 | 2690 | 0.015 sec | 0.016 sec | 13 GB | 128 GB |\n| 12XL | 1,000,000 | 32 | 80 | 100 | 3900 | 0.014 sec | 0.016 sec | 13 GB | 192 GB |\n| 16XL | 1,000,000 | 32 | 80 | 100 | 4200 | 0.014 sec | 0.016 sec | 20 GB | 256 GB |\n\nAccuracy was 0.99 for benchmarks.\n"
    }
  },
  {
    "chunk_id": "0e0b2226-8b9f-4c3f-8acc-6461af2df3f6",
    "metadata": {
      "token_count": 620,
      "source_url": "https://supabase.com/docs/guides/ai/choosing-compute-addon",
      "page_title": "Choosing your Compute Add-on | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Choosing your Compute Add-on",
        "h2": "HNSW \\#",
        "h3": "960 dimensions \\#"
      },
      "text": "This benchmark uses the [gist-960](http://corpus-texmex.irisa.fr/) dataset, which contains 1,000,000 embeddings of images. Each embedding is 960 dimensions.\n\ngist-960\n\n| Compute Size | Vectors | m | ef\\_construction | ef\\_search | QPS | Latency Mean | Latency p95 | RAM Usage | RAM |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| Micro | 30,000 | 16 | 64 | 65 | 430 | 0.024 sec | 0.034 sec | 1.2 GB (Swap) | 1 GB |\n| Small | 100,000 | 32 | 80 | 60 | 260 | 0.040 sec | 0.054 sec | 2.2 GB (Swap) | 2 GB |\n| Medium | 250,000 | 32 | 80 | 90 | 120 | 0.083 sec | 0.106 sec | 4 GB | 4 GB |\n| Large | 500,000 | 32 | 80 | 120 | 160 | 0.063 sec | 0.087 sec | 7 GB | 8 GB |\n| XL | 1,000,000 | 32 | 80 | 200 | 200 | 0.049 sec | 0.072 sec | 13 GB | 16 GB |\n| 2XL | 1,000,000 | 32 | 80 | 200 | 340 | 0.025 sec | 0.029 sec | 17 GB | 32 GB |\n| 4XL | 1,000,000 | 32 | 80 | 200 | 630 | 0.031 sec | 0.050 sec | 18 GB | 64 GB |\n| 8XL | 1,000,000 | 32 | 80 | 200 | 1100 | 0.034 sec | 0.048 sec | 19 GB | 128 GB |\n| 12XL | 1,000,000 | 32 | 80 | 200 | 1420 | 0.041 sec | 0.095 sec | 21 GB | 192 GB |\n| 16XL | 1,000,000 | 32 | 80 | 200 | 1650 | 0.037 sec | 0.081 sec | 23 GB | 256 GB |\n\nAccuracy was 0.99 for benchmarks.\n\nQPS can also be improved by increasing [<code>m</code> and <code>ef_construction</code>](/docs/guides/ai/going-to-prod#hnsw-understanding-efconstruction--efsearch--and-m). This will allow you to use a smaller value for <code>ef_search</code> and increase QPS.\n",
      "overlap_text": {
        "previous_chunk_id": "60d1cdef-6fd4-4184-addf-d3496ca541ab",
        "text": " | 1,000,000 | 32 | 80 | 100 | 4200 | 0.014 sec | 0.016 sec | 20 GB | 256 GB |\n\nAccuracy was 0.99 for benchmarks.\n"
      }
    }
  },
  {
    "chunk_id": "223cb608-38cf-4eae-bf67-aa5ba7b47c5a",
    "metadata": {
      "token_count": 824,
      "source_url": "https://supabase.com/docs/guides/ai/choosing-compute-addon",
      "page_title": "Choosing your Compute Add-on | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Choosing your Compute Add-on",
        "h2": "HNSW \\#",
        "h3": "1536 dimensions \\#"
      },
      "text": "This benchmark uses the [dbpedia-entities-openai-1M](https://huggingface.co/datasets/KShivendu/dbpedia-entities-openai-1M) dataset, which contains 1,000,000 embeddings of text. And 224,482 embeddings from [Wikipedia articles](https://huggingface.co/datasets/Supabase/wikipedia-en-embeddings) for compute add-ons <code>large</code> and below. Each embedding is 1536 dimensions created with the [OpenAI Embeddings API](https://platform.openai.com/docs/guides/embeddings).\n\nOpenAI-1536\n\n| Compute Size | Vectors | m | ef\\_construction | ef\\_search | QPS | Latency Mean | Latency p95 | RAM Usage | RAM |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| Micro | 15,000 | 16 | 40 | 40 | 480 | 0.011 sec | 0.016 sec | 1.2 GB (Swap) | 1 GB |\n| Small | 50,000 | 32 | 64 | 100 | 175 | 0.031 sec | 0.051 sec | 2.2 GB (Swap) | 2 GB |\n| Medium | 100,000 | 32 | 64 | 100 | 240 | 0.083 sec | 0.126 sec | 4 GB | 4 GB |\n| Large | 224,482 | 32 | 64 | 100 | 280 | 0.017 sec | 0.028 sec | 8 GB | 8 GB |\n| XL | 500,000 | 24 | 56 | 100 | 360 | 0.055 sec | 0.135 sec | 13 GB | 16 GB |\n| 2XL | 1,000,000 | 24 | 56 | 250 | 560 | 0.036 sec | 0.058 sec | 32 GB | 32 GB |\n| 4XL | 1,000,000 | 24 | 56 | 250 | 950 | 0.021 sec | 0.033 sec | 39 GB | 64 GB |\n| 8XL | 1,000,000 | 24 | 56 | 250 | 1650 | 0.016 sec | 0.023 sec | 40 GB | 128 GB |\n| 12XL | 1,000,000 | 24 | 56 | 250 | 1900 | 0.015 sec | 0.021 sec | 38 GB | 192 GB |\n| 16XL | 1,000,000 | 24 | 56 | 250 | 2200 | 0.015 sec | 0.020 sec | 40 GB | 256 GB |\n\nAccuracy was 0.99 for benchmarks.\n\nQPS can also be improved by increasing [<code>m</code> and <code>ef_construction</code>](/docs/guides/ai/going-to-prod#hnsw-understanding-efconstruction--efsearch--and-m). This will allow you to use a smaller value for <code>ef_search</code> and increase QPS. For example, increasing <code>m</code> to 32 and <code>ef_construction</code> to 80 for 4XL will increase QPS to 1280.\n\nIt is possible to upload more vectors to a single table if Memory allows it (for example, 4XL plan and higher for OpenAI embeddings). But it will affect the performance of the queries: QPS will be lower, and latency will be higher. Scaling should be almost linear, but it is recommended to benchmark your workload to find the optimal number of vectors per table and per database instance.\n",
      "overlap_text": {
        "previous_chunk_id": "0e0b2226-8b9f-4c3f-8acc-6461af2df3f6",
        "text": "/docs/guides/ai/going-to-prod#hnsw-understanding-efconstruction--efsearch--and-m). This will allow you to use a smaller value for <code>ef_search</code> and increase QPS.\n"
      }
    }
  },
  {
    "chunk_id": "5c7eaa2d-c853-48cc-b13f-27c413bb8573",
    "metadata": {
      "token_count": 535,
      "source_url": "https://supabase.com/docs/guides/ai/choosing-compute-addon",
      "page_title": "Choosing your Compute Add-on | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Choosing your Compute Add-on",
        "h2": "IVFFlat \\#",
        "h3": "384 dimensions \\#"
      },
      "text": "This benchmark uses the dbpedia-entities-openai-1M dataset containing 1,000,000 embeddings of text, regenerated for 384 dimension embeddings. Each embedding is generated using [gte-small](https://huggingface.co/Supabase/gte-small).\n\ngte-small-384, accuracy=.98gte-small-384, accuracy=.99\n\n| Compute Size | Vectors | Lists | Probes | QPS | Latency Mean | Latency p95 | RAM Usage | RAM |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| Micro | 100,000 | 500 | 50 | 205 | 0.048 sec | 0.066 sec | 1.2 GB (Swap) | 1 GB |\n| Small | 250,000 | 1000 | 60 | 160 | 0.062 sec | 0.079 sec | 2 GB | 2 GB |\n| Medium | 500,000 | 2000 | 80 | 120 | 0.082 sec | 0.104 sec | 3.2 GB | 4 GB |\n| Large | 1,000,000 | 5000 | 150 | 75 | 0.269 sec | 0.375 sec | 6.5 GB | 8 GB |\n| XL | 1,000,000 | 5000 | 150 | 150 | 0.131 sec | 0.178 sec | 9 GB | 16 GB |\n| 2XL | 1,000,000 | 5000 | 150 | 300 | 0.066 sec | 0.099 sec | 10 GB | 32 GB |\n| 4XL | 1,000,000 | 5000 | 150 | 570 | 0.035 sec | 0.046 sec | 10 GB | 64 GB |\n| 8XL | 1,000,000 | 5000 | 150 | 1400 | 0.023 sec | 0.028 sec | 12 GB | 128 GB |\n| 12XL | 1,000,000 | 5000 | 150 | 1550 | 0.030 sec | 0.039 sec | 12 GB | 192 GB |\n| 16XL | 1,000,000 | 5000 | 150 | 1800 | 0.030 sec | 0.039 sec | 16 GB | 256 GB |\n",
      "overlap_text": {
        "previous_chunk_id": "223cb608-38cf-4eae-bf67-aa5ba7b47c5a",
        "text": " But it will affect the performance of the queries: QPS will be lower, and latency will be higher. Scaling should be almost linear, but it is recommended to benchmark your workload to find the optimal number of vectors per table and per database instance.\n"
      }
    }
  },
  {
    "chunk_id": "924ef72c-a000-4aae-85ac-5287b9ba5230",
    "metadata": {
      "token_count": 477,
      "source_url": "https://supabase.com/docs/guides/ai/choosing-compute-addon",
      "page_title": "Choosing your Compute Add-on | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Choosing your Compute Add-on",
        "h2": "IVFFlat \\#",
        "h3": "960 dimensions \\#"
      },
      "text": "This benchmark uses the [gist-960](http://corpus-texmex.irisa.fr/) dataset, which contains 1,000,000 embeddings of images. Each embedding is 960 dimensions.\n\ngist-960, probes = 10\n\n| Compute Size | Vectors | Lists | QPS | Latency Mean | Latency p95 | RAM Usage | RAM |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| Micro | 30,000 | 30 | 75 | 0.065 sec | 0.088 sec | 1.1 GB (Swap) | 1 GB |\n| Small | 100,000 | 100 | 78 | 0.064 sec | 0.092 sec | 1.8 GB | 2 GB |\n| Medium | 250,000 | 250 | 58 | 0.085 sec | 0.129 sec | 3.2 GB | 4 GB |\n| Large | 500,000 | 500 | 55 | 0.088 sec | 0.140 sec | 5 GB | 8 GB |\n| XL | 1,000,000 | 1000 | 110 | 0.046 sec | 0.070 sec | 14 GB | 16 GB |\n| 2XL | 1,000,000 | 1000 | 235 | 0.083 sec | 0.136 sec | 10 GB | 32 GB |\n| 4XL | 1,000,000 | 1000 | 420 | 0.071 sec | 0.106 sec | 11 GB | 64 GB |\n| 8XL | 1,000,000 | 1000 | 815 | 0.072 sec | 0.106 sec | 13 GB | 128 GB |\n| 12XL | 1,000,000 | 1000 | 1150 | 0.052 sec | 0.078 sec | 15.5 GB | 192 GB |\n| 16XL | 1,000,000 | 1000 | 1345 | 0.072 sec | 0.106 sec | 17.5 GB | 256 GB |\n",
      "overlap_text": {
        "previous_chunk_id": "5c7eaa2d-c853-48cc-b13f-27c413bb8573",
        "text": "12 GB | 192 GB |\n| 16XL | 1,000,000 | 5000 | 150 | 1800 | 0.030 sec | 0.039 sec | 16 GB | 256 GB |\n"
      }
    }
  },
  {
    "chunk_id": "629762ff-4117-4b62-a7fe-1cc31c494c58",
    "metadata": {
      "token_count": 811,
      "source_url": "https://supabase.com/docs/guides/ai/choosing-compute-addon",
      "page_title": "Choosing your Compute Add-on | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Choosing your Compute Add-on",
        "h2": "Performance tips \\#",
        "h3": "Finetune index parameters \\#"
      },
      "text": "This benchmark uses the [dbpedia-entities-openai-1M](https://huggingface.co/datasets/KShivendu/dbpedia-entities-openai-1M) dataset, which contains 1,000,000 embeddings of text. Each embedding is 1536 dimensions created with the [OpenAI Embeddings API](https://platform.openai.com/docs/guides/embeddings).\n\nOpenAI-1536, probes = 10OpenAI-1536, probes = 40\n\n| Compute Size | Vectors | Lists | QPS | Latency Mean | Latency p95 | RAM Usage | RAM |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| Micro | 20,000 | 40 | 135 | 0.372 sec | 0.412 sec | 1.2 GB (Swap) | 1 GB |\n| Small | 50,000 | 100 | 140 | 0.357 sec | 0.398 sec | 1.8 GB | 2 GB |\n| Medium | 100,000 | 200 | 130 | 0.383 sec | 0.446 sec | 3.7 GB | 4 GB |\n| Large | 250,000 | 500 | 130 | 0.378 sec | 0.434 sec | 7 GB | 8 GB |\n| XL | 500,000 | 1000 | 235 | 0.213 sec | 0.271 sec | 13.5 GB | 16 GB |\n| 2XL | 1,000,000 | 2000 | 380 | 0.133 sec | 0.236 sec | 30 GB | 32 GB |\n| 4XL | 1,000,000 | 2000 | 720 | 0.068 sec | 0.120 sec | 35 GB | 64 GB |\n| 8XL | 1,000,000 | 2000 | 1250 | 0.039 sec | 0.066 sec | 38 GB | 128 GB |\n| 12XL | 1,000,000 | 2000 | 1600 | 0.030 sec | 0.052 sec | 41 GB | 192 GB |\n| 16XL | 1,000,000 | 2000 | 1790 | 0.029 sec | 0.051 sec | 45 GB | 256 GB |\n\nFor 1,000,000 vectors 10 probes results to accuracy of 0.91. And for 500,000 vectors and below 10 probes results to accuracy in the range of 0.95 - 0.99. To increase accuracy, you need to increase the number of probes.\n\nIt is possible to upload more vectors to a single table if Memory allows it (for example, 4XL plan and higher for OpenAI embeddings). But it will affect the performance of the queries: QPS will be lower, and latency will be higher. Scaling should be almost linear, but it is recommended to benchmark your workload to find the optimal number of vectors per table and per database instance.\nThere are various ways to improve your pgvector performance. Here are some tips:\nIt's useful to execute a few thousand \u201cwarm-up\u201d queries before going into production. This helps help with RAM utilization. This can also help to determine that you've selected the right compute size for your workload.\nYou can increase the Requests per Second by increasing <code>m</code> and <code>ef_construction</code> or <code>lists</code>. This also has an important caveat: building the index takes longer with higher values for these parameters.\n\nHNSWIVFFlat\n\nCheck out more tips and the complete step-by-step guide in [Going to Production for AI applications](going-to-prod).\n",
      "overlap_text": {
        "previous_chunk_id": "924ef72c-a000-4aae-85ac-5287b9ba5230",
        "text": ".5 GB | 192 GB |\n| 16XL | 1,000,000 | 1000 | 1345 | 0.072 sec | 0.106 sec | 17.5 GB | 256 GB |\n"
      }
    }
  },
  {
    "chunk_id": "0ef48d1e-300c-4db0-bfb8-34734cd5698f",
    "metadata": {
      "token_count": 148,
      "source_url": "https://supabase.com/docs/guides/ai/choosing-compute-addon",
      "page_title": "Choosing your Compute Add-on | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Choosing your Compute Add-on",
        "h2": "Benchmark methodology \\#",
        "h3": ""
      },
      "text": "We follow techniques outlined in the [ANN Benchmarks](https://github.com/erikbern/ann-benchmarks) methodology. A Python test runner is responsible for uploading the data, creating the index, and running the queries. The pgvector engine is implemented using [vecs](https://github.com/supabase/vecs), a Python client for pgvector.\n\nEach test is run for a minimum of 30-40 minutes. They include a series of experiments executed at different concurrency levels to measure the engine's performance under different load types. The results are then averaged.\n\nAs a general recommendation, we suggest using a concurrency level of 5 or more for most workloads and 30 or more for high-load workloads.\n",
      "overlap_text": {
        "previous_chunk_id": "629762ff-4117-4b62-a7fe-1cc31c494c58",
        "text": " also has an important caveat: building the index takes longer with higher values for these parameters.\n\nHNSWIVFFlat\n\nCheck out more tips and the complete step-by-step guide in [Going to Production for AI applications](going-to-prod).\n"
      }
    }
  },
  {
    "chunk_id": "fcf16cfc-82dc-4cdb-887e-922c8c1106b1",
    "metadata": {
      "token_count": 146,
      "source_url": "https://supabase.com/docs/guides/ai/rag-with-permissions",
      "page_title": "RAG with Permissions | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "RAG with Permissions",
        "h2": "Fine-grain access control with Retrieval Augmented Generation.",
        "h3": ""
      },
      "text": "AI & Vectors\nSince pgvector is built on top of Postgres, you can implement fine-grain access control on your vector database using [Row Level Security (RLS)](/docs/guides/database/postgres/row-level-security). This means you can restrict which documents are returned during a vector similarity search to users that have access to them. Supabase also supports [Foreign Data Wrappers (FDW)](/docs/guides/database/extensions/wrappers/overview) which means you can use an external database or data source to determine these permissions if your user data doesn't exist in Supabase.\n\nUse this guide to learn how to restrict access to documents when performing retrieval augmented generation (RAG).\n"
    }
  },
  {
    "chunk_id": "835a6225-8c90-4370-87ba-9687050cd435",
    "metadata": {
      "token_count": 699,
      "source_url": "https://supabase.com/docs/guides/ai/rag-with-permissions",
      "page_title": "RAG with Permissions | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "RAG with Permissions",
        "h2": "Alternative scenarios \\#",
        "h3": ""
      },
      "text": "In a typical RAG setup, your documents are chunked into small subsections and similarity is performed over those sections:\n\n`\n_16\n-- Track documents/pages/files/etc\n_16\ncreate table documents (\n_16\nid bigint primary key generated always as identity,\n_16\nname text not null,\n_16\nowner_id uuid not null references auth.users (id) default auth.uid(),\n_16\ncreated_at timestamp with time zone not null default now()\n_16\n);\n_16\n_16\n-- Store the content and embedding vector for each section in the document\n_16\n-- with a reference to original document (one-to-many)\n_16\ncreate table document_sections (\n_16\nid bigint primary key generated always as identity,\n_16\ndocument_id bigint not null references documents (id),\n_16\ncontent text not null,\n_16\nembedding vector (384)\n_16\n);\n`\n\nNotice how we record the <code>owner_id</code> on each document. Let's create an RLS policy that restricts access to <code>document_sections</code> based on whether or not they own the linked document:\n\n`\n_12\n-- enable row level security\n_12\nalter table document_sections enable row level security;\n_12\n_12\n-- setup RLS for select operations\n_12\ncreate policy \"Users can query their own document sections\"\n_12\non document_sections for select to authenticated using (\n_12\ndocument_id in (\n_12\n    select id\n_12\n    from documents\n_12\n    where (owner_id = (select auth.uid()))\n_12\n)\n_12\n);\n`\n\nIn this example, the current user is determined using the built-in <code>auth.uid()</code> function when the query is executed through your project's auto-generated [REST API](/docs/guides/api). If you are connecting to your Supabase database through a direct Postgres connection, see [Direct Postgres Connection](#direct-postgres-connection) below for directions on how to achieve the same access control.\n\nNow every <code>select</code> query executed on <code>document_sections</code> will implicitly filter the returned sections based on whether or not the current user has access to them.\n\nFor example, executing:\n\n`\n_10\nselect * from document_sections;\n`\n\nas an authenticated user will only return rows that they are the owner of (as determined by the linked document). More importantly, semantic search over these sections (or any additional filtering for that matter) will continue to respect these RLS policies:\n\n`\n_10\n-- Perform inner product similarity based on a match_threshold\n_10\nselect *\n_10\nfrom document_sections\n_10\nwhere document_sections.embedding <#> embedding < -match_threshold\n_10\norder by document_sections.embedding <#> embedding;\n`\n\nThe above example only configures <code>select</code> access to users. If you wanted, you could create more RLS policies for inserts, updates, and deletes in order to apply the same permission logic for those other operations. See [Row Level Security](/docs/guides/database/postgres/row-level-security) for a more in-depth guide on RLS policies.\nEvery app has its own unique requirements and may differ from the above example. Here are some alternative scenarios we often see and how they are implemented in Supabase.\n",
      "overlap_text": {
        "previous_chunk_id": "fcf16cfc-82dc-4cdb-887e-922c8c1106b1",
        "text": "/overview) which means you can use an external database or data source to determine these permissions if your user data doesn't exist in Supabase.\n\nUse this guide to learn how to restrict access to documents when performing retrieval augmented generation (RAG).\n"
      }
    }
  },
  {
    "chunk_id": "0daed6f7-d279-4e12-bc37-b31f8bc3f2cb",
    "metadata": {
      "token_count": 212,
      "source_url": "https://supabase.com/docs/guides/ai/rag-with-permissions",
      "page_title": "RAG with Permissions | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "RAG with Permissions",
        "h2": "Alternative scenarios \\#",
        "h3": "Documents owned by multiple people \\#"
      },
      "text": "Instead of a one-to-many relationship between <code>users</code> and <code>documents</code>, you may require a many-to-many relationship so that multiple people can access the same document. Let's reimplement this using a join table:\n\n`\n_10\ncreate table document_owners (\n_10\nid bigint primary key generated always as identity,\n_10\nowner_id uuid not null references auth.users (id) default auth.uid(),\n_10\ndocument_id bigint not null references documents (id)\n_10\n);\n`\n\nThen your RLS policy would change to:\n\n`\n_10\ncreate policy \"Users can query their own document sections\"\n_10\non document_sections for select to authenticated using (\n_10\ndocument_id in (\n_10\n    select document_id\n_10\n    from document_owners\n_10\n    where (owner_id = (select auth.uid()))\n_10\n)\n_10\n);\n`\n\nInstead of directly querying the <code>documents</code> table, we query the join table.\n",
      "overlap_text": {
        "previous_chunk_id": "835a6225-8c90-4370-87ba-9687050cd435",
        "text": "gres/row-level-security) for a more in-depth guide on RLS policies.\nEvery app has its own unique requirements and may differ from the above example. Here are some alternative scenarios we often see and how they are implemented in Supabase.\n"
      }
    }
  },
  {
    "chunk_id": "1618fbe0-c474-4498-acbb-9c047fd6bfa3",
    "metadata": {
      "token_count": 774,
      "source_url": "https://supabase.com/docs/guides/ai/rag-with-permissions",
      "page_title": "RAG with Permissions | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "RAG with Permissions",
        "h2": "Alternative scenarios \\#",
        "h3": "User and document data live outside of Supabase \\#"
      },
      "text": "You may have an existing system that stores users, documents, and their permissions in a separate database. Let's explore the scenario where this data exists in another Postgres database. We'll use a foreign data wrapper (FDW) to connect to the external DB from within your Supabase DB:\n\nRLS is latency-sensitive, so extra caution should be taken before implementing this method. Use the [query plan analyzer](https://supabase.com/docs/guides/platform/performance#optimizing-poor-performing-queries) to measure execution times for your queries to ensure they are within expected ranges. For enterprise applications, contact [enterprise@supabase.io](mailto:enterprise@supabase.io).\n\nFor data sources other than Postgres, see [Foreign Data Wrappers](/docs/guides/database/extensions/wrappers/overview) for a list of external sources supported today. If your data lives in a source not provided in the list, please contact [support](https://supabase.com/dashboard/support/new) and we'll be happy to discuss your use case.\n\nLet's assume your external DB contains a <code>users</code> and <code>documents</code> table like this:\n\n`\n_12\ncreate table public.users (\n_12\nid bigint primary key generated always as identity,\n_12\nemail text not null,\n_12\ncreated_at timestamp with time zone not null default now()\n_12\n);\n_12\n_12\ncreate table public.documents (\n_12\nid bigint primary key generated always as identity,\n_12\nname text not null,\n_12\nowner_id bigint not null references public.users (id),\n_12\ncreated_at timestamp with time zone not null default now()\n_12\n);\n`\n\nIn your Supabase DB, let's create foreign tables that link to the above tables:\n\n`\n_16\ncreate schema external;\n_16\ncreate extension postgres_fdw with schema extensions;\n_16\n_16\n-- Setup the foreign server\n_16\ncreate server foreign_server\n_16\nforeign data wrapper postgres_fdw\n_16\noptions (host '<db-host>', port '<db-port>', dbname '<db-name>');\n_16\n_16\n-- Map local 'authenticated' role to external 'postgres' user\n_16\ncreate user mapping for authenticated\n_16\nserver foreign_server\n_16\noptions (user 'postgres', password '<user-password>');\n_16\n_16\n-- Import foreign 'users' and 'documents' tables into 'external' schema\n_16\nimport foreign schema public limit to (users, documents)\n_16\nfrom server foreign_server into external;\n`\n\nThis example maps the <code>authenticated</code> role in Supabase to the <code>postgres</code> user in the external DB. In production, it's best to create a custom user on the external DB that has the minimum permissions necessary to access the information you need.\n\nOn the Supabase DB, we use the built-in <code>authenticated</code> role which is automatically used when end users make authenticated requests over your auto-generated REST API. If you plan to connect to your Supabase DB over a direct Postgres connection instead of the REST API, you can change this to any user you like. See [Direct Postgres Connection](#direct-postgres-connection) for more info.\n\nWe'll store <code>document_sections</code> and their embeddings in Supabase so that we can perform similarity search over them via pgvector.\n\n`\n_10\ncreate table document_sections (\n_10\nid bigint primary key generated always as identity,\n_10\ndocument_id bigint not null,\n_10\ncontent text not null,\n_10\nembedding vector (384)\n_10\n);\n`\n\n",
      "overlap_text": {
        "previous_chunk_id": "0daed6f7-d279-4e12-bc37-b31f8bc3f2cb",
        "text": "\n    from document_owners\n_10\n    where (owner_id = (select auth.uid()))\n_10\n)\n_10\n);\n`\n\nInstead of directly querying the <code>documents</code> table, we query the join table.\n"
      }
    }
  },
  {
    "chunk_id": "b3d9be74-86ff-4aca-a2dd-7038d7d3d72d",
    "metadata": {
      "token_count": 527,
      "source_url": "https://supabase.com/docs/guides/ai/rag-with-permissions",
      "page_title": "RAG with Permissions | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "RAG with Permissions",
        "h2": "Alternative scenarios \\#",
        "h3": "# Direct Postgres connection \\#"
      },
      "text": "We maintain a reference to the foreign document via <code>document_id</code>, but without a foreign key reference since foreign keys can only be added to local tables. Be sure to use the same ID data type that you use on your external documents table.\n\nSince we're managing users and authentication outside of Supabase, we have two options:\n\n1. Make a direct Postgres connection to the Supabase DB and set the current user every request\n2. Issue a custom JWT from your system and use it to authenticate with the REST API\nYou can directly connect to your Supabase Postgres DB using the [connection info](/dashboard/project/_/settings/database) on your project's database settings page. To use RLS with this method, we use a custom session variable that contains the current user's ID:\n\n`\n_12\n-- enable row level security\n_12\nalter table document_sections enable row level security;\n_12\n_12\n-- setup RLS for select operations\n_12\ncreate policy \"Users can query their own document sections\"\n_12\non document_sections for select to authenticated using (\n_12\ndocument_id in (\n_12\n    select id\n_12\n    from external.documents\n_12\n    where owner_id = current_setting('app.current_user_id')::bigint\n_12\n)\n_12\n);\n`\n\nThe session variable is accessed through the <code>current_setting()</code> function. We name the variable <code>app.current_user_id</code> here, but you can modify this to any name you like. We also cast it to a <code>bigint</code> since that was the data type of the <code>user.id</code> column. Change this to whatever data type you use for your ID.\n\nNow for every request, we set the user's ID at the beginning of the session:\n\n`\n_10\nset app.current_user_id = '<current-user-id>';\n`\n\nThen all subsequent queries will inherit the permission of that user:\n\n`\n_10\n-- Only document sections owned by the user are returned\n_10\nselect *\n_10\nfrom document_sections\n_10\nwhere document_sections.embedding <#> embedding < -match_threshold\n_10\norder by document_sections.embedding <#> embedding;\n`\n\nYou might be tempted to discard RLS completely and simply filter by user within the <code>where</code> clause. Though this will work, we recommend RLS as a general best practice since RLS is always applied even as new queries and application logic is introduced in the future.\n",
      "overlap_text": {
        "previous_chunk_id": "1618fbe0-c474-4498-acbb-9c047fd6bfa3",
        "text": "_10\ncreate table document_sections (\n_10\nid bigint primary key generated always as identity,\n_10\ndocument_id bigint not null,\n_10\ncontent text not null,\n_10\nembedding vector (384)\n_10\n);\n`\n\n"
      }
    }
  },
  {
    "chunk_id": "5f11cf92-08ce-4b9a-884d-e227918daab7",
    "metadata": {
      "token_count": 433,
      "source_url": "https://supabase.com/docs/guides/ai/rag-with-permissions",
      "page_title": "RAG with Permissions | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "RAG with Permissions",
        "h2": "Alternative scenarios \\#",
        "h3": "Other scenarios \\#"
      },
      "text": "If you would like to use the auto-generated REST API to query your Supabase database using JWTs from an external auth provider, you can get your auth provider to issue a custom JWT for Supabase.\n\nSee the [Clerk Supabase docs](https://clerk.com/docs/integrations/databases/supabase) for an example of how this can be done. Modify the instructions to work with your own auth provider as needed.\n\nNow we can simply use the same RLS policy from our first example:\n\n`\n_12\n-- enable row level security\n_12\nalter table document_sections enable row level security;\n_12\n_12\n-- setup RLS for select operations\n_12\ncreate policy \"Users can query their own document sections\"\n_12\non document_sections for select to authenticated using (\n_12\ndocument_id in (\n_12\n    select id\n_12\n    from documents\n_12\n    where (owner_id = (select auth.uid()))\n_12\n)\n_12\n);\n`\n\nUnder the hood, <code>auth.uid()</code> references <code>current_setting('request.jwt.claim.sub')</code> which corresponds to the JWT's <code>sub</code> (subject) claim. This setting is automatically set at the beginning of each request to the REST API.\n\nAll subsequent queries will inherit the permission of that user:\n\n`\n_10\n-- Only document sections owned by the user are returned\n_10\nselect *\n_10\nfrom document_sections\n_10\nwhere document_sections.embedding <#> embedding < -match_threshold\n_10\norder by document_sections.embedding <#> embedding;\n`\nThere are endless approaches to this problem based on the complexities of each system. Luckily Postgres comes with all the primitives needed to provide access control in the way that works best for your project.\n\nIf the examples above didn't fit your use case or you need to adjust them slightly to better fit your existing system, feel free to reach out to [support](https://supabase.com/dashboard/support/new) and we'll be happy to assist you.\n",
      "overlap_text": {
        "previous_chunk_id": "b3d9be74-86ff-4aca-a2dd-7038d7d3d72d",
        "text": " completely and simply filter by user within the <code>where</code> clause. Though this will work, we recommend RLS as a general best practice since RLS is always applied even as new queries and application logic is introduced in the future.\n"
      }
    }
  },
  {
    "chunk_id": "517f0e5d-62e7-4810-833e-8e8ed9ca6844",
    "metadata": {
      "token_count": 485,
      "source_url": "https://supabase.com/docs/guides/ai/langchain",
      "page_title": "LangChain | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "LangChain",
        "h2": "Usage \\#",
        "h3": ""
      },
      "text": "AI & Vectors\n[LangChain](https://langchain.com/) is a popular framework for working with AI, Vectors, and embeddings. LangChain supports using Supabase as a [vector store](https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/supabase), using the <code>pgvector</code> extension.\nPrepare you database with the relevant tables:\n\nDashboardSQL\n\n1. Go to the [SQL Editor](https://supabase.com/dashboard/project/_/sql) page in the Dashboard.\n2. Click **LangChain** in the Quick start section.\n3. Click **Run**.\nYou can now search your documents using any Node.js application. This is intended to be run on a secure server route.\n\n``\n_28\nimport { SupabaseVectorStore } from 'langchain/vectorstores/supabase'\n_28\nimport { OpenAIEmbeddings } from 'langchain/embeddings/openai'\n_28\nimport { createClient } from '@supabase/supabase-js'\n_28\n_28\nconst supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY\n_28\nif (!supabaseKey) throw new Error(<code>Expected SUPABASE_SERVICE_ROLE_KEY</code>)\n_28\n_28\nconst url = process.env.SUPABASE_URL\n_28\nif (!url) throw new Error(<code>Expected env var SUPABASE_URL</code>)\n_28\n_28\nexport const run = async () => {\n_28\nconst client = createClient(url, supabaseKey)\n_28\n_28\nconst vectorStore = await SupabaseVectorStore.fromTexts(\n_28\n    ['Hello world', 'Bye bye', \"What's this?\"],\n_28\n    [{ id: 2 }, { id: 1 }, { id: 3 }],\n_28\n    new OpenAIEmbeddings(),\n_28\n    {\n_28\n      client,\n_28\n      tableName: 'documents',\n_28\n      queryName: 'match_documents',\n_28\n    }\n_28\n)\n_28\n_28\nconst resultOne = await vectorStore.similaritySearch('Hello world', 1)\n_28\n_28\nconsole.log(resultOne)\n_28\n}\n``\n"
    }
  },
  {
    "chunk_id": "4c7093a0-3628-4d97-9be4-fd0d59d98087",
    "metadata": {
      "token_count": 465,
      "source_url": "https://supabase.com/docs/guides/ai/langchain",
      "page_title": "LangChain | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "LangChain",
        "h2": "Usage \\#",
        "h3": "Simple metadata filtering \\#"
      },
      "text": "Given the above <code>match_documents</code> Postgres function, you can also pass a filter parameter to only return documents with a specific metadata field value. This filter parameter is a JSON object, and the <code>match_documents</code> function will use the Postgres JSONB Containment operator <code>@></code> to filter documents by the metadata field values you specify. See details on the [Postgres JSONB Containment operator](https://www.postgresql.org/docs/current/datatype-json.html#JSON-CONTAINMENT) for more information.\n\n``\n_32\nimport { SupabaseVectorStore } from 'langchain/vectorstores/supabase'\n_32\nimport { OpenAIEmbeddings } from 'langchain/embeddings/openai'\n_32\nimport { createClient } from '@supabase/supabase-js'\n_32\n_32\n// First, follow set-up instructions above\n_32\n_32\nconst privateKey = process.env.SUPABASE_SERVICE_ROLE_KEY\n_32\nif (!privateKey) throw new Error(<code>Expected env var SUPABASE_SERVICE_ROLE_KEY</code>)\n_32\n_32\nconst url = process.env.SUPABASE_URL\n_32\nif (!url) throw new Error(<code>Expected env var SUPABASE_URL</code>)\n_32\n_32\nexport const run = async () => {\n_32\nconst client = createClient(url, privateKey)\n_32\n_32\nconst vectorStore = await SupabaseVectorStore.fromTexts(\n_32\n    ['Hello world', 'Hello world', 'Hello world'],\n_32\n    [{ user_id: 2 }, { user_id: 1 }, { user_id: 3 }],\n_32\n    new OpenAIEmbeddings(),\n_32\n    {\n_32\n      client,\n_32\n      tableName: 'documents',\n_32\n      queryName: 'match_documents',\n_32\n    }\n_32\n)\n_32\n_32\nconst result = await vectorStore.similaritySearch('Hello world', 1, {\n_32\n    user_id: 3,\n_32\n})\n_32\n_32\nconsole.log(result)\n_32\n}\n``\n",
      "overlap_text": {
        "previous_chunk_id": "517f0e5d-62e7-4810-833e-8e8ed9ca6844",
        "text": "_documents',\n_28\n    }\n_28\n)\n_28\n_28\nconst resultOne = await vectorStore.similaritySearch('Hello world', 1)\n_28\n_28\nconsole.log(resultOne)\n_28\n}\n``\n"
      }
    }
  },
  {
    "chunk_id": "6af7cb39-858e-40da-a26b-62eab90f22b7",
    "metadata": {
      "token_count": 800,
      "source_url": "https://supabase.com/docs/guides/ai/langchain",
      "page_title": "LangChain | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "LangChain",
        "h2": "Usage \\#",
        "h3": "Advanced metadata filtering \\#"
      },
      "text": "You can also use query builder-style filtering ( [similar to how the Supabase JavaScript library works](https://supabase.com/docs/reference/javascript/using-filters)) instead of passing an object. Note that since the filter properties will be in the metadata column, you need to use arrow operators ( <code>-></code> for integer or <code>->></code> for text) as defined in [Postgrest API documentation](https://postgrest.org/en/stable/references/api/tables_views.html?highlight=operators#json-columns) and specify the data type of the property (e.g. the column should look something like <code>metadata->some_int_value::int</code>).\n\n``\n_62\nimport { SupabaseFilterRPCCall, SupabaseVectorStore } from 'langchain/vectorstores/supabase'\n_62\nimport { OpenAIEmbeddings } from 'langchain/embeddings/openai'\n_62\nimport { createClient } from '@supabase/supabase-js'\n_62\n_62\n// First, follow set-up instructions above\n_62\n_62\nconst privateKey = process.env.SUPABASE_SERVICE_ROLE_KEY\n_62\nif (!privateKey) throw new Error(<code>Expected env var SUPABASE_SERVICE_ROLE_KEY</code>)\n_62\n_62\nconst url = process.env.SUPABASE_URL\n_62\nif (!url) throw new Error(<code>Expected env var SUPABASE_URL</code>)\n_62\n_62\nexport const run = async () => {\n_62\nconst client = createClient(url, privateKey)\n_62\n_62\nconst embeddings = new OpenAIEmbeddings()\n_62\n_62\nconst store = new SupabaseVectorStore(embeddings, {\n_62\n    client,\n_62\n    tableName: 'documents',\n_62\n})\n_62\n_62\nconst docs = [\\\n_62\\\n    {\\\n_62\\\n      pageContent:\\\n_62\\\n        'This is a long text, but it actually means something because vector database does not understand Lorem Ipsum. So I would need to expand upon the notion of quantum fluff, a theoretical concept where subatomic particles coalesce to form transient multidimensional spaces. Yet, this abstraction holds no real-world application or comprehensible meaning, reflecting a cosmic puzzle.',\\\n_62\\\n      metadata: { b: 1, c: 10, stuff: 'right' },\\\n_62\\\n    },\\\n_62\\\n    {\\\n_62\\\n      pageContent:\\\n_62\\\n        'This is a long text, but it actually means something because vector database does not understand Lorem Ipsum. So I would need to proceed by discussing the echo of virtual tweets in the binary corridors of the digital universe. Each tweet, like a pixelated canary, hums in an unseen frequency, a fascinatingly perplexing phenomenon that, while conjuring vivid imagery, lacks any concrete implication or real-world relevance, portraying a paradox of multidimensional spaces in the age of cyber folklore.',\\\n_62\\\n      metadata: { b: 2, c: 9, stuff: 'right' },\\\n_62\\\n    },\\\n_62\\\n    { pageContent: 'hello', metadata: { b: 1, c: 9, stuff: 'right' } },\\\n_62\\\n    { pageContent: 'hello', metadata: { b: 1, c: 9, stuff: 'wrong' } },\\\n_62\\\n    { pageContent: 'hi', metadata: { b: 2, c: 8, stuff: 'right' } },\\\n_62\\\n    { pageContent: 'bye', metadata: { b: 3, c: 7, stuff: 'right' } },\\\n_62\\\n",
      "overlap_text": {
        "previous_chunk_id": "4c7093a0-3628-4d97-9be4-fd0d59d98087",
        "text": "\n_32\nconst result = await vectorStore.similaritySearch('Hello world', 1, {\n_32\n    user_id: 3,\n_32\n})\n_32\n_32\nconsole.log(result)\n_32\n}\n``\n"
      }
    }
  },
  {
    "chunk_id": "da26c13c-5baf-4e09-992e-74c9f615ed8f",
    "metadata": {
      "token_count": 460,
      "source_url": "https://supabase.com/docs/guides/ai/langchain",
      "page_title": "LangChain | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "LangChain",
        "h2": "Resources \\#",
        "h3": "Advanced metadata filtering \\#"
      },
      "text": "    { pageContent: \"what's this\", metadata: { b: 4, c: 6, stuff: 'right' } },\\\n_62\\\n]\n_62\n_62\nawait store.addDocuments(docs)\n_62\n_62\nconst funcFilterA: SupabaseFilterRPCCall = (rpc) =>\n_62\n    rpc\n_62\n      .filter('metadata->b::int', 'lt', 3)\n_62\n      .filter('metadata->c::int', 'gt', 7)\n_62\n      .textSearch('content', <code>'multidimensional' & 'spaces'</code>, {\n_62\n        config: 'english',\n_62\n      })\n_62\n_62\nconst resultA = await store.similaritySearch('quantum', 4, funcFilterA)\n_62\n_62\nconst funcFilterB: SupabaseFilterRPCCall = (rpc) =>\n_62\n    rpc\n_62\n      .filter('metadata->b::int', 'lt', 3)\n_62\n      .filter('metadata->c::int', 'gt', 7)\n_62\n      .filter('metadata->>stuff', 'eq', 'right')\n_62\n_62\nconst resultB = await store.similaritySearch('hello', 2, funcFilterB)\n_62\n_62\nconsole.log(resultA, resultB)\n_62\n}\n``\nLangChain supports the concept of a hybrid search, which combines Similarity Search with Full Text Search. Read the official docs to get started: [Supabase Hybrid Search](https://js.langchain.com/docs/modules/indexes/retrievers/supabase-hybrid).\n\nYou can install the LangChain Hybrid Search function though our [database.dev package manager](https://database.dev/langchain/hybrid_search).\n- Official [LangChain site](https://langchain.com/).\n- Official [LangChain docs](https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/supabase).\n- Supabase [Hybrid Search](https://js.langchain.com/docs/modules/indexes/retrievers/supabase-hybrid).\n",
      "overlap_text": {
        "previous_chunk_id": "6af7cb39-858e-40da-a26b-62eab90f22b7",
        "text": "2, c: 8, stuff: 'right' } },\\\n_62\\\n    { pageContent: 'bye', metadata: { b: 3, c: 7, stuff: 'right' } },\\\n_62\\\n"
      }
    }
  },
  {
    "chunk_id": "acf21900-c3fe-4e64-92ff-55ee815d95f6",
    "metadata": {
      "token_count": 728,
      "source_url": "https://supabase.com/docs/guides/ai/vector-columns",
      "page_title": "Vector columns | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Vector columns",
        "h2": "Usage \\#",
        "h3": "Storing a vector / embedding \\#"
      },
      "text": "AI & Vectors\nSupabase offers a number of different ways to store and query vectors within Postgres. The SQL included in this guide is applicable for clients in all programming languages. If you are a Python user see your [Python client options](/docs/guides/ai/python-clients) after reading the <code>Learn</code> section.\n\nVectors in Supabase are enabled via [pgvector](https://github.com/pgvector/pgvector/), a PostgreSQL extension for storing and querying vectors in Postgres. It can be used to store [embeddings](/docs/guides/ai/concepts#what-are-embeddings).\nDashboardSQL\n\n1. Go to the [Database](https://supabase.com/dashboard/project/_/database/tables) page in the Dashboard.\n2. Click on **Extensions** in the sidebar.\n3. Search for \"vector\" and enable the extension.\nAfter enabling the <code>vector</code> extension, you will get access to a new data type called <code>vector</code>. The size of the vector (indicated in parenthesis) represents the number of dimensions stored in that vector.\n\n`\n_10\ncreate table documents (\n_10\nid serial primary key,\n_10\ntitle text not null,\n_10\nbody text not null,\n_10\nembedding vector(384)\n_10\n);\n`\n\nIn the above SQL snippet, we create a <code>documents</code> table with a column called <code>embedding</code> (note this is just a regular Postgres column - you can name it whatever you like). We give the <code>embedding</code> column a <code>vector</code> data type with 384 dimensions. Change this to the number of dimensions produced by your embedding model. For example, if you are [generating embeddings](/docs/guides/ai/quickstarts/generate-text-embeddings) using the open source [<code>gte-small</code>](https://huggingface.co/Supabase/gte-small) model, you would set this number to 384 since that model produces 384 dimensions.\n\nIn general, embeddings with fewer dimensions perform best. See our [analysis on fewer dimensions in pgvector](https://supabase.com/blog/fewer-dimensions-are-better-pgvector).\nIn this example we'll generate a vector using Transformers.js, then store it in the database using the Supabase JavaScript client.\n\n`\n_21\nimport { pipeline } from '@xenova/transformers'\n_21\nconst generateEmbedding = await pipeline('feature-extraction', 'Supabase/gte-small')\n_21\n_21\nconst title = 'First post!'\n_21\nconst body = 'Hello world!'\n_21\n_21\n// Generate a vector using Transformers.js\n_21\nconst output = await generateEmbedding(body, {\n_21\npooling: 'mean',\n_21\nnormalize: true,\n_21\n})\n_21\n_21\n// Extract the embedding output\n_21\nconst embedding = Array.from(output.data)\n_21\n_21\n// Store the vector in Postgres\n_21\nconst { data, error } = await supabase.from('documents').insert({\n_21\ntitle,\n_21\nbody,\n_21\nembedding,\n_21\n})\n`\n\nThis example uses the JavaScript Supabase client, but you can modify it to work with any [supported language library](/docs#client-libraries).\n"
    }
  },
  {
    "chunk_id": "add31619-f285-4d30-b385-2c61255581ab",
    "metadata": {
      "token_count": 734,
      "source_url": "https://supabase.com/docs/guides/ai/vector-columns",
      "page_title": "Vector columns | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Vector columns",
        "h2": "Usage \\#",
        "h3": "Querying a vector / embedding \\#"
      },
      "text": "Similarity search is the most common use case for vectors. <code>pgvector</code> support 3 new operators for computing distance:\n\n| Operator | Description |\n| --- | --- |\n| <code><-></code> | Euclidean distance |\n| <code><#></code> | negative inner product |\n| <code><=></code> | cosine distance |\n\nChoosing the right operator depends on your needs. Dot product tends to be the fastest if your vectors are normalized. For more information on how embeddings work and how they relate to each other, see [What are Embeddings?](/docs/guides/ai/concepts#what-are-embeddings).\n\nSupabase client libraries like <code>supabase-js</code> connect to your Postgres instance via [PostgREST](/docs/guides/getting-started/architecture#postgrest-api). PostgREST does not currently support <code>pgvector</code> similarity operators, so we'll need to wrap our query in a Postgres function and call it via the <code>rpc()</code> method:\n\n`\n_23\ncreate or replace function match_documents (\n_23\nquery_embedding vector(384),\n_23\nmatch_threshold float,\n_23\nmatch_count int\n_23\n)\n_23\nreturns table (\n_23\nid bigint,\n_23\ntitle text,\n_23\nbody text,\n_23\nsimilarity float\n_23\n)\n_23\nlanguage sql stable\n_23\nas $$\n_23\nselect\n_23\n    documents.id,\n_23\n    documents.title,\n_23\n    documents.body,\n_23\n    1 - (documents.embedding <=> query_embedding) as similarity\n_23\nfrom documents\n_23\nwhere 1 - (documents.embedding <=> query_embedding) > match_threshold\n_23\norder by (documents.embedding <=> query_embedding) asc\n_23\nlimit match_count;\n_23\n$$;\n`\n\nThis function takes a <code>query_embedding</code> argument and compares it to all other embeddings in the <code>documents</code> table. Each comparison returns a similarity score. If the similarity is greater than the <code>match_threshold</code> argument, it is returned. The number of rows returned is limited by the <code>match_count</code> argument.\n\nFeel free to modify this method to fit the needs of your application. The <code>match_threshold</code> ensures that only documents that have a minimum similarity to the <code>query_embedding</code> are returned. Without this, you may end up returning documents that subjectively don't match. This value will vary for each application - you will need to perform your own testing to determine the threshold that makes sense for your app.\n\nIf you index your vector column, ensure that the <code>order by</code> sorts by the distance function directly (rather than sorting by the calculated <code>similarity</code> column, which may lead to the index being ignored and poor performance).\n\nTo execute the function from your client library, call <code>rpc()</code> with the name of your Postgres function:\n\n`\n_10\nconst { data: documents } = await supabaseClient.rpc('match_documents', {\n_10\nquery_embedding: embedding, // Pass the embedding you want to compare\n_10\nmatch_threshold: 0.78, // Choose an appropriate threshold for your data\n_10\nmatch_count: 10, // Choose the number of matches\n_10\n})\n`\n\n",
      "overlap_text": {
        "previous_chunk_id": "acf21900-c3fe-4e64-92ff-55ee815d95f6",
        "text": "21\ntitle,\n_21\nbody,\n_21\nembedding,\n_21\n})\n`\n\nThis example uses the JavaScript Supabase client, but you can modify it to work with any [supported language library](/docs#client-libraries).\n"
      }
    }
  },
  {
    "chunk_id": "1cb2ce0f-2305-496e-9b98-d16a6a7bc278",
    "metadata": {
      "token_count": 191,
      "source_url": "https://supabase.com/docs/guides/ai/vector-columns",
      "page_title": "Vector columns | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Vector columns",
        "h2": "Usage \\#",
        "h3": "Indexes \\#"
      },
      "text": "In this example <code>embedding</code> would be another embedding you wish to compare against your table of pre-generated embedding documents. For example if you were building a search engine, every time the user submits their query you would first generate an embedding on the search query itself, then pass it into the above <code>rpc()</code> function to match.\n\nBe sure to use embeddings produced from the same embedding model when calculating distance. Comparing embeddings from two different models will produce no meaningful result.\n\nVectors and embeddings can be used for much more than search. Learn more about embeddings at [What are Embeddings?](/docs/guides/ai/concepts#what-are-embeddings).\nOnce your vector table starts to grow, you will likely want to add an index to speed up queries. See [Vector indexes](/docs/guides/ai/vector-indexes) to learn how vector indexes work and how to create them.\n",
      "overlap_text": {
        "previous_chunk_id": "add31619-f285-4d30-b385-2c61255581ab",
        "text": " // Pass the embedding you want to compare\n_10\nmatch_threshold: 0.78, // Choose an appropriate threshold for your data\n_10\nmatch_count: 10, // Choose the number of matches\n_10\n})\n`\n\n"
      }
    }
  },
  {
    "chunk_id": "4196c4f3-a6c6-4989-a91f-8210d6b38156",
    "metadata": {
      "token_count": 610,
      "source_url": "https://supabase.com/docs/guides/ai/hugging-face",
      "page_title": "Hugging Face Inference API | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Hugging Face Inference API",
        "h2": "Access token \\#",
        "h3": "Audio \\#"
      },
      "text": "AI & Vectors\n[Hugging Face](https://huggingface.co) is an open source hub for AI/ML models and tools. With over 100,000 machine learning models available, Hugging Face provides a great way to integrate specialized AI & ML tasks into your application.\n\nThere are 3 ways to use Hugging Face models in your application:\n\n1. Use the [Transformers](https://huggingface.co/docs/transformers/index) Python library to perform inference in a Python backend.\n2. [Generate embeddings](/docs/guides/ai/quickstarts/generate-text-embeddings) directly in Edge Functions using Transformers.js.\n3. Use Hugging Face's hosted [Inference API](https://huggingface.co/inference-api) to execute AI tasks remotely on Hugging Face servers. This guide will walk you through this approach.\nBelow are some of the types of tasks you can perform with Hugging Face:\n- [Summarization](https://huggingface.co/tasks/summarization)\n- [Text classification](https://huggingface.co/tasks/text-classification)\n- [Text generation](https://huggingface.co/tasks/text-generation)\n- [Translation](https://huggingface.co/tasks/translation)\n- [Fill in the blank](https://huggingface.co/tasks/fill-mask)\n- [Image to text](https://huggingface.co/tasks/image-to-text)\n- [Text to image](https://huggingface.co/tasks/text-to-image)\n- [Image classification](https://huggingface.co/tasks/image-classification)\n- [Video classification](https://huggingface.co/tasks/video-classification)\n- [Object detection](https://huggingface.co/tasks/object-detection)\n- [Image segmentation](https://huggingface.co/tasks/image-segmentation)\n- [Text to speech](https://huggingface.co/tasks/text-to-speech)\n- [Speech to text](https://huggingface.co/tasks/automatic-speech-recognition)\n- [Audio classification](https://huggingface.co/tasks/audio-classification)\n\nSee a [full list of tasks](https://huggingface.co/tasks).\nFirst generate a Hugging Face access token for your app:\n\n[https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n\nName your token based on the app its being used for and the environment. For example, if you are building an image generation app you might create 2 tokens:\n\n- \"My Image Generator (Dev)\"\n- \"My Image Generator (Prod)\"\n\nSince we will be using this token for the inference API, choose the <code>read</code> role.\n\nThough it is possible to use the Hugging Face inference API today without an access token, [you may be rate limited](https://huggingface.co/docs/huggingface.js/inference/README#usage).\n\nTo ensure you don't experience any unexpected downtime or errors, we recommend creating an access token.\n"
    }
  },
  {
    "chunk_id": "06e3d159-aec4-467a-b669-6cd4b280d021",
    "metadata": {
      "token_count": 795,
      "source_url": "https://supabase.com/docs/guides/ai/hugging-face",
      "page_title": "Hugging Face Inference API | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Hugging Face Inference API",
        "h2": "Edge Functions \\#",
        "h3": ""
      },
      "text": "Edge Functions are server-side TypeScript functions that run on-demand. Since Edge Functions run on a server, you can safely give them access to your Hugging Face access token.\n\nYou will need the <code>supabase</code> CLI [installed](/docs/guides/cli) for the following commands to work.\n\nTo create a new Edge Function, navigate to your local project and initialize Supabase if you haven't already:\n\n`\n_10\nsupabase init\n`\n\nThen create an Edge Function:\n\n`\n_10\nsupabase functions new text-to-image\n`\n\nCreate a file called <code>.env.local</code> to store your Hugging Face access token:\n\n`\n_10\nHUGGING_FACE_ACCESS_TOKEN=<your-token-here>\n`\n\nLet's modify the Edge Function to import Hugging Face's inference client and perform a <code>text-to-image</code> request:\n\n`\n_20\nimport { serve } from 'https://deno.land/std@0.168.0/http/server.ts'\n_20\nimport { HfInference } from 'https://esm.sh/@huggingface/inference@2.3.2'\n_20\n_20\nconst hf = new HfInference(Deno.env.get('HUGGING_FACE_ACCESS_TOKEN'))\n_20\n_20\nserve(async (req) => {\n_20\nconst { prompt } = await req.json()\n_20\n_20\nconst image = await hf.textToImage(\n_20\n    {\n_20\n      inputs: prompt,\n_20\n      model: 'stabilityai/stable-diffusion-2',\n_20\n    },\n_20\n    {\n_20\n      use_cache: false,\n_20\n    }\n_20\n)\n_20\n_20\nreturn new Response(image)\n_20\n})\n`\n\n1. This function creates a new instance of <code>HfInference</code> using the <code>HUGGING_FACE_ACCESS_TOKEN</code> environment variable.\n\n2. It expects a POST request that includes a JSON request body. The JSON body should include a parameter called <code>prompt</code> that represents the text-to-image prompt that we will pass to Hugging Face's inference API.\n\n3. Next we call <code>textToImage()</code>, passing in the user's prompt along with the model that we would like to use for the image generation. Today Hugging Face recommends <code>stabilityai/stable-diffusion-2</code>, but you can change this to any other text-to-image model. You can see a list of which models are supported for each task by navigating to their [models page](https://huggingface.co/models?pipeline_tag=text-to-image) and filtering by task.\n\n4. We set <code>use_cache</code> to <code>false</code> so that repeat queries with the same prompt will produce new images. If the task and model you are using is deterministic (will always produce the same result based on the same input), consider setting <code>use_cache</code> to <code>true</code> for faster responses.\n\n5. The <code>image</code> result returned from the API will be a <code>Blob</code>. We can pass the <code>Blob</code> directly into a <code>new Response()</code> which will automatically set the content type and body of the response from the <code>image</code>.\n\nFinally let's serve the Edge Function locally to test it:\n\n`\n_10\nsupabase functions serve --env-file .env.local --no-verify-jwt\n`\n\nRemember to pass in the <code>.env.local</code> file using the <code>--env-file</code> parameter so that the Edge Function can access the <code>HUGGING_FACE_ACCESS_TOKEN</code>.\n\n",
      "overlap_text": {
        "previous_chunk_id": "4196c4f3-a6c6-4989-a91f-8210d6b38156",
        "text": " today without an access token, [you may be rate limited](https://huggingface.co/docs/huggingface.js/inference/README#usage).\n\nTo ensure you don't experience any unexpected downtime or errors, we recommend creating an access token.\n"
      }
    }
  },
  {
    "chunk_id": "51ddf97d-c14b-4df6-8243-d48cf37da2b6",
    "metadata": {
      "token_count": 312,
      "source_url": "https://supabase.com/docs/guides/ai/hugging-face",
      "page_title": "Hugging Face Inference API | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Hugging Face Inference API",
        "h2": "Resources \\#",
        "h3": ""
      },
      "text": "For demo purposes we set <code>--no-verify-jwt</code> to make it easy to test the Edge Function without passing in a JWT token. In a real application you will need to pass the JWT as a <code>Bearer</code> token in the <code>Authorization</code> header.\n\nAt this point, you can make an API request to your Edge Function using your preferred frontend framework (Next.js, React, Expo, etc). We can also test from the terminal using <code>curl</code>:\n\n`\n_10\ncurl --output result.jpg --location --request POST 'http://localhost:54321/functions/v1/text-to-image' \\\n_10\n  --header 'Content-Type: application/json' \\\n_10\n  --data '{\"prompt\":\"Llama wearing sunglasses\"}'\n`\n\nIn this example, your generated image will save to <code>result.jpg</code>:\n\n![Llama wearing sunglasses example](https://supabase.com/docs/img/ai/hugging-face/llama-sunglasses-example.png)\nYou can now create an Edge Function that invokes a Hugging Face task using your model of choice.\n\nTry running some other [AI tasks](#ai-tasks).\n- Official [Hugging Face site](https://huggingface.co/).\n- Official [Hugging Face JS docs](https://huggingface.co/docs/huggingface.js).\n- [Generate image captions](/docs/guides/ai/examples/huggingface-image-captioning) using Hugging Face.\n",
      "overlap_text": {
        "previous_chunk_id": "06e3d159-aec4-467a-b669-6cd4b280d021",
        "text": "-verify-jwt\n`\n\nRemember to pass in the <code>.env.local</code> file using the <code>--env-file</code> parameter so that the Edge Function can access the <code>HUGGING_FACE_ACCESS_TOKEN</code>.\n\n"
      }
    }
  },
  {
    "chunk_id": "a0d2dc31-5de9-487f-81b4-1a1755e2e971",
    "metadata": {
      "token_count": 838,
      "source_url": "https://supabase.com/docs/guides/ai/examples/mixpeek-video-search",
      "page_title": "Video Search with Mixpeek Multimodal Embeddings | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Generate embedding using Mixpeek",
        "h2": "Create embeddings for your videos \\#",
        "h3": ""
      },
      "text": "AI & Vectors\nThe [Mixpeek Embed API](https://docs.mixpeek.com/api-documentation/inference/embed) allows you to generate embeddings for various types of content, including videos and text. You can use these embeddings for:\n\n- Text-to-Video / Video-To-Text / Video-to-Video / Text-to-Text Search\n- Fine-tuning on your own video and text data\n\nThis guide demonstrates how to implement video search using Mixpeek Embed for video processing and embedding, and Supabase Vector for storing and querying embeddings.\n\nYou can find the full application code as a Python Poetry project on [GitHub](https://github.com/yourusername/your-repo-name).\n[Poetry](https://python-poetry.org/) provides packaging and dependency management for Python. If you haven't already, install poetry via pip:\n\n`\n_10\npip install poetry\n`\n\nThen initialize a new project:\n\n`\n_10\npoetry new video-search\n`\nIf you haven't already, [install the Supabase CLI](https://supabase.com/docs/guides/cli), then initialize Supabase in the root of your newly created poetry project:\n\n`\n_10\nsupabase init\n`\n\nNext, start your local Supabase stack:\n\n`\n_10\nsupabase start\n`\n\nThis will start up the Supabase stack locally and print out a bunch of environment details, including your local <code>DB URL</code>. Make a note of that for later use.\nAdd the following dependencies to your project:\n\n- [<code>supabase</code>](https://github.com/supabase-community/supabase-py): Supabase Python Client\n- [<code>mixpeek</code>](https://github.com/mixpeek/python-client): Mixpeek Python Client for embedding generation\n\n`\n_10\npoetry add supabase mixpeek\n`\nAt the top of your main Python script, import the dependencies and store your environment variables:\n\n`\n_10\nfrom supabase import create_client, Client\n_10\nfrom mixpeek import Mixpeek\n_10\nimport os\n_10\n_10\nSUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n_10\nSUPABASE_KEY = os.getenv(\"SUPABASE_API_KEY\")\n_10\nMIXPEEK_API_KEY = os.getenv(\"MIXPEEK_API_KEY\")\n`\nNext, create a <code>seed</code> method, which will create a new Supabase table, generate embeddings for your video chunks, and insert the embeddings into your database:\n\n`\n_46\ndef seed():\n_46\n_46\n    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n_46\n    mixpeek = Mixpeek(MIXPEEK_API_KEY)\n_46\n_46\n_46\n    supabase.table(\"video_chunks\").create({\n_46\n        \"id\": \"text\",\n_46\n        \"start_time\": \"float8\",\n_46\n        \"end_time\": \"float8\",\n_46\n        \"embedding\": \"vector(768)\",\n_46\n        \"metadata\": \"jsonb\"\n_46\n    })\n_46\n_46\n_46\n    video_url = \"https://example.com/your_video.mp4\"\n_46\n    processed_chunks = mixpeek.tools.video.process(\n_46\n        video_source=video_url,\n_46\n        chunk_interval=1,  # 1 second intervals\n_46\n        resolution=[720, 1280]\n_46\n    )\n_46\n_46\n    for chunk in processed_chunks:\n_46\n        print(f\"Processing video chunk: {chunk['start_time']}\")\n_46\n_46\n_46\n        embed_response = mixpeek.embed.video(\n_46\n            model_id=\"vuse-generic-v1\",\n_46\n            input=chunk['base64_chunk'],\n_46\n            input_type=\"base64\"\n_46\n        )\n_46\n_46\n"
    }
  },
  {
    "chunk_id": "1cf36a98-3854-483e-8748-dbe6223836f1",
    "metadata": {
      "token_count": 743,
      "source_url": "https://supabase.com/docs/guides/ai/examples/mixpeek-video-search",
      "page_title": "Video Search with Mixpeek Multimodal Embeddings | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Display the results",
        "h2": "Conclusion \\#",
        "h3": ""
      },
      "text": "_46\n        supabase.table(\"video_chunks\").insert({\n_46\n            \"id\": f\"chunk_{chunk['start_time']}\",\n_46\n            \"start_time\": chunk[\"start_time\"],\n_46\n            \"end_time\": chunk[\"end_time\"],\n_46\n            \"embedding\": embed_response['embedding'],\n_46\n            \"metadata\": {\"video_url\": video_url}\n_46\n        }).execute()\n_46\n_46\n    print(\"Video processed and embeddings inserted\")\n_46\n_46\n_46\n    supabase.query(\"CREATE INDEX ON video_chunks USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100)\").execute()\n_46\n    print(\"Created index\")\n`\n\nAdd this method as a script in your <code>pyproject.toml</code> file:\n\n`\n_10\n[tool.poetry.scripts]\n_10\nseed = \"video_search.main:seed\"\n_10\nsearch = \"video_search.main:search\"\n`\n\nAfter activating the virtual environment with <code>poetry shell</code>, you can now run your seed script via <code>poetry run seed</code>. You can inspect the generated embeddings in your local database by visiting the local Supabase dashboard at [localhost:54323](http://localhost:54323/project/default/editor).\nWith Supabase Vector, you can query your embeddings. You can use either a video clip as search input or alternatively, you can generate an embedding from a string input and use that as the query input:\n\n`\n_32\ndef search():\n_32\n_32\n    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n_32\n    mixpeek = Mixpeek(MIXPEEK_API_KEY)\n_32\n_32\n_32\n    query_string = \"a car chase scene\"\n_32\n    text_emb = mixpeek.embed.video(\n_32\n        model_id=\"vuse-generic-v1\",\n_32\n        input=query_string,\n_32\n        input_type=\"text\"\n_32\n    )\n_32\n_32\n_32\n    results = supabase.rpc(\n_32\n        'match_video_chunks',\n_32\n        {\n_32\n            'query_embedding': text_emb['embedding'],\n_32\n            'match_threshold': 0.8,\n_32\n            'match_count': 5\n_32\n        }\n_32\n    ).execute()\n_32\n_32\n_32\n    if results.data:\n_32\n        for result in results.data:\n_32\n            print(f\"Matched chunk from {result['start_time']} to {result['end_time']} seconds\")\n_32\n            print(f\"Video URL: {result['metadata']['video_url']}\")\n_32\n            print(f\"Similarity: {result['similarity']}\")\n_32\n            print(\"---\")\n_32\n    else:\n_32\n        print(\"No matching video chunks found\")\n`\n\nThis query will return the top 5 most similar video chunks from your database.\n\nYou can now test it out by running <code>poetry run search</code>, and you will be presented with the most relevant video chunks to the query \"a car chase scene\".\nWith just a couple of Python scripts, you are able to implement video search as well as reverse video search using Mixpeek Embed and Supabase Vector. This approach allows for powerful semantic search capabilities that can be integrated into various applications, enabling you to search through video content using both text and video queries.\n",
      "overlap_text": {
        "previous_chunk_id": "a0d2dc31-5de9-487f-81b4-1a1755e2e971",
        "text": "peek.embed.video(\n_46\n            model_id=\"vuse-generic-v1\",\n_46\n            input=chunk['base64_chunk'],\n_46\n            input_type=\"base64\"\n_46\n        )\n_46\n_46\n"
      }
    }
  },
  {
    "chunk_id": "750b7cee-40cb-4fb3-8189-b17d197d4a5a",
    "metadata": {
      "token_count": 699,
      "source_url": "https://supabase.com/docs/guides/ai/vector-indexes/hnsw-indexes",
      "page_title": "HNSW indexes | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "HNSW indexes",
        "h2": "How does HNSW work? \\#",
        "h3": "Navigable Small World \\#"
      },
      "text": "AI & Vectors\nHNSW is an algorithm for approximate nearest neighbor search. It is a frequently used index type that can improve performance when querying highly-dimensional vectors, like those representing embeddings.\nThe way you create an HNSW index depends on the distance operator you are using. <code>pgvector</code> includes 3 distance operators:\n\n| Operator | Description | [**Operator class**](https://www.postgresql.org/docs/current/sql-createopclass.html) |\n| --- | --- | --- |\n| <code><-></code> | Euclidean distance | <code>vector_l2_ops</code> |\n| <code><#></code> | negative inner product | <code>vector_ip_ops</code> |\n| <code><=></code> | cosine distance | <code>vector_cosine_ops</code> |\n\nUse the following SQL commands to create an HNSW index for the operator(s) used in your queries.\n`\n_10\ncreate index on items using hnsw (column_name vector_l2_ops);\n`\n`\n_10\ncreate index on items using hnsw (column_name vector_ip_ops);\n`\n`\n_10\ncreate index on items using hnsw (column_name vector_cosine_ops);\n`\n\nCurrently vectors with up to 2,000 dimensions can be indexed.\nHNSW uses proximity graphs (graphs connecting nodes based on distance between them) to approximate nearest-neighbor search. To understand HNSW, we can break it down into 2 parts:\n\n- **Hierarchical (H):** The algorithm operates over multiple layers\n- **Navigable Small World (NSW):** Each vector is a node within a graph and is connected to several other nodes\nThe hierarchical aspect of HNSW builds off of the idea of skip lists.\n\nSkip lists are multi-layer linked lists. The bottom layer is a regular linked list connecting an ordered sequence of elements. Each new layer above removes some elements from the underlying layer (based on a fixed probability), producing a sparser subsequence that \u201cskips\u201d over elements.\n\nWhen searching for an element, the algorithm begins at the top layer and traverses its linked list horizontally. If the target element is found, the algorithm stops and returns it. Otherwise if the next element in the list is greater than the target (or <code>NULL</code>), the algorithm drops down to the next layer below. Since each layer below is less sparse than the layer above (with the bottom layer connecting all elements), the target will eventually be found. Skip lists offer O(log n) average complexity for both search and insertion/deletion.\nA navigable small world (NSW) is a special type of proximity graph that also includes long-range connections between nodes. These long-range connections support the \u201csmall world\u201d property of the graph, meaning almost every node can be reached from any other node within a few hops. Without these additional long-range connections, many hops would be required to reach a far-away node.\n\nThe \u201cnavigable\u201d part of NSW specifically refers to the ability to logarithmically scale the greedy search algorithm on the graph, an algorithm that attempts to make only the locally optimal choice at each hop. Without this property, the graph may still be considered a small world with short paths between far-away nodes, but the greedy algorithm tends to miss them. Greedy search is ideal for NSW because it is quick to navigate and has low computational costs.\n"
    }
  },
  {
    "chunk_id": "bdb0ffc5-1217-44db-bdcb-b70f131bccbb",
    "metadata": {
      "token_count": 248,
      "source_url": "https://supabase.com/docs/guides/ai/vector-indexes/hnsw-indexes",
      "page_title": "HNSW indexes | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "HNSW indexes",
        "h2": "Resources \\#",
        "h3": "**Hierarchical +** Navigable Small World \\#"
      },
      "text": "HNSW combines these two concepts. From the hierarchical perspective, the bottom layer consists of a NSW made up of short links between nodes. Each layer above \u201cskips\u201d elements and creates longer links between nodes further away from each other.\n\nJust like skip lists, search starts at the top layer and works its way down until it finds the target element. However, instead of comparing a scalar value at each layer to determine whether or not to descend to the layer below, a multi-dimensional distance measure (such as Euclidean distance) is used.\nHNSW should be your default choice when creating a vector index. Add the index when you don't need 100% accuracy and are willing to trade a small amount of accuracy for a lot of throughput.\n\nUnlike IVFFlat indexes, you are safe to build an HNSW index immediately after the table is created. HNSW indexes are based on graphs which inherently are not affected by the same limitations as IVFFlat. As new data is added to the table, the index will be filled automatically and the index structure will remain optimal.\nRead more about indexing on <code>pgvector</code>'s [GitHub page](https://github.com/pgvector/pgvector#indexing).\n",
      "overlap_text": {
        "previous_chunk_id": "750b7cee-40cb-4fb3-8189-b17d197d4a5a",
        "text": ". Without this property, the graph may still be considered a small world with short paths between far-away nodes, but the greedy algorithm tends to miss them. Greedy search is ideal for NSW because it is quick to navigate and has low computational costs.\n"
      }
    }
  },
  {
    "chunk_id": "d611becb-03f3-41f3-8fc5-62e8078b2a4d",
    "metadata": {
      "token_count": 259,
      "source_url": "https://supabase.com/docs/guides/ai/python-clients",
      "page_title": "Choosing a Client | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Choosing a Client",
        "h2": "",
        "h3": ""
      },
      "text": "AI & Vectors\nAs described in [Structured & Unstructured Embeddings](/docs/guides/ai/structured-unstructured), AI workloads come in many forms.\n\nFor data science or ephemeral workloads, the [Supabase Vecs](https://supabase.github.io/vecs/) client gets you started quickly. All you need is a connection string and vecs handles setting up your database to store and query vectors with associated metadata.\n\nYou can get your connection string from the [**Database Settings**](https://supabase.com/dashboard/project/_/settings/database) page in your dashboard. Make sure to check **Use connection pooling**, then copy the URI. Also, change the URI scheme from <code>postgres</code> to <code>postgresql</code>. <code>vecs</code> uses SQLAlchemy under the hood, which only supports <code>postgresql</code> as a dialect.\n\nFor production python applications with version controlled migrations, we recommend adding first class vector support to your toolchain by [registering the vector type with your ORM](https://github.com/pgvector/pgvector-python). pgvector provides bindings for the most commonly used SQL drivers/libraries including Django, SQLAlchemy, SQLModel, psycopg, asyncpg and Peewee.\n"
    }
  },
  {
    "chunk_id": "2a93e079-687b-458b-b9ca-c652287411ce",
    "metadata": {
      "token_count": 433,
      "source_url": "https://supabase.com/docs/guides/ai/examples/huggingface-image-captioning",
      "page_title": "Generate image captions using Hugging Face | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Generate image captions using Hugging Face",
        "h2": "Generate TypeScript types \\#",
        "h3": ""
      },
      "text": "AI & Vectors\nWe can combine Hugging Face with [Supabase Storage](https://supabase.com/storage) and [Database Webhooks](https://supabase.com/docs/guides/database/webhooks) to automatically caption for any image we upload to a storage bucket.\n[Hugging Face](https://huggingface.co/) is the collaboration platform for the machine learning community.\n\n[Huggingface.js](https://huggingface.co/docs/huggingface.js/index) provides a convenient way to make calls to 100,000+ Machine Learning models, making it easy to incorporate AI functionality into your [Supabase Edge Functions](https://supabase.com/edge-functions).\n- Open your Supabase project dashboard or [create a new project](https://supabase.com/dashboard/projects).\n- [Create a new bucket](https://supabase.com/dashboard/project/_/storage/buckets) called <code>images</code>.\n- Generate TypeScript types from remote Database.\n- Create a new Database table called <code>image_caption</code>.\n  - Create <code>id</code> column of type <code>uuid</code> which references <code>storage.objects.id</code>.\n  - Create a <code>caption</code> column of type <code>text</code>.\n- Regenerate TypeScript types to include new <code>image_caption</code> table.\n- Deploy the function to Supabase: <code>supabase functions deploy huggingface-image-captioning</code>.\n- Create the Database Webhook in the [Supabase Dashboard](https://supabase.com/dashboard/project/_/database/hooks) to trigger the <code>huggingface-image-captioning</code> function anytime a record is added to the <code>storage.objects</code> table.\nTo generate the types.ts file for the storage and public schemas, run the following command in the terminal:\n\n`\n_10\nsupabase gen types typescript --project-id=your-project-ref --schema=storage,public > supabase/functions/huggingface-image-captioning/types.ts\n`\n"
    }
  },
  {
    "chunk_id": "5aacaf24-d0f6-4362-b7c8-ee11e42b4f83",
    "metadata": {
      "token_count": 622,
      "source_url": "https://supabase.com/docs/guides/ai/examples/huggingface-image-captioning",
      "page_title": "Generate image captions using Hugging Face | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Generate image captions using Hugging Face",
        "h2": "Code \\#",
        "h3": ""
      },
      "text": "Find the complete code on [GitHub](https://github.com/supabase/supabase/tree/master/examples/edge-functions/supabase/functions/huggingface-image-captioning).\n\n``\n_49\nimport { serve } from 'https://deno.land/std@0.168.0/http/server.ts'\n_49\nimport { HfInference } from 'https://esm.sh/@huggingface/inference@2.3.2'\n_49\nimport { createClient } from 'jsr:@supabase/supabase-js@2'\n_49\nimport { Database } from './types.ts'\n_49\n_49\nconsole.log('Hello from <code>huggingface-image-captioning</code> function!')\n_49\n_49\nconst hf = new HfInference(Deno.env.get('HUGGINGFACE_ACCESS_TOKEN'))\n_49\n_49\ntype SoRecord = Database['storage']['Tables']['objects']['Row']\n_49\ninterface WebhookPayload {\n_49\ntype: 'INSERT' | 'UPDATE' | 'DELETE'\n_49\ntable: string\n_49\nrecord: SoRecord\n_49\nschema: 'public'\n_49\nold_record: null | SoRecord\n_49\n}\n_49\n_49\nserve(async (req) => {\n_49\nconst payload: WebhookPayload = await req.json()\n_49\nconst soRecord = payload.record\n_49\nconst supabaseAdminClient = createClient<Database>(\n_49\n    // Supabase API URL - env var exported by default when deployed.\n_49\n    Deno.env.get('SUPABASE_URL') ?? '',\n_49\n    // Supabase API SERVICE ROLE KEY - env var exported by default when deployed.\n_49\n    Deno.env.get('SUPABASE_SERVICE_ROLE_KEY') ?? ''\n_49\n)\n_49\n_49\n// Construct image url from storage\n_49\nconst { data, error } = await supabaseAdminClient.storage\n_49\n    .from(soRecord.bucket_id!)\n_49\n    .createSignedUrl(soRecord.path_tokens!.join('/'), 60)\n_49\nif (error) throw error\n_49\nconst { signedUrl } = data\n_49\n_49\n// Run image captioning with Huggingface\n_49\nconst imgDesc = await hf.imageToText({\n_49\n    data: await (await fetch(signedUrl)).blob(),\n_49\n    model: 'nlpconnect/vit-gpt2-image-captioning',\n_49\n})\n_49\n_49\n// Store image caption in Database table\n_49\nawait supabaseAdminClient\n_49\n    .from('image_caption')\n_49\n    .insert({ id: soRecord.id!, caption: imgDesc.generated_text })\n_49\n    .throwOnError()\n_49\n_49\nreturn new Response('ok')\n_49\n})\n``\n",
      "overlap_text": {
        "previous_chunk_id": "2a93e079-687b-458b-b9ca-c652287411ce",
        "text": " public schemas, run the following command in the terminal:\n\n`\n_10\nsupabase gen types typescript --project-id=your-project-ref --schema=storage,public > supabase/functions/huggingface-image-captioning/types.ts\n`\n"
      }
    }
  },
  {
    "chunk_id": "2fc30000-027a-4bbf-bb9a-59ffe1e89596",
    "metadata": {
      "token_count": 514,
      "source_url": "https://supabase.com/docs/guides/ai/google-colab",
      "page_title": "Google Colab | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "create vector store client",
        "h2": "Connect to your database \\#",
        "h3": ""
      },
      "text": "AI & Vectors\n[![](https://supabase.com/docs/img/ai/colab-badge.svg)](https://colab.research.google.com/github/supabase/supabase/blob/master/examples/ai/vector_hello_world.ipynb)\n\nGoogle Colab is a hosted Jupyter Notebook service. It provides free access to computing resources, including GPUs and TPUs, and is well-suited to machine learning, data science, and education. We can use Colab to manage collections using [Supabase Vecs](/docs/guides/ai/vecs-python-client).\n\nIn this tutorial we'll connect to a database running on the Supabase [platform](https://supabase.com/dashboard/). If you don't already have a database, you can create one here: [database.new](https://database.new).\nStart by visiting [colab.research.google.com](https://colab.research.google.com/). There you can create a new notebook.\n\n![Google Colab new notebook](https://supabase.com/docs/img/ai/google-colab/colab-new.png)\nWe'll use the Supabase Vector client, [Vecs](/docs/guides/ai/vecs-python-client), to manage our collections.\n\nAt the top of the notebook add the notebook paste the following code and hit the \"execute\" button ( <code>ctrl+enter</code>):\n\n`\n_10\npip install vecs\n`\n\n![Install vecs](https://supabase.com/docs/img/ai/google-colab/install-vecs.png)\nFind the Postgres pooler connection string for your Supabase project in the [database settings](https://supabase.com/dashboard/project/_/settings/database) of the dashboard. Copy the \"URI\" format, which should look something like <code>postgres://postgres.xxxx:password@xxxx.pooler.supabase.com:6543/postgres</code>\n\nCreate a new code block below the install block ( <code>ctrl+m b</code>) and add the following code using the Postgres URI you copied above:\n\n`\n_10\nimport vecs\n_10\n_10\nDB_CONNECTION = \"postgres://postgres.xxxx:password@xxxx.pooler.supabase.com:6543/postgres\"\n_10\n_10\n_10\nvx = vecs.create_client(DB_CONNECTION)\n`\n\nExecute the code block ( <code>ctrl+enter</code>). If no errors were returned then your connection was successful.\n"
    }
  },
  {
    "chunk_id": "ecd33bec-b9ab-4839-9657-98d1636a3ff0",
    "metadata": {
      "token_count": 585,
      "source_url": "https://supabase.com/docs/guides/ai/google-colab",
      "page_title": "Google Colab | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "create vector store client",
        "h2": "Resources \\#",
        "h3": ""
      },
      "text": "Now we're going to create a new collection and insert some documents.\n\nCreate a new code block below the install block ( <code>ctrl+m b</code>). Add the following code to the code block and execute it ( <code>ctrl+enter</code>):\n\n`\n_16\ncollection = vx.get_or_create_collection(name=\"colab_collection\", dimension=3)\n_16\n_16\ncollection.upsert(\n_16\n    vectors=[\\\n_16\\\n        (\\\n_16\\\n         \"vec0\",           # the vector's identifier\\\n_16\\\n         [0.1, 0.2, 0.3],  # the vector. list or np.array\\\n_16\\\n         {\"year\": 1973}    # associated  metadata\\\n_16\\\n        ),\\\n_16\\\n        (\\\n_16\\\n         \"vec1\",\\\n_16\\\n         [0.7, 0.8, 0.9],\\\n_16\\\n         {\"year\": 2012}\\\n_16\\\n        )\\\n_16\\\n    ]\n_16\n)\n`\n\nThis will create a table inside your database within the <code>vecs</code> schema, called <code>colab_collection</code>. You can view the inserted items in the [Table Editor](https://supabase.com/dashboard/project/_/editor/), by selecting the <code>vecs</code> schema from the schema dropdown.\n\n![Colab documents](https://supabase.com/docs/img/ai/google-colab/colab-documents.png)\nNow we can search for documents based on their similarity. Create a new code block and execute the following code:\n\n`\n_10\ncollection.query(\n_10\n    query_vector=[0.4,0.5,0.6],  # required\n_10\n    limit=5,                     # number of records to return\n_10\n    filters={},                  # metadata filters\n_10\n    measure=\"cosine_distance\",   # distance measure to use\n_10\n    include_value=False,         # should distance measure values be returned?\n_10\n    include_metadata=False,      # should record metadata be returned?\n_10\n)\n`\n\nYou will see that this returns two documents in an array <code>['vec1', 'vec0']</code>:\n\n![Colab results](https://supabase.com/docs/img/ai/google-colab/colab-results.png)\n\nIt also returns a warning:\n\n`\n_10\nQuery does not have a covering index for cosine_distance.\n`\n\nYou can lean more about creating indexes in the [Vecs documentation](https://supabase.github.io/vecs/api/#create-an-index).\n- Vecs API: [supabase.github.io/vecs/api](https://supabase.github.io/vecs/api)\n",
      "overlap_text": {
        "previous_chunk_id": "2fc30000-027a-4bbf-bb9a-59ffe1e89596",
        "text": ":6543/postgres\"\n_10\n_10\n_10\nvx = vecs.create_client(DB_CONNECTION)\n`\n\nExecute the code block ( <code>ctrl+enter</code>). If no errors were returned then your connection was successful.\n"
      }
    }
  },
  {
    "chunk_id": "31218dc8-d40e-4943-86a1-b7fca05b7983",
    "metadata": {
      "token_count": 551,
      "source_url": "https://supabase.com/docs/guides/ai/examples/headless-vector-search",
      "page_title": "Adding generative Q&A for your documentation | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Adding generative Q&A for your documentation",
        "h2": "Usage \\#",
        "h3": "Prepare your database \\#"
      },
      "text": "AI & Vectors\nSupabase provides a [Headless Search Toolkit](https://github.com/supabase/headless-vector-search) for adding \"Generative Q&A\" to your documentation. The toolkit is \"headless\", so that you can integrate it into your existing website and style it to match your website theme.\n\nYou can see how this works with the Supabase docs. Just hit <code>cmd+k</code> and \"ask\" for something like \"what are the features of supabase?\". You will see that the response is streamed back, using the information provided in the docs:\n\n![headless search](https://supabase.com/docs/img/ai/headless-search/headless.png)\n- Supabase: Database & Edge Functions.\n- OpenAI: Embeddings and completions.\n- GitHub Actions: for ingesting your markdown docs.\nThis toolkit consists of 2 parts:\n\n- The [Headless Vector Search](https://github.com/supabase/headless-vector-search) template which you can deploy in your own organization.\n- A [GitHub Action](https://github.com/supabase/embeddings-generator) which will ingest your markdown files, convert them to embeddings, and store them in your database.\nThere are 3 steps to build similarity search inside your documentation:\n\n1. Prepare your database.\n2. Ingest your documentation.\n3. Add a search interface.\nTo prepare, create a [new Supabase project](https://database.new) and store the database and API credentials, which you can find in the project [settings](https://supabase.com/dashboard/project/_/settings).\n\nNow we can use the [Headless Vector Search](https://github.com/supabase/headless-vector-search#set-up) instructions to set up the database:\n\n1. Clone the repo to your local machine: <code>git clone git@github.com:supabase/headless-vector-search.git</code>\n2. Link the repo to your remote project: <code>supabase link --project-ref XXX</code>\n3. Apply the database migrations: <code>supabase db push</code>\n4. Set your OpenAI key as a secret: <code>supabase secrets set OPENAI_API_KEY=sk-xxx</code>\n5. Deploy the Edge Functions: <code>supabase functions deploy --no-verify-jwt</code>\n6. Expose <code>docs</code> schema via API in Supabase Dashboard [settings](https://supabase.com/dashboard/project/_/settings/api) \\> <code>API Settings</code> \\> <code>Exposed schemas</code>\n"
    }
  },
  {
    "chunk_id": "9599bade-b948-47b8-9b6c-79042119c1be",
    "metadata": {
      "token_count": 781,
      "source_url": "https://supabase.com/docs/guides/ai/examples/headless-vector-search",
      "page_title": "Adding generative Q&A for your documentation | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Adding generative Q&A for your documentation",
        "h2": "Resources \\#",
        "h3": "Add a search interface \\#"
      },
      "text": "Now we need to push your documentation into the database as embeddings. You can do this manually, but to make it easier we've created a [GitHub Action](https://github.com/marketplace/actions/supabase-embeddings-generator) which can update your database every time there is a Pull Request.\n\nIn your knowledge base repository, create a new action called <code>.github/workflows/generate_embeddings.yml</code> with the following content:\n\n`\n_17\nname: 'generate_embeddings'\n_17\non: # run on main branch changes\n_17\npush:\n_17\n    branches:\n_17\n      - main\n_17\n_17\njobs:\n_17\ngenerate:\n_17\n    runs-on: ubuntu-latest\n_17\n    steps:\n_17\n      - uses: actions/checkout@v3\n_17\n      - uses: supabase/embeddings-generator@v0.0.x # Update this to the latest version.\n_17\n        with:\n_17\n          supabase-url: 'https://your-project-ref.supabase.co' # Update this to your project URL.\n_17\n          supabase-service-role-key: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}\n_17\n          openai-key: ${{ secrets.OPENAI_API_KEY }}\n_17\n          docs-root-path: 'docs' # the path to the root of your md(x) files\n`\n\nMake sure to choose the latest version, and set your <code>SUPABASE_SERVICE_ROLE_KEY</code> and <code>OPENAI_API_KEY</code> as repository secrets in your repo settings (settings > secrets > actions).\nNow inside your docs, you need to create a search interface. Because this is a headless interface, you can use it with any language. The only requirement is that you send the user query to the <code>query</code> Edge Function, which will stream an answer back from OpenAI. It might look something like this:\n\n``\n_31\nconst onSubmit = (e: Event) => {\n_31\ne.preventDefault()\n_31\nanswer.value = \"\"\n_31\nisLoading.value = true\n_31\n_31\nconst query = new URLSearchParams({ query: inputRef.current!.value })\n_31\nconst projectUrl = <code>https://your-project-ref.supabase.co/functions/v1</code>\n_31\nconst queryURL = <code>${projectURL}/${query}</code>\n_31\nconst eventSource = new EventSource(queryURL)\n_31\n_31\neventSource.addEventListener(\"error\", (err) => {\n_31\n    isLoading.value = false\n_31\n    console.error(err)\n_31\n})\n_31\n_31\neventSource.addEventListener(\"message\", (e: MessageEvent) => {\n_31\n    isLoading.value = false\n_31\n_31\n    if (e.data === \"[DONE]\") {\n_31\n      eventSource.close()\n_31\n      return\n_31\n    }\n_31\n_31\n    const completionResponse: CreateCompletionResponse = JSON.parse(e.data)\n_31\n    const text = completionResponse.choices[0].text\n_31\n_31\n    answer.value += text\n_31\n});\n_31\n_31\nisLoading.value = true\n_31\n}\n``\n- Read about how we built [ChatGPT for the Supabase Docs](https://supabase.com/blog/chatgpt-supabase-docs).\n- Read the pgvector Docs for [Embeddings and vector similarity](/docs/guides/database/extensions/pgvector)\n- See how to build something like this from scratch [using Next.js](/docs/guides/ai/examples/nextjs-vector-search).\n",
      "overlap_text": {
        "previous_chunk_id": "31218dc8-d40e-4943-86a1-b7fca05b7983",
        "text": "code>docs</code> schema via API in Supabase Dashboard [settings](https://supabase.com/dashboard/project/_/settings/api) \\> <code>API Settings</code> \\> <code>Exposed schemas</code>\n"
      }
    }
  },
  {
    "chunk_id": "2cc8a1b4-22aa-46b9-bc2c-04a6f93bf526",
    "metadata": {
      "token_count": 742,
      "source_url": "https://supabase.com/docs/guides/ai/engineering-for-scale",
      "page_title": "Engineering for Scale | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "metadata filtering",
        "h2": "Enterprise workloads \\#",
        "h3": "# Connecting your remote database \\#"
      },
      "text": "AI & Vectors\nContent sources for vectors can be extremely large. As you grow you should run your Vector workloads across several secondary databases (sometimes called \"pods\"), which allows each collection to scale independently.\nFor small workloads, it's typical to store your data in a single database.\n\nIf you've used [Vecs](/docs/guides/ai/vecs-python-client) to create 3 different collections, you can expose collections to your web or mobile application using [views](/docs/guides/database/tables#views):\n\nFor example, with 3 collections, called <code>docs</code>, <code>posts</code>, and <code>images</code>, we could expose the \"docs\" inside the public schema like this:\n\n`\n_10\ncreate view public.docs as\n_10\nselect\n_10\nid,\n_10\nembedding,\n_10\nmetadata, # Expose the metadata as JSON\n_10\n(metadata->>'url')::text as url # Extract the URL as a string\n_10\nfrom vector\n`\n\nYou can then use any of the client libraries to access your collections within your applications:\n\n`\n_10\nconst { data, error } = await supabase\n_10\n.from('docs')\n_10\n.select('id, embedding, metadata')\n_10\n.eq('url', '/hello-world')\n`\nAs you move into production, we recommend splitting your collections into separate projects. This is because it allows your vector stores to scale independently of your production data. Vectors typically grow faster than operational data, and they have different resource requirements. Running them on separate databases removes the single-point-of-failure.\n\nYou can use as many secondary databases as you need to manage your collections. With this architecture, you have 2 options for accessing collections within your application:\n\n1. Query the collections directly using Vecs.\n2. Access the collections from your Primary database through a Wrapper.\n\nYou can use both of these in tandem to suit your use-case. We recommend option <code>1</code> wherever possible, as it offers the most scalability.\nVecs provides methods for querying collections, either using a [cosine similarity function](https://supabase.github.io/vecs/api/#basic) or with [metadata filtering](https://supabase.github.io/vecs/api/#metadata-filtering).\n\n`\n_10\n_10\ndocs.query(query_vector=[0.4,0.5,0.6], limit=5)\n_10\n_10\n_10\ndocs.query(\n_10\n    query_vector=[0.4,0.5,0.6],\n_10\n    limit=5,\n_10\n    filters={\"year\": {\"$eq\": 2012}}, # metadata filters\n_10\n)\n`\nSupabase supports [Foreign Data Wrappers](/blog/postgres-foreign-data-wrappers-rust). Wrappers allow you to connect two databases together so that you can query them over the network.\n\nThis involves 2 steps: connecting to your remote database from the primary and creating a Foreign Table.\nInside your Primary database we need to provide the credentials to access the secondary database:\n\n`\n_10\ncreate extension postgres_fdw;\n_10\n_10\ncreate server docs_server\n_10\nforeign data wrapper postgres_fdw\n_10\noptions (host 'db.xxx.supabase.co', port '5432', dbname 'postgres');\n_10\n_10\ncreate user mapping for docs_user\n_10\nserver docs_server\n_10\noptions (user 'postgres', password 'password');\n`\n"
    }
  },
  {
    "chunk_id": "2af02b91-775d-4c6e-a6b9-71694d699888",
    "metadata": {
      "token_count": 197,
      "source_url": "https://supabase.com/docs/guides/ai/engineering-for-scale",
      "page_title": "Engineering for Scale | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "metadata filtering",
        "h2": "",
        "h3": "Enterprise architecture \\#"
      },
      "text": "We can now create a foreign table to access the data in our secondary project.\n\n`\n_10\ncreate foreign table docs (\n_10\nid text not null,\n_10\nembedding vector(384),\n_10\nmetadata jsonb,\n_10\nurl text\n_10\n)\n_10\nserver docs_server\n_10\noptions (schema_name 'public', table_name 'docs');\n`\n\nThis looks very similar to our View example above, and you can continue to use the client libraries to access your collections through the foreign table:\n\n`\n_10\nconst { data, error } = await supabase\n_10\n.from('docs')\n_10\n.select('id, embedding, metadata')\n_10\n.eq('url', '/hello-world')\n`\nThis diagram provides an example architecture that allows you to access the collections either with our client libraries or using Vecs. You can add as many secondary databases as you need (in this example we only show one):\n",
      "overlap_text": {
        "previous_chunk_id": "2cc8a1b4-22aa-46b9-bc2c-04a6f93bf526",
        "text": "db.xxx.supabase.co', port '5432', dbname 'postgres');\n_10\n_10\ncreate user mapping for docs_user\n_10\nserver docs_server\n_10\noptions (user 'postgres', password 'password');\n`\n"
      }
    }
  },
  {
    "chunk_id": "06540bdb-20b1-42ba-809d-55417db14792",
    "metadata": {
      "token_count": 443,
      "source_url": "https://supabase.com/docs/guides/ai/keyword-search",
      "page_title": "Keyword search | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Keyword search",
        "h2": "See also \\#",
        "h3": ""
      },
      "text": "AI & Vectors\nKeyword search involves locating documents or records that contain specific words or phrases, primarily based on the exact match between the search terms and the text within the data. It differs from [semantic search](/docs/guides/ai/semantic-search), which interprets the meaning behind the query to provide results that are contextually related, even if the exact words aren't present in the text. Semantic search considers synonyms, intent, and natural language nuances to provide a more nuanced approach to information retrieval.\n\nIn Postgres, keyword search is implemented using [full-text search](/docs/guides/database/full-text-search). It supports indexing and text analysis for data retrieval, focusing on records that match the search criteria. Postgres' full-text search extends beyond simple keyword matching to address linguistic nuances, making it effective for applications that require precise text queries.\nKeyword search is particularly useful in scenarios where precision and specificity matter. It's more effective than semantic search when users are looking for information using exact terminology or specific identifiers. It ensures that results directly contain those terms, reducing the chance of retrieving irrelevant information that might be semantically related but not what the user seeks.\n\nFor example in technical or academic research databases, researchers often search for specific studies, compounds, or concepts identified by certain terms or codes. Searching for a specific chemical compound using its exact molecular formula or a unique identifier will yield more focused and relevant results compared to a semantic search, which could return a wide range of documents discussing the compound in different contexts. Keyword search ensures documents that explicitly mention the exact term are found, allowing users to access the precise data they need efficiently.\n\nIt's also possible to combine keyword search with semantic search to get the best of both worlds. See [Hybrid search](/docs/guides/ai/hybrid-search) for more details.\nFor an in-depth guide to Postgres' full-text search, including how to store, index, and query records, see [Full text search](/docs/guides/database/full-text-search).\n- [Semantic search](/docs/guides/ai/semantic-search)\n- [Hybrid search](/docs/guides/ai/hybrid-search)\n"
    }
  },
  {
    "chunk_id": "e737ec01-0acc-4ffd-a113-095ae47817d6",
    "metadata": {
      "token_count": 379,
      "source_url": "https://supabase.com/docs/guides/ai/vector-indexes",
      "page_title": "Vector indexes | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Vector indexes",
        "h2": "Resources \\#",
        "h3": ""
      },
      "text": "AI & Vectors\nOnce your vector table starts to grow, you will likely want to add an index to speed up queries. Without indexes, you'll be performing a sequential scan which can be a resource-intensive operation when you have many records.\nToday <code>pgvector</code> supports two types of indexes:\n\n- [HNSW](/docs/guides/ai/vector-indexes/hnsw-indexes)\n- [IVFFlat](/docs/guides/ai/vector-indexes/ivf-indexes)\n\nIn general we recommend using [HNSW](/docs/guides/ai/vector-indexes/hnsw-indexes) because of its [performance](https://supabase.com/blog/increase-performance-pgvector-hnsw#hnsw-performance-1536-dimensions) and [robustness against changing data](/docs/guides/ai/vector-indexes/hnsw-indexes#when-should-you-create-hnsw-indexes).\nIndexes can be used to improve performance of nearest neighbor search using various distance measures. <code>pgvector</code> includes 3 distance operators:\n\n| Operator | Description | [**Operator class**](https://www.postgresql.org/docs/current/sql-createopclass.html) |\n| --- | --- | --- |\n| <code><-></code> | Euclidean distance | <code>vector_l2_ops</code> |\n| <code><#></code> | negative inner product | <code>vector_ip_ops</code> |\n| <code><=></code> | cosine distance | <code>vector_cosine_ops</code> |\n\nCurrently vectors with up to 2,000 dimensions can be indexed.\nRead more about indexing on <code>pgvector</code>'s [GitHub page](https://github.com/pgvector/pgvector#indexing).\n"
    }
  },
  {
    "chunk_id": "1dbcb608-fd58-47a2-8b9f-24788e997f01",
    "metadata": {
      "token_count": 751,
      "source_url": "https://supabase.com/docs/guides/ai/concepts",
      "page_title": "Concepts | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Concepts",
        "h2": "See also \\#",
        "h3": ""
      },
      "text": "AI & Vectors\nEmbeddings are core to many AI and vector applications. This guide covers these concepts. If you prefer to get started right away, see our guide on [Generating Embeddings](/docs/guides/ai/quickstarts/generate-text-embeddings).\nEmbeddings capture the \"relatedness\" of text, images, video, or other types of information. This relatedness is most commonly used for:\n\n- **Search:** how similar is a search term to a body of text?\n- **Recommendations:** how similar are two products?\n- **Classifications:** how do we categorize a body of text?\n- **Clustering:** how do we identify trends?\n\nLet's explore an example of text embeddings. Say we have three phrases:\n\n1. \"The cat chases the mouse\"\n2. \"The kitten hunts rodents\"\n3. \"I like ham sandwiches\"\n\nYour job is to group phrases with similar meaning. If you are a human, this should be obvious. Phrases 1 and 2 are almost identical, while phrase 3 has a completely different meaning.\n\nAlthough phrases 1 and 2 are similar, they share no common vocabulary (besides \"the\"). Yet their meanings are nearly identical. How can we teach a computer that these are the same?\nHumans use words and symbols to communicate language. But words in isolation are mostly meaningless - we need to draw from shared knowledge & experience in order to make sense of them. The phrase \u201cYou should Google it\u201d only makes sense if you know that Google is a search engine and that people have been using it as a verb.\n\nIn the same way, we need to train a neural network model to understand human language. An effective model should be trained on millions of different examples to understand what each word, phrase, sentence, or paragraph could mean in different contexts.\n\nSo how does this relate to embeddings?\nEmbeddings compress discrete information (words & symbols) into distributed continuous-valued data (vectors). If we took our phrases from before and plot them on a chart, it might look something like this:\n\n![Vector similarity](https://supabase.com/docs/img/ai/vector-similarity.png)\n\nPhrases 1 and 2 would be plotted close to each other, since their meanings are similar. We would expect phrase 3 to live somewhere far away since it isn't related. If we had a fourth phrase, \u201cSally ate Swiss cheese\u201d, this might exist somewhere between phrase 3 (cheese can go on sandwiches) and phrase 1 (mice like Swiss cheese).\n\nIn this example we only have 2 dimensions: the X and Y axis. In reality, we would need many more dimensions to effectively capture the complexities of human language.\nCompared to our 2-dimensional example above, most embedding models will output many more dimensions. For example the open source [<code>gte-small</code>](https://huggingface.co/Supabase/gte-small) model outputs 384 dimensions.\n\nWhy is this useful? Once we have generated embeddings on multiple texts, it is trivial to calculate how similar they are using vector math operations like cosine distance. A common use case for this is search. Your process might look something like this:\n\n1. Pre-process your knowledge base and generate embeddings for each page\n2. Store your embeddings to be referenced later\n3. Build a search page that prompts your user for input\n4. Take user's input, generate a one-time embedding, then perform a similarity search against your pre-processed embeddings.\n5. Return the most similar pages to the user\n- [Structured and Unstructured embeddings](/docs/guides/ai/structured-unstructured)\n"
    }
  },
  {
    "chunk_id": "b9df2ab6-84a1-472e-800b-3386b9839507",
    "metadata": {
      "token_count": 721,
      "source_url": "https://supabase.com/docs/guides/ai/going-to-prod",
      "page_title": "Going to Production | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Going to Production",
        "h2": "HNSW, understanding <code>ef_construction</code>, <code>ef_search</code>, and <code>m</code> \\#",
        "h3": ""
      },
      "text": "AI & Vectors\nThis guide will help you to prepare your application for production. We'll provide actionable steps to help you scale your application, ensure that it is reliable, can handle the load, and provide optimal accuracy for your use case.\n\nSee our [Engineering for Scale](/docs/guides/ai/engineering-for-scale) guide for more information about engineering at scale.\nSequential scans will result in significantly higher latencies and lower throughput, guaranteeing 100% accuracy and not being RAM bound.\n\nThere are a couple of cases where you might not need indexes:\n\n- You have a small dataset and don't need to scale it.\n- You are not expecting high amounts of vector search queries per second.\n- You need to guarantee 100% accuracy.\n\nYou don't have to create indexes in these cases and can use sequential scans instead. This type of workload will not be RAM bound and will not require any additional resources but will result in higher latencies and lower throughput. Extra CPU cores may help to improve queries per second, but it will not help to improve latency.\n\nOn the other hand, if you need to scale your application, you will need to [create indexes](/docs/guides/ai/vector-indexes). This will result in lower latencies and higher throughput, but will require additional RAM to make use of Postgres Caching. Also, using indexes will result in lower accuracy, since you are replacing exact (KNN) search with approximate (ANN) search.\n<code>pgvector</code> supports two types of indexes: HNSW and IVFFlat. We recommend using [HNSW](/docs/guides/ai/vector-indexes/hnsw-indexes) because of its [performance](https://supabase.com/blog/increase-performance-pgvector-hnsw#hnsw-performance-1536-dimensions) and [robustness against changing data](/docs/guides/ai/vector-indexes/hnsw-indexes#when-should-you-create-hnsw-indexes).\nIndex build parameters:\n\n- <code>m</code> is the number of bi-directional links created for every new element during construction. Higher <code>m</code> is suitable for datasets with high dimensionality and/or high accuracy requirements. Reasonable values for <code>m</code> are between 2 and 100. Range 12-48 is a good starting point for most use cases (16 is the default value).\n\n- <code>ef_construction</code> is the size of the dynamic list for the nearest neighbors (used during the construction algorithm). Higher <code>ef_construction</code> will result in better index quality and higher accuracy, but it will also increase the time required to build the index. <code>ef_construction</code> has to be at least 2 \\* <code>m</code> (64 is the default value). At some point, increasing <code>ef_construction</code> does not improve the quality of the index. You can measure accuracy when <code>ef_search</code> = <code>ef_construction</code>: if accuracy is lower than 0.9, then there is room for improvement.\n\nSearch parameters:\n\n- <code>ef_search</code> is the size of the dynamic list for the nearest neighbors (used during the search). Increasing <code>ef_search</code> will result in better accuracy, but it will also increase the time required to execute a query (40 is the default value).\n"
    }
  },
  {
    "chunk_id": "63029b0b-002f-41ff-9f63-d13ebc11017c",
    "metadata": {
      "token_count": 661,
      "source_url": "https://supabase.com/docs/guides/ai/going-to-prod",
      "page_title": "Going to Production | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Going to Production",
        "h2": "Performance tips when using indexes \\#",
        "h3": ""
      },
      "text": "Indexes used for approximate vector similarity search in pgvector divides a dataset into partitions. The number of these partitions is defined by the <code>lists</code> constant. The <code>probes</code> controls how many lists are going to be searched during a query.\n\nThe values of lists and probes directly affect accuracy and queries per second (QPS).\n\n- Higher <code>lists</code> means an index will be built slower, but you can achieve better QPS and accuracy.\n- Higher <code>probes</code> means that select queries will be slower, but you can achieve better accuracy.\n- <code>lists</code> and <code>probes</code> are not independent. Higher <code>lists</code> means that you will have to use higher <code>probes</code> to achieve the same accuracy.\n\nYou can find more examples of how <code>lists</code> and <code>probes</code> constants affect accuracy and QPS in [pgvector 0.4.0 performance](https://supabase.com/blog/pgvector-performance) blogpost.\nFirst, a few generic tips which you can pick and choose from:\n\n1. The Supabase managed platform will automatically optimize Postgres configs for you based on your compute addon. But if you self-host, consider **adjusting your Postgres config** based on RAM & CPU cores. See [example optimizations](https://gist.github.com/egor-romanov/323e2847851bbd758081511785573c08) for more details.\n2. Prefer <code>inner-product</code> to <code>L2</code> or <code>Cosine</code> distances if your vectors are normalized (like <code>text-embedding-ada-002</code>). If embeddings are not normalized, <code>Cosine</code> distance should give the best results with an index.\n3. **Pre-warm your database.** Implement the warm-up technique before transitioning to production or running benchmarks.\n   - Use [pg\\_prewarm](https://www.postgresql.org/docs/current/pgprewarm.html) to load the index into RAM <code>select pg_prewarm('vecs.docs_vec_idx');</code>. This will help to avoid cold cache issues.\n   - Execute 10,000 to 50,000 \"warm-up\" queries before each benchmark/prod. This will help to utilize cache and buffers more efficiently.\n4. **Establish your workload.** Finetune <code>m</code> and <code>ef_construction</code> or <code>lists</code> constants for the pgvector index to accelerate your queries (at the expense of a slower build times). For instance, for benchmarks with 1,000,000 OpenAI embeddings, we set <code>m</code> and <code>ef_construction</code> to 32 and 80, and it resulted in 35% higher QPS than 24 and 56 values respectively.\n5. **Benchmark your own specific workloads.** Doing this during cache warm-up helps gauge the best value for the index build parameters, balancing accuracy with queries per second (QPS).\n",
      "overlap_text": {
        "previous_chunk_id": "b9df2ab6-84a1-472e-800b-3386b9839507",
        "text": " the size of the dynamic list for the nearest neighbors (used during the search). Increasing <code>ef_search</code> will result in better accuracy, but it will also increase the time required to execute a query (40 is the default value).\n"
      }
    }
  },
  {
    "chunk_id": "542eab34-7461-4862-8d24-63003d494a13",
    "metadata": {
      "token_count": 736,
      "source_url": "https://supabase.com/docs/guides/ai/going-to-prod",
      "page_title": "Going to Production | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "Going to Production",
        "h2": "Useful links \\#",
        "h3": ""
      },
      "text": "1. Decide if you are going to use indexes or not. You can skip the rest of this guide if you do not use indexes.\n2. Over-provision RAM during preparation. You can scale down in step <code>5</code>, but it's better to start with a larger size to get the best results for RAM requirements. (We'd recommend at least 8XL if you're using Supabase.)\n3. Upload your data to the database. If you use the [<code>vecs</code>](/docs/guides/ai/python/api) library, it will automatically generate an index with default parameters.\n4. Run a benchmark using randomly generated queries and observe the results. Again, you can use the <code>vecs</code> library with the <code>ann-benchmarks</code> tool. Do it with default values for index build parameters, you can later adjust them to get the best results.\n5. Monitor the RAM usage, and save it as a note for yourself. You would likely want to use a compute add-on in the future that has the same amount of RAM that was used at the moment (both actual RAM usage and RAM used for cache and buffers).\n6. Scale down your compute add-on to the one that would have the same amount of RAM used at the moment.\n7. Repeat step 3 to load the data into RAM. You should see QPS increase on subsequent runs, and stop when it no longer increases.\n8. Run a benchmark using real queries and observe the results. You can use the <code>vecs</code> library for that as well with <code>ann-benchmarks</code> tool. Tweak <code>ef_search</code> for HNSW or <code>probes</code> for IVFFlat until you see that both accuracy and QPS match your requirements.\n9. If you want higher QPS you can increase <code>m</code> and <code>ef_construction</code> for HNSW or <code>lists</code> for IVFFlat parameters (consider switching from IVF to HNSW). You have to rebuild the index with a higher <code>m</code> and <code>ef_construction</code> values and repeat steps 6-7 to find the best combination of <code>m</code>, <code>ef_construction</code> and <code>ef_search</code> constants to achieve the best QPS and accuracy values. Higher <code>m</code>, <code>ef_construction</code> mean that index will build slower, but you can achieve better QPS and accuracy. Higher <code>ef_search</code> mean that select queries will be slower, but you can achieve better accuracy.\nDon't forget to check out the general [Production Checklist](/docs/guides/platform/going-into-prod) to ensure your project is secure, performant, and will remain available for your users.\n\nYou can look at our [Choosing Compute Add-on](/docs/guides/ai/choosing-compute-addon) guide to get a basic understanding of how much compute you might need for your workload.\n\nOr take a look at our [pgvector 0.5.0 performance](https://supabase.com/blog/increase-performance-pgvector-hnsw) and [pgvector 0.4.0 performance](https://supabase.com/blog/pgvector-performance) blog posts to see what pgvector is capable of and how the above technique can be used to achieve the best results.\n",
      "overlap_text": {
        "previous_chunk_id": "63029b0b-002f-41ff-9f63-d13ebc11017c",
        "text": " higher QPS than 24 and 56 values respectively.\n5. **Benchmark your own specific workloads.** Doing this during cache warm-up helps gauge the best value for the index build parameters, balancing accuracy with queries per second (QPS).\n"
      }
    }
  },
  {
    "chunk_id": "34ae4e75-bed6-4a08-affa-e8226d4190a5",
    "metadata": {
      "token_count": 771,
      "source_url": "https://supabase.com/docs/guides/ai/quickstarts/hello-world",
      "page_title": "Creating and managing collections | Supabase Docs"
    },
    "data": {
      "headers": {
        "h1": "create vector store client",
        "h2": "Next steps \\#",
        "h3": ""
      },
      "text": "AI & Vectors\nThis guide will walk you through a basic [\"Hello World\"](https://github.com/supabase/supabase/blob/master/examples/ai/vector_hello_world.ipynb) example using Colab and Supabase Vecs. You'll learn how to:\n\n1. Launch a Postgres database that uses pgvector to store embeddings\n2. Launch a notebook that connects to your database\n3. Create a vector collection\n4. Add data to the collection\n5. Query the collection\nLet's create a new Postgres database. This is as simple as starting a new Project in Supabase:\n\n1. [Create a new project](https://database.new/) in the Supabase dashboard.\n2. Enter your project details. Remember to store your password somewhere safe.\n\nYour database will be available in less than a minute.\n\n**Finding your credentials:**\n\nYou can find your project credentials inside the project [settings](https://supabase.com/dashboard/project/_/settings/), including:\n\n- [Database credentials](https://supabase.com/dashboard/project/_/settings/database): connection strings and connection pooler details.\n- [API credentials](https://supabase.com/dashboard/project/_/settings/database): your serverless API URL and <code>anon</code> / <code>service_role</code> keys.\nLaunch our [<code>vector_hello_world</code>](https://github.com/supabase/supabase/blob/master/examples/ai/vector_hello_world.ipynb) notebook in Colab:\n\n[![](https://supabase.com/docs/img/ai/colab-badge.svg)](https://colab.research.google.com/github/supabase/supabase/blob/master/examples/ai/vector_hello_world.ipynb)\n\nAt the top of the notebook, you'll see a button <code>Copy to Drive</code>. Click this button to copy the notebook to your Google Drive.\nInside the Notebook, find the cell which specifies the <code>DB_CONNECTION</code>. It will contain some code like this:\n\n`\n_10\nimport vecs\n_10\n_10\nDB_CONNECTION = \"postgresql://<user>:<password>@<host>:<port>/<db_name>\"\n_10\n_10\n_10\nvx = vecs.create_client(DB_CONNECTION)\n`\n\nReplace the <code>DB_CONNECTION</code> with your own connection string for your database. You can find the Postgres connection string in the [Database Settings](https://supabase.com/dashboard/project/_/settings/database) of your Supabase project.\n\nSQLAlchemy requires the connection string to start with <code>postgresql://</code> (instead of <code>postgres://</code>). Don't forget to rename this after copying the string from the dashboard.\n\nYou must use the \"connection pooling\" string (domain ending in <code>*.pooler.supabase.com</code>) with Google Colab since Colab does not support IPv6.\nNow all that's left is to step through the notebook. You can do this by clicking the \"execute\" button ( <code>ctrl+enter</code>) at the top left of each code cell. The notebook guides you through the process of creating a collection, adding data to it, and querying it.\n\nYou can view the inserted items in the [Table Editor](https://supabase.com/dashboard/project/_/editor/), by selecting the <code>vecs</code> schema from the schema dropdown.\n\n![Colab documents](https://supabase.com/docs/img/ai/google-colab/colab-documents.png)\nYou can now start building your own applications with Vecs. Check our [examples](/docs/guides/ai#examples) for ideas.\n"
    }
  }
]