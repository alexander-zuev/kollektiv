{
  "too_small": [
    {
      "id": "01afaeb5-58a7-471d-8ca9-dfb992762ce4",
      "size": 4,
      "headers": {
        "h1": "Textembed",
        "h2": "",
        "h3": ""
      },
      "text": "Back to top\n"
    },
    {
      "id": "1497151f-0e16-4955-a839-48c5b19987dc",
      "size": 65,
      "headers": {
        "h1": "Upstage",
        "h2": "UpstageEmbedding \\#",
        "h3": ""
      },
      "text": "Bases: <code>OpenAIEmbedding</code>\n\nClass for Upstage embeddings.\n\nSource code in <code>llama-index-integrations/embeddings/llama-index-embeddings-upstage/llama_index/embeddings/upstage/base.py</code>\n\n|     |     |\n| --- | --- |\n"
    },
    {
      "id": "ef0a0598-dd3b-4a51-8aa4-cd6940ae504c",
      "size": 75,
      "headers": {
        "h1": "Fastembed",
        "h2": "FastEmbedEmbedding \\#",
        "h3": ""
      },
      "text": ": np.ndarray = next(self._model.query_embed(query))<br>        return query_embeddings.tolist()<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        return self._get_query_embedding(query)<br></code>`` |\n\nBack to top\n\nHi, how can I help you?\n\nðŸ¦™\n"
    },
    {
      "id": "ca5e00f9-1712-4f1a-8e14-99ffb5753bc1",
      "size": 81,
      "headers": {
        "h1": "Index",
        "h2": "BaseEvaluator \\#",
        "h3": ""
      },
      "text": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/#llama_index.core.evaluation.BaseEvaluator)\nEvaluation modules.\nBases: <code>PromptMixin</code>\n\nBase Evaluator class.\n\nSource code in <code>llama-index-core/llama_index/core/evaluation/base.py</code>\n\n|     |     |\n| --- | --- |\n"
    },
    {
      "id": "3aa61d76-b100-4a13-ae47-312dd4f5fd72",
      "size": 94,
      "headers": {
        "h1": "Clip",
        "h2": "ClipEmbedding \\#",
        "h3": ""
      },
      "text": "br>        import torch<br>        with torch.no_grad():<br>            image = (<br>                self._preprocess(Image.open(img_file_path))<br>                .unsqueeze(0)<br>                .to(self._device)<br>            )<br>            return self._model.encode_image(image).tolist()[0]<br></code>`` |\n\nBack to top\n\nHi, how can I help you?\n\nðŸ¦™\n"
    },
    {
      "id": "8acc61cc-b8e8-4c20-9650-07bb522dfe40",
      "size": 92,
      "headers": {
        "h1": "Retrieval",
        "h2": "BaseRetrievalEvaluator \\#",
        "h3": ""
      },
      "text": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/retrieval/#llama_index.core.evaluation.BaseRetrievalEvaluator)\nEvaluation modules.\nBases: <code>BaseModel</code>\n\nBase Retrieval Evaluator class.\n\nSource code in <code>llama-index-core/llama_index/core/evaluation/retrieval/base.py</code>\n\n|     |     |\n| --- | --- |\n"
    },
    {
      "id": "efc1738f-ca86-423d-ac0c-14c142d52003",
      "size": 99,
      "headers": {
        "h1": "Simple",
        "h2": "SimpleChatEngine \\#",
        "h3": ""
      },
      "text": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/simple/#llama_index.core.chat_engine.SimpleChatEngine)\nBases: <code>BaseChatEngine</code>\n\nSimple Chat Engine.\n\nHave a conversation with the LLM.\nThis does not make use of a knowledge base.\n\nSource code in <code>llama-index-core/llama_index/core/chat_engine/simple.py</code>\n\n|     |     |\n| --- | --- |\n"
    },
    {
      "id": "99866017-0b0d-475b-9f83-565a28d992e6",
      "size": 93,
      "headers": {
        "h1": "Together",
        "h2": "TogetherEmbedding \\#",
        "h3": ""
      },
      "text": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/together/#llama_index.embeddings.together.TogetherEmbedding)\nBases: <code>BaseEmbedding</code>\n\nSource code in <code>llama-index-integrations/embeddings/llama-index-embeddings-together/llama_index/embeddings/together/base.py</code>\n\n|     |     |\n| --- | --- |\n"
    },
    {
      "id": "9ec0bba6-9b63-4712-abb1-7552e80b4725",
      "size": 90,
      "headers": {
        "h1": "Cloudflare workersai",
        "h2": "CloudflareEmbedding \\#",
        "h3": ""
      },
      "text": "<br>                json={\"text\": texts},<br>                headers=headers,<br>            ) as response:<br>                resp = await response.json()<br>                if \"result\" not in resp:<br>                    raise RuntimeError(\"Failed to fetch embeddings asynchronously\")<br>                return resp[\"result\"][\"data\"]<br></code>`` |\n\nBack to top\n\nHi, how can I help you?\n\nðŸ¦™\n"
    },
    {
      "id": "c0c2dc68-8c2d-4630-a6e2-8e4299c345ea",
      "size": 97,
      "headers": {
        "h1": "Jinaai",
        "h2": "JinaEmbedding \\#",
        "h3": ""
      },
      "text": " = []<br>        for img_file_path in img_file_paths:<br>            if is_local(img_file_path):<br>                input.append({\"bytes\": get_bytes_str(img_file_path)})<br>            else:<br>                input.append({\"url\": img_file_path})<br>        return await self._api.aget_embeddings(input=input)<br></code>`` |\n\nBack to top\n\nHi, how can I help you?\n\nðŸ¦™\n"
    },
    {
      "id": "9131deb2-36b8-4f7e-8f03-d6dfc8368509",
      "size": 92,
      "headers": {
        "h1": "Gemini",
        "h2": "GeminiEmbedding \\#",
        "h3": ""
      },
      "text": "]:<br>        \"\"\"Asynchronously get text embedding.\"\"\"<br>        return self._get_text_embedding(text)<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Asynchronously get text embeddings.\"\"\"<br>        return self._get_text_embeddings(texts)<br></code>`` |\n\nBack to top\n\nHi, how can I help you?\n\nðŸ¦™\n"
    },
    {
      "id": "db6df76a-20a6-4d63-8db5-ff0ef65c340d",
      "size": 94,
      "headers": {
        "h1": "Llamafile",
        "h2": "LlamafileEmbedding \\#",
        "h3": ""
      },
      "text": "=f\"{self.base_url}/embedding\",<br>                headers={\"Content-Type\": \"application/json\"},<br>                json=request_body,<br>            )<br>            response.encoding = \"utf-8\"<br>            response.raise_for_status()<br>            return [output[\"embedding\"] for output in response.json()[\"results\"]]<br></code>`` |\n\nBack to top\n\nHi, how can I help you?\n\nðŸ¦™\n"
    },
    {
      "id": "46520439-9338-4a7b-ab26-276800da9718",
      "size": 72,
      "headers": {
        "h1": "Nomic",
        "h2": "NomicEmbedding \\#",
        "h3": ""
      },
      "text": "Bases: <code>MultiModalEmbedding</code>\n\nNomicEmbedding uses the Nomic API to generate embeddings.\n\nSource code in <code>llama-index-integrations/embeddings/llama-index-embeddings-nomic/llama_index/embeddings/nomic/base.py</code>\n\n|     |     |\n| --- | --- |\n"
    },
    {
      "id": "1fff629b-a533-48f8-a0a2-15e15cbb86f4",
      "size": 99,
      "headers": {
        "h1": "Entity",
        "h2": "EntityExtractor \\#",
        "h3": ""
      },
      "text": ">                        metadata[metadata_label] = set()<br>                    metadata[metadata_label].add(self.span_joiner.join(span[\"span\"]))<br>        # convert metadata from set to list<br>        for metadata in metadata_list:<br>            for key, val in metadata.items():<br>                metadata[key] = list(val)<br>        return metadata_list<br></code>`` |\n\nBack to top\n\nHi, how can I help you?\n\nðŸ¦™\n"
    },
    {
      "id": "1cfc584d-a462-4040-b43b-0dc844fec7ad",
      "size": 66,
      "headers": {
        "h1": "Xinference",
        "h2": "XinferenceEmbedding \\#",
        "h3": ""
      },
      "text": "Bases: <code>BaseEmbedding</code>\n\nClass for Xinference embeddings.\n\nSource code in <code>llama-index-integrations/embeddings/llama-index-embeddings-xinference/llama_index/embeddings/xinference/base.py</code>\n\n|     |     |\n| --- | --- |\n"
    },
    {
      "id": "017428b7-1b89-404d-975a-d4f6465bcdd3",
      "size": 63,
      "headers": {
        "h1": "Ipex llm",
        "h2": "IpexLLMEmbedding \\#",
        "h3": ""
      },
      "text": "Bases: <code>BaseEmbedding</code>\n\nSource code in <code>llama-index-integrations/embeddings/llama-index-embeddings-ipex-llm/llama_index/embeddings/ipex_llm/base.py</code>\n\n|     |     |\n| --- | --- |\n"
    },
    {
      "id": "1771010c-cc29-4c90-9f38-bafae3bf4866",
      "size": 87,
      "headers": {
        "h1": "Ipex llm",
        "h2": "IpexLLMEmbedding \\#",
        "h3": ""
      },
      "text": ") -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._embed(text, prompt_name=\"text\")<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        return self._embed(texts, prompt_name=\"text\")<br></code>`` |\n\nBack to top\n"
    },
    {
      "id": "d93c9384-79e3-4238-9126-33416a9568ab",
      "size": 69,
      "headers": {
        "h1": "Promptlayer",
        "h2": "PromptLayerHandler \\#",
        "h3": ""
      },
      "text": "Bases: <code>BaseCallbackHandler</code>\n\nCallback handler for sending to promptlayer.com.\n\nSource code in <code>llama-index-integrations/callbacks/llama-index-callbacks-promptlayer/llama_index/callbacks/promptlayer/base.py</code>\n\n|     |     |\n| --- | --- |\n"
    },
    {
      "id": "10d7c6b6-1993-4a50-9e9c-2857ac00de46",
      "size": 87,
      "headers": {
        "h1": "Promptlayer",
        "h2": "PromptLayerHandler \\#",
        "h3": ""
      },
      "text": "extra_args,<br>                    **event_data[\"kwargs\"],<br>                },<br>                self.pl_tags,<br>                [resp],<br>                event_data[\"request_start_time\"],<br>                request_end_time,<br>                self._promptlayer_api_key,<br>                return_pl_id=self.return_pl_id,<br>            )<br></code>`` |\n\nBack to top\n"
    },
    {
      "id": "87ed05b8-71a5-4a7a-8532-67c9260c2cfa",
      "size": 83,
      "headers": {
        "h1": "Adapter",
        "h2": "AdapterEmbeddingModel \\#",
        "h3": ""
      },
      "text": " text: str) -> List[float]:<br>        return self._base_embed_model._get_text_embedding(text)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        return await self._base_embed_model._aget_text_embedding(text)<br></code>`` |\n\nBack to top\n\nHi, how can I help you?\n\nðŸ¦™\n"
    },
    {
      "id": "d96227aa-1bde-4e99-a7ae-68344a43e841",
      "size": 61,
      "headers": {
        "h1": "Response",
        "h2": "ResponseEvaluator<code>module-attribute</code>\\#",
        "h3": ""
      },
      "text": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/response/#llama_index.core.evaluation.ResponseEvaluator)\nEvaluation modules.\n```\nResponseEvaluator = FaithfulnessEvaluator\n\n```\n\nBack to top\n\nHi, how can I help you?\n\nðŸ¦™\n"
    },
    {
      "id": "3f3fce24-5d33-4b3b-b241-dd0de745c909",
      "size": 83,
      "headers": {
        "h1": "React",
        "h2": "ReActAgent \\#",
        "h3": ""
      },
      "text": "Bases: <code>AgentRunner</code>\n\nReAct agent.\n\nSubclasses AgentRunner with a ReActAgentWorker.\n\nFor the legacy implementation see:\n\n```\nfrom llama_index.core.agent.legacy.react.base import ReActAgent\n\n```\n\nSource code in <code>llama-index-core/llama_index/core/agent/react/base.py</code>\n\n|     |     |\n| --- | --- |\n"
    },
    {
      "id": "4a2b6c28-5a81-422b-8aea-a0c0f3f7f32a",
      "size": 98,
      "headers": {
        "h1": "Coa",
        "h2": "CoAAgentWorker \\#",
        "h3": ""
      },
      "text": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/agent/coa/#llama_index.agent.coa.CoAAgentWorker)\nBases: <code>BaseAgentWorker</code>\n\nChain-of-abstraction Agent Worker.\n\nSource code in <code>llama-index-integrations/agent/llama-index-agent-coa/llama_index/agent/coa/step.py</code>\n\n|     |     |\n| --- | --- |\n"
    },
    {
      "id": "52ccd76c-3ce4-442f-8c9f-0e5d4bd575a1",
      "size": 98,
      "headers": {
        "h1": "Alibabacloud aisearch",
        "h2": "AlibabaCloudAISearchEmbedding \\#",
        "h3": ""
      },
      "text": "Bases: <code>BaseEmbedding</code>\n\nFor further details, please visit <code>https://help.aliyun.com/zh/open-search/search-platform/developer-reference/text-embedding-api-details</code>.\n\nSource code in <code>llama-index-integrations/embeddings/llama-index-embeddings-alibabacloud-aisearch/llama_index/embeddings/alibabacloud_aisearch/base.py</code>\n\n|     |     |\n| --- | --- |\n"
    },
    {
      "id": "df9ac44c-678d-45d3-aacb-35c8661f6f5a",
      "size": 86,
      "headers": {
        "h1": "Alibabacloud aisearch",
        "h2": "AlibabaCloudAISearchEmbedding \\#",
        "h3": ""
      },
      "text": "_type=\"document\",<br>        )<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"The asynchronous version of _get_text_embeddings.\"\"\"<br>        return await self._aget_embeddings(<br>            texts,<br>            input_type=\"document\",<br>        )<br></code>`` |\n\nBack to top\n"
    },
    {
      "id": "31e04505-2ac6-435e-b67e-210101c4c42f",
      "size": 82,
      "headers": {
        "h1": "Context",
        "h2": "ContextChatEngine \\#",
        "h3": ""
      },
      "text": "Bases: <code>BaseChatEngine</code>\n\nContext Chat Engine.\n\nUses a retriever to retrieve a context, set the context in the system prompt,\nand then uses an LLM to generate a response, for a fluid chat experience.\n\nSource code in <code>llama-index-core/llama_index/core/chat_engine/context.py</code>\n\n|     |     |\n| --- | --- |\n"
    },
    {
      "id": "ed40795f-a748-4c34-a901-8dc42a7b212d",
      "size": 83,
      "headers": {
        "h1": "Index",
        "h2": "BaseExtractor \\#",
        "h3": ""
      },
      "text": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/extractors/#llama_index.core.extractors.interface.BaseExtractor)\nNode parser interface.\nBases: <code>TransformComponent</code>\n\nMetadata extractor.\n\nSource code in <code>llama-index-core/llama_index/core/extractors/interface.py</code>\n\n|     |     |\n| --- | --- |\n"
    },
    {
      "id": "94a4fca0-9f81-445f-b055-7f3672b95d60",
      "size": 89,
      "headers": {
        "h1": "Index",
        "h2": "BaseEmbedding \\#",
        "h3": ""
      },
      "text": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/#llama_index.core.embeddings.BaseEmbedding)\nBases: <code>TransformComponent</code>, <code>DispatcherSpanMixin</code>\n\nBase class for embeddings.\n\nSource code in <code>llama-index-core/llama_index/core/base/embeddings/base.py</code>\n\n|     |     |\n| --- | --- |\n"
    },
    {
      "id": "94cd465d-14b5-4ff7-84e1-85a2b0dc703d",
      "size": 94,
      "headers": {
        "h1": "Index",
        "h2": "resolve\\_embed\\_model \\#",
        "h3": ""
      },
      "text": "    if embed_model is None:<br>        print(\"Embeddings have been explicitly disabled. Using MockEmbedding.\")<br>        embed_model = MockEmbedding(embed_dim=1)<br>    assert isinstance(embed_model, BaseEmbedding)<br>    embed_model.callback_manager = callback_manager or Settings.callback_manager<br>    return embed_model<br></code>`` |\n\nBack to top\n\nHi, how can I help you?\n\nðŸ¦™\n"
    },
    {
      "id": "2308c6c1-ccaf-43a4-bf6d-cf90c675efca",
      "size": 63,
      "headers": {
        "h1": "Sagemaker endpoint",
        "h2": "SageMakerEmbedding \\#",
        "h3": ""
      },
      "text": "Bases: <code>BaseEmbedding</code>\n\nSource code in <code>llama-index-integrations/embeddings/llama-index-embeddings-sagemaker-endpoint/llama_index/embeddings/sagemaker_endpoint/base.py</code>\n\n|     |     |\n| --- | --- |\n"
    },
    {
      "id": "33cd3387-ff9b-4b78-a4f9-ffdde8ba7d06",
      "size": 93,
      "headers": {
        "h1": "Instructor",
        "h2": "InstructorEmbedding \\#",
        "h3": ""
      },
      "text": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/instructor/#llama_index.embeddings.instructor.InstructorEmbedding)\nBases: <code>BaseEmbedding</code>\n\nSource code in <code>llama-index-integrations/embeddings/llama-index-embeddings-instructor/llama_index/embeddings/instructor/base.py</code>\n\n|     |     |\n| --- | --- |\n"
    },
    {
      "id": "363fa5bc-322e-4b96-ace2-5c95017a4640",
      "size": 68,
      "headers": {
        "h1": "Instructor",
        "h2": "InstructorEmbedding \\#",
        "h3": ""
      },
      "text": "[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        text_pairs = [self._format_text(text) for text in texts]<br>        return self._embed(text_pairs)<br></code>`` |\n\nBack to top\n\nHi, how can I help you?\n\nðŸ¦™\n"
    },
    {
      "id": "a4832883-d03c-43c9-a87d-82d519d5a88b",
      "size": 96,
      "headers": {
        "h1": "Bedrock",
        "h2": "BedrockEmbedding \\#",
        "h3": ""
      },
      "text": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/bedrock/#llama_index.embeddings.bedrock.BedrockEmbedding)\nBases: <code>BaseEmbedding</code>\n\nSource code in <code>llama-index-integrations/embeddings/llama-index-embeddings-bedrock/llama_index/embeddings/bedrock/base.py</code>\n\n|     |     |\n| --- | --- |\n"
    },
    {
      "id": "d8d4fd8f-3f03-49c6-a8c1-f303acc4952d",
      "size": 78,
      "headers": {
        "h1": "Mistralai",
        "h2": "MistralAIEmbedding \\#",
        "h3": ""
      },
      "text": ">        \"\"\"Asynchronously get text embeddings.\"\"\"<br>        embedding_response = await self._client.embeddings.create_async(<br>            model=self.model_name, inputs=texts<br>        )<br>        return [embed.embedding for embed in embedding_response.data]<br></code>`` |\n\nBack to top\n\nHi, how can I help you?\n\nðŸ¦™\n"
    },
    {
      "id": "4fb03bf1-1d18-4ac1-83cb-faa001729684",
      "size": 68,
      "headers": {
        "h1": "API Reference \\#",
        "h2": "",
        "h3": ""
      },
      "text": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/#api-reference)\nLlamaIndex provides thorough documentation of modules and integrations used in the framework.\n\nUse the navigation or search to find the classes you are interested in!\n\nBack to top\n\nHi, how can I help you?\n\nðŸ¦™\n"
    },
    {
      "id": "c8b9d15e-3d60-432b-822c-a8815713ddc7",
      "size": 69,
      "headers": {
        "h1": "Correctness",
        "h2": "CorrectnessEvaluator \\#",
        "h3": ""
      },
      "text": ",<br>            response=response,<br>            passing=score >= self._score_threshold if score is not None else None,<br>            score=score,<br>            feedback=reasoning,<br>        )<br></code>`` |\n\nBack to top\n\nHi, how can I help you?\n\nðŸ¦™\n"
    },
    {
      "id": "74e4b5ba-607c-4bc7-901f-ceb51c9d27a1",
      "size": 85,
      "headers": {
        "h1": "Llm rails",
        "h2": "LLMRailsEmbedding \\#",
        "h3": ""
      },
      "text": "br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        return await self._aget_embedding(query)<br>    async def _aget_text_embedding(self, query: str) -> List[float]:<br>        return await self._aget_embedding(query)<br></code>`` |\n\nBack to top\n\nHi, how can I help you?\n\nðŸ¦™\n"
    },
    {
      "id": "2d2c8354-a632-4228-983a-a8b0d34c6236",
      "size": 99,
      "headers": {
        "h1": "Vertex",
        "h2": "VertexTextEmbedding \\#",
        "h3": ""
      },
      "text": "_request(<br>            texts=[query],<br>            embed_mode=self.embed_mode,<br>            is_query=True,<br>            model_name=self.model_name,<br>        )<br>        embeddings = await self._model.get_embeddings_async(<br>            texts, **self.additional_kwargs<br>        )<br>        return embeddings[0].values<br></code>`` |\n\nBack to top\n\nHi, how can I help you?\n\nðŸ¦™\n"
    }
  ],
  "too_large": [
    {
      "id": "74ad0491-6f7e-4d6d-b806-d7bf7b94d9b0",
      "size": 6695,
      "headers": {
        "h1": "Core Agent Classes \\#",
        "h2": "Runners \\#",
        "h3": "AgentRunner \\#"
      },
      "text": "| ``<code><br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>472<br>473<br>474<br>475<br>476<br>477<br>478<br>479<br>480<br>481<br>482<br>483<br>484<br>485<br>486<br>487<br>488<br>489<br>490<br>491<br>492<br>493<br>494<br>495<br>496<br>497<br>498<br>499<br>500<br>501<br>502<br>503<br>504<br>505<br>506<br>507<br>508<br>509<br>510<br>511<br>512<br>513<br>514<br>515<br>516<br>517<br>518<br>519<br>520<br>521<br>522<br>523<br>524<br>525<br>526<br>527<br>528<br>529<br>530<br>531<br>532<br>533<br>534<br>535<br>536<br>537<br>538<br>539<br>540<br>541<br>542<br>543<br>544<br>545<br>546<br>547<br>548<br>549<br>550<br>551<br>552<br>553<br>554<br>555<br>556<br>557<br>558<br>559<br>560<br>561<br>562<br>563<br>564<br>565<br>566<br>567<br>568<br>569<br>570<br>571<br>572<br>573<br>574<br>575<br>576<br>577<br>578<br>579<br>580<br>581<br>582<br>583<br>584<br>585<br>586<br>587<br>588<br>589<br>590<br>591<br>592<br>593<br>594<br>595<br>596<br>597<br>598<br>599<br>600<br>601<br>602<br>603<br>604<br>605<br>606<br>607<br>608<br>609<br>610<br>611<br>612<br>613<br>614<br>615<br>616<br>617<br>618<br>619<br>620<br>621<br>622<br>623<br>624<br>625<br>626<br>627<br>628<br>629<br>630<br>631<br>632<br>633<br>634<br>635<br>636<br>637<br>638<br>639<br>640<br>641<br>642<br>643<br>644<br>645<br>646<br>647<br>648<br>649<br>650<br>651<br>652<br>653<br>654<br>655<br>656<br>657<br>658<br>659<br>660<br>661<br>662<br>663<br>664<br>665<br>666<br>667<br>668<br>669<br>670<br>671<br>672<br>673<br>674<br>675<br>676<br>677<br>678<br>679<br>680<br>681<br>682<br>683<br>684<br>685<br>686<br>687<br>688<br>689<br>690<br>691<br>692<br>693<br>694<br>695<br>696<br>697<br>698<br>699<br>700<br>701<br>702<br>703<br>704<br>705<br>706<br>707<br>708<br>709<br>710<br>711<br>712<br>713<br>714<br>715<br>716<br>717<br>718<br>719<br>720<br>721<br>722<br>723<br>724<br>725<br>726<br>727<br>728<br>729<br>730<br>731<br>732<br>733<br>734<br>735<br>736<br></code>`<code> | </code>`<code><br>class AgentRunner(BaseAgentRunner):<br>    \"\"\"Agent runner.<br>    Top-level agent orchestrator that can create tasks, run each step in a task,<br>    or run a task e2e. Stores state and keeps track of tasks.<br>    Args:<br>        agent_worker (BaseAgentWorker): step executor<br>        chat_history (Optional[List[ChatMessage]], optional): chat history. Defaults to None.<br>        state (Optional[AgentState], optional): agent state. Defaults to None.<br>        memory (Optional[BaseMemory], optional): memory. Defaults to None.<br>        llm (Optional[LLM], optional): LLM. Defaults to None.<br>        callback_manager (Optional[CallbackManager], optional): callback manager. Defaults to None.<br>        init_task_state_kwargs (Optional[dict], optional): init task state kwargs. Defaults to None.<br>    \"\"\"<br>    # # TODO: implement this in Pydantic<br>    def __init__(<br>        self,<br>        agent_worker: BaseAgentWorker,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        state: Optional[AgentState] = None,<br>        memory: Optional[BaseMemory] = None,<br>        llm: Optional[LLM] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        init_task_state_kwargs: Optional[dict] = None,<br>        delete_task_on_finish: bool = False,<br>        default_tool_choice: str = \"auto\",<br>        verbose: bool = False,<br>    ) -> None:<br>        \"\"\"Initialize.\"\"\"<br>        self.agent_worker = agent_worker<br>        self.state = state or AgentState()<br>        self.memory = memory or ChatMemoryBuffer.from_defaults(chat_history, llm=llm)<br>        # get and set callback manager<br>        if callback_manager is not None:<br>            self.agent_worker.set_callback_manager(callback_manager)<br>            self.callback_manager = callback_manager<br>        else:<br>            # TODO: This is *temporary*<br>            # Stopgap before having a callback on the BaseAgentWorker interface.<br>            # Doing that requires a bit more refactoring to make sure existing code<br>            # doesn't break.<br>            if hasattr(self.agent_worker, \"callback_manager\"):<br>                self.callback_manager = (<br>                    self.agent_worker.callback_manager or CallbackManager()<br>                )<br>            else:<br>                self.callback_manager = CallbackManager()<br>        self.init_task_state_kwargs = init_task_state_kwargs or {}<br>        self.delete_task_on_finish = delete_task_on_finish<br>        self.default_tool_choice = default_tool_choice<br>        self.verbose = verbose<br>    @staticmethod<br>    def from_llm(<br>        tools: Optional[List[BaseTool]] = None,<br>        llm: Optional[LLM] = None,<br>        **kwargs: Any,<br>    ) -> \"AgentRunner\":<br>        from llama_index.core.agent import ReActAgent<br>        if os.getenv(\"IS_TESTING\"):<br>            return ReActAgent.from_tools(<br>                tools=tools,<br>                llm=llm,<br>                **kwargs,<br>            )<br>        try:<br>            from llama_index.llms.openai import OpenAI  # pants: no-infer-dep<br>            from llama_index.llms.openai.utils import (<br>                is_function_calling_model,<br>            )  # pants: no-infer-dep<br>        except ImportError:<br>            raise ImportError(<br>                \"</code>llama-index-llms-openai<code> package not found. Please \"<br>                \"install by running </code>pip install llama-index-llms-openai<code>.\"<br>            )<br>        if isinstance(llm, OpenAI) and is_function_calling_model(llm.model):<br>            from llama_index.agent.openai import OpenAIAgent  # pants: no-infer-dep<br>            return OpenAIAgent.from_tools(<br>                tools=tools,<br>                llm=llm,<br>                **kwargs,<br>            )<br>        else:<br>            return ReActAgent.from_tools(<br>                tools=tools,<br>                llm=llm,<br>                **kwargs,<br>            )<br>    @property<br>    def chat_history(self) -> List[ChatMessage]:<br>        return self.memory.get_all()<br>    def reset(self) -> None:<br>        self.memory.reset()<br>        self.state.reset()<br>    def create_task(self, input: str, **kwargs: Any) -> Task:<br>        \"\"\"Create task.\"\"\"<br>        if not self.init_task_state_kwargs:<br>            extra_state = kwargs.pop(\"extra_state\", {})<br>        else:<br>            if \"extra_state\" in kwargs:<br>                raise ValueError(<br>                    \"Cannot specify both </code>extra_state<code> and </code>init_task_state_kwargs<code>\"<br>                )<br>            else:<br>                extra_state = self.init_task_state_kwargs<br>        callback_manager = kwargs.pop(\"callback_manager\", self.callback_manager)<br>        task = Task(<br>            input=input,<br>            memory=self.memory,<br>            extra_state=extra_state,<br>            callback_manager=callback_manager,<br>            **kwargs,<br>        )<br>        # # put input into memory<br>        # self.memory.put(ChatMessage(content=input, role=MessageRole.USER))<br>        # get initial step from task, and put it in the step queue<br>        initial_step = self.agent_worker.initialize_step(task)<br>        task_state = TaskState(<br>            task=task,<br>            step_queue=deque([initial_step]),<br>        )<br>        # add it to state<br>        self.state.task_dict[task.task_id] = task_state<br>        return task<br>    def delete_task(<br>        self,<br>        task_id: str,<br>    ) -> None:<br>        \"\"\"Delete task.<br>        NOTE: this will not delete any previous executions from memory.<br>        \"\"\"<br>        self.state.task_dict.pop(task_id)<br>    def list_tasks(self, **kwargs: Any) -> List[Task]:<br>        \"\"\"List tasks.\"\"\"<br>        return [task_state.task for task_state in self.state.task_dict.values()]<br>    def get_task(self, task_id: str, **kwargs: Any) -> Task:<br>        \"\"\"Get task.\"\"\"<br>        return self.state.get_task(task_id)<br>    def get_upcoming_steps(self, task_id: str, **kwargs: Any) -> List[TaskStep]:<br>        \"\"\"Get upcoming steps.\"\"\"<br>        return list(self.state.get_step_queue(task_id))<br>    def get_completed_steps(self, task_id: str, **kwargs: Any) -> List[TaskStepOutput]:<br>        \"\"\"Get completed steps.\"\"\"<br>        return self.state.get_completed_steps(task_id)<br>    def get_task_output(self, task_id: str, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Get task output.\"\"\"<br>        completed_steps = self.get_completed_steps(task_id)<br>        if len(completed_steps) == 0:<br>            raise ValueError(f\"No completed steps for task_id: {task_id}\")<br>        return completed_steps[-1]<br>    def get_completed_tasks(self, **kwargs: Any) -> List[Task]:<br>        \"\"\"Get completed tasks.\"\"\"<br>        task_states = list(self.state.task_dict.values())<br>        completed_tasks = []<br>        for task_state in task_states:<br>            completed_steps = self.get_completed_steps(task_state.task.task_id)<br>            if len(completed_steps) > 0 and completed_steps[-1].is_last:<br>                completed_tasks.append(task_state.task)<br>        return completed_tasks<br>    @dispatcher.span<br>    def _run_step(<br>        self,<br>        task_id: str,<br>        step: Optional[TaskStep] = None,<br>        input: Optional[str] = None,<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Execute step.\"\"\"<br>        task = self.state.get_task(task_id)<br>        step_queue = self.state.get_step_queue(task_id)<br>        step = step or step_queue.popleft()<br>        if input is not None:<br>            step.input = input<br>        dispatcher.event(<br>            AgentRunStepStartEvent(task_id=task_id, step=step, input=input)<br>        )<br>        if self.verbose:<br>            print(f\"> Running step {step.step_id}. Step input: {step.input}\")<br>        # TODO: figure out if you can dynamically swap in different step executors<br>        # not clear when you would do that by theoretically possible<br>        if mode == ChatResponseMode.WAIT:<br>            cur_step_output = self.agent_worker.run_step(step, task, **kwargs)<br>        elif mode == ChatResponseMode.STREAM:<br>            cur_step_output = self.agent_worker.stream_step(step, task, **kwargs)<br>        else:<br>            raise ValueError(f\"Invalid mode: {mode}\")<br>        # append cur_step_output next steps to queue<br>        next_steps = cur_step_output.next_steps<br>        step_queue.extend(next_steps)<br>        # add cur_step_output to completed steps<br>        completed_steps = self.state.get_completed_steps(task_id)<br>        completed_steps.append(cur_step_output)<br>        dispatcher.event(AgentRunStepEndEvent(step_output=cur_step_output))<br>        return cur_step_output<br>    @dispatcher.span<br>    async def _arun_step(<br>        self,<br>        task_id: str,<br>        step: Optional[TaskStep] = None,<br>        input: Optional[str] = None,<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Execute step.\"\"\"<br>        dispatcher.event(<br>            AgentRunStepStartEvent(task_id=task_id, step=step, input=input)<br>        )<br>        task = self.state.get_task(task_id)<br>        step_queue = self.state.get_step_queue(task_id)<br>        step = step or step_queue.popleft()<br>        if input is not None:<br>            step.input = input<br>        if self.verbose:<br>            print(f\"> Running step {step.step_id}. Step input: {step.input}\")<br>        # TODO: figure out if you can dynamically swap in different step executors<br>        # not clear when you would do that by theoretically possible<br>        if mode == ChatResponseMode.WAIT:<br>            cur_step_output = await self.agent_worker.arun_step(step, task, **kwargs)<br>        elif mode == ChatResponseMode.STREAM:<br>            cur_step_output = await self.agent_worker.astream_step(step, task, **kwargs)<br>        else:<br>            raise ValueError(f\"Invalid mode: {mode}\")<br>        # append cur_step_output next steps to queue<br>        next_steps = cur_step_output.next_steps<br>        step_queue.extend(next_steps)<br>        # add cur_step_output to completed steps<br>        completed_steps = self.state.get_completed_steps(task_id)<br>        completed_steps.append(cur_step_output)<br>        dispatcher.event(AgentRunStepEndEvent(step_output=cur_step_output))<br>        return cur_step_output<br>    @dispatcher.span<br>    def run_step(<br>        self,<br>        task_id: str,<br>        input: Optional[str] = None,<br>        step: Optional[TaskStep] = None,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        step = validate_step_from_args(task_id, input, step, **kwargs)<br>        return self._run_step(<br>            task_id, step, input=input, mode=ChatResponseMode.WAIT, **kwargs<br>        )<br>    @dispatcher.span<br>    async def arun_step(<br>        self,<br>        task_id: str,<br>        input: Optional[str] = None,<br>        step: Optional[TaskStep] = None,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        step = validate_step_from_args(task_id, input, step, **kwargs)<br>        return await self._arun_step(<br>            task_id, step, input=input, mode=ChatResponseMode.WAIT, **kwargs<br>        )<br>    @dispatcher.span<br>    def stream_step(<br>        self,<br>        task_id: str,<br>        input: Optional[str] = None,<br>        step: Optional[TaskStep] = None,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        step = validate_step_from_args(task_id, input, step, **kwargs)<br>        return self._run_step(<br>            task_id, step, input=input, mode=ChatResponseMode.STREAM, **kwargs<br>        )<br>    @dispatcher.span<br>    async def astream_step(<br>        self,<br>        task_id: str,<br>        input: Optional[str] = None,<br>        step: Optional[TaskStep] = None,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async stream).\"\"\"<br>        step = validate_step_from_args(task_id, input, step, **kwargs)<br>        return await self._arun_step(<br>            task_id, step, input=input, mode=ChatResponseMode.STREAM, **kwargs<br>        )<br>    @dispatcher.span<br>    def finalize_response(<br>        self,<br>        task_id: str,<br>        step_output: Optional[TaskStepOutput] = None,<br>    ) -> AGENT_CHAT_RESPONSE_TYPE:<br>        \"\"\"Finalize response.\"\"\"<br>        if step_output is None:<br>            step_output = self.state.get_completed_steps(task_id)[-1]<br>        if not step_output.is_last:<br>            raise ValueError(<br>                \"finalize_response can only be called on the last step output\"<br>            )<br>        if not isinstance(<br>            step_output.output,<br>            (AgentChatResponse, StreamingAgentChatResponse),<br>        ):<br>            raise ValueError(<br>                \"When </code>is_last<code> is True, cur_step_output.output must be \"<br>                f\"AGENT_CHAT_RESPONSE_TYPE: {step_output.output}\"<br>            )<br>        # finalize task<br>        self.agent_worker.finalize_task(self.state.get_task(task_id))<br>        if self.delete_task_on_finish:<br>            self.delete_task(task_id)<br>        # Attach all sources generated across all steps<br>        step_output.output.sources = self.get_task(task_id).extra_state.get(<br>            \"sources\", []<br>        )<br>        step_output.output.set_source_nodes()<br>        return cast(AGENT_CHAT_RESPONSE_TYPE, step_output.output)<br>    @dispatcher.span<br>    def _chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Union[str, dict] = \"auto\",<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>    ) -> AGENT_CHAT_RESPONSE_TYPE:<br>        \"\"\"Chat with step executor.\"\"\"<br>        if chat_history is not None:<br>            self.memory.set(chat_history)<br>        task = self.create_task(message)<br>        result_output = None<br>        dispatcher.event(AgentChatWithStepStartEvent(user_msg=message))<br>        while True:<br>            # pass step queue in as argument, assume step executor is stateless<br>            cur_step_output = self._run_step(<br>                task.task_id, mode=mode, tool_choice=tool_choice<br>            )<br>            if cur_step_output.is_last:<br>                result_output = cur_step_output<br>                break<br>            # ensure tool_choice does not cause endless loops<br>            tool_choice = \"auto\"<br>        result = self.finalize_response(<br>            task.task_id,<br>            result_output,<br>        )<br>        dispatcher.event(AgentChatWithStepEndEvent(response=result))<br>        return result<br>    @dispatcher.span<br>    async def _achat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Union[str, dict] = \"auto\",<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>    ) -> AGENT_CHAT_RESPONSE_TYPE:<br>        \"\"\"Chat with step executor.\"\"\"<br>        if chat_history is not None:<br>            self.memory.set(chat_history)<br>        task = self.create_task(message)<br>        result_output = None<br>        dispatcher.event(AgentChatWithStepStartEvent(user_msg=message))<br>        while True:<br>            # pass step queue in as argument, assume step executor is stateless<br>            cur_step_output = await self._arun_step(<br>                task.task_id, mode=mode, tool_choice=tool_choice<br>            )<br>            if cur_step_output.is_last:<br>                result_output = cur_step_output<br>                break<br>            # ensure tool_choice does not cause endless loops<br>            tool_choice = \"auto\"<br>        result = self.finalize_response(<br>            task.task_id,<br>            result_output,<br>        )<br>        dispatcher.event(AgentChatWithStepEndEvent(response=result))<br>        return result<br>    @dispatcher.span<br>    @trace_method(\"chat\")<br>    def chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Optional[Union[str, dict]] = None,<br>    ) -> AgentChatResponse:<br>        # override tool choice is provided as input.<br>        if tool_choice is None:<br>            tool_choice = self.default_tool_choice<br>        with self.callback_manager.event(<br>            CBEventType.AGENT_STEP,<br>            payload={EventPayload.MESSAGES: [message]},<br>        ) as e:<br>            chat_response = self._chat(<br>                message=message,<br>                chat_history=chat_history,<br>                tool_choice=tool_choice,<br>                mode=ChatResponseMode.WAIT,<br>            )<br>            assert isinstance(chat_response, AgentChatResponse)<br>            e.on_end(payload={EventPayload.RESPONSE: chat_response})<br>        return chat_response<br>    @dispatcher.span<br>    @trace_method(\"chat\")<br>    async def achat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Optional[Union[str, dict]] = None,<br>    ) -> AgentChatResponse:<br>        # override tool choice is provided as input.<br>        if tool_choice is None:<br>            tool_choice = self.default_tool_choice<br>        with self.callback_manager.event(<br>            CBEventType.AGENT_STEP,<br>            payload={EventPayload.MESSAGES: [message]},<br>        ) as e:<br>            chat_response = await self._achat(<br>                message=message,<br>                chat_history=chat_history,<br>                tool_choice=tool_choice,<br>                mode=ChatResponseMode.WAIT,<br>            )<br>            assert isinstance(chat_response, AgentChatResponse)<br>            e.on_end(payload={EventPayload.RESPONSE: chat_response})<br>        return chat_response<br>    @dispatcher.span<br>    @trace_method(\"chat\")<br>    def stream_chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Optional[Union[str, dict]] = None,<br>    ) -> StreamingAgentChatResponse:<br>        # override tool choice is provided as input.<br>        if tool_choice is None:<br>            tool_choice = self.default_tool_choice<br>        with self.callback_manager.event(<br>            CBEventType.AGENT_STEP,<br>            payload={EventPayload.MESSAGES: [message]},<br>        ) as e:<br>            chat_response = self._chat(<br>                message, chat_history, tool_choice, mode=ChatResponseMode.STREAM<br>            )<br>            assert isinstance(chat_response, StreamingAgentChatResponse) or (<br>                isinstance(chat_response, AgentChatResponse)<br>                and chat_response.is_dummy_stream<br>            )<br>            e.on_end(payload={EventPayload.RESPONSE: chat_response})<br>        return chat_response  # type: ignore<br>    @dispatcher.span<br>    @trace_method(\"chat\")<br>    async def astream_chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Optional[Union[str, dict]] = None,<br>    ) -> StreamingAgentChatResponse:<br>        # override tool choice is provided as input.<br>        if tool_choice is None:<br>            tool_choice = self.default_tool_choice<br>        with self.callback_manager.event(<br>            CBEventType.AGENT_STEP,<br>            payload={EventPayload.MESSAGES: [message]},<br>        ) as e:<br>            chat_response = await self._achat(<br>                message, chat_history, tool_choice, mode=ChatResponseMode.STREAM<br>            )<br>            assert isinstance(chat_response, StreamingAgentChatResponse) or (<br>                isinstance(chat_response, AgentChatResponse)<br>                and chat_response.is_dummy_stream<br>            )<br>            e.on_end(payload={EventPayload.RESPONSE: chat_response})<br>        return chat_response  # type: ignore<br>    def undo_step(self, task_id: str) -> None:<br>        \"\"\"Undo previous step.\"\"\"<br>        raise NotImplementedError(\"undo_step not implemented\")<br></code>`` |\n"
    },
    {
      "id": "f0bd1658-b897-457c-9f7a-f2deb8f47399",
      "size": 5221,
      "headers": {
        "h1": "Core Agent Classes \\#",
        "h2": "Runners \\#",
        "h3": "ParallelAgentRunner \\#"
      },
      "text": "| ``<code><br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>472<br>473<br>474<br>475<br>476<br>477<br>478<br>479<br>480<br>481<br>482<br>483<br>484<br>485<br>486<br>487<br>488<br>489<br>490<br>491<br>492<br>493<br>494<br>495<br>496<br></code>`<code> | </code>`<code><br>class ParallelAgentRunner(BaseAgentRunner):<br>    \"\"\"Parallel agent runner.<br>    Executes steps in queue in parallel. Requires async support.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        agent_worker: BaseAgentWorker,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        state: Optional[DAGAgentState] = None,<br>        memory: Optional[BaseMemory] = None,<br>        llm: Optional[LLM] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        init_task_state_kwargs: Optional[dict] = None,<br>        delete_task_on_finish: bool = False,<br>    ) -> None:<br>        \"\"\"Initialize.\"\"\"<br>        self.memory = memory or ChatMemoryBuffer.from_defaults(chat_history, llm=llm)<br>        self.state = state or DAGAgentState()<br>        self.callback_manager = callback_manager or CallbackManager([])<br>        self.init_task_state_kwargs = init_task_state_kwargs or {}<br>        self.agent_worker = agent_worker<br>        self.delete_task_on_finish = delete_task_on_finish<br>    @property<br>    def chat_history(self) -> List[ChatMessage]:<br>        return self.memory.get_all()<br>    def reset(self) -> None:<br>        self.memory.reset()<br>    def create_task(self, input: str, **kwargs: Any) -> Task:<br>        \"\"\"Create task.\"\"\"<br>        task = Task(<br>            input=input,<br>            memory=self.memory,<br>            extra_state=self.init_task_state_kwargs,<br>            **kwargs,<br>        )<br>        # # put input into memory<br>        # self.memory.put(ChatMessage(content=input, role=MessageRole.USER))<br>        # add it to state<br>        # get initial step from task, and put it in the step queue<br>        initial_step = self.agent_worker.initialize_step(task)<br>        task_state = DAGTaskState(<br>            task=task,<br>            root_step=initial_step,<br>            step_queue=deque([initial_step]),<br>        )<br>        self.state.task_dict[task.task_id] = task_state<br>        return task<br>    def delete_task(<br>        self,<br>        task_id: str,<br>    ) -> None:<br>        \"\"\"Delete task.<br>        NOTE: this will not delete any previous executions from memory.<br>        \"\"\"<br>        self.state.task_dict.pop(task_id)<br>    def get_completed_tasks(self, **kwargs: Any) -> List[Task]:<br>        \"\"\"Get completed tasks.\"\"\"<br>        task_states = list(self.state.task_dict.values())<br>        return [<br>            task_state.task<br>            for task_state in task_states<br>            if len(task_state.completed_steps) > 0<br>            and task_state.completed_steps[-1].is_last<br>        ]<br>    def get_task_output(self, task_id: str, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Get task output.\"\"\"<br>        task_state = self.state.task_dict[task_id]<br>        if len(task_state.completed_steps) == 0:<br>            raise ValueError(f\"No completed steps for task_id: {task_id}\")<br>        return task_state.completed_steps[-1]<br>    def list_tasks(self, **kwargs: Any) -> List[Task]:<br>        \"\"\"List tasks.\"\"\"<br>        task_states = list(self.state.task_dict.values())<br>        return [task_state.task for task_state in task_states]<br>    def get_task(self, task_id: str, **kwargs: Any) -> Task:<br>        \"\"\"Get task.\"\"\"<br>        return self.state.get_task(task_id)<br>    def get_upcoming_steps(self, task_id: str, **kwargs: Any) -> List[TaskStep]:<br>        \"\"\"Get upcoming steps.\"\"\"<br>        return list(self.state.get_step_queue(task_id))<br>    def get_completed_steps(self, task_id: str, **kwargs: Any) -> List[TaskStepOutput]:<br>        \"\"\"Get completed steps.\"\"\"<br>        return self.state.get_completed_steps(task_id)<br>    def run_steps_in_queue(<br>        self,<br>        task_id: str,<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>        **kwargs: Any,<br>    ) -> List[TaskStepOutput]:<br>        \"\"\"Execute steps in queue.<br>        Run all steps in queue, clearing it out.<br>        Assume that all steps can be run in parallel.<br>        \"\"\"<br>        return asyncio_run(self.arun_steps_in_queue(task_id, mode=mode, **kwargs))<br>    async def arun_steps_in_queue(<br>        self,<br>        task_id: str,<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>        **kwargs: Any,<br>    ) -> List[TaskStepOutput]:<br>        \"\"\"Execute all steps in queue.<br>        All steps in queue are assumed to be ready.<br>        \"\"\"<br>        # first pop all steps from step_queue<br>        steps: List[TaskStep] = []<br>        while len(self.state.get_step_queue(task_id)) > 0:<br>            steps.append(self.state.get_step_queue(task_id).popleft())<br>        # take every item in the queue, and run it<br>        tasks = []<br>        for step in steps:<br>            tasks.append(self._arun_step(task_id, step=step, mode=mode, **kwargs))<br>        return await asyncio.gather(*tasks)<br>    def _run_step(<br>        self,<br>        task_id: str,<br>        step: Optional[TaskStep] = None,<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Execute step.\"\"\"<br>        task = self.state.get_task(task_id)<br>        task_queue = self.state.get_step_queue(task_id)<br>        step = step or task_queue.popleft()<br>        if not step.is_ready:<br>            raise ValueError(f\"Step {step.step_id} is not ready\")<br>        if mode == ChatResponseMode.WAIT:<br>            cur_step_output: TaskStepOutput = self.agent_worker.run_step(<br>                step, task, **kwargs<br>            )<br>        elif mode == ChatResponseMode.STREAM:<br>            cur_step_output = self.agent_worker.stream_step(step, task, **kwargs)<br>        else:<br>            raise ValueError(f\"Invalid mode: {mode}\")<br>        for next_step in cur_step_output.next_steps:<br>            if next_step.is_ready:<br>                task_queue.append(next_step)<br>        # add cur_step_output to completed steps<br>        completed_steps = self.state.get_completed_steps(task_id)<br>        completed_steps.append(cur_step_output)<br>        return cur_step_output<br>    async def _arun_step(<br>        self,<br>        task_id: str,<br>        step: Optional[TaskStep] = None,<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Execute step.\"\"\"<br>        task = self.state.get_task(task_id)<br>        task_queue = self.state.get_step_queue(task_id)<br>        step = step or task_queue.popleft()<br>        if not step.is_ready:<br>            raise ValueError(f\"Step {step.step_id} is not ready\")<br>        if mode == ChatResponseMode.WAIT:<br>            cur_step_output = await self.agent_worker.arun_step(step, task, **kwargs)<br>        elif mode == ChatResponseMode.STREAM:<br>            cur_step_output = await self.agent_worker.astream_step(step, task, **kwargs)<br>        else:<br>            raise ValueError(f\"Invalid mode: {mode}\")<br>        for next_step in cur_step_output.next_steps:<br>            if next_step.is_ready:<br>                task_queue.append(next_step)<br>        # add cur_step_output to completed steps<br>        completed_steps = self.state.get_completed_steps(task_id)<br>        completed_steps.append(cur_step_output)<br>        return cur_step_output<br>    def run_step(<br>        self,<br>        task_id: str,<br>        input: Optional[str] = None,<br>        step: Optional[TaskStep] = None,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        return self._run_step(task_id, step, mode=ChatResponseMode.WAIT, **kwargs)<br>    async def arun_step(<br>        self,<br>        task_id: str,<br>        input: Optional[str] = None,<br>        step: Optional[TaskStep] = None,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        return await self._arun_step(<br>            task_id, step, mode=ChatResponseMode.WAIT, **kwargs<br>        )<br>    def stream_step(<br>        self,<br>        task_id: str,<br>        input: Optional[str] = None,<br>        step: Optional[TaskStep] = None,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        return self._run_step(task_id, step, mode=ChatResponseMode.STREAM, **kwargs)<br>    async def astream_step(<br>        self,<br>        task_id: str,<br>        input: Optional[str] = None,<br>        step: Optional[TaskStep] = None,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async stream).\"\"\"<br>        return await self._arun_step(<br>            task_id, step, mode=ChatResponseMode.STREAM, **kwargs<br>        )<br>    def finalize_response(<br>        self,<br>        task_id: str,<br>        step_output: Optional[TaskStepOutput] = None,<br>    ) -> AGENT_CHAT_RESPONSE_TYPE:<br>        \"\"\"Finalize response.\"\"\"<br>        if step_output is None:<br>            step_output = self.state.get_completed_steps(task_id)[-1]<br>        if not step_output.is_last:<br>            raise ValueError(<br>                \"finalize_response can only be called on the last step output\"<br>            )<br>        if not isinstance(<br>            step_output.output,<br>            (AgentChatResponse, StreamingAgentChatResponse),<br>        ):<br>            raise ValueError(<br>                \"When </code>is_last<code> is True, cur_step_output.output must be \"<br>                f\"AGENT_CHAT_RESPONSE_TYPE: {step_output.output}\"<br>            )<br>        # finalize task<br>        self.agent_worker.finalize_task(self.state.get_task(task_id))<br>        if self.delete_task_on_finish:<br>            self.delete_task(task_id)<br>        return cast(AGENT_CHAT_RESPONSE_TYPE, step_output.output)<br>    def _chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Union[str, dict] = \"auto\",<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>    ) -> AGENT_CHAT_RESPONSE_TYPE:<br>        \"\"\"Chat with step executor.\"\"\"<br>        if chat_history is not None:<br>            self.memory.set(chat_history)<br>        task = self.create_task(message)<br>        result_output = None<br>        while True:<br>            # pass step queue in as argument, assume step executor is stateless<br>            cur_step_outputs = self.run_steps_in_queue(task.task_id, mode=mode)<br>            # check if a step output is_last<br>            is_last = any(<br>                cur_step_output.is_last for cur_step_output in cur_step_outputs<br>            )<br>            if is_last:<br>                if len(cur_step_outputs) > 1:<br>                    raise ValueError(<br>                        \"More than one step output returned in final step.\"<br>                    )<br>                cur_step_output = cur_step_outputs[0]<br>                result_output = cur_step_output<br>                break<br>        return self.finalize_response(<br>            task.task_id,<br>            result_output,<br>        )<br>    async def _achat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Union[str, dict] = \"auto\",<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>    ) -> AGENT_CHAT_RESPONSE_TYPE:<br>        \"\"\"Chat with step executor.\"\"\"<br>        if chat_history is not None:<br>            self.memory.set(chat_history)<br>        task = self.create_task(message)<br>        result_output = None<br>        while True:<br>            # pass step queue in as argument, assume step executor is stateless<br>            cur_step_outputs = await self.arun_steps_in_queue(task.task_id, mode=mode)<br>            # check if a step output is_last<br>            is_last = any(<br>                cur_step_output.is_last for cur_step_output in cur_step_outputs<br>            )<br>            if is_last:<br>                if len(cur_step_outputs) > 1:<br>                    raise ValueError(<br>                        \"More than one step output returned in final step.\"<br>                    )<br>                cur_step_output = cur_step_outputs[0]<br>                result_output = cur_step_output<br>                break<br>        return self.finalize_response(<br>            task.task_id,<br>            result_output,<br>        )<br>    @trace_method(\"chat\")<br>    def chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Union[str, dict] = \"auto\",<br>    ) -> AgentChatResponse:<br>        with self.callback_manager.event(<br>            CBEventType.AGENT_STEP,<br>            payload={EventPayload.MESSAGES: [message]},<br>        ) as e:<br>            chat_response = self._chat(<br>                message, chat_history, tool_choice, mode=ChatResponseMode.WAIT<br>            )<br>            assert isinstance(chat_response, AgentChatResponse)<br>            e.on_end(payload={EventPayload.RESPONSE: chat_response})<br>        return chat_response<br>    @trace_method(\"chat\")<br>    async def achat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Union[str, dict] = \"auto\",<br>    ) -> AgentChatResponse:<br>        with self.callback_manager.event(<br>            CBEventType.AGENT_STEP,<br>            payload={EventPayload.MESSAGES: [message]},<br>        ) as e:<br>            chat_response = await self._achat(<br>                message, chat_history, tool_choice, mode=ChatResponseMode.WAIT<br>            )<br>            assert isinstance(chat_response, AgentChatResponse)<br>            e.on_end(payload={EventPayload.RESPONSE: chat_response})<br>        return chat_response<br>    @trace_method(\"chat\")<br>    def stream_chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Union[str, dict] = \"auto\",<br>    ) -> StreamingAgentChatResponse:<br>        with self.callback_manager.event(<br>            CBEventType.AGENT_STEP,<br>            payload={EventPayload.MESSAGES: [message]},<br>        ) as e:<br>            chat_response = self._chat(<br>                message, chat_history, tool_choice, mode=ChatResponseMode.STREAM<br>            )<br>            e.on_end(payload={EventPayload.RESPONSE: chat_response})<br>        return chat_response  # type: ignore<br>    @trace_method(\"chat\")<br>    async def astream_chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Union[str, dict] = \"auto\",<br>    ) -> StreamingAgentChatResponse:<br>        with self.callback_manager.event(<br>            CBEventType.AGENT_STEP,<br>            payload={EventPayload.MESSAGES: [message]},<br>        ) as e:<br>            chat_response = await self._achat(<br>                message, chat_history, tool_choice, mode=ChatResponseMode.STREAM<br>            )<br>            e.on_end(payload={EventPayload.RESPONSE: chat_response})<br>        return chat_response  # type: ignore<br>    def undo_step(self, task_id: str) -> None:<br>        \"\"\"Undo previous step.\"\"\"<br>        raise NotImplementedError(\"undo_step not implemented\")<br></code>`` |\n"
    },
    {
      "id": "03e70fc9-0abb-4abe-9b05-1d79348ce12c",
      "size": 2964,
      "headers": {
        "h1": "Core Agent Classes \\#",
        "h2": "Workers \\#",
        "h3": "CustomSimpleAgentWorker \\#"
      },
      "text": "| ``<code><br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br></code>`<code> | </code>`<code><br>class CustomSimpleAgentWorker(BaseModel, BaseAgentWorker):<br>    \"\"\"Custom simple agent worker.<br>    This is \"simple\" in the sense that some of the scaffolding is setup already.<br>    Assumptions:<br>    - assumes that the agent has tools, llm, callback manager, and tool retriever<br>    - has a </code>from_tools<code> convenience function<br>    - assumes that the agent is sequential, and doesn't take in any additional<br>    intermediate inputs.<br>    Args:<br>        tools (Sequence[BaseTool]): Tools to use for reasoning<br>        llm (LLM): LLM to use<br>        callback_manager (CallbackManager): Callback manager<br>        tool_retriever (Optional[ObjectRetriever[BaseTool]]): Tool retriever<br>        verbose (bool): Whether to print out reasoning steps<br>    \"\"\"<br>    model_config = ConfigDict(arbitrary_types_allowed=True)<br>    tools: Sequence[BaseTool] = Field(..., description=\"Tools to use for reasoning\")<br>    llm: LLM = Field(..., description=\"LLM to use\")<br>    callback_manager: CallbackManager = Field(<br>        default_factory=lambda: CallbackManager([]), exclude=True<br>    )<br>    tool_retriever: Optional[ObjectRetriever[BaseTool]] = Field(<br>        default=None, description=\"Tool retriever\"<br>    )<br>    verbose: bool = Field(False, description=\"Whether to print out reasoning steps\")<br>    _get_tools: Callable[[str], Sequence[BaseTool]] = PrivateAttr()<br>    def __init__(<br>        self,<br>        tools: Sequence[BaseTool],<br>        llm: LLM,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        **kwargs: Any,<br>    ) -> None:<br>        callback_manager = callback_manager or CallbackManager([])<br>        super().__init__(<br>            tools=tools,<br>            llm=llm,<br>            callback_manager=callback_manager or CallbackManager([]),<br>            tool_retriever=tool_retriever,<br>            verbose=verbose,<br>            **kwargs,<br>        )<br>        if len(tools) > 0 and tool_retriever is not None:<br>            raise ValueError(\"Cannot specify both tools and tool_retriever\")<br>        elif len(tools) > 0:<br>            self._get_tools = lambda _: tools<br>        elif tool_retriever is not None:<br>            tool_retriever_c = cast(ObjectRetriever[BaseTool], tool_retriever)<br>            self._get_tools = lambda message: tool_retriever_c.retrieve(message)<br>        else:<br>            self._get_tools = lambda _: []<br>    @classmethod<br>    def from_tools(<br>        cls,<br>        tools: Optional[Sequence[BaseTool]] = None,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        llm: Optional[LLM] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        **kwargs: Any,<br>    ) -> \"CustomSimpleAgentWorker\":<br>        \"\"\"Convenience constructor method from set of of BaseTools (Optional).\"\"\"<br>        llm = llm or Settings.llm<br>        if callback_manager is not None:<br>            llm.callback_manager = callback_manager<br>        return cls(<br>            tools=tools or [],<br>            tool_retriever=tool_retriever,<br>            llm=llm,<br>            callback_manager=callback_manager or CallbackManager([]),<br>            verbose=verbose,<br>            **kwargs,<br>        )<br>    @abstractmethod<br>    def _initialize_state(self, task: Task, **kwargs: Any) -> Dict[str, Any]:<br>        \"\"\"Initialize state.\"\"\"<br>    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>        \"\"\"Initialize step from task.\"\"\"<br>        sources: List[ToolOutput] = []<br>        # temporary memory for new messages<br>        new_memory = ChatMemoryBuffer.from_defaults()<br>        # initialize initial state<br>        initial_state = {<br>            \"sources\": sources,<br>            \"memory\": new_memory,<br>        }<br>        step_state = self._initialize_state(task, **kwargs)<br>        # if intersecting keys, error<br>        if set(step_state.keys()).intersection(set(initial_state.keys())):<br>            raise ValueError(<br>                f\"Step state keys {step_state.keys()} and initial state keys {initial_state.keys()} intersect.\"<br>                f\"*NOTE*: initial state keys {initial_state.keys()} are reserved.\"<br>            )<br>        step_state.update(initial_state)<br>        return TaskStep(<br>            task_id=task.task_id,<br>            step_id=str(uuid.uuid4()),<br>            input=task.input,<br>            step_state=step_state,<br>        )<br>    def get_tools(self, input: str) -> List[AsyncBaseTool]:<br>        \"\"\"Get tools.\"\"\"<br>        return [adapt_to_async_tool(t) for t in self._get_tools(input)]<br>    def _get_task_step_response(<br>        self, agent_response: AGENT_CHAT_RESPONSE_TYPE, step: TaskStep, is_done: bool<br>    ) -> TaskStepOutput:<br>        \"\"\"Get task step response.\"\"\"<br>        if is_done:<br>            new_steps = []<br>        else:<br>            new_steps = [<br>                step.get_next_step(<br>                    step_id=str(uuid.uuid4()),<br>                    # NOTE: input is unused<br>                    input=None,<br>                )<br>            ]<br>        return TaskStepOutput(<br>            output=agent_response,<br>            task_step=step,<br>            is_last=is_done,<br>            next_steps=new_steps,<br>        )<br>    @abstractmethod<br>    def _run_step(<br>        self, state: Dict[str, Any], task: Task, input: Optional[str] = None<br>    ) -> Tuple[AgentChatResponse, bool]:<br>        \"\"\"Run step.<br>        Returns:<br>            Tuple of (agent_response, is_done)<br>        \"\"\"<br>    async def _arun_step(<br>        self, state: Dict[str, Any], task: Task, input: Optional[str] = None<br>    ) -> Tuple[AgentChatResponse, bool]:<br>        \"\"\"Run step (async).<br>        Can override this method if you want to run the step asynchronously.<br>        Returns:<br>            Tuple of (agent_response, is_done)<br>        \"\"\"<br>        raise NotImplementedError(<br>            \"This agent does not support async.\" \"Please implement _arun_step.\"<br>        )<br>    @trace_method(\"run_step\")<br>    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        agent_response, is_done = self._run_step(<br>            step.step_state, task, input=step.input<br>        )<br>        response = self._get_task_step_response(agent_response, step, is_done)<br>        # sync step state with task state<br>        task.extra_state.update(step.step_state)<br>        return response<br>    @trace_method(\"run_step\")<br>    async def arun_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        agent_response, is_done = await self._arun_step(<br>            step.step_state, task, input=step.input<br>        )<br>        response = self._get_task_step_response(agent_response, step, is_done)<br>        task.extra_state.update(step.step_state)<br>        return response<br>    @trace_method(\"run_step\")<br>    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        raise NotImplementedError(\"This agent does not support streaming.\")<br>    @trace_method(\"run_step\")<br>    async def astream_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async stream).\"\"\"<br>        raise NotImplementedError(\"This agent does not support streaming.\")<br>    @abstractmethod<br>    def _finalize_task(self, state: Dict[str, Any], **kwargs: Any) -> None:<br>        \"\"\"Finalize task, after all the steps are completed.<br>        State is all the step states.<br>        \"\"\"<br>    def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>        \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>        # add new messages to memory<br>        task.memory.set(task.memory.get() + task.extra_state[\"memory\"].get_all())<br>        # reset new memory<br>        task.extra_state[\"memory\"].reset()<br>        self._finalize_task(task.extra_state, **kwargs)<br>    def set_callback_manager(self, callback_manager: CallbackManager) -> None:<br>        \"\"\"Set callback manager.\"\"\"<br>        # TODO: make this abstractmethod (right now will break some agent impls)<br>        self.callback_manager = callback_manager<br></code>`` |\n"
    },
    {
      "id": "45121071-663a-48f3-aa49-4e3e943c130f",
      "size": 5511,
      "headers": {
        "h1": "Core Agent Classes \\#",
        "h2": "Workers \\#",
        "h3": "MultimodalReActAgentWorker \\#"
      },
      "text": "| ``<code><br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>472<br>473<br>474<br>475<br>476<br>477<br>478<br>479<br>480<br>481<br>482<br>483<br>484<br>485<br>486<br>487<br>488<br>489<br>490<br>491<br>492<br>493<br>494<br>495<br>496<br>497<br>498<br>499<br>500<br>501<br>502<br>503<br>504<br>505<br>506<br>507<br>508<br>509<br>510<br>511<br>512<br>513<br>514<br>515<br>516<br>517<br>518<br>519<br>520<br>521<br>522<br>523<br>524<br></code>`<code> | </code>`<code><br>class MultimodalReActAgentWorker(BaseAgentWorker):<br>    \"\"\"Multimodal ReAct Agent worker.<br>    **NOTE**: This is a BETA feature.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        tools: Sequence[BaseTool],<br>        multi_modal_llm: MultiModalLLM,<br>        max_iterations: int = 10,<br>        react_chat_formatter: Optional[ReActChatFormatter] = None,<br>        output_parser: Optional[ReActOutputParser] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>    ) -> None:<br>        self._multi_modal_llm = multi_modal_llm<br>        self.callback_manager = callback_manager or CallbackManager([])<br>        self._max_iterations = max_iterations<br>        self._react_chat_formatter = react_chat_formatter or ReActChatFormatter(<br>            system_header=REACT_MM_CHAT_SYSTEM_HEADER<br>        )<br>        self._output_parser = output_parser or ReActOutputParser()<br>        self._verbose = verbose<br>        try:<br>            from llama_index.multi_modal_llms.openai.utils import (<br>                generate_openai_multi_modal_chat_message,<br>            )  # pants: no-infer-dep<br>            self._add_user_step_to_reasoning = partial(<br>                add_user_step_to_reasoning,<br>                generate_chat_message_fn=generate_openai_multi_modal_chat_message,  # type: ignore<br>            )<br>        except ImportError:<br>            raise ImportError(<br>                \"</code>llama-index-multi-modal-llms-openai<code> package cannot be found. \"<br>                \"Please install it by using </code>pip install <code>llama-index-multi-modal-llms-openai</code>\"<br>            )<br>        if len(tools) > 0 and tool_retriever is not None:<br>            raise ValueError(\"Cannot specify both tools and tool_retriever\")<br>        elif len(tools) > 0:<br>            self._get_tools = lambda _: tools<br>        elif tool_retriever is not None:<br>            tool_retriever_c = cast(ObjectRetriever[BaseTool], tool_retriever)<br>            self._get_tools = lambda message: tool_retriever_c.retrieve(message)<br>        else:<br>            self._get_tools = lambda _: []<br>    @classmethod<br>    def from_tools(<br>        cls,<br>        tools: Optional[Sequence[BaseTool]] = None,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        multi_modal_llm: Optional[MultiModalLLM] = None,<br>        max_iterations: int = 10,<br>        react_chat_formatter: Optional[ReActChatFormatter] = None,<br>        output_parser: Optional[ReActOutputParser] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        **kwargs: Any,<br>    ) -> \"MultimodalReActAgentWorker\":<br>        \"\"\"Convenience constructor method from set of of BaseTools (Optional).<br>        NOTE: kwargs should have been exhausted by this point. In other words<br>        the various upstream components such as BaseSynthesizer (response synthesizer)<br>        or BaseRetriever should have picked up off their respective kwargs in their<br>        constructions.<br>        Returns:<br>            ReActAgent<br>        \"\"\"<br>        if multi_modal_llm is None:<br>            try:<br>                from llama_index.multi_modal_llms.openai import (<br>                    OpenAIMultiModal,<br>                )  # pants: no-infer-dep<br>                multi_modal_llm = multi_modal_llm or OpenAIMultiModal(<br>                    model=\"gpt-4-vision-preview\", max_new_tokens=1000<br>                )<br>            except ImportError:<br>                raise ImportError(<br>                    \"<code>llama-index-multi-modal-llms-openai</code> package cannot be found. \"<br>                    \"Please install it by using <code>pip install </code>llama-index-multi-modal-llms-openai<code>\"<br>                )<br>        return cls(<br>            tools=tools or [],<br>            tool_retriever=tool_retriever,<br>            multi_modal_llm=multi_modal_llm,<br>            max_iterations=max_iterations,<br>            react_chat_formatter=react_chat_formatter,<br>            output_parser=output_parser,<br>            callback_manager=callback_manager,<br>            verbose=verbose,<br>        )<br>    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>        \"\"\"Initialize step from task.\"\"\"<br>        sources: List[ToolOutput] = []<br>        current_reasoning: List[BaseReasoningStep] = []<br>        # temporary memory for new messages<br>        new_memory = ChatMemoryBuffer.from_defaults()<br>        # validation<br>        if \"image_docs\" not in task.extra_state:<br>            raise ValueError(\"Image docs not found in task extra state.\")<br>        # initialize task state<br>        task_state = {<br>            \"sources\": sources,<br>            \"current_reasoning\": current_reasoning,<br>            \"new_memory\": new_memory,<br>        }<br>        task.extra_state.update(task_state)<br>        return TaskStep(<br>            task_id=task.task_id,<br>            step_id=str(uuid.uuid4()),<br>            input=task.input,<br>            step_state={\"is_first\": True, \"image_docs\": task.extra_state[\"image_docs\"]},<br>        )<br>    def get_tools(self, input: str) -> List[AsyncBaseTool]:<br>        \"\"\"Get tools.\"\"\"<br>        return [adapt_to_async_tool(t) for t in self._get_tools(input)]<br>    def _extract_reasoning_step(<br>        self, output: ChatResponse, is_streaming: bool = False<br>    ) -> Tuple[str, List[BaseReasoningStep], bool]:<br>        \"\"\"<br>        Extracts the reasoning step from the given output.<br>        This method parses the message content from the output,<br>        extracts the reasoning step, and determines whether the processing is<br>        complete. It also performs validation checks on the output and<br>        handles possible errors.<br>        \"\"\"<br>        if output.message.content is None:<br>            raise ValueError(\"Got empty message.\")<br>        message_content = output.message.content<br>        current_reasoning = []<br>        try:<br>            reasoning_step = self._output_parser.parse(message_content, is_streaming)<br>        except BaseException as exc:<br>            raise ValueError(f\"Could not parse output: {message_content}\") from exc<br>        if self._verbose:<br>            print_text(f\"{reasoning_step.get_content()}\\n\", color=\"pink\")<br>        current_reasoning.append(reasoning_step)<br>        if reasoning_step.is_done:<br>            return message_content, current_reasoning, True<br>        reasoning_step = cast(ActionReasoningStep, reasoning_step)<br>        if not isinstance(reasoning_step, ActionReasoningStep):<br>            raise ValueError(f\"Expected ActionReasoningStep, got {reasoning_step}\")<br>        return message_content, current_reasoning, False<br>    def _process_actions(<br>        self,<br>        task: Task,<br>        tools: Sequence[AsyncBaseTool],<br>        output: ChatResponse,<br>        is_streaming: bool = False,<br>    ) -> Tuple[List[BaseReasoningStep], bool]:<br>        tools_dict: Dict[str, AsyncBaseTool] = {<br>            tool.metadata.get_name(): tool for tool in tools<br>        }<br>        _, current_reasoning, is_done = self._extract_reasoning_step(<br>            output, is_streaming<br>        )<br>        if is_done:<br>            return current_reasoning, True<br>        # call tool with input<br>        reasoning_step = cast(ActionReasoningStep, current_reasoning[-1])<br>        tool = tools_dict[reasoning_step.action]<br>        with self.callback_manager.event(<br>            CBEventType.FUNCTION_CALL,<br>            payload={<br>                EventPayload.FUNCTION_CALL: reasoning_step.action_input,<br>                EventPayload.TOOL: tool.metadata,<br>            },<br>        ) as event:<br>            tool_output = tool.call(**reasoning_step.action_input)<br>            event.on_end(payload={EventPayload.FUNCTION_OUTPUT: str(tool_output)})<br>        task.extra_state[\"sources\"].append(tool_output)<br>        observation_step = ObservationReasoningStep(<br>            observation=str(tool_output), return_direct=tool.metadata.return_direct<br>        )<br>        current_reasoning.append(observation_step)<br>        if self._verbose:<br>            print_text(f\"{observation_step.get_content()}\\n\", color=\"blue\")<br>        return current_reasoning, tool.metadata.return_direct<br>    async def _aprocess_actions(<br>        self,<br>        task: Task,<br>        tools: Sequence[AsyncBaseTool],<br>        output: ChatResponse,<br>        is_streaming: bool = False,<br>    ) -> Tuple[List[BaseReasoningStep], bool]:<br>        tools_dict = {tool.metadata.name: tool for tool in tools}<br>        _, current_reasoning, is_done = self._extract_reasoning_step(<br>            output, is_streaming<br>        )<br>        if is_done:<br>            return current_reasoning, True<br>        # call tool with input<br>        reasoning_step = cast(ActionReasoningStep, current_reasoning[-1])<br>        tool = tools_dict[reasoning_step.action]<br>        with self.callback_manager.event(<br>            CBEventType.FUNCTION_CALL,<br>            payload={<br>                EventPayload.FUNCTION_CALL: reasoning_step.action_input,<br>                EventPayload.TOOL: tool.metadata,<br>            },<br>        ) as event:<br>            tool_output = await tool.acall(**reasoning_step.action_input)<br>            event.on_end(payload={EventPayload.FUNCTION_OUTPUT: str(tool_output)})<br>        task.extra_state[\"sources\"].append(tool_output)<br>        observation_step = ObservationReasoningStep(<br>            observation=str(tool_output), return_direct=tool.metadata.return_direct<br>        )<br>        current_reasoning.append(observation_step)<br>        if self._verbose:<br>            print_text(f\"{observation_step.get_content()}\\n\", color=\"blue\")<br>        return current_reasoning, tool.metadata.return_direct<br>    def _get_response(<br>        self,<br>        current_reasoning: List[BaseReasoningStep],<br>        sources: List[ToolOutput],<br>    ) -> AgentChatResponse:<br>        \"\"\"Get response from reasoning steps.\"\"\"<br>        if len(current_reasoning) == 0:<br>            raise ValueError(\"No reasoning steps were taken.\")<br>        elif len(current_reasoning) == self._max_iterations:<br>            raise ValueError(\"Reached max iterations.\")<br>        if isinstance(current_reasoning[-1], ResponseReasoningStep):<br>            response_step = cast(ResponseReasoningStep, current_reasoning[-1])<br>            response_str = response_step.response<br>        elif (<br>            isinstance(current_reasoning[-1], ObservationReasoningStep)<br>            and current_reasoning[-1].return_direct<br>        ):<br>            response_str = current_reasoning[-1].observation<br>        else:<br>            response_str = current_reasoning[-1].get_content()<br>        # TODO: add sources from reasoning steps<br>        return AgentChatResponse(response=response_str, sources=sources)<br>    def _get_task_step_response(<br>        self, agent_response: AGENT_CHAT_RESPONSE_TYPE, step: TaskStep, is_done: bool<br>    ) -> TaskStepOutput:<br>        \"\"\"Get task step response.\"\"\"<br>        if is_done:<br>            new_steps = []<br>        else:<br>            new_steps = [<br>                step.get_next_step(<br>                    step_id=str(uuid.uuid4()),<br>                    # NOTE: input is unused<br>                    input=None,<br>                )<br>            ]<br>        return TaskStepOutput(<br>            output=agent_response,<br>            task_step=step,<br>            is_last=is_done,<br>            next_steps=new_steps,<br>        )<br>    def _run_step(<br>        self,<br>        step: TaskStep,<br>        task: Task,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        # This is either not None on the first step or if the user specifies<br>        # an intermediate step in the middle<br>        if step.input is not None:<br>            self._add_user_step_to_reasoning(<br>                step=step,<br>                memory=task.extra_state[\"new_memory\"],<br>                current_reasoning=task.extra_state[\"current_reasoning\"],<br>                verbose=self._verbose,<br>            )<br>        # TODO: see if we want to do step-based inputs<br>        tools = self.get_tools(task.input)<br>        input_chat = self._react_chat_formatter.format(<br>            tools,<br>            chat_history=task.memory.get_all()<br>            + task.extra_state[\"new_memory\"].get_all(),<br>            current_reasoning=task.extra_state[\"current_reasoning\"],<br>        )<br>        # send prompt<br>        chat_response = self._multi_modal_llm.chat(input_chat)<br>        # given react prompt outputs, call tools or return response<br>        reasoning_steps, is_done = self._process_actions(<br>            task, tools, output=chat_response<br>        )<br>        task.extra_state[\"current_reasoning\"].extend(reasoning_steps)<br>        agent_response = self._get_response(<br>            task.extra_state[\"current_reasoning\"], task.extra_state[\"sources\"]<br>        )<br>        if is_done:<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(content=agent_response.response, role=MessageRole.ASSISTANT)<br>            )<br>        return self._get_task_step_response(agent_response, step, is_done)<br>    async def _arun_step(<br>        self,<br>        step: TaskStep,<br>        task: Task,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        if step.input is not None:<br>            self._add_user_step_to_reasoning(<br>                step=step,<br>                memory=task.extra_state[\"new_memory\"],<br>                current_reasoning=task.extra_state[\"current_reasoning\"],<br>                verbose=self._verbose,<br>            )<br>        # TODO: see if we want to do step-based inputs<br>        tools = self.get_tools(task.input)<br>        input_chat = self._react_chat_formatter.format(<br>            tools,<br>            chat_history=task.memory.get_all()<br>            + task.extra_state[\"new_memory\"].get_all(),<br>            current_reasoning=task.extra_state[\"current_reasoning\"],<br>        )<br>        # send prompt<br>        chat_response = await self._multi_modal_llm.achat(input_chat)<br>        # given react prompt outputs, call tools or return response<br>        reasoning_steps, is_done = await self._aprocess_actions(<br>            task, tools, output=chat_response<br>        )<br>        task.extra_state[\"current_reasoning\"].extend(reasoning_steps)<br>        agent_response = self._get_response(<br>            task.extra_state[\"current_reasoning\"], task.extra_state[\"sources\"]<br>        )<br>        if is_done:<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(content=agent_response.response, role=MessageRole.ASSISTANT)<br>            )<br>        return self._get_task_step_response(agent_response, step, is_done)<br>    def _run_step_stream(<br>        self,<br>        step: TaskStep,<br>        task: Task,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        raise NotImplementedError(\"Stream step not implemented yet.\")<br>    async def _arun_step_stream(<br>        self,<br>        step: TaskStep,<br>        task: Task,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        raise NotImplementedError(\"Stream step not implemented yet.\")<br>    @trace_method(\"run_step\")<br>    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        return self._run_step(step, task)<br>    @trace_method(\"run_step\")<br>    async def arun_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        return await self._arun_step(step, task)<br>    @trace_method(\"run_step\")<br>    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        # TODO: figure out if we need a different type for TaskStepOutput<br>        return self._run_step_stream(step, task)<br>    @trace_method(\"run_step\")<br>    async def astream_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async stream).\"\"\"<br>        return await self._arun_step_stream(step, task)<br>    def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>        \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>        # add new messages to memory<br>        task.memory.set(<br>            task.memory.get_all() + task.extra_state[\"new_memory\"].get_all()<br>        )<br>        # reset new memory<br>        task.extra_state[\"new_memory\"].reset()<br>    def set_callback_manager(self, callback_manager: CallbackManager) -> None:<br>        \"\"\"Set callback manager.\"\"\"<br>        # TODO: make this abstractmethod (right now will break some agent impls)<br>        self.callback_manager = callback_manager<br></code>`` |\n"
    },
    {
      "id": "6897c2db-6208-4149-ad27-f642c380802e",
      "size": 2577,
      "headers": {
        "h1": "Core Agent Classes \\#",
        "h2": "Workers \\#",
        "h3": "QueryPipelineAgentWorker \\#"
      },
      "text": "| ``<code><br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br></code>`<code> | </code>`<code><br>@deprecated(\"Use </code>FnAgentWorker<code> instead to build a stateful agent.\")<br>class QueryPipelineAgentWorker(BaseModel, BaseAgentWorker):<br>    \"\"\"Query Pipeline agent worker.<br>    NOTE: This is now deprecated. Use </code>FnAgentWorker<code> instead to build a stateful agent.<br>    Barebones agent worker that takes in a query pipeline.<br>    **Default Workflow**: The default workflow assumes that you compose<br>    a query pipeline with </code>StatefulFnComponent<code> objects. This allows you to store, update<br>    and retrieve state throughout the executions of the query pipeline by the agent.<br>    The task and step state of the agent are stored in this </code>state<code> variable via a special key.<br>    Of course you can choose to store other variables in this state as well.<br>    **Deprecated Workflow**: The deprecated workflow assumes that the first component in the<br>    query pipeline is an </code>AgentInputComponent<code> and last is </code>AgentFnComponent<code>.<br>    Args:<br>        pipeline (QueryPipeline): Query pipeline<br>    \"\"\"<br>    model_config = ConfigDict(arbitrary_types_allowed=True)<br>    pipeline: QueryPipeline = Field(..., description=\"Query pipeline\")<br>    callback_manager: CallbackManager = Field(..., exclude=True)<br>    task_key: str = Field(\"task\", description=\"Key to store task in state\")<br>    step_state_key: str = Field(\"step_state\", description=\"Key to store step in state\")<br>    def __init__(<br>        self,<br>        pipeline: QueryPipeline,<br>        callback_manager: Optional[CallbackManager] = None,<br>        **kwargs: Any,<br>    ) -> None:<br>        \"\"\"Initialize.\"\"\"<br>        if callback_manager is not None:<br>            # set query pipeline callback<br>            pipeline.set_callback_manager(callback_manager)<br>        else:<br>            callback_manager = pipeline.callback_manager<br>        super().__init__(<br>            pipeline=pipeline,<br>            callback_manager=callback_manager,<br>            **kwargs,<br>        )<br>        # validate query pipeline<br>        # self.agent_input_component<br>        self.agent_components<br>    @property<br>    def agent_input_component(self) -> AgentInputComponent:<br>        \"\"\"Get agent input component.<br>        NOTE: This is deprecated and will be removed in the future.<br>        \"\"\"<br>        root_key = self.pipeline.get_root_keys()[0]<br>        if not isinstance(self.pipeline.module_dict[root_key], AgentInputComponent):<br>            raise ValueError(<br>                \"Query pipeline first component must be AgentInputComponent, got \"<br>                f\"{self.pipeline.module_dict[root_key]}\"<br>            )<br>        return cast(AgentInputComponent, self.pipeline.module_dict[root_key])<br>    @property<br>    def agent_components(self) -> Sequence[BaseAgentComponent]:<br>        \"\"\"Get agent output component.\"\"\"<br>        return _get_agent_components(self.pipeline)<br>    def preprocess(self, task: Task, step: TaskStep) -> None:<br>        \"\"\"Preprocessing flow.<br>        This runs preprocessing to propagate the task and step as variables<br>        to relevant components in the query pipeline.<br>        Contains deprecated flow of updating agent components.<br>        But also contains main flow of updating StatefulFnComponent components.<br>        \"\"\"<br>        # NOTE: this is deprecated<br>        # partial agent output component with task and step<br>        for agent_fn_component in self.agent_components:<br>            agent_fn_component.partial(task=task, state=step.step_state)<br>        # update stateful components<br>        self.pipeline.update_state(<br>            {self.task_key: task, self.step_state_key: step.step_state}<br>        )<br>    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>        \"\"\"Initialize step from task.\"\"\"<br>        sources: List[ToolOutput] = []<br>        # temporary memory for new messages<br>        new_memory = ChatMemoryBuffer.from_defaults()<br>        # initialize initial state<br>        initial_state = {<br>            \"sources\": sources,<br>            \"memory\": new_memory,<br>        }<br>        return TaskStep(<br>            task_id=task.task_id,<br>            step_id=str(uuid.uuid4()),<br>            input=task.input,<br>            step_state=initial_state,<br>        )<br>    def _get_task_step_response(<br>        self, agent_response: AGENT_CHAT_RESPONSE_TYPE, step: TaskStep, is_done: bool<br>    ) -> TaskStepOutput:<br>        \"\"\"Get task step response.\"\"\"<br>        if is_done:<br>            new_steps = []<br>        else:<br>            new_steps = [<br>                step.get_next_step(<br>                    step_id=str(uuid.uuid4()),<br>                    # NOTE: input is unused<br>                    input=None,<br>                )<br>            ]<br>        return TaskStepOutput(<br>            output=agent_response,<br>            task_step=step,<br>            is_last=is_done,<br>            next_steps=new_steps,<br>        )<br>    @trace_method(\"run_step\")<br>    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        self.preprocess(task, step)<br>        # HACK: do a try/except for now. Fine since old agent components are deprecated<br>        try:<br>            self.agent_input_component<br>            uses_deprecated = True<br>        except ValueError:<br>            uses_deprecated = False<br>        if uses_deprecated:<br>            agent_response, is_done = self.pipeline.run(<br>                state=step.step_state, task=task<br>            )<br>        else:<br>            agent_response, is_done = self.pipeline.run()<br>        response = self._get_task_step_response(agent_response, step, is_done)<br>        # sync step state with task state<br>        task.extra_state.update(step.step_state)<br>        return response<br>    @trace_method(\"run_step\")<br>    async def arun_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        self.preprocess(task, step)<br>        # HACK: do a try/except for now. Fine since old agent components are deprecated<br>        try:<br>            self.agent_input_component<br>            uses_deprecated = True<br>        except ValueError:<br>            uses_deprecated = False<br>        if uses_deprecated:<br>            agent_response, is_done = await self.pipeline.arun(<br>                state=step.step_state, task=task<br>            )<br>        else:<br>            agent_response, is_done = await self.pipeline.arun()<br>        response = self._get_task_step_response(agent_response, step, is_done)<br>        task.extra_state.update(step.step_state)<br>        return response<br>    @trace_method(\"run_step\")<br>    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        raise NotImplementedError(\"This agent does not support streaming.\")<br>    @trace_method(\"run_step\")<br>    async def astream_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async stream).\"\"\"<br>        raise NotImplementedError(\"This agent does not support streaming.\")<br>    def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>        \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>        # add new messages to memory<br>        task.memory.set(task.memory.get() + task.extra_state[\"memory\"].get_all())<br>        # reset new memory<br>        task.extra_state[\"memory\"].reset()<br>    def set_callback_manager(self, callback_manager: CallbackManager) -> None:<br>        \"\"\"Set callback manager.\"\"\"<br>        # TODO: make this abstractmethod (right now will break some agent impls)<br>        self.callback_manager = callback_manager<br>        self.pipeline.set_callback_manager(callback_manager)<br></code>`` |\n"
    },
    {
      "id": "1922c378-79cd-442f-a254-ce09328c747e",
      "size": 6690,
      "headers": {
        "h1": "Wandb",
        "h2": "WandbCallbackHandler \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>472<br>473<br>474<br>475<br>476<br>477<br>478<br>479<br>480<br>481<br>482<br>483<br>484<br>485<br>486<br>487<br>488<br>489<br>490<br>491<br>492<br>493<br>494<br>495<br>496<br>497<br>498<br>499<br>500<br>501<br>502<br>503<br>504<br>505<br>506<br>507<br>508<br>509<br>510<br>511<br>512<br>513<br>514<br>515<br>516<br>517<br>518<br>519<br>520<br>521<br>522<br>523<br>524<br>525<br>526<br>527<br>528<br>529<br>530<br>531<br>532<br>533<br>534<br>535<br>536<br>537<br>538<br>539<br>540<br>541<br>542<br>543<br>544<br>545<br>546<br>547<br>548<br>549<br>550<br>551<br>552<br>553<br>554<br>555<br>556<br>557<br>558<br>559<br>560<br>561<br>562<br>563<br>564<br>565<br>566<br>567<br>568<br>569<br>570<br></code>`<code> | </code>`<code><br>class WandbCallbackHandler(BaseCallbackHandler):<br>    \"\"\"Callback handler that logs events to wandb.<br>    NOTE: this is a beta feature. The usage within our codebase, and the interface<br>    may change.<br>    Use the </code>WandbCallbackHandler<code> to log trace events to wandb. This handler is<br>    useful for debugging and visualizing the trace events. It captures the payload of<br>    the events and logs them to wandb. The handler also tracks the start and end of<br>    events. This is particularly useful for debugging your LLM calls.<br>    The </code>WandbCallbackHandler<code> can also be used to log the indices and graphs to wandb<br>    using the </code>persist_index<code> method. This will save the indexes as artifacts in wandb.<br>    The </code>load_storage_context<code> method can be used to load the indexes from wandb<br>    artifacts. This method will return a </code>StorageContext<code> object that can be used to<br>    build the index, using </code>load_index_from_storage<code>, </code>load_indices_from_storage<code> or<br>    </code>load_graph_from_storage<code> functions.<br>    Args:<br>        event_starts_to_ignore (Optional[List[CBEventType]]): list of event types to<br>            ignore when tracking event starts.<br>        event_ends_to_ignore (Optional[List[CBEventType]]): list of event types to<br>            ignore when tracking event ends.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        run_args: Optional[WandbRunArgs] = None,<br>        tokenizer: Optional[Callable[[str], List]] = None,<br>        event_starts_to_ignore: Optional[List[CBEventType]] = None,<br>        event_ends_to_ignore: Optional[List[CBEventType]] = None,<br>    ) -> None:<br>        try:<br>            import wandb<br>            from wandb.sdk.data_types import trace_tree<br>            self._wandb = wandb<br>            self._trace_tree = trace_tree<br>        except ImportError:<br>            raise ImportError(<br>                \"WandbCallbackHandler requires wandb. \"<br>                \"Please install it with </code>pip install wandb<code>.\"<br>            )<br>        from llama_index.core.indices import (<br>            ComposableGraph,<br>            GPTEmptyIndex,<br>            GPTKeywordTableIndex,<br>            GPTRAKEKeywordTableIndex,<br>            GPTSimpleKeywordTableIndex,<br>            GPTSQLStructStoreIndex,<br>            GPTTreeIndex,<br>            GPTVectorStoreIndex,<br>            SummaryIndex,<br>        )<br>        self._IndexType = (<br>            ComposableGraph,<br>            GPTKeywordTableIndex,<br>            GPTSimpleKeywordTableIndex,<br>            GPTRAKEKeywordTableIndex,<br>            SummaryIndex,<br>            GPTEmptyIndex,<br>            GPTTreeIndex,<br>            GPTVectorStoreIndex,<br>            GPTSQLStructStoreIndex,<br>        )<br>        self._run_args = run_args<br>        # Check if a W&B run is already initialized; if not, initialize one<br>        self._ensure_run(should_print_url=(self._wandb.run is None))  # type: ignore[attr-defined]<br>        self._event_pairs_by_id: Dict[str, List[CBEvent]] = defaultdict(list)<br>        self._cur_trace_id: Optional[str] = None<br>        self._trace_map: Dict[str, List[str]] = defaultdict(list)<br>        self.tokenizer = tokenizer or get_tokenizer()<br>        self._token_counter = TokenCounter(tokenizer=self.tokenizer)<br>        event_starts_to_ignore = (<br>            event_starts_to_ignore if event_starts_to_ignore else []<br>        )<br>        event_ends_to_ignore = event_ends_to_ignore if event_ends_to_ignore else []<br>        super().__init__(<br>            event_starts_to_ignore=event_starts_to_ignore,<br>            event_ends_to_ignore=event_ends_to_ignore,<br>        )<br>    def on_event_start(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        parent_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> str:<br>        \"\"\"Store event start data by event type.<br>        Args:<br>            event_type (CBEventType): event type to store.<br>            payload (Optional[Dict[str, Any]]): payload to store.<br>            event_id (str): event id to store.<br>            parent_id (str): parent event id.<br>        \"\"\"<br>        event = CBEvent(event_type, payload=payload, id_=event_id)<br>        self._event_pairs_by_id[event.id_].append(event)<br>        return event.id_<br>    def on_event_end(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> None:<br>        \"\"\"Store event end data by event type.<br>        Args:<br>            event_type (CBEventType): event type to store.<br>            payload (Optional[Dict[str, Any]]): payload to store.<br>            event_id (str): event id to store.<br>        \"\"\"<br>        event = CBEvent(event_type, payload=payload, id_=event_id)<br>        self._event_pairs_by_id[event.id_].append(event)<br>        self._trace_map = defaultdict(list)<br>    def start_trace(self, trace_id: Optional[str] = None) -> None:<br>        \"\"\"Launch a trace.\"\"\"<br>        self._trace_map = defaultdict(list)<br>        self._cur_trace_id = trace_id<br>        self._start_time = datetime.now()<br>    def end_trace(<br>        self,<br>        trace_id: Optional[str] = None,<br>        trace_map: Optional[Dict[str, List[str]]] = None,<br>    ) -> None:<br>        # Ensure W&B run is initialized<br>        self._ensure_run()<br>        self._trace_map = trace_map or defaultdict(list)<br>        self._end_time = datetime.now()<br>        # Log the trace map to wandb<br>        # We can control what trace ids we want to log here.<br>        self.log_trace_tree()<br>        # TODO (ayulockin): Log the LLM token counts to wandb when weave is ready<br>    def log_trace_tree(self) -> None:<br>        \"\"\"Log the trace tree to wandb.\"\"\"<br>        try:<br>            child_nodes = self._trace_map[\"root\"]<br>            root_span = self._convert_event_pair_to_wb_span(<br>                self._event_pairs_by_id[child_nodes[0]],<br>                trace_id=self._cur_trace_id if len(child_nodes) > 1 else None,<br>            )<br>            if len(child_nodes) == 1:<br>                child_nodes = self._trace_map[child_nodes[0]]<br>                root_span = self._build_trace_tree(child_nodes, root_span)<br>            else:<br>                root_span = self._build_trace_tree(child_nodes, root_span)<br>            if root_span:<br>                root_trace = self._trace_tree.WBTraceTree(root_span)<br>                if self._wandb.run:  # type: ignore[attr-defined]<br>                    self._wandb.run.log({\"trace\": root_trace})  # type: ignore[attr-defined]<br>                self._wandb.termlog(\"Logged trace tree to W&B.\")  # type: ignore[attr-defined]<br>        except Exception as e:<br>            print(f\"Failed to log trace tree to W&B: {e}\")<br>            # ignore errors to not break user code<br>    def persist_index(<br>        self, index: \"IndexType\", index_name: str, persist_dir: Union[str, None] = None<br>    ) -> None:<br>        \"\"\"Upload an index to wandb as an artifact. You can learn more about W&B<br>        artifacts here: https://docs.wandb.ai/guides/artifacts.<br>        For the </code>ComposableGraph<code> index, the root id is stored as artifact metadata.<br>        Args:<br>            index (IndexType): index to upload.<br>            index_name (str): name of the index. This will be used as the artifact name.<br>            persist_dir (Union[str, None]): directory to persist the index. If None, a<br>                temporary directory will be created and used.<br>        \"\"\"<br>        if persist_dir is None:<br>            persist_dir = f\"{self._wandb.run.dir}/storage\"  # type: ignore<br>            _default_persist_dir = True<br>        if not os.path.exists(persist_dir):<br>            os.makedirs(persist_dir)<br>        if isinstance(index, self._IndexType):<br>            try:<br>                index.storage_context.persist(persist_dir)  # type: ignore<br>                metadata = None<br>                # For the </code>ComposableGraph<code> index, store the root id as metadata<br>                if isinstance(index, self._IndexType[0]):<br>                    root_id = index.root_id<br>                    metadata = {\"root_id\": root_id}<br>                self._upload_index_as_wb_artifact(persist_dir, index_name, metadata)<br>            except Exception as e:<br>                # Silently ignore errors to not break user code<br>                self._print_upload_index_fail_message(e)<br>        # clear the default storage dir<br>        if _default_persist_dir:<br>            shutil.rmtree(persist_dir, ignore_errors=True)<br>    def load_storage_context(<br>        self, artifact_url: str, index_download_dir: Union[str, None] = None<br>    ) -> \"StorageContext\":<br>        \"\"\"Download an index from wandb and return a storage context.<br>        Use this storage context to load the index into memory using<br>        </code>load_index_from_storage<code>, </code>load_indices_from_storage<code> or<br>        </code>load_graph_from_storage<code> functions.<br>        Args:<br>            artifact_url (str): url of the artifact to download. The artifact url will<br>                be of the form: </code>entity/project/index_name:version<code> and can be found in<br>                the W&B UI.<br>            index_download_dir (Union[str, None]): directory to download the index to.<br>        \"\"\"<br>        from llama_index.core.storage.storage_context import StorageContext<br>        artifact = self._wandb.use_artifact(artifact_url, type=\"storage_context\")  # type: ignore[attr-defined]<br>        artifact_dir = artifact.download(root=index_download_dir)<br>        return StorageContext.from_defaults(persist_dir=artifact_dir)<br>    def _upload_index_as_wb_artifact(<br>        self, dir_path: str, artifact_name: str, metadata: Optional[Dict]<br>    ) -> None:<br>        \"\"\"Utility function to upload a dir to W&B as an artifact.\"\"\"<br>        artifact = self._wandb.Artifact(artifact_name, type=\"storage_context\")  # type: ignore[attr-defined]<br>        if metadata:<br>            artifact.metadata = metadata<br>        artifact.add_dir(dir_path)<br>        self._wandb.run.log_artifact(artifact)  # type: ignore<br>    def _build_trace_tree(<br>        self, events: List[str], span: \"trace_tree.Span\"<br>    ) -> \"trace_tree.Span\":<br>        \"\"\"Build the trace tree from the trace map.\"\"\"<br>        for child_event in events:<br>            child_span = self._convert_event_pair_to_wb_span(<br>                self._event_pairs_by_id[child_event]<br>            )<br>            child_span = self._build_trace_tree(<br>                self._trace_map[child_event], child_span<br>            )<br>            span.add_child_span(child_span)<br>        return span<br>    def _convert_event_pair_to_wb_span(<br>        self,<br>        event_pair: List[CBEvent],<br>        trace_id: Optional[str] = None,<br>    ) -> \"trace_tree.Span\":<br>        \"\"\"Convert a pair of events to a wandb trace tree span.\"\"\"<br>        start_time_ms, end_time_ms = self._get_time_in_ms(event_pair)<br>        if trace_id is None:<br>            event_type = event_pair[0].event_type<br>            span_kind = self._map_event_type_to_span_kind(event_type)<br>        else:<br>            event_type = trace_id  # type: ignore<br>            span_kind = None<br>        wb_span = self._trace_tree.Span(<br>            name=f\"{event_type}\",<br>            span_kind=span_kind,<br>            start_time_ms=start_time_ms,<br>            end_time_ms=end_time_ms,<br>        )<br>        inputs, outputs, wb_span = self._add_payload_to_span(wb_span, event_pair)<br>        wb_span.add_named_result(inputs=inputs, outputs=outputs)  # type: ignore<br>        return wb_span<br>    def _map_event_type_to_span_kind(<br>        self, event_type: CBEventType<br>    ) -> Union[None, \"trace_tree.SpanKind\"]:<br>        \"\"\"Map a CBEventType to a wandb trace tree SpanKind.\"\"\"<br>        if event_type == CBEventType.CHUNKING:<br>            span_kind = None<br>        elif event_type == CBEventType.NODE_PARSING:<br>            span_kind = None<br>        elif event_type == CBEventType.EMBEDDING:<br>            # TODO: add span kind for EMBEDDING when it's available<br>            span_kind = None<br>        elif event_type == CBEventType.LLM:<br>            span_kind = self._trace_tree.SpanKind.LLM<br>        elif event_type == CBEventType.QUERY:<br>            span_kind = self._trace_tree.SpanKind.AGENT<br>        elif event_type == CBEventType.AGENT_STEP:<br>            span_kind = self._trace_tree.SpanKind.AGENT<br>        elif event_type == CBEventType.RETRIEVE:<br>            span_kind = self._trace_tree.SpanKind.TOOL<br>        elif event_type == CBEventType.SYNTHESIZE:<br>            span_kind = self._trace_tree.SpanKind.CHAIN<br>        elif event_type == CBEventType.TREE:<br>            span_kind = self._trace_tree.SpanKind.CHAIN<br>        elif event_type == CBEventType.SUB_QUESTION:<br>            span_kind = self._trace_tree.SpanKind.CHAIN<br>        elif event_type == CBEventType.RERANKING:<br>            span_kind = self._trace_tree.SpanKind.CHAIN<br>        elif event_type == CBEventType.FUNCTION_CALL:<br>            span_kind = self._trace_tree.SpanKind.TOOL<br>        else:<br>            span_kind = None<br>        return span_kind<br>    def _add_payload_to_span(<br>        self, span: \"trace_tree.Span\", event_pair: List[CBEvent]<br>    ) -> Tuple[Optional[Dict[str, Any]], Optional[Dict[str, Any]], \"trace_tree.Span\"]:<br>        \"\"\"Add the event's payload to the span.\"\"\"<br>        assert len(event_pair) == 2<br>        event_type = event_pair[0].event_type<br>        inputs = None<br>        outputs = None<br>        if event_type == CBEventType.NODE_PARSING:<br>            # TODO: disabled full detailed inputs/outputs due to UI lag<br>            inputs, outputs = self._handle_node_parsing_payload(event_pair)<br>        elif event_type == CBEventType.LLM:<br>            inputs, outputs, span = self._handle_llm_payload(event_pair, span)<br>        elif event_type == CBEventType.QUERY:<br>            inputs, outputs = self._handle_query_payload(event_pair)<br>        elif event_type == CBEventType.EMBEDDING:<br>            inputs, outputs = self._handle_embedding_payload(event_pair)<br>        return inputs, outputs, span<br>    def _handle_node_parsing_payload(<br>        self, event_pair: List[CBEvent]<br>    ) -> Tuple[Dict[str, Any], Dict[str, Any]]:<br>        \"\"\"Handle the payload of a NODE_PARSING event.\"\"\"<br>        inputs = event_pair[0].payload<br>        outputs = event_pair[-1].payload<br>        if inputs and EventPayload.DOCUMENTS in inputs:<br>            documents = inputs.pop(EventPayload.DOCUMENTS)<br>            inputs[\"num_documents\"] = len(documents)<br>        if outputs and EventPayload.NODES in outputs:<br>            nodes = outputs.pop(EventPayload.NODES)<br>            outputs[\"num_nodes\"] = len(nodes)<br>        return inputs or {}, outputs or {}<br>    def _handle_llm_payload(<br>        self, event_pair: List[CBEvent], span: \"trace_tree.Span\"<br>    ) -> Tuple[Dict[str, Any], Dict[str, Any], \"trace_tree.Span\"]:<br>        \"\"\"Handle the payload of a LLM event.\"\"\"<br>        inputs = event_pair[0].payload<br>        outputs = event_pair[-1].payload<br>        assert isinstance(inputs, dict) and isinstance(outputs, dict)<br>        # Get </code>original_template<code> from Prompt<br>        if EventPayload.PROMPT in inputs:<br>            inputs[EventPayload.PROMPT] = inputs[EventPayload.PROMPT]<br>        # Format messages<br>        if EventPayload.MESSAGES in inputs:<br>            inputs[EventPayload.MESSAGES] = \"\\n\".join(<br>                [str(x) for x in inputs[EventPayload.MESSAGES]]<br>            )<br>        token_counts = get_llm_token_counts(self._token_counter, outputs)<br>        metadata = {<br>            \"formatted_prompt_tokens_count\": token_counts.prompt_token_count,<br>            \"prediction_tokens_count\": token_counts.completion_token_count,<br>            \"total_tokens_used\": token_counts.total_token_count,<br>        }<br>        span.attributes = metadata<br>        # Make </code>response<code> part of </code>outputs<code><br>        outputs = {EventPayload.RESPONSE: str(outputs[EventPayload.RESPONSE])}<br>        return inputs, outputs, span<br>    def _handle_query_payload(<br>        self, event_pair: List[CBEvent]<br>    ) -> Tuple[Optional[Dict[str, Any]], Dict[str, Any]]:<br>        \"\"\"Handle the payload of a QUERY event.\"\"\"<br>        inputs = event_pair[0].payload<br>        outputs = event_pair[-1].payload<br>        if outputs:<br>            response_obj = outputs[EventPayload.RESPONSE]<br>            response = str(outputs[EventPayload.RESPONSE])<br>            if type(response).__name__ == \"Response\":<br>                response = response_obj.response<br>            elif type(response).__name__ == \"StreamingResponse\":<br>                response = response_obj.get_response().response<br>        else:<br>            response = \" \"<br>        outputs = {\"response\": response}<br>        return inputs, outputs<br>    def _handle_embedding_payload(<br>        self,<br>        event_pair: List[CBEvent],<br>    ) -> Tuple[Optional[Dict[str, Any]], Dict[str, Any]]:<br>        event_pair[0].payload<br>        outputs = event_pair[-1].payload<br>        chunks = []<br>        if outputs:<br>            chunks = outputs.get(EventPayload.CHUNKS, [])<br>        return {}, {\"num_chunks\": len(chunks)}<br>    def _get_time_in_ms(self, event_pair: List[CBEvent]) -> Tuple[int, int]:<br>        \"\"\"Get the start and end time of an event pair in milliseconds.\"\"\"<br>        start_time = datetime.strptime(event_pair[0].time, TIMESTAMP_FORMAT)<br>        end_time = datetime.strptime(event_pair[1].time, TIMESTAMP_FORMAT)<br>        start_time_in_ms = int(<br>            (start_time - datetime(1970, 1, 1)).total_seconds() * 1000<br>        )<br>        end_time_in_ms = int((end_time - datetime(1970, 1, 1)).total_seconds() * 1000)<br>        return start_time_in_ms, end_time_in_ms<br>    def _ensure_run(self, should_print_url: bool = False) -> None:<br>        \"\"\"Ensures an active W&B run exists.<br>        If not, will start a new run with the provided run_args.<br>        \"\"\"<br>        if self._wandb.run is None:  # type: ignore[attr-defined]<br>            # Make a shallow copy of the run args, so we don't modify the original<br>            run_args = self._run_args or {}  # type: ignore<br>            run_args: dict = {**run_args}  # type: ignore<br>            # Prefer to run in silent mode since W&B has a lot of output<br>            # which can be undesirable when dealing with text-based models.<br>            if \"settings\" not in run_args:  # type: ignore<br>                run_args[\"settings\"] = {\"silent\": True}  # type: ignore<br>            # Start the run and add the stream table<br>            self._wandb.init(**run_args)  # type: ignore[attr-defined]<br>            self._wandb.run._label(repo=\"llama_index\")  # type: ignore<br>            if should_print_url:<br>                self._print_wandb_init_message(<br>                    self._wandb.run.settings.run_url  # type: ignore<br>                )<br>    def _print_wandb_init_message(self, run_url: str) -> None:<br>        \"\"\"Print a message to the terminal when W&B is initialized.\"\"\"<br>        self._wandb.termlog(  # type: ignore[attr-defined]<br>            f\"Streaming LlamaIndex events to W&B at {run_url}\\n\"<br>            \"</code>WandbCallbackHandler<code> is currently in beta.\\n\"<br>            \"Please report any issues to https://github.com/wandb/wandb/issues \"<br>            \"with the tag </code>llamaindex<code>.\"<br>        )<br>    def _print_upload_index_fail_message(self, e: Exception) -> None:<br>        \"\"\"Print a message to the terminal when uploading the index fails.\"\"\"<br>        self._wandb.termlog(  # type: ignore[attr-defined]<br>            f\"Failed to upload index to W&B with the following error: {e}\\n\"<br>        )<br>    def finish(self) -> None:<br>        \"\"\"Finish the callback handler.\"\"\"<br>        self._wandb.finish()  # type: ignore[attr-defined]<br></code>`` |\n"
    },
    {
      "id": "fffc50a5-27ba-44af-92b2-1d369e653784",
      "size": 1197,
      "headers": {
        "h1": "Google",
        "h2": "GeminiEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br></code>`<code> | </code>`<code><br>class GeminiEmbedding(BaseEmbedding):<br>    \"\"\"Google Gemini embeddings.<br>    Args:<br>        model_name (str): Model for embedding.<br>            Defaults to \"models/embedding-001\".<br>        api_key (Optional[str]): API key to access the model. Defaults to None.<br>    \"\"\"<br>    _model: Any = PrivateAttr()<br>    title: Optional[str] = Field(<br>        default=\"\",<br>        description=\"Title is only applicable for retrieval_document tasks, and is used to represent a document title. For other tasks, title is invalid.\",<br>    )<br>    task_type: Optional[str] = Field(<br>        default=\"retrieval_document\",<br>        description=\"The task for embedding model.\",<br>    )<br>    def __init__(<br>        self,<br>        model_name: str = \"models/embedding-001\",<br>        task_type: Optional[str] = \"retrieval_document\",<br>        api_key: Optional[str] = None,<br>        title: Optional[str] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>        **kwargs: Any,<br>    ):<br>        super().__init__(<br>            model_name=model_name,<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            title=title,<br>            task_type=task_type,<br>            **kwargs,<br>        )<br>        gemini.configure(api_key=api_key)<br>        self._model = gemini<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"GeminiEmbedding\"<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self._model.embed_content(<br>            model=self.model_name,<br>            content=query,<br>            title=self.title,<br>            task_type=self.task_type,<br>        )[\"embedding\"]<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._model.embed_content(<br>            model=self.model_name,<br>            content=text,<br>            title=self.title,<br>            task_type=self.task_type,<br>        )[\"embedding\"]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        return [<br>            self._model.embed_content(<br>                model=self.model_name,<br>                content=text,<br>                title=self.title,<br>                task_type=self.task_type,<br>            )[\"embedding\"]<br>            for text in texts<br>        ]<br>    ### Async methods ###<br>    # need to wait async calls from Gemini side to be implemented.<br>    # Issue: https://github.com/google/generative-ai-python/issues/125<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        return self._get_query_embedding(query)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously get text embedding.\"\"\"<br>        return self._get_text_embedding(text)<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Asynchronously get text embeddings.\"\"\"<br>        return self._get_text_embeddings(texts)<br></code>`` |\n"
    },
    {
      "id": "6c12da73-71db-45f1-b4e9-92a7d24f3adc",
      "size": 2277,
      "headers": {
        "h1": "Upstage",
        "h2": "UpstageEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br></code>`<code> | </code>`<code><br>class UpstageEmbedding(OpenAIEmbedding):<br>    \"\"\"<br>    Class for Upstage embeddings.<br>    \"\"\"<br>    additional_kwargs: Dict[str, Any] = Field(<br>        default_factory=dict, description=\"Additional kwargs for the Upstage API.\"<br>    )<br>    api_key: str = Field(description=\"The Upstage API key.\")<br>    api_base: Optional[str] = Field(<br>        default=DEFAULT_UPSTAGE_API_BASE, description=\"The base URL for Upstage API.\"<br>    )<br>    dimensions: Optional[int] = Field(<br>        None,<br>        description=\"Not supported yet. The number of dimensions the resulting output embeddings should have.\",<br>    )<br>    def __init__(<br>        self,<br>        model: str = \"solar-embedding-1-large\",<br>        embed_batch_size: int = 100,<br>        dimensions: Optional[int] = None,<br>        additional_kwargs: Dict[str, Any] = None,<br>        api_key: Optional[str] = None,<br>        api_base: Optional[str] = None,<br>        max_retries: int = 10,<br>        timeout: float = 60.0,<br>        reuse_client: bool = True,<br>        callback_manager: Optional[CallbackManager] = None,<br>        default_headers: Optional[Dict[str, str]] = None,<br>        http_client: Optional[httpx.Client] = None,<br>        **kwargs: Any,<br>    ) -> None:<br>        additional_kwargs = additional_kwargs or {}<br>        if dimensions is not None:<br>            warnings.warn(\"Received dimensions argument. This is not supported yet.\")<br>            additional_kwargs[\"dimensions\"] = dimensions<br>        if embed_batch_size > MAX_EMBED_BATCH_SIZE:<br>            raise ValueError(<br>                f\"embed_batch_size should be less than or equal to {MAX_EMBED_BATCH_SIZE}.\"<br>            )<br>        if \"upstage_api_key\" in kwargs:<br>            api_key = kwargs.pop(\"upstage_api_key\")<br>        api_key, api_base = resolve_upstage_credentials(<br>            api_key=api_key, api_base=api_base<br>        )<br>        if \"model_name\" in kwargs:<br>            model = kwargs.pop(\"model_name\")<br>        # if model endswith with \"-query\" or \"-passage\", remove the suffix and print a warning<br>        if model.endswith((\"-query\", \"-passage\")):<br>            model = model.rsplit(\"-\", 1)[0]<br>            logger.warning(<br>                f\"Model name should not end with '-query' or '-passage'. The suffix has been removed. \"<br>                f\"Model name: {model}\"<br>            )<br>        super().__init__(<br>            embed_batch_size=embed_batch_size,<br>            dimensions=dimensions,<br>            callback_manager=callback_manager,<br>            model_name=model,<br>            additional_kwargs=additional_kwargs,<br>            api_key=api_key,<br>            api_base=api_base,<br>            max_retries=max_retries,<br>            reuse_client=reuse_client,<br>            timeout=timeout,<br>            default_headers=default_headers,<br>            **kwargs,<br>        )<br>        self._client = None<br>        self._aclient = None<br>        self._http_client = http_client<br>        self._query_engine, self._text_engine = get_engine(model)<br>    def class_name(cls) -> str:<br>        return \"UpstageEmbedding\"<br>    def _get_credential_kwargs(self, is_async: bool = False) -> Dict[str, Any]:<br>        return {<br>            \"api_key\": self.api_key,<br>            \"base_url\": self.api_base,<br>            \"max_retries\": self.max_retries,<br>            \"timeout\": self.timeout,<br>            \"default_headers\": self.default_headers,<br>            \"http_client\": self._async_http_client if is_async else self._http_client,<br>        }<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        client = self._get_client()<br>        text = query.replace(\"\\n\", \" \")<br>        return (<br>            client.embeddings.create(<br>                input=text, model=self._query_engine, **self.additional_kwargs<br>            )<br>            .data[0]<br>            .embedding<br>        )<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        client = self._get_aclient()<br>        text = query.replace(\"\\n\", \" \")<br>        return (<br>            (<br>                await client.embeddings.create(<br>                    input=text, model=self._query_engine, **self.additional_kwargs<br>                )<br>            )<br>            .data[0]<br>            .embedding<br>        )<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        client = self._get_client()<br>        return (<br>            client.embeddings.create(<br>                input=text, model=self._text_engine, **self.additional_kwargs<br>            )<br>            .data[0]<br>            .embedding<br>        )<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously get text embedding.\"\"\"<br>        client = self._get_aclient()<br>        return (<br>            (<br>                await client.embeddings.create(<br>                    input=text, model=self._text_engine, **self.additional_kwargs<br>                )<br>            )<br>            .data[0]<br>            .embedding<br>        )<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        client = self._get_client()<br>        batch_size = min(self.embed_batch_size, len(texts))<br>        texts = [text.replace(\"\\n\", \" \") for text in texts]<br>        embeddings = []<br>        for i in range(0, len(texts), batch_size):<br>            batch = texts[i : i + batch_size]<br>            response = client.embeddings.create(<br>                input=batch, model=self._text_engine, **self.additional_kwargs<br>            )<br>            embeddings.extend([r.embedding for r in response.data])<br>        return embeddings<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Asynchronously get text embeddings.\"\"\"<br>        client = self._get_aclient()<br>        batch_size = min(self.embed_batch_size, len(texts))<br>        texts = [text.replace(\"\\n\", \" \") for text in texts]<br>        embeddings = []<br>        for i in range(0, len(texts), batch_size):<br>            batch = texts[i : i + batch_size]<br>            response = await client.embeddings.create(<br>                input=batch, model=self._text_engine, **self.additional_kwargs<br>            )<br>            embeddings.extend([r.embedding for r in response.data])<br>        return embeddings<br></code>`` |\n"
    },
    {
      "id": "be4106bb-53bb-45ee-a970-3d9c66dddcfb",
      "size": 1916,
      "headers": {
        "h1": "Azure openai",
        "h2": "AzureOpenAIEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br></code>`<code> | </code>`<code><br>class AzureOpenAIEmbedding(OpenAIEmbedding):<br>    azure_endpoint: Optional[str] = Field(<br>        default=None, description=\"The Azure endpoint to use.\", validate_default=True<br>    )<br>    azure_deployment: Optional[str] = Field(<br>        default=None, description=\"The Azure deployment to use.\", validate_default=True<br>    )<br>    api_base: str = Field(<br>        default=\"\",<br>        description=\"The base URL for Azure deployment.\",<br>        validate_default=True,<br>    )<br>    api_version: str = Field(<br>        default=\"\",<br>        description=\"The version for Azure OpenAI API.\",<br>        validate_default=True,<br>    )<br>    azure_ad_token_provider: Optional[AnnotatedProvider] = Field(<br>        default=None,<br>        description=\"Callback function to provide Azure AD token.\",<br>        exclude=True,<br>    )<br>    use_azure_ad: bool = Field(<br>        description=\"Indicates if Microsoft Entra ID (former Azure AD) is used for token authentication\"<br>    )<br>    _azure_ad_token: Any = PrivateAttr(default=None)<br>    _client: AzureOpenAI = PrivateAttr()<br>    _aclient: AsyncAzureOpenAI = PrivateAttr()<br>    def __init__(<br>        self,<br>        mode: str = OpenAIEmbeddingMode.TEXT_SEARCH_MODE,<br>        model: str = OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        additional_kwargs: Optional[Dict[str, Any]] = None,<br>        api_key: Optional[str] = None,<br>        api_version: Optional[str] = None,<br>        # azure specific<br>        azure_endpoint: Optional[str] = None,<br>        azure_deployment: Optional[str] = None,<br>        azure_ad_token_provider: Optional[AzureADTokenProvider] = None,<br>        use_azure_ad: bool = False,<br>        deployment_name: Optional[str] = None,<br>        max_retries: int = 10,<br>        reuse_client: bool = True,<br>        callback_manager: Optional[CallbackManager] = None,<br>        num_workers: Optional[int] = None,<br>        # custom httpx client<br>        http_client: Optional[httpx.Client] = None,<br>        async_http_client: Optional[httpx.AsyncClient] = None,<br>        **kwargs: Any,<br>    ):<br>        azure_endpoint = get_from_param_or_env(<br>            \"azure_endpoint\", azure_endpoint, \"AZURE_OPENAI_ENDPOINT\", \"\"<br>        )<br>        azure_deployment = resolve_from_aliases(<br>            azure_deployment,<br>            deployment_name,<br>        )<br>        super().__init__(<br>            mode=mode,<br>            model=model,<br>            embed_batch_size=embed_batch_size,<br>            additional_kwargs=additional_kwargs,<br>            api_key=api_key,<br>            api_version=api_version,<br>            azure_endpoint=azure_endpoint,<br>            azure_deployment=azure_deployment,<br>            azure_ad_token_provider=azure_ad_token_provider,<br>            use_azure_ad=use_azure_ad,<br>            max_retries=max_retries,<br>            reuse_client=reuse_client,<br>            callback_manager=callback_manager,<br>            http_client=http_client,<br>            async_http_client=async_http_client,<br>            num_workers=num_workers,<br>            **kwargs,<br>        )<br>    @model_validator(mode=\"before\")<br>    @classmethod<br>    def validate_env(cls, values: Dict[str, Any]) -> Dict[str, Any]:<br>        \"\"\"Validate necessary credentials are set.\"\"\"<br>        if (<br>            values.get(\"api_base\") == \"https://api.openai.com/v1\"<br>            and values.get(\"azure_endpoint\") is None<br>        ):<br>            raise ValueError(<br>                \"You must set OPENAI_API_BASE to your Azure endpoint. \"<br>                \"It should look like https://YOUR_RESOURCE_NAME.openai.azure.com/\"<br>            )<br>        if values.get(\"api_version\") is None:<br>            raise ValueError(\"You must set OPENAI_API_VERSION for Azure OpenAI.\")<br>        return values<br>    def _get_client(self) -> AzureOpenAI:<br>        if not self.reuse_client:<br>            return AzureOpenAI(**self._get_credential_kwargs())<br>        if self._client is None:<br>            self._client = AzureOpenAI(**self._get_credential_kwargs())<br>        return self._client<br>    def _get_aclient(self) -> AsyncAzureOpenAI:<br>        if not self.reuse_client:<br>            return AsyncAzureOpenAI(**self._get_credential_kwargs(is_async=True))<br>        if self._aclient is None:<br>            self._aclient = AsyncAzureOpenAI(<br>                **self._get_credential_kwargs(is_async=True)<br>            )<br>        return self._aclient<br>    def _get_credential_kwargs(self, is_async: bool = False) -> Dict[str, Any]:<br>        if self.use_azure_ad:<br>            self._azure_ad_token = refresh_openai_azuread_token(self._azure_ad_token)<br>            self.api_key = self._azure_ad_token.token<br>        else:<br>            self.api_key = get_from_param_or_env(<br>                \"api_key\", self.api_key, \"AZURE_OPENAI_API_KEY\"<br>            )<br>        return {<br>            \"api_key\": self.api_key,<br>            \"azure_ad_token_provider\": self.azure_ad_token_provider,<br>            \"azure_endpoint\": self.azure_endpoint,<br>            \"azure_deployment\": self.azure_deployment,<br>            \"api_version\": self.api_version,<br>            \"default_headers\": self.default_headers,<br>            \"http_client\": self._async_http_client if is_async else self._http_client,<br>        }<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"AzureOpenAIEmbedding\"<br></code>`` |\n"
    },
    {
      "id": "7041e0fa-0fde-45d2-8b66-235e1b1a2150",
      "size": 2585,
      "headers": {
        "h1": "Databricks",
        "h2": "DatabricksEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br></code>`<code> | </code>``<code><br>class DatabricksEmbedding(BaseEmbedding):<br>    \"\"\"Databricks class for text embedding.<br>    Databricks adheres to the OpenAI API, so this integration aligns closely with the existing OpenAIEmbedding class.<br>    Args:<br>        model (str): The unique ID of the embedding model as served by the Databricks endpoint.<br>        endpoint (Optional[str]): The url of the Databricks endpoint. Can be set as an environment variable (</code>DATABRICKS_SERVING_ENDPOINT<code>).<br>        api_key (Optional[str]): The Databricks API key to use. Can be set as an environment variable (</code>DATABRICKS_TOKEN<code>).<br>    Examples:<br>        </code>pip install llama-index-embeddings-databricks<code><br>        </code>`<code>python<br>        import os<br>        from llama_index.core import Settings<br>        from llama_index.embeddings.databricks import DatabricksEmbedding<br>        # Set up the DatabricksEmbedding class with the required model, API key and serving endpoint<br>        os.environ[\"DATABRICKS_TOKEN\"] = \"<MY TOKEN>\"<br>        os.environ[\"DATABRICKS_SERVING_ENDPOINT\"] = \"<MY ENDPOINT>\"<br>        embed_model  = DatabricksEmbedding(model=\"databricks-bge-large-en\")<br>        Settings.embed_model = embed_model<br>        # Embed some text<br>        embeddings = embed_model.get_text_embedding(\"The DatabricksEmbedding integration works great.\")<br>        </code>`<code><br>    \"\"\"<br>    additional_kwargs: Dict[str, Any] = Field(<br>        default_factory=dict, description=\"Additional kwargs as for the OpenAI API.\"<br>    )<br>    model: str = Field(<br>        description=\"The ID of a model hosted on the databricks endpoint.\"<br>    )<br>    api_key: str = Field(description=\"The Databricks API key.\")<br>    endpoint: str = Field(description=\"The Databricks API endpoint.\")<br>    max_retries: int = Field(<br>        default=10, description=\"Maximum number of retries.\", gte=0<br>    )<br>    timeout: float = Field(default=60.0, description=\"Timeout for each request.\", gte=0)<br>    default_headers: Optional[Dict[str, str]] = Field(<br>        default=None, description=\"The default headers for API requests.\"<br>    )<br>    reuse_client: bool = Field(<br>        default=True,<br>        description=(<br>            \"Reuse the client between requests. When doing anything with large \"<br>            \"volumes of async API calls, setting this to false can improve stability.\"<br>        ),<br>    )<br>    _query_engine: str = PrivateAttr()<br>    _text_engine: str = PrivateAttr()<br>    _client: Optional[OpenAI] = PrivateAttr()<br>    _aclient: Optional[AsyncOpenAI] = PrivateAttr()<br>    _http_client: Optional[httpx.Client] = PrivateAttr()<br>    def __init__(<br>        self,<br>        model: str,<br>        endpoint: Optional[str] = None,<br>        embed_batch_size: int = 100,<br>        additional_kwargs: Optional[Dict[str, Any]] = None,<br>        api_key: Optional[str] = None,<br>        max_retries: int = 10,<br>        timeout: float = 60.0,<br>        reuse_client: bool = True,<br>        callback_manager: Optional[CallbackManager] = None,<br>        default_headers: Optional[Dict[str, str]] = None,<br>        http_client: Optional[httpx.Client] = None,<br>        num_workers: Optional[int] = None,<br>        **kwargs: Any,<br>    ) -> None:<br>        additional_kwargs = additional_kwargs or {}<br>        api_key = get_from_param_or_env(\"api_key\", api_key, \"DATABRICKS_TOKEN\")<br>        endpoint = get_from_param_or_env(<br>            \"endpoint\", endpoint, \"DATABRICKS_SERVING_ENDPOINT\"<br>        )<br>        super().__init__(<br>            model=model,<br>            endpoint=endpoint,<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            model_name=model,<br>            additional_kwargs=additional_kwargs,<br>            api_key=api_key,<br>            max_retries=max_retries,<br>            reuse_client=reuse_client,<br>            timeout=timeout,<br>            default_headers=default_headers,<br>            num_workers=num_workers,<br>            **kwargs,<br>        )<br>        self._client = None<br>        self._aclient = None<br>        self._http_client = http_client<br>    def _get_client(self) -> OpenAI:<br>        if not self.reuse_client:<br>            return OpenAI(**self._get_credential_kwargs())<br>        if self._client is None:<br>            self._client = OpenAI(**self._get_credential_kwargs())<br>        return self._client<br>    def _get_aclient(self) -> AsyncOpenAI:<br>        if not self.reuse_client:<br>            return AsyncOpenAI(**self._get_credential_kwargs())<br>        if self._aclient is None:<br>            self._aclient = AsyncOpenAI(**self._get_credential_kwargs())<br>        return self._aclient<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"DatabricksEmbedding\"<br>    def _get_credential_kwargs(self) -> Dict[str, Any]:<br>        return {<br>            \"api_key\": self.api_key,<br>            \"base_url\": self.endpoint,<br>            \"max_retries\": self.max_retries,<br>            \"timeout\": self.timeout,<br>            \"default_headers\": self.default_headers,<br>            \"http_client\": self._http_client,<br>        }<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        client = self._get_client()<br>        return get_embedding(<br>            client,<br>            query,<br>            engine=self.model,<br>            **self.additional_kwargs,<br>        )<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        aclient = self._get_aclient()<br>        return await aget_embedding(<br>            aclient,<br>            query,<br>            engine=self.model,<br>            **self.additional_kwargs,<br>        )<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        client = self._get_client()<br>        return get_embedding(<br>            client,<br>            text,<br>            engine=self.model,<br>            **self.additional_kwargs,<br>        )<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously get text embedding.\"\"\"<br>        aclient = self._get_aclient()<br>        return await aget_embedding(<br>            aclient,<br>            text,<br>            engine=self.model,<br>            **self.additional_kwargs,<br>        )<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.<br>        By default, this is a wrapper around _get_text_embedding.<br>        Can be overridden for batch queries.<br>        \"\"\"<br>        client = self._get_client()<br>        return get_embeddings(<br>            client,<br>            texts,<br>            engine=self.model,<br>            **self.additional_kwargs,<br>        )<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Asynchronously get text embeddings.\"\"\"<br>        aclient = self._get_aclient()<br>        return await aget_embeddings(<br>            aclient,<br>            texts,<br>            engine=self.model,<br>            **self.additional_kwargs,<br>        )<br></code>``` |\n"
    },
    {
      "id": "ea9018ad-0b98-4597-bacc-f31dd7d7717d",
      "size": 1691,
      "headers": {
        "h1": "Tonic validate",
        "h2": "TonicValidateEvaluator \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br></code>`<code> | </code>`<code><br>class TonicValidateEvaluator(BaseEvaluator):<br>    \"\"\"Tonic Validate's validate scorer. Calculates all of Tonic Validate's metrics.<br>    See https://docs.tonic.ai/validate/ for more details.<br>    Args:<br>        metrics(List[Metric]): The metrics to use. Defaults to all of Tonic Validate's<br>            metrics.<br>        model_evaluator(str): The OpenAI service to use. Specifies the chat completion<br>            model to use as the LLM evaluator. Defaults to \"gpt-4\".<br>    \"\"\"<br>    def __init__(<br>        self, metrics: Optional[List[Any]] = None, model_evaluator: str = \"gpt-4\"<br>    ):<br>        if metrics is None:<br>            metrics = [<br>                AnswerConsistencyMetric(),<br>                AnswerSimilarityMetric(),<br>                AugmentationAccuracyMetric(),<br>                AugmentationPrecisionMetric(),<br>                RetrievalPrecisionMetric(),<br>            ]<br>        self.metrics = metrics<br>        self.model_evaluator = model_evaluator<br>        self.validate_scorer = ValidateScorer(metrics, model_evaluator)<br>    def _calculate_average_score(self, run: Any) -> float:<br>        from tonic_validate.metrics.answer_similarity_metric import (<br>            AnswerSimilarityMetric,<br>        )<br>        ave_score = 0.0<br>        metric_cnt = 0<br>        for metric_name, score in run.overall_scores.items():<br>            if metric_name == AnswerSimilarityMetric.name:<br>                ave_score += score / 5<br>            else:<br>                ave_score += score<br>            metric_cnt += 1<br>        return ave_score / metric_cnt<br>    async def aevaluate(<br>        self,<br>        query: Optional[str] = None,<br>        response: Optional[str] = None,<br>        contexts: Optional[Sequence[str]] = None,<br>        reference_response: Optional[str] = None,<br>        **kwargs: Any,<br>    ) -> TonicValidateEvaluationResult:<br>        from tonic_validate.classes.benchmark import BenchmarkItem<br>        from tonic_validate.classes.llm_response import LLMResponse<br>        benchmark_item = BenchmarkItem(question=query, answer=reference_response)<br>        llm_response = LLMResponse(<br>            llm_answer=response,<br>            llm_context_list=contexts,<br>            benchmark_item=benchmark_item,<br>        )<br>        responses = [llm_response]<br>        run = self.validate_scorer.score_run(responses)<br>        ave_score = self._calculate_average_score(run)<br>        return TonicValidateEvaluationResult(<br>            query=query,<br>            contexts=contexts,<br>            response=response,<br>            score=ave_score,<br>            score_dict=run.run_data[0].scores,<br>        )<br>    async def aevaluate_run(<br>        self,<br>        queries: List[str],<br>        responses: List[str],<br>        contexts_list: List[List[str]],<br>        reference_responses: List[str],<br>        **kwargs: Any,<br>    ) -> Any:<br>        \"\"\"Evaluates a batch of responses.<br>        Returns a Tonic Validate Run object, which can be logged to the Tonic Validate<br>        UI. See https://docs.tonic.ai/validate/ for more details.<br>        \"\"\"<br>        from tonic_validate.classes.benchmark import BenchmarkItem<br>        from tonic_validate.classes.llm_response import LLMResponse<br>        llm_responses = []<br>        for query, response, contexts, reference_response in zip(<br>            queries, responses, contexts_list, reference_responses<br>        ):<br>            benchmark_item = BenchmarkItem(question=query, answer=reference_response)<br>            llm_response = LLMResponse(<br>                llm_answer=response,<br>                llm_context_list=contexts,<br>                benchmark_item=benchmark_item,<br>            )<br>            llm_responses.append(llm_response)<br>        return self.validate_scorer.score_run(llm_responses)<br>    def evaluate_run(<br>        self,<br>        queries: List[str],<br>        responses: List[str],<br>        contexts_list: List[List[str]],<br>        reference_responses: List[str],<br>        **kwargs: Any,<br>    ) -> Any:<br>        \"\"\"Evaluates a batch of responses.<br>        Returns a Tonic Validate Run object, which can be logged to the Tonic Validate<br>        UI. See https://docs.tonic.ai/validate/ for more details.<br>        \"\"\"<br>        return asyncio.run(<br>            self.aevaluate_run(<br>                queries=queries,<br>                responses=responses,<br>                contexts_list=contexts_list,<br>                reference_responses=reference_responses,<br>                **kwargs,<br>            )<br>        )<br>    def _get_prompts(self) -> PromptDictType:<br>        return {}<br>    def _get_prompt_modules(self) -> PromptMixinType:<br>        return {}<br>    def _update_prompts(self, prompts_dict: PromptDictType) -> None:<br>        return<br></code>`` |\n"
    },
    {
      "id": "8aee8ff0-6478-4b5b-ad0a-ea2544966bb3",
      "size": 1164,
      "headers": {
        "h1": "Fastembed",
        "h2": "FastEmbedEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>89<br>90<br>91<br>92<br>93<br>94<br>95<br>96<br>97<br>98<br>99<br></code>`<code> | </code>`<code><br>class FastEmbedEmbedding(BaseEmbedding):<br>    \"\"\"<br>    Qdrant FastEmbedding models.<br>    FastEmbed is a lightweight, fast, Python library built for embedding generation.<br>    See more documentation at:<br>    * https://github.com/qdrant/fastembed/<br>    * https://qdrant.github.io/fastembed/.<br>    To use this class, you must install the </code>fastembed<code> Python package.<br>    </code>pip install fastembed<code><br>    Example:<br>        from llama_index.embeddings.fastembed import FastEmbedEmbedding<br>        fastembed = FastEmbedEmbedding()<br>    \"\"\"<br>    model_name: str = Field(<br>        \"BAAI/bge-small-en-v1.5\",<br>        description=\"Name of the FastEmbedding model to use.\\n\"<br>        \"Defaults to 'BAAI/bge-small-en-v1.5'.\\n\"<br>        \"Find the list of supported models at \"<br>        \"https://qdrant.github.io/fastembed/examples/Supported_Models/\",<br>    )<br>    max_length: int = Field(<br>        512,<br>        description=\"The maximum number of tokens. Defaults to 512.\\n\"<br>        \"Unknown behavior for values > 512.\",<br>    )<br>    cache_dir: Optional[str] = Field(<br>        None,<br>        description=\"The path to the cache directory.\\n\"<br>        \"Defaults to </code>local_cache<code> in the parent directory\",<br>    )<br>    threads: Optional[int] = Field(<br>        None,<br>        description=\"The number of threads single onnxruntime session can use.\\n\"<br>        \"Defaults to None\",<br>    )<br>    doc_embed_type: Literal[\"default\", \"passage\"] = Field(<br>        \"default\",<br>        description=\"Type of embedding method to use for documents.\\n\"<br>        \"Available options are 'default' and 'passage'.\",<br>    )<br>    _model: Any = PrivateAttr()<br>    @classmethod<br>    def class_name(self) -> str:<br>        return \"FastEmbedEmbedding\"<br>    def __init__(<br>        self,<br>        model_name: Optional[str] = \"BAAI/bge-small-en-v1.5\",<br>        max_length: Optional[int] = 512,<br>        cache_dir: Optional[str] = None,<br>        threads: Optional[int] = None,<br>        doc_embed_type: Literal[\"default\", \"passage\"] = \"default\",<br>    ):<br>        super().__init__(<br>            model_name=model_name,<br>            max_length=max_length,<br>            threads=threads,<br>            doc_embed_type=doc_embed_type,<br>        )<br>        self._model = TextEmbedding(<br>            model_name=model_name,<br>            max_length=max_length,<br>            cache_dir=cache_dir,<br>            threads=threads,<br>        )<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        embeddings: List[np.ndarray]<br>        if self.doc_embed_type == \"passage\":<br>            embeddings = list(self._model.passage_embed(text))<br>        else:<br>            embeddings = list(self._model.embed(text))<br>        return embeddings[0].tolist()<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        query_embeddings: np.ndarray = next(self._model.query_embed(query))<br>        return query_embeddings.tolist()<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        return self._get_query_embedding(query)<br></code>`` |\n"
    },
    {
      "id": "d2d8479b-d300-4268-aba0-c0ea446db52f",
      "size": 2681,
      "headers": {
        "h1": "Index",
        "h2": "StreamingAgentChatResponse<code>dataclass</code>\\#",
        "h3": ""
      },
      "text": "| ``<code><br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br></code>`<code> | </code>`<code><br>@dataclass<br>class StreamingAgentChatResponse:<br>    \"\"\"Streaming chat response to user and writing to chat history.\"\"\"<br>    response: str = \"\"<br>    sources: List[ToolOutput] = field(default_factory=list)<br>    chat_stream: Optional[ChatResponseGen] = None<br>    achat_stream: Optional[ChatResponseAsyncGen] = None<br>    source_nodes: List[NodeWithScore] = field(default_factory=list)<br>    unformatted_response: str = \"\"<br>    queue: Queue = field(default_factory=Queue)<br>    aqueue: Optional[asyncio.Queue] = None<br>    # flag when chat message is a function call<br>    is_function: Optional[bool] = None<br>    # flag when processing done<br>    is_done = False<br>    # signal when a new item is added to the queue<br>    new_item_event: Optional[asyncio.Event] = None<br>    # NOTE: async code uses two events rather than one since it yields<br>    # control when waiting for queue item<br>    # signal when the OpenAI functions stop executing<br>    is_function_false_event: Optional[asyncio.Event] = None<br>    # signal when an OpenAI function is being executed<br>    is_function_not_none_thread_event: Event = field(default_factory=Event)<br>    # Track if an exception occurred<br>    exception: Optional[Exception] = None<br>    def set_source_nodes(self) -> None:<br>        if self.sources and not self.source_nodes:<br>            for tool_output in self.sources:<br>                if isinstance(tool_output.raw_output, (Response, StreamingResponse)):<br>                    self.source_nodes.extend(tool_output.raw_output.source_nodes)<br>    def __post_init__(self) -> None:<br>        self.set_source_nodes()<br>    def __str__(self) -> str:<br>        if self.is_done and not self.queue.empty() and not self.is_function:<br>            while self.queue.queue:<br>                delta = self.queue.queue.popleft()<br>                self.unformatted_response += delta<br>            self.response = self.unformatted_response.strip()<br>        return self.response<br>    def _ensure_async_setup(self) -> None:<br>        if self.aqueue is None:<br>            self.aqueue = asyncio.Queue()<br>        if self.new_item_event is None:<br>            self.new_item_event = asyncio.Event()<br>        if self.is_function_false_event is None:<br>            self.is_function_false_event = asyncio.Event()<br>    def put_in_queue(self, delta: Optional[str]) -> None:<br>        self.queue.put_nowait(delta)<br>        self.is_function_not_none_thread_event.set()<br>    def aput_in_queue(self, delta: Optional[str]) -> None:<br>        assert self.aqueue is not None<br>        assert self.new_item_event is not None<br>        self.aqueue.put_nowait(delta)<br>        self.new_item_event.set()<br>    @dispatcher.span<br>    def write_response_to_history(<br>        self,<br>        memory: BaseMemory,<br>        on_stream_end_fn: Optional[Callable] = None,<br>    ) -> None:<br>        if self.chat_stream is None:<br>            raise ValueError(<br>                \"chat_stream is None. Cannot write to history without chat_stream.\"<br>            )<br>        # try/except to prevent hanging on error<br>        dispatcher.event(StreamChatStartEvent())<br>        try:<br>            final_text = \"\"<br>            for chat in self.chat_stream:<br>                self.is_function = is_function(chat.message)<br>                if chat.delta:<br>                    dispatcher.event(<br>                        StreamChatDeltaReceivedEvent(<br>                            delta=chat.delta,<br>                        )<br>                    )<br>                    self.put_in_queue(chat.delta)<br>                final_text += chat.delta or \"\"<br>            if self.is_function is not None:  # if loop has gone through iteration<br>                # NOTE: this is to handle the special case where we consume some of the<br>                # chat stream, but not all of it (e.g. in react agent)<br>                chat.message.content = final_text.strip()  # final message<br>                memory.put(chat.message)<br>        except Exception as e:<br>            dispatcher.event(StreamChatErrorEvent(exception=e))<br>            self.exception = e<br>            # This act as is_done events for any consumers waiting<br>            self.is_function_not_none_thread_event.set()<br>            # force the queue reader to see the exception<br>            self.put_in_queue(\"\")<br>            raise<br>        dispatcher.event(StreamChatEndEvent())<br>        self.is_done = True<br>        # This act as is_done events for any consumers waiting<br>        self.is_function_not_none_thread_event.set()<br>        if on_stream_end_fn is not None and not self.is_function:<br>            on_stream_end_fn()<br>    @dispatcher.span<br>    async def awrite_response_to_history(<br>        self,<br>        memory: BaseMemory,<br>        on_stream_end_fn: Optional[Callable] = None,<br>    ) -> None:<br>        self._ensure_async_setup()<br>        assert self.aqueue is not None<br>        assert self.is_function_false_event is not None<br>        assert self.new_item_event is not None<br>        if self.achat_stream is None:<br>            raise ValueError(<br>                \"achat_stream is None. Cannot asynchronously write to \"<br>                \"history without achat_stream.\"<br>            )<br>        # try/except to prevent hanging on error<br>        dispatcher.event(StreamChatStartEvent())<br>        try:<br>            final_text = \"\"<br>            async for chat in self.achat_stream:<br>                self.is_function = is_function(chat.message)<br>                if chat.delta:<br>                    dispatcher.event(<br>                        StreamChatDeltaReceivedEvent(<br>                            delta=chat.delta,<br>                        )<br>                    )<br>                    self.aput_in_queue(chat.delta)<br>                final_text += chat.delta or \"\"<br>                self.new_item_event.set()<br>                if self.is_function is False:<br>                    self.is_function_false_event.set()<br>            if self.is_function is not None:  # if loop has gone through iteration<br>                # NOTE: this is to handle the special case where we consume some of the<br>                # chat stream, but not all of it (e.g. in react agent)<br>                chat.message.content = final_text.strip()  # final message<br>                memory.put(chat.message)<br>        except Exception as e:<br>            dispatcher.event(StreamChatErrorEvent(exception=e))<br>            self.exception = e<br>            # These act as is_done events for any consumers waiting<br>            self.is_function_false_event.set()<br>            self.new_item_event.set()<br>            # force the queue reader to see the exception<br>            self.aput_in_queue(\"\")<br>            raise<br>        dispatcher.event(StreamChatEndEvent())<br>        self.is_done = True<br>        # These act as is_done events for any consumers waiting<br>        self.is_function_false_event.set()<br>        self.new_item_event.set()<br>        if on_stream_end_fn is not None and not self.is_function:<br>            on_stream_end_fn()<br>    @property<br>    def response_gen(self) -> Generator[str, None, None]:<br>        while not self.is_done or not self.queue.empty():<br>            if self.exception is not None:<br>                raise self.exception<br>            try:<br>                delta = self.queue.get(block=False)<br>                self.unformatted_response += delta<br>                yield delta<br>            except Empty:<br>                # Queue is empty, but we're not done yet. Sleep for 0 secs to release the GIL and allow other threads to run.<br>                time.sleep(0)<br>        self.response = self.unformatted_response.strip()<br>    async def async_response_gen(self) -> AsyncGenerator[str, None]:<br>        self._ensure_async_setup()<br>        assert self.aqueue is not None<br>        while True:<br>            if not self.aqueue.empty() or not self.is_done:<br>                if self.exception is not None:<br>                    raise self.exception<br>                try:<br>                    delta = await asyncio.wait_for(self.aqueue.get(), timeout=0.1)<br>                except asyncio.TimeoutError:<br>                    if self.is_done:<br>                        break<br>                    continue<br>                if delta is not None:<br>                    self.unformatted_response += delta<br>                    yield delta<br>            else:<br>                break<br>        self.response = self.unformatted_response.strip()<br>    def print_response_stream(self) -> None:<br>        for token in self.response_gen:<br>            print(token, end=\"\", flush=True)<br>    async def aprint_response_stream(self) -> None:<br>        async for token in self.async_response_gen():<br>            print(token, end=\"\", flush=True)<br></code>`` |\n"
    },
    {
      "id": "d59036a1-234c-472c-9076-3d44e05330b8",
      "size": 1714,
      "headers": {
        "h1": "Clarifai",
        "h2": "ClarifaiEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br></code>`<code> | </code>`<code><br>class ClarifaiEmbedding(BaseEmbedding):<br>    \"\"\"Clarifai embeddings class.<br>    Clarifai uses Personal Access Tokens(PAT) to validate requests.<br>    You can create and manage PATs under your Clarifai account security settings.<br>    Export your PAT as an environment variable by running </code>export CLARIFAI_PAT={PAT}<code><br>    \"\"\"<br>    model_url: Optional[str] = Field(<br>        description=f\"Full URL of the model. e.g. </code>{EXAMPLE_URL}<code>\"<br>    )<br>    model_id: Optional[str] = Field(description=\"Model ID.\")<br>    model_version_id: Optional[str] = Field(description=\"Model Version ID.\")<br>    app_id: Optional[str] = Field(description=\"Clarifai application ID of the model.\")<br>    user_id: Optional[str] = Field(description=\"Clarifai user ID of the model.\")<br>    pat: Optional[str] = Field(<br>        description=\"Personal Access Tokens(PAT) to validate requests.\"<br>    )<br>    _model: Any = PrivateAttr()<br>    def __init__(<br>        self,<br>        model_name: Optional[str] = None,<br>        model_url: Optional[str] = None,<br>        model_version_id: Optional[str] = \"\",<br>        app_id: Optional[str] = None,<br>        user_id: Optional[str] = None,<br>        pat: Optional[str] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>    ):<br>        embed_batch_size = min(128, embed_batch_size)<br>        if pat is None and os.environ.get(\"CLARIFAI_PAT\") is not None:<br>            pat = os.environ.get(\"CLARIFAI_PAT\")<br>        if not pat and os.environ.get(\"CLARIFAI_PAT\") is None:<br>            raise ValueError(<br>                \"Set </code>CLARIFAI_PAT<code> as env variable or pass </code>pat<code> as constructor argument\"<br>            )<br>        if model_url is not None and model_name is not None:<br>            raise ValueError(\"You can only specify one of model_url or model_name.\")<br>        if model_url is None and model_name is None:<br>            raise ValueError(\"You must specify one of model_url or model_name.\")<br>        if model_name is not None:<br>            if app_id is None or user_id is None:<br>                raise ValueError(<br>                    f\"Missing one app ID or user ID of the model: {app_id=}, {user_id=}\"<br>                )<br>            else:<br>                model = Model(<br>                    user_id=user_id,<br>                    app_id=app_id,<br>                    model_id=model_name,<br>                    model_version={\"id\": model_version_id},<br>                    pat=pat,<br>                )<br>        if model_url is not None:<br>            model = Model(model_url, pat=pat)<br>            model_name = model.id<br>        super().__init__(<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            model_name=model_name,<br>        )<br>        self._model = model<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"ClarifaiEmbedding\"<br>    def _embed(self, sentences: List[str]) -> List[List[float]]:<br>        \"\"\"Embed sentences.\"\"\"<br>        try:<br>            from clarifai.client.input import Inputs<br>        except ImportError:<br>            raise ImportError(\"ClarifaiEmbedding requires </code>pip install clarifai<code>.\")<br>        embeddings = []<br>        try:<br>            for i in range(0, len(sentences), self.embed_batch_size):<br>                batch = sentences[i : i + self.embed_batch_size]<br>                input_batch = [<br>                    Inputs.get_text_input(input_id=str(id), raw_text=inp)<br>                    for id, inp in enumerate(batch)<br>                ]<br>                predict_response = self._model.predict(input_batch)<br>                embeddings.extend(<br>                    [<br>                        list(output.data.embeddings[0].vector)<br>                        for output in predict_response.outputs<br>                    ]<br>                )<br>        except Exception as e:<br>            logger.error(f\"Predict failed, exception: {e}\")<br>        return embeddings<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self._embed([query])[0]<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding async.\"\"\"<br>        return self._get_query_embedding(query)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding async.\"\"\"<br>        return self._get_text_embedding(text)<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._embed([text])[0]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        return self._embed(texts)<br></code>`` |\n"
    },
    {
      "id": "50f3d122-faea-4328-9c7e-17ee7458dc25",
      "size": 1032,
      "headers": {
        "h1": "Index",
        "h2": "BaseEvaluator \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br></code>`<code> | </code>`<code><br>class BaseEvaluator(PromptMixin):<br>    \"\"\"Base Evaluator class.\"\"\"<br>    def _get_prompt_modules(self) -> PromptMixinType:<br>        \"\"\"Get prompt modules.\"\"\"<br>        return {}<br>    def evaluate(<br>        self,<br>        query: Optional[str] = None,<br>        response: Optional[str] = None,<br>        contexts: Optional[Sequence[str]] = None,<br>        **kwargs: Any,<br>    ) -> EvaluationResult:<br>        \"\"\"Run evaluation with query string, retrieved contexts,<br>        and generated response string.<br>        Subclasses can override this method to provide custom evaluation logic and<br>        take in additional arguments.<br>        \"\"\"<br>        return asyncio_run(<br>            self.aevaluate(<br>                query=query,<br>                response=response,<br>                contexts=contexts,<br>                **kwargs,<br>            )<br>        )<br>    @abstractmethod<br>    async def aevaluate(<br>        self,<br>        query: Optional[str] = None,<br>        response: Optional[str] = None,<br>        contexts: Optional[Sequence[str]] = None,<br>        **kwargs: Any,<br>    ) -> EvaluationResult:<br>        \"\"\"Run evaluation with query string, retrieved contexts,<br>        and generated response string.<br>        Subclasses can override this method to provide custom evaluation logic and<br>        take in additional arguments.<br>        \"\"\"<br>        raise NotImplementedError<br>    def evaluate_response(<br>        self,<br>        query: Optional[str] = None,<br>        response: Optional[Response] = None,<br>        **kwargs: Any,<br>    ) -> EvaluationResult:<br>        \"\"\"Run evaluation with query string and generated Response object.<br>        Subclasses can override this method to provide custom evaluation logic and<br>        take in additional arguments.<br>        \"\"\"<br>        response_str: Optional[str] = None<br>        contexts: Optional[Sequence[str]] = None<br>        if response is not None:<br>            response_str = response.response<br>            contexts = [node.get_content() for node in response.source_nodes]<br>        return self.evaluate(<br>            query=query, response=response_str, contexts=contexts, **kwargs<br>        )<br>    async def aevaluate_response(<br>        self,<br>        query: Optional[str] = None,<br>        response: Optional[Response] = None,<br>        **kwargs: Any,<br>    ) -> EvaluationResult:<br>        \"\"\"Run evaluation with query string and generated Response object.<br>        Subclasses can override this method to provide custom evaluation logic and<br>        take in additional arguments.<br>        \"\"\"<br>        response_str: Optional[str] = None<br>        contexts: Optional[Sequence[str]] = None<br>        if response is not None:<br>            response_str = response.response<br>            contexts = [node.get_content() for node in response.source_nodes]<br>        return await self.aevaluate(<br>            query=query, response=response_str, contexts=contexts, **kwargs<br>        )<br></code>`` |\n"
    },
    {
      "id": "86b8595c-5b53-4886-9d39-af085081b4a3",
      "size": 4404,
      "headers": {
        "h1": "Index",
        "h2": "BatchEvalRunner \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br></code>`<code> | </code>``<code><br>class BatchEvalRunner:<br>    \"\"\"<br>    Batch evaluation runner.<br>    Args:<br>        evaluators (Dict[str, BaseEvaluator]): Dictionary of evaluators.<br>        workers (int): Number of workers to use for parallelization.<br>            Defaults to 2.<br>        show_progress (bool): Whether to show progress bars. Defaults to False.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        evaluators: Dict[str, BaseEvaluator],<br>        workers: int = 2,<br>        show_progress: bool = False,<br>    ):<br>        self.evaluators = evaluators<br>        self.workers = workers<br>        self.semaphore = asyncio.Semaphore(self.workers)<br>        self.show_progress = show_progress<br>        self.asyncio_mod = asyncio_module(show_progress=self.show_progress)<br>    def _format_results(<br>        self, results: List[Tuple[str, EvaluationResult]]<br>    ) -> Dict[str, List[EvaluationResult]]:<br>        \"\"\"Format results.\"\"\"<br>        # Format results<br>        results_dict: Dict[str, List[EvaluationResult]] = {<br>            name: [] for name in self.evaluators<br>        }<br>        for name, result in results:<br>            results_dict[name].append(result)<br>        return results_dict<br>    def _validate_and_clean_inputs(<br>        self,<br>        *inputs_list: Any,<br>    ) -> List[Any]:<br>        \"\"\"<br>        Validate and clean input lists.<br>        Enforce that at least one of the inputs is not None.<br>        Make sure that all inputs have the same length.<br>        Make sure that None inputs are replaced with [None] * len(inputs).<br>        \"\"\"<br>        assert len(inputs_list) > 0<br>        # first, make sure at least one of queries or response_strs is not None<br>        input_len: Optional[int] = None<br>        for inputs in inputs_list:<br>            if inputs is not None:<br>                input_len = len(inputs)<br>                break<br>        if input_len is None:<br>            raise ValueError(\"At least one item in inputs_list must be provided.\")<br>        new_inputs_list = []<br>        for inputs in inputs_list:<br>            if inputs is None:<br>                new_inputs_list.append([None] * input_len)<br>            else:<br>                if len(inputs) != input_len:<br>                    raise ValueError(\"All inputs must have the same length.\")<br>                new_inputs_list.append(inputs)<br>        return new_inputs_list<br>    def _validate_nested_eval_kwargs_types(<br>        self, eval_kwargs_lists: Dict[str, Any]<br>    ) -> Dict[str, Any]:<br>        \"\"\"<br>        Ensure eval kwargs are acceptable format.<br>            either a Dict[str, List] or a Dict[str, Dict[str, List]].<br>        Allows use of different kwargs (e.g. references) with different evaluators<br>            while keeping backwards compatibility for single evaluators<br>        \"\"\"<br>        if not isinstance(eval_kwargs_lists, dict):<br>            raise ValueError(<br>                f\"eval_kwargs_lists must be a dict. Got {eval_kwargs_lists}\"<br>            )<br>        for evaluator, eval_kwargs in eval_kwargs_lists.items():<br>            if isinstance(eval_kwargs, list):<br>                # maintain backwards compatibility - for use with single evaluator<br>                eval_kwargs_lists[evaluator] = self._validate_and_clean_inputs(<br>                    eval_kwargs<br>                )[0]<br>            elif isinstance(eval_kwargs, dict):<br>                # for use with multiple evaluators<br>                for k in eval_kwargs:<br>                    v = eval_kwargs[k]<br>                    if not isinstance(v, list):<br>                        raise ValueError(<br>                            f\"nested inner values in eval_kwargs must be a list. Got {evaluator}: {k}: {v}\"<br>                        )<br>                    eval_kwargs_lists[evaluator][k] = self._validate_and_clean_inputs(<br>                        v<br>                    )[0]<br>            else:<br>                raise ValueError(<br>                    f\"eval_kwargs must be a list or a dict. Got {evaluator}: {eval_kwargs}\"<br>                )<br>        return eval_kwargs_lists<br>    def _get_eval_kwargs(<br>        self, eval_kwargs_lists: Dict[str, Any], idx: int<br>    ) -> Dict[str, Any]:<br>        \"\"\"<br>        Get eval kwargs from eval_kwargs_lists at a given idx.<br>        Since eval_kwargs_lists is a dict of lists, we need to get the<br>        value at idx for each key.<br>        \"\"\"<br>        return {k: v[idx] for k, v in eval_kwargs_lists.items()}<br>    async def aevaluate_response_strs(<br>        self,<br>        queries: Optional[List[str]] = None,<br>        response_strs: Optional[List[str]] = None,<br>        contexts_list: Optional[List[List[str]]] = None,<br>        **eval_kwargs_lists: Dict[str, Any],<br>    ) -> Dict[str, List[EvaluationResult]]:<br>        \"\"\"<br>        Evaluate query, response pairs.<br>        This evaluates queries, responses, contexts as string inputs.<br>        Can supply additional kwargs to the evaluator in eval_kwargs_lists.<br>        Args:<br>            queries (Optional[List[str]]): List of query strings. Defaults to None.<br>            response_strs (Optional[List[str]]): List of response strings.<br>                Defaults to None.<br>            contexts_list (Optional[List[List[str]]]): List of context lists.<br>                Defaults to None.<br>            **eval_kwargs_lists (Dict[str, Any]): Dict of either dicts or lists<br>                of kwargs to pass to evaluator. Defaults to None.<br>                    multiple evaluators: {evaluator: {kwarg: [list of values]},...}<br>                    single evaluator:    {kwarg: [list of values]}<br>        \"\"\"<br>        queries, response_strs, contexts_list = self._validate_and_clean_inputs(<br>            queries, response_strs, contexts_list<br>        )<br>        eval_kwargs_lists = self._validate_nested_eval_kwargs_types(eval_kwargs_lists)<br>        # boolean to check if using multi kwarg evaluator<br>        multi_kwargs = len(eval_kwargs_lists) > 0 and isinstance(<br>            next(iter(eval_kwargs_lists.values())), dict<br>        )<br>        # run evaluations<br>        eval_jobs = []<br>        for idx, query in enumerate(cast(List[str], queries)):<br>            response_str = cast(List, response_strs)[idx]<br>            contexts = cast(List, contexts_list)[idx]<br>            for name, evaluator in self.evaluators.items():<br>                if multi_kwargs:<br>                    # multi-evaluator - get appropriate runtime kwargs if present<br>                    kwargs = (<br>                        eval_kwargs_lists[name] if name in eval_kwargs_lists else {}<br>                    )<br>                else:<br>                    # single evaluator (maintain backwards compatibility)<br>                    kwargs = eval_kwargs_lists<br>                eval_kwargs = self._get_eval_kwargs(kwargs, idx)<br>                eval_jobs.append(<br>                    eval_worker(<br>                        self.semaphore,<br>                        evaluator,<br>                        name,<br>                        query=query,<br>                        response_str=response_str,<br>                        contexts=contexts,<br>                        eval_kwargs=eval_kwargs,<br>                    )<br>                )<br>        results = await self.asyncio_mod.gather(*eval_jobs)<br>        # Format results<br>        return self._format_results(results)<br>    async def aevaluate_responses(<br>        self,<br>        queries: Optional[List[str]] = None,<br>        responses: Optional[List[Response]] = None,<br>        **eval_kwargs_lists: Dict[str, Any],<br>    ) -> Dict[str, List[EvaluationResult]]:<br>        \"\"\"<br>        Evaluate query, response pairs.<br>        This evaluates queries and response objects.<br>        Args:<br>            queries (Optional[List[str]]): List of query strings. Defaults to None.<br>            responses (Optional[List[Response]]): List of response objects.<br>                Defaults to None.<br>            **eval_kwargs_lists (Dict[str, Any]): Dict of either dicts or lists<br>                of kwargs to pass to evaluator. Defaults to None.<br>                    multiple evaluators: {evaluator: {kwarg: [list of values]},...}<br>                    single evaluator:    {kwarg: [list of values]}<br>        \"\"\"<br>        queries, responses = self._validate_and_clean_inputs(queries, responses)<br>        eval_kwargs_lists = self._validate_nested_eval_kwargs_types(eval_kwargs_lists)<br>        # boolean to check if using multi kwarg evaluator<br>        multi_kwargs = len(eval_kwargs_lists) > 0 and isinstance(<br>            next(iter(eval_kwargs_lists.values())), dict<br>        )<br>        # run evaluations<br>        eval_jobs = []<br>        for idx, query in enumerate(cast(List[str], queries)):<br>            response = cast(List, responses)[idx]<br>            for name, evaluator in self.evaluators.items():<br>                if multi_kwargs:<br>                    # multi-evaluator - get appropriate runtime kwargs if present<br>                    kwargs = (<br>                        eval_kwargs_lists[name] if name in eval_kwargs_lists else {}<br>                    )<br>                else:<br>                    # single evaluator (maintain backwards compatibility)<br>                    kwargs = eval_kwargs_lists<br>                eval_kwargs = self._get_eval_kwargs(kwargs, idx)<br>                eval_jobs.append(<br>                    eval_response_worker(<br>                        self.semaphore,<br>                        evaluator,<br>                        name,<br>                        query=query,<br>                        response=response,<br>                        eval_kwargs=eval_kwargs,<br>                    )<br>                )<br>        results = await self.asyncio_mod.gather(*eval_jobs)<br>        # Format results<br>        return self._format_results(results)<br>    async def aevaluate_queries(<br>        self,<br>        query_engine: BaseQueryEngine,<br>        queries: Optional[List[str]] = None,<br>        **eval_kwargs_lists: Dict[str, Any],<br>    ) -> Dict[str, List[EvaluationResult]]:<br>        \"\"\"<br>        Evaluate queries.<br>        Args:<br>            query_engine (BaseQueryEngine): Query engine.<br>            queries (Optional[List[str]]): List of query strings. Defaults to None.<br>            **eval_kwargs_lists (Dict[str, Any]): Dict of lists of kwargs to<br>                pass to evaluator. Defaults to None.<br>        \"\"\"<br>        if queries is None:<br>            raise ValueError(\"</code>queries<code> must be provided\")<br>        # gather responses<br>        response_jobs = []<br>        for query in queries:<br>            response_jobs.append(response_worker(self.semaphore, query_engine, query))<br>        responses = await self.asyncio_mod.gather(*response_jobs)<br>        return await self.aevaluate_responses(<br>            queries=queries,<br>            responses=responses,<br>            **eval_kwargs_lists,<br>        )<br>    def evaluate_response_strs(<br>        self,<br>        queries: Optional[List[str]] = None,<br>        response_strs: Optional[List[str]] = None,<br>        contexts_list: Optional[List[List[str]]] = None,<br>        **eval_kwargs_lists: Dict[str, Any],<br>    ) -> Dict[str, List[EvaluationResult]]:<br>        \"\"\"<br>        Evaluate query, response pairs.<br>        Sync version of aevaluate_response_strs.<br>        \"\"\"<br>        return asyncio_run(<br>            self.aevaluate_response_strs(<br>                queries=queries,<br>                response_strs=response_strs,<br>                contexts_list=contexts_list,<br>                **eval_kwargs_lists,<br>            )<br>        )<br>    def evaluate_responses(<br>        self,<br>        queries: Optional[List[str]] = None,<br>        responses: Optional[List[Response]] = None,<br>        **eval_kwargs_lists: Dict[str, Any],<br>    ) -> Dict[str, List[EvaluationResult]]:<br>        \"\"\"<br>        Evaluate query, response objs.<br>        Sync version of aevaluate_responses.<br>        \"\"\"<br>        return asyncio_run(<br>            self.aevaluate_responses(<br>                queries=queries,<br>                responses=responses,<br>                **eval_kwargs_lists,<br>            )<br>        )<br>    def evaluate_queries(<br>        self,<br>        query_engine: BaseQueryEngine,<br>        queries: Optional[List[str]] = None,<br>        **eval_kwargs_lists: Dict[str, Any],<br>    ) -> Dict[str, List[EvaluationResult]]:<br>        \"\"\"<br>        Evaluate queries.<br>        Sync version of aevaluate_queries.<br>        \"\"\"<br>        return asyncio_run(<br>            self.aevaluate_queries(<br>                query_engine=query_engine,<br>                queries=queries,<br>                **eval_kwargs_lists,<br>            )<br>        )<br>    def upload_eval_results(<br>        self,<br>        project_name: str,<br>        app_name: str,<br>        results: Dict[str, List[EvaluationResult]],<br>    ) -> None:<br>        \"\"\"<br>        Upload the evaluation results to LlamaCloud.<br>        Args:<br>            project_name (str): The name of the project.<br>            app_name (str): The name of the app.<br>            results (Dict[str, List[EvaluationResult]]):<br>                The evaluation results, a mapping of metric name to a list of EvaluationResult objects.<br>        Examples:<br>            </code>`<code>python<br>            results = batch_runner.evaluate_responses(...)<br>            batch_runner.upload_eval_results(<br>                project_name=\"my_project\",<br>                app_name=\"my_app\",<br>                results=results<br>            )<br>            </code>`<code><br>        \"\"\"<br>        from llama_index.core.evaluation.eval_utils import upload_eval_results<br>        upload_eval_results(<br>            project_name=project_name, app_name=app_name, results=results<br>        )<br></code>``` |\n"
    },
    {
      "id": "d79de168-d0a8-4bd6-bc45-0ed29ec93bf7",
      "size": 3530,
      "headers": {
        "h1": "Lats",
        "h2": "LATSAgentWorker \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br></code>`<code> | </code>`<code><br>class LATSAgentWorker(CustomSimpleAgentWorker):<br>    \"\"\"Agent worker that performs a step of Language Agent Tree Search.<br>    Source paper: https://arxiv.org/pdf/2310.04406v2.pdf.<br>    Continues iterating until there's no errors / task is done.<br>    \"\"\"<br>    num_expansions: int = Field(default=2, description=\"Number of expansions to do.\")<br>    reflection_prompt: PromptTemplate = Field(..., description=\"Reflection prompt.\")<br>    candiate_expansion_prompt: PromptTemplate = Field(<br>        ..., description=\"Candidate expansion prompt.\"<br>    )<br>    max_rollouts: int = Field(<br>        default=5,<br>        description=(<br>            \"Max rollouts. By default, -1 means that we keep going until the first solution is found.\"<br>        ),<br>    )<br>    chat_formatter: ReActChatFormatter = Field(<br>        default_factory=ReActChatFormatter, description=\"Chat formatter.\"<br>    )<br>    def __init__(<br>        self,<br>        tools: List[BaseTool],<br>        llm: Optional[LLM] = None,<br>        num_expansions: int = 2,<br>        max_rollouts: int = 5,<br>        reflection_prompt: Optional[PromptTemplate] = None,<br>        candiate_expansion_prompt: Optional[PromptTemplate] = None,<br>        **kwargs: Any,<br>    ) -> None:<br>        \"\"\"Init params.\"\"\"<br>        # validate that all tools are query engine tools<br>        llm = llm or Settings.llm<br>        super().__init__(<br>            tools=tools,<br>            llm=llm,<br>            num_expansions=num_expansions,<br>            max_rollouts=max_rollouts,<br>            reflection_prompt=reflection_prompt or DEFAULT_REFLECTION_PROMPT,<br>            candiate_expansion_prompt=candiate_expansion_prompt<br>            or DEFAULT_CANDIDATES_PROMPT,<br>            **kwargs,<br>        )<br>    def _initialize_state(self, task: Task, **kwargs: Any) -> Dict[str, Any]:<br>        \"\"\"Initialize state.\"\"\"<br>        # initialize root node<br>        root_node = SearchNode(<br>            current_reasoning=[ObservationReasoningStep(observation=task.input)],<br>            evaluation=Evaluation(score=1),  # evaluation for root node is blank<br>        )<br>        return {\"count\": 0, \"solution_queue\": [], \"root_node\": root_node}<br>    async def _arun_candidate(<br>        self,<br>        node: SearchNode,<br>        task: Task,<br>    ) -> List[BaseReasoningStep]:<br>        \"\"\"Generate candidate for a given node.<br>        Generically we sample the action space to generate new candidate nodes.<br>        Practically since we're using a ReAct powered agent, this means<br>        using generating a ReAct trajectory, running a tool.<br>        \"\"\"<br>        output_parser = ReActOutputParser()<br>        # format react prompt<br>        formatted_prompt = self.chat_formatter.format(<br>            self.tools,<br>            chat_history=task.memory.get(),<br>            current_reasoning=node.current_reasoning,<br>        )<br>        # run LLM<br>        response = await self.llm.achat(formatted_prompt)<br>        # parse output into reasoning step<br>        try:<br>            reasoning_step = output_parser.parse(response.message.content)<br>        except ValueError as e:<br>            reasoning_step = ResponseReasoningStep(<br>                thought=response.message.content,<br>                response=f\"Encountered an error parsing: {e!s}\",<br>            )<br>        # get response or run tool<br>        if reasoning_step.is_done:<br>            reasoning_step = cast(ResponseReasoningStep, reasoning_step)<br>            current_reasoning = [reasoning_step]<br>        else:<br>            reasoning_step = cast(ActionReasoningStep, reasoning_step)<br>            tool_selection = ToolSelection(<br>                tool_id=reasoning_step.action,<br>                tool_name=reasoning_step.action,<br>                tool_kwargs=reasoning_step.action_input,<br>            )<br>            try:<br>                tool_output = await acall_tool_with_selection(<br>                    tool_selection, self.tools, verbose=self.verbose<br>                )<br>            except Exception as e:<br>                tool_output = f\"Encountered error: {e!s}\"<br>            observation_step = ObservationReasoningStep(observation=str(tool_output))<br>            current_reasoning = [reasoning_step, observation_step]<br>        return current_reasoning<br>    async def _aevaluate(<br>        self,<br>        cur_node: SearchNode,<br>        current_reasoning: List[BaseReasoningStep],<br>        input: str,<br>    ) -> float:<br>        \"\"\"Evaluate.\"\"\"<br>        all_reasoning = cur_node.current_reasoning + current_reasoning<br>        history_str = \"\\n\".join([s.get_content() for s in all_reasoning])<br>        evaluation = await self.llm.astructured_predict(<br>            Evaluation,<br>            prompt=self.reflection_prompt,<br>            query=input,<br>            conversation_history=history_str,<br>        )<br>        if self.verbose:<br>            print_text(<br>                f\"> Evaluation for input {input}\\n: {evaluation}\\n\\n\", color=\"pink\"<br>            )<br>        return evaluation<br>    async def _get_next_candidates(<br>        self,<br>        cur_node: SearchNode,<br>        input: str,<br>    ) -> List[str]:<br>        \"\"\"Get next candidates.\"\"\"<br>        # get candidates<br>        history_str = \"\\n\".join([s.get_content() for s in cur_node.current_reasoning])<br>        candidates = await self.llm.astructured_predict(<br>            Candidates,<br>            prompt=self.candiate_expansion_prompt,<br>            query=input,<br>            conversation_history=history_str,<br>            num_candidates=self.num_expansions,<br>        )<br>        candidate_strs = candidates.candidates[: self.num_expansions]<br>        if self.verbose:<br>            print_text(f\"> Got candidates: {candidate_strs}\\n\", color=\"yellow\")<br>        # ensure we have the right number of candidates<br>        if len(candidate_strs) < self.num_expansions:<br>            return (candidate_strs * self.num_expansions)[: self.num_expansions]<br>        else:<br>            return candidate_strs[: self.num_expansions]<br>    def _update_state(<br>        self,<br>        node: SearchNode,<br>        current_reasoning: List[BaseReasoningStep],<br>        evaluation: Evaluation,<br>    ) -> SearchNode:<br>        \"\"\"Update state.\"\"\"<br>        # create child node<br>        new_node = SearchNode(<br>            current_reasoning=node.current_reasoning + current_reasoning,<br>            parent=node,<br>            children=[],<br>            evaluation=evaluation,<br>        )<br>        node.children.append(new_node)<br>        # backpropagate the reward<br>        new_node.backpropagate(evaluation.score)<br>        return new_node<br>    def _run_step(<br>        self, state: Dict[str, Any], task: Task, input: Optional[str] = None<br>    ) -> Tuple[AgentChatResponse, bool]:<br>        \"\"\"Run step.<br>        Returns:<br>            Tuple of (agent_response, is_done)<br>        \"\"\"<br>        return asyncio.run(self._arun_step(state, task, input))<br>    async def _arun_step(<br>        self, state: Dict[str, Any], task: Task, input: Optional[str] = None<br>    ) -> Tuple[AgentChatResponse, bool]:<br>        \"\"\"Run step.<br>        Returns:<br>            Tuple of (agent_response, is_done)<br>        \"\"\"<br>        root_node = state[\"root_node\"]<br>        cur_node = root_node.get_best_leaf()<br>        if self.verbose:<br>            print_text(<br>                f\"> Selecting node to expand: {cur_node.answer}\\n\", color=\"green\"<br>            )<br>        # expand the given node, generate n candidate nodes<br>        new_candidates = await self._get_next_candidates(<br>            cur_node,<br>            task.input,<br>        )<br>        new_nodes = []<br>        for candidate in new_candidates:<br>            new_nodes.append(<br>                self._update_state(<br>                    cur_node,<br>                    [ObservationReasoningStep(observation=candidate)],<br>                    Evaluation(score=1),  # evaluation for candidate node is blank<br>                )<br>            )<br>        # expand the given node, generate n candidates<br>        # for each candidate, run tool, get response<br>        solution_queue: List[SearchNode] = state[\"solution_queue\"]<br>        # first, generate the candidates<br>        candidate_jobs = [<br>            self._arun_candidate(new_node, task) for new_node in new_nodes<br>        ]<br>        all_new_reasoning_steps = await asyncio.gather(*candidate_jobs)<br>        if self.verbose:<br>            for new_reasoning_steps in all_new_reasoning_steps:<br>                out_txt = \"\\n\".join([s.get_content() for s in new_reasoning_steps])<br>                print_text(f\"> Generated new reasoning step: {out_txt}\\n\", color=\"blue\")<br>        # then, evaluate the candidates<br>        eval_jobs = [<br>            self._aevaluate(new_node, new_reasoning_steps, task.input)<br>            for new_node, new_reasoning_steps in zip(new_nodes, all_new_reasoning_steps)<br>        ]<br>        evaluations = await asyncio.gather(*eval_jobs)<br>        # then, update the state<br>        for new_reasoning_steps, cur_new_node, evaluation in zip(<br>            all_new_reasoning_steps, new_nodes, evaluations<br>        ):<br>            new_node = self._update_state(cur_new_node, new_reasoning_steps, evaluation)<br>            if new_node.is_done:<br>                if self.verbose:<br>                    print_text(<br>                        f\"> Found solution node: {new_node.answer}\\n\", color=\"cyan\"<br>                    )<br>                solution_queue.append(new_node)<br>        # check if done<br>        state[\"count\"] += 1<br>        if self.max_rollouts == -1 and solution_queue:<br>            is_done = True<br>        elif self.max_rollouts > 0 and state[\"count\"] >= self.max_rollouts:<br>            is_done = True<br>        else:<br>            is_done = False<br>        # determine response<br>        if solution_queue:<br>            best_solution_node = max(solution_queue, key=lambda x: x.score)<br>            response = best_solution_node.answer<br>        else:<br>            response = \"I am still thinking.\"<br>        if self.verbose:<br>            print_text(f\"> Got final response: {response!s}\\n\", color=\"green\")<br>        # return response<br>        return AgentChatResponse(response=str(response)), is_done<br>    def _finalize_task(self, state: Dict[str, Any], **kwargs) -> None:<br>        \"\"\"Finalize task.\"\"\"<br></code>`` |\n"
    },
    {
      "id": "722a9089-6174-45d9-bab7-db57b56edc5d",
      "size": 1540,
      "headers": {
        "h1": "Clip",
        "h2": "ClipEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br></code>`<code> | </code>`<code><br>class ClipEmbedding(MultiModalEmbedding):<br>    \"\"\"CLIP embedding models for encoding text and image for Multi-Modal purpose.<br>    This class provides an interface to generate embeddings using a model<br>    deployed in OpenAI CLIP. At the initialization it requires a model name<br>    of CLIP.<br>    Note:<br>        Requires </code>clip<code> package to be available in the PYTHONPATH. It can be installed with<br>        </code>pip install git+https://github.com/openai/CLIP.git<code>.<br>    \"\"\"<br>    embed_batch_size: int = Field(default=DEFAULT_EMBED_BATCH_SIZE, gt=0)<br>    _clip: Any = PrivateAttr()<br>    _model: Any = PrivateAttr()<br>    _preprocess: Any = PrivateAttr()<br>    _device: Any = PrivateAttr()<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"ClipEmbedding\"<br>    def __init__(<br>        self,<br>        *,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        model_name: str = DEFAULT_CLIP_MODEL,<br>        **kwargs: Any,<br>    ):<br>        \"\"\"Initializes the ClipEmbedding class.<br>        During the initialization the </code>clip<code> package is imported.<br>        Args:<br>            embed_batch_size (int, optional): The batch size for embedding generation. Defaults to 10,<br>                must be > 0 and <= 100.<br>            model_name (str): The model name of Clip model.<br>        Raises:<br>            ImportError: If the </code>clip<code> package is not available in the PYTHONPATH.<br>            ValueError: If the model cannot be fetched from Open AI. or if the embed_batch_size<br>                is not in the range (0, 100].<br>        \"\"\"<br>        if embed_batch_size <= 0:<br>            raise ValueError(f\"Embed batch size {embed_batch_size}  must be > 0.\")<br>        try:<br>            import clip<br>            import torch<br>        except ImportError:<br>            raise ImportError(<br>                \"ClipEmbedding requires </code>pip install git+https://github.com/openai/CLIP.git<code> and torch.\"<br>            )<br>        super().__init__(<br>            embed_batch_size=embed_batch_size, model_name=model_name, **kwargs<br>        )<br>        try:<br>            self._device = \"cuda\" if torch.cuda.is_available() else \"cpu\"<br>            is_local_path = os.path.exists(self.model_name)<br>            if not is_local_path and self.model_name not in AVAILABLE_CLIP_MODELS:<br>                raise ValueError(<br>                    f\"Model name {self.model_name} is not available in CLIP.\"<br>                )<br>            self._model, self._preprocess = clip.load(<br>                self.model_name, device=self._device<br>            )<br>        except Exception as e:<br>            logger.error(\"Error while loading clip model.\")<br>            raise ValueError(\"Unable to fetch the requested embeddings model\") from e<br>    # TEXT EMBEDDINGS<br>    async def _aget_query_embedding(self, query: str) -> Embedding:<br>        return self._get_query_embedding(query)<br>    def _get_text_embedding(self, text: str) -> Embedding:<br>        return self._get_text_embeddings([text])[0]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[Embedding]:<br>        results = []<br>        for text in texts:<br>            try:<br>                import clip<br>            except ImportError:<br>                raise ImportError(<br>                    \"ClipEmbedding requires </code>pip install git+https://github.com/openai/CLIP.git<code> and torch.\"<br>                )<br>            text_embedding = self._model.encode_text(<br>                clip.tokenize(text).to(self._device)<br>            )<br>            results.append(text_embedding.tolist()[0])<br>        return results<br>    def _get_query_embedding(self, query: str) -> Embedding:<br>        return self._get_text_embedding(query)<br>    # IMAGE EMBEDDINGS<br>    async def _aget_image_embedding(self, img_file_path: ImageType) -> Embedding:<br>        return self._get_image_embedding(img_file_path)<br>    def _get_image_embedding(self, img_file_path: ImageType) -> Embedding:<br>        import torch<br>        with torch.no_grad():<br>            image = (<br>                self._preprocess(Image.open(img_file_path))<br>                .unsqueeze(0)<br>                .to(self._device)<br>            )<br>            return self._model.encode_image(image).tolist()[0]<br></code>`` |\n"
    },
    {
      "id": "851671b3-a73f-448f-a9ef-0440328f7535",
      "size": 1439,
      "headers": {
        "h1": "Retrieval",
        "h2": "BaseRetrievalEvaluator \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br></code>`<code> | </code>`<code><br>class BaseRetrievalEvaluator(BaseModel):<br>    \"\"\"Base Retrieval Evaluator class.\"\"\"<br>    model_config = ConfigDict(arbitrary_types_allowed=True)<br>    metrics: List[BaseRetrievalMetric] = Field(<br>        ..., description=\"List of metrics to evaluate\"<br>    )<br>    @classmethod<br>    def from_metric_names(<br>        cls, metric_names: List[str], **kwargs: Any<br>    ) -> \"BaseRetrievalEvaluator\":<br>        \"\"\"Create evaluator from metric names.<br>        Args:<br>            metric_names (List[str]): List of metric names<br>            **kwargs: Additional arguments for the evaluator<br>        \"\"\"<br>        metric_types = resolve_metrics(metric_names)<br>        return cls(metrics=[metric() for metric in metric_types], **kwargs)<br>    @abstractmethod<br>    async def _aget_retrieved_ids_and_texts(<br>        self, query: str, mode: RetrievalEvalMode = RetrievalEvalMode.TEXT<br>    ) -> Tuple[List[str], List[str]]:<br>        \"\"\"Get retrieved ids and texts.\"\"\"<br>        raise NotImplementedError<br>    def evaluate(<br>        self,<br>        query: str,<br>        expected_ids: List[str],<br>        expected_texts: Optional[List[str]] = None,<br>        mode: RetrievalEvalMode = RetrievalEvalMode.TEXT,<br>        **kwargs: Any,<br>    ) -> RetrievalEvalResult:<br>        \"\"\"Run evaluation results with query string and expected ids.<br>        Args:<br>            query (str): Query string<br>            expected_ids (List[str]): Expected ids<br>        Returns:<br>            RetrievalEvalResult: Evaluation result<br>        \"\"\"<br>        return asyncio_run(<br>            self.aevaluate(<br>                query=query,<br>                expected_ids=expected_ids,<br>                expected_texts=expected_texts,<br>                mode=mode,<br>                **kwargs,<br>            )<br>        )<br>    # @abstractmethod<br>    async def aevaluate(<br>        self,<br>        query: str,<br>        expected_ids: List[str],<br>        expected_texts: Optional[List[str]] = None,<br>        mode: RetrievalEvalMode = RetrievalEvalMode.TEXT,<br>        **kwargs: Any,<br>    ) -> RetrievalEvalResult:<br>        \"\"\"Run evaluation with query string, retrieved contexts,<br>        and generated response string.<br>        Subclasses can override this method to provide custom evaluation logic and<br>        take in additional arguments.<br>        \"\"\"<br>        retrieved_ids, retrieved_texts = await self._aget_retrieved_ids_and_texts(<br>            query, mode<br>        )<br>        metric_dict = {}<br>        for metric in self.metrics:<br>            eval_result = metric.compute(<br>                query, expected_ids, retrieved_ids, expected_texts, retrieved_texts<br>            )<br>            metric_dict[metric.metric_name] = eval_result<br>        return RetrievalEvalResult(<br>            query=query,<br>            expected_ids=expected_ids,<br>            expected_texts=expected_texts,<br>            retrieved_ids=retrieved_ids,<br>            retrieved_texts=retrieved_texts,<br>            mode=mode,<br>            metric_dict=metric_dict,<br>        )<br>    async def aevaluate_dataset(<br>        self,<br>        dataset: EmbeddingQAFinetuneDataset,<br>        workers: int = 2,<br>        show_progress: bool = False,<br>        **kwargs: Any,<br>    ) -> List[RetrievalEvalResult]:<br>        \"\"\"Run evaluation with dataset.\"\"\"<br>        semaphore = asyncio.Semaphore(workers)<br>        async def eval_worker(<br>            query: str, expected_ids: List[str], mode: RetrievalEvalMode<br>        ) -> RetrievalEvalResult:<br>            async with semaphore:<br>                return await self.aevaluate(query, expected_ids=expected_ids, mode=mode)<br>        response_jobs = []<br>        mode = RetrievalEvalMode.from_str(dataset.mode)<br>        for query_id, query in dataset.queries.items():<br>            expected_ids = dataset.relevant_docs[query_id]<br>            response_jobs.append(eval_worker(query, expected_ids, mode))<br>        if show_progress:<br>            from tqdm.asyncio import tqdm_asyncio<br>            eval_results = await tqdm_asyncio.gather(*response_jobs)<br>        else:<br>            eval_results = await asyncio.gather(*response_jobs)<br>        return eval_results<br></code>`` |\n"
    },
    {
      "id": "8301489a-5e6f-411a-ba01-5f903a832f9a",
      "size": 1652,
      "headers": {
        "h1": "Gigachat",
        "h2": "GigaChatEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 13<br> 14<br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br></code>`<code> | </code>`<code><br>class GigaChatEmbedding(BaseEmbedding):<br>    \"\"\"<br>    GigaChat encoder class for generating embeddings.<br>    Attributes:<br>        _client (Optional[GigaChat]): Instance of the GigaChat client.<br>        type (str): Type identifier for the encoder, which is \"gigachat\".<br>    Example:<br>        .. code-block:: python<br>            from langchain_community.embeddings.gigachat import GigaChatEmbeddings<br>            embeddings = GigaChatEmbeddings(<br>                credentials=..., scope=..., verify_ssl_certs=False<br>            )<br>    \"\"\"<br>    _client: Optional[GigaChat] = PrivateAttr()<br>    type: str = \"gigachat\"<br>    def __init__(<br>        self,<br>        name: Optional[str] = \"Embeddings\",<br>        auth_data: Optional[str] = None,<br>        scope: Optional[str] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>        **kwargs: Any,<br>    ) -> None:<br>        auth_data = get_from_param_or_env(<br>            \"auth_data\", auth_data, \"GIGACHAT_AUTH_DATA\", \"\"<br>        )<br>        if not auth_data:<br>            raise ValueError(<br>                \"You must provide an AUTH DATA to use GigaChat. \"<br>                \"You can either pass it in as an argument or set it </code>GIGACHAT_AUTH_DATA<code>.\"<br>            )<br>        if scope is None:<br>            raise ValueError(<br>                \"\"\"<br>                GigaChat scope cannot be 'None'.<br>                Set 'GIGACHAT_API_PERS' for personal use or 'GIGACHAT_API_CORP' for corporate use.<br>                \"\"\"<br>            )<br>        super().__init__(<br>            model_name=name,<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            **kwargs,<br>        )<br>        try:<br>            self._client = GigaChat(<br>                scope=scope, credentials=auth_data, verify_ssl_certs=False<br>            )<br>        except Exception as e:<br>            raise ValueError(f\"GigaChat client failed to initialize. Error: {e}\") from e<br>    @classmethod<br>    def class_name(cls) -> str:<br>        \"\"\"Return the class name.\"\"\"<br>        return \"GigaChatEmbedding\"<br>    def _get_query_embeddings(self, queries: List[str]) -> List[List[float]]:<br>        \"\"\"Synchronously Embed documents using a GigaChat embeddings model.<br>        Args:<br>            queries: The list of documents to embed.<br>        Returns:<br>            List of embeddings, one for each document.<br>        \"\"\"<br>        embeddings = self._client.embeddings(queries).data<br>        return [embeds_obj.embedding for embeds_obj in embeddings]<br>    async def _aget_query_embeddings(self, queries: List[str]) -> List[List[float]]:<br>        \"\"\"Asynchronously embed documents using a GigaChat embeddings model.<br>        Args:<br>            queries: The list of documents to embed.<br>        Returns:<br>            List of embeddings, one for each document.<br>        \"\"\"<br>        embeddings = (await self._client.aembeddings(queries)).data<br>        return [embeds_obj.embedding for embeds_obj in embeddings]<br>    def _get_query_embedding(self, query: List[str]) -> List[float]:<br>        \"\"\"Synchronously embed a document using GigaChat embeddings model.<br>        Args:<br>            query: The document to embed.<br>        Returns:<br>            Embeddings for the document.<br>        \"\"\"<br>        return self._client.embeddings(query).data[0].embedding<br>    async def _aget_query_embedding(self, query: List[str]) -> List[float]:<br>        \"\"\"Asynchronously embed a query using GigaChat embeddings model.<br>        Args:<br>            query: The document to embed.<br>        Returns:<br>            Embeddings for the document.<br>        \"\"\"<br>        return (await self._client.aembeddings(query)).data[0].embedding<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Synchronously embed a text using GigaChat embeddings model.<br>        Args:<br>            text: The text to embed.<br>        Returns:<br>            Embeddings for the text.<br>        \"\"\"<br>        return self._client.embeddings([text]).data[0].embedding<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously embed a text using GigaChat embeddings model.<br>        Args:<br>            text: The text to embed.<br>        Returns:<br>            Embeddings for the text.<br>        \"\"\"<br>        return (await self._client.aembeddings([text])).data[0].embedding<br></code>`` |\n"
    },
    {
      "id": "fcea923e-2b2f-416e-92e0-0022f717b15f",
      "size": 2156,
      "headers": {
        "h1": "Simple",
        "h2": "SimpleChatEngine \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br></code>`<code> | </code>`<code><br>class SimpleChatEngine(BaseChatEngine):<br>    \"\"\"<br>    Simple Chat Engine.<br>    Have a conversation with the LLM.<br>    This does not make use of a knowledge base.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        llm: LLM,<br>        memory: BaseMemory,<br>        prefix_messages: List[ChatMessage],<br>        callback_manager: Optional[CallbackManager] = None,<br>    ) -> None:<br>        self._llm = llm<br>        self._memory = memory<br>        self._prefix_messages = prefix_messages<br>        self.callback_manager = callback_manager or CallbackManager([])<br>    @classmethod<br>    def from_defaults(<br>        cls,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        memory: Optional[BaseMemory] = None,<br>        memory_cls: Type[BaseMemory] = ChatMemoryBuffer,<br>        system_prompt: Optional[str] = None,<br>        prefix_messages: Optional[List[ChatMessage]] = None,<br>        llm: Optional[LLM] = None,<br>        **kwargs: Any,<br>    ) -> \"SimpleChatEngine\":<br>        \"\"\"Initialize a SimpleChatEngine from default parameters.\"\"\"<br>        llm = llm or Settings.llm<br>        chat_history = chat_history or []<br>        memory = memory or memory_cls.from_defaults(chat_history=chat_history, llm=llm)<br>        if system_prompt is not None:<br>            if prefix_messages is not None:<br>                raise ValueError(<br>                    \"Cannot specify both system_prompt and prefix_messages\"<br>                )<br>            prefix_messages = [<br>                ChatMessage(content=system_prompt, role=llm.metadata.system_role)<br>            ]<br>        prefix_messages = prefix_messages or []<br>        return cls(<br>            llm=llm,<br>            memory=memory,<br>            prefix_messages=prefix_messages,<br>            callback_manager=Settings.callback_manager,<br>        )<br>    @trace_method(\"chat\")<br>    def chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> AgentChatResponse:<br>        if chat_history is not None:<br>            self._memory.set(chat_history)<br>        self._memory.put(ChatMessage(content=message, role=\"user\"))<br>        if hasattr(self._memory, \"tokenizer_fn\"):<br>            initial_token_count = len(<br>                self._memory.tokenizer_fn(<br>                    \" \".join([(m.content or \"\") for m in self._prefix_messages])<br>                )<br>            )<br>        else:<br>            initial_token_count = 0<br>        all_messages = self._prefix_messages + self._memory.get(<br>            initial_token_count=initial_token_count<br>        )<br>        chat_response = self._llm.chat(all_messages)<br>        ai_message = chat_response.message<br>        self._memory.put(ai_message)<br>        return AgentChatResponse(response=str(chat_response.message.content))<br>    @trace_method(\"chat\")<br>    def stream_chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> StreamingAgentChatResponse:<br>        if chat_history is not None:<br>            self._memory.set(chat_history)<br>        self._memory.put(ChatMessage(content=message, role=\"user\"))<br>        if hasattr(self._memory, \"tokenizer_fn\"):<br>            initial_token_count = len(<br>                self._memory.tokenizer_fn(<br>                    \" \".join([(m.content or \"\") for m in self._prefix_messages])<br>                )<br>            )<br>        else:<br>            initial_token_count = 0<br>        all_messages = self._prefix_messages + self._memory.get(<br>            initial_token_count=initial_token_count<br>        )<br>        chat_response = StreamingAgentChatResponse(<br>            chat_stream=self._llm.stream_chat(all_messages)<br>        )<br>        thread = Thread(<br>            target=chat_response.write_response_to_history, args=(self._memory,)<br>        )<br>        thread.start()<br>        return chat_response<br>    @trace_method(\"chat\")<br>    async def achat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> AgentChatResponse:<br>        if chat_history is not None:<br>            self._memory.set(chat_history)<br>        self._memory.put(ChatMessage(content=message, role=\"user\"))<br>        if hasattr(self._memory, \"tokenizer_fn\"):<br>            initial_token_count = len(<br>                self._memory.tokenizer_fn(<br>                    \" \".join([(m.content or \"\") for m in self._prefix_messages])<br>                )<br>            )<br>        else:<br>            initial_token_count = 0<br>        all_messages = self._prefix_messages + self._memory.get(<br>            initial_token_count=initial_token_count<br>        )<br>        chat_response = await self._llm.achat(all_messages)<br>        ai_message = chat_response.message<br>        self._memory.put(ai_message)<br>        return AgentChatResponse(response=str(chat_response.message.content))<br>    @trace_method(\"chat\")<br>    async def astream_chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> StreamingAgentChatResponse:<br>        if chat_history is not None:<br>            self._memory.set(chat_history)<br>        self._memory.put(ChatMessage(content=message, role=\"user\"))<br>        if hasattr(self._memory, \"tokenizer_fn\"):<br>            initial_token_count = len(<br>                self._memory.tokenizer_fn(<br>                    \" \".join([(m.content or \"\") for m in self._prefix_messages])<br>                )<br>            )<br>        else:<br>            initial_token_count = 0<br>        all_messages = self._prefix_messages + self._memory.get(<br>            initial_token_count=initial_token_count<br>        )<br>        chat_response = StreamingAgentChatResponse(<br>            achat_stream=await self._llm.astream_chat(all_messages)<br>        )<br>        asyncio.create_task(chat_response.awrite_response_to_history(self._memory))<br>        return chat_response<br>    def reset(self) -> None:<br>        self._memory.reset()<br>    @property<br>    def chat_history(self) -> List[ChatMessage]:<br>        \"\"\"Get chat history.\"\"\"<br>        return self._memory.get_all()<br></code>`` |\n"
    },
    {
      "id": "68b90bf7-5cca-480f-aeb2-46bcd00ff816",
      "size": 1366,
      "headers": {
        "h1": "Context relevancy",
        "h2": "ContextRelevancyEvaluator \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br></code>`<code> | </code>`<code><br>class ContextRelevancyEvaluator(BaseEvaluator):<br>    \"\"\"Context relevancy evaluator.<br>    Evaluates the relevancy of retrieved contexts to a query.<br>    This evaluator considers the query string and retrieved contexts.<br>    Args:<br>        raise_error(Optional[bool]):<br>            Whether to raise an error if the response is invalid.<br>            Defaults to False.<br>        eval_template(Optional[Union[str, BasePromptTemplate]]):<br>            The template to use for evaluation.<br>        refine_template(Optional[Union[str, BasePromptTemplate]]):<br>            The template to use for refinement.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        llm: Optional[LLM] = None,<br>        raise_error: bool = False,<br>        eval_template: str | BasePromptTemplate | None = None,<br>        refine_template: str | BasePromptTemplate | None = None,<br>        score_threshold: float = _DEFAULT_SCORE_THRESHOLD,<br>        parser_function: Callable[<br>            [str], Tuple[Optional[float], Optional[str]]<br>        ] = _default_parser_function,<br>    ) -> None:<br>        \"\"\"Init params.\"\"\"<br>        from llama_index.core import Settings<br>        self._llm = llm or Settings.llm<br>        self._raise_error = raise_error<br>        self._eval_template: BasePromptTemplate<br>        if isinstance(eval_template, str):<br>            self._eval_template = PromptTemplate(eval_template)<br>        else:<br>            self._eval_template = eval_template or DEFAULT_EVAL_TEMPLATE<br>        self._refine_template: BasePromptTemplate<br>        if isinstance(refine_template, str):<br>            self._refine_template = PromptTemplate(refine_template)<br>        else:<br>            self._refine_template = refine_template or DEFAULT_REFINE_TEMPLATE<br>        self.parser_function = parser_function<br>        self.score_threshold = score_threshold<br>    def _get_prompts(self) -> PromptDictType:<br>        \"\"\"Get prompts.\"\"\"<br>        return {<br>            \"eval_template\": self._eval_template,<br>            \"refine_template\": self._refine_template,<br>        }<br>    def _update_prompts(self, prompts: PromptDictType) -> None:<br>        \"\"\"Update prompts.\"\"\"<br>        if \"eval_template\" in prompts:<br>            self._eval_template = prompts[\"eval_template\"]<br>        if \"refine_template\" in prompts:<br>            self._refine_template = prompts[\"refine_template\"]<br>    async def aevaluate(<br>        self,<br>        query: str | None = None,<br>        response: str | None = None,<br>        contexts: Sequence[str] | None = None,<br>        sleep_time_in_seconds: int = 0,<br>        **kwargs: Any,<br>    ) -> EvaluationResult:<br>        \"\"\"Evaluate whether the contexts is relevant to the query.\"\"\"<br>        del kwargs  # Unused<br>        del response  # Unused<br>        if query is None or contexts is None:<br>            raise ValueError(\"Both query and contexts must be provided\")<br>        docs = [Document(text=context) for context in contexts]<br>        index = SummaryIndex.from_documents(docs)<br>        await asyncio.sleep(sleep_time_in_seconds)<br>        query_engine = index.as_query_engine(<br>            llm=self._llm,<br>            text_qa_template=self._eval_template,<br>            refine_template=self._refine_template,<br>        )<br>        response_obj = await query_engine.aquery(query)<br>        raw_response_txt = str(response_obj)<br>        score, reasoning = self.parser_function(raw_response_txt)<br>        invalid_result, invalid_reason = False, None<br>        if score is None and reasoning is None:<br>            if self._raise_error:<br>                raise ValueError(\"The response is invalid\")<br>            invalid_result = True<br>            invalid_reason = \"Unable to parse the output string.\"<br>        if score:<br>            score /= self.score_threshold<br>        return EvaluationResult(<br>            query=query,<br>            contexts=contexts,<br>            score=score,<br>            feedback=raw_response_txt,<br>            invalid_result=invalid_result,<br>            invalid_reason=invalid_reason,<br>        )<br></code>`` |\n"
    },
    {
      "id": "0c81156a-85c9-4ff7-9f4c-dbb3ff5981b5",
      "size": 1805,
      "headers": {
        "h1": "Together",
        "h2": "TogetherEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 12<br> 13<br> 14<br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br></code>`<code> | </code>`<code><br>class TogetherEmbedding(BaseEmbedding):<br>    api_base: str = Field(<br>        default=\"https://api.together.xyz/v1\",<br>        description=\"The base URL for the Together API.\",<br>    )<br>    api_key: str = Field(<br>        default=\"\",<br>        description=\"The API key for the Together API. If not set, will attempt to use the TOGETHER_API_KEY environment variable.\",<br>    )<br>    def __init__(<br>        self,<br>        model_name: str,<br>        api_key: Optional[str] = None,<br>        api_base: str = \"https://api.together.xyz/v1\",<br>        **kwargs: Any,<br>    ) -> None:<br>        api_key = api_key or os.environ.get(\"TOGETHER_API_KEY\", None)<br>        super().__init__(<br>            model_name=model_name,<br>            api_key=api_key,<br>            api_base=api_base,<br>            **kwargs,<br>        )<br>    def _generate_embedding(self, text: str, model_api_string: str) -> Embedding:<br>        \"\"\"Generate embeddings from Together API.<br>        Args:<br>            text: str. An input text sentence or document.<br>            model_api_string: str. An API string for a specific embedding model of your choice.<br>        Returns:<br>            embeddings: a list of float numbers. Embeddings correspond to your given text.<br>        \"\"\"<br>        headers = {<br>            \"accept\": \"application/json\",<br>            \"content-type\": \"application/json\",<br>            \"Authorization\": f\"Bearer {self.api_key}\",<br>        }<br>        session = requests.session()<br>        while True:<br>            response = session.post(<br>                self.api_base.strip(\"/\") + \"/embeddings\",<br>                headers=headers,<br>                json={\"input\": text, \"model\": model_api_string},<br>            )<br>            if response.status_code != 200:<br>                if response.status_code == 429:<br>                    \"\"\"Rate limit exceeded, wait for reset\"\"\"<br>                    reset_time = int(response.headers.get(\"X-RateLimit-Reset\", 0))<br>                    if reset_time > 0:<br>                        time.sleep(reset_time)<br>                        continue<br>                    else:<br>                        \"\"\"Rate limit reset time has passed, retry immediately\"\"\"<br>                        continue<br>                \"\"\" Handle other non-200 status codes \"\"\"<br>                raise ValueError(<br>                    f\"Request failed with status code {response.status_code}: {response.text}\"<br>                )<br>            return response.json()[\"data\"][0][\"embedding\"]<br>    async def _agenerate_embedding(self, text: str, model_api_string: str) -> Embedding:<br>        \"\"\"Async generate embeddings from Together API.<br>        Args:<br>            text: str. An input text sentence or document.<br>            model_api_string: str. An API string for a specific embedding model of your choice.<br>        Returns:<br>            embeddings: a list of float numbers. Embeddings correspond to your given text.<br>        \"\"\"<br>        headers = {<br>            \"accept\": \"application/json\",<br>            \"content-type\": \"application/json\",<br>            \"Authorization\": f\"Bearer {self.api_key}\",<br>        }<br>        async with httpx.AsyncClient() as client:<br>            while True:<br>                response = await client.post(<br>                    self.api_base.strip(\"/\") + \"/embeddings\",<br>                    headers=headers,<br>                    json={\"input\": text, \"model\": model_api_string},<br>                )<br>                if response.status_code != 200:<br>                    if response.status_code == 429:<br>                        \"\"\"Rate limit exceeded, wait for reset\"\"\"<br>                        reset_time = int(response.headers.get(\"X-RateLimit-Reset\", 0))<br>                        if reset_time > 0:<br>                            await asyncio.sleep(reset_time)<br>                            continue<br>                        else:<br>                            \"\"\"Rate limit reset time has passed, retry immediately\"\"\"<br>                            continue<br>                    \"\"\" Handle other non-200 status codes\"\"\"<br>                    raise ValueError(<br>                        f\"Request failed with status code {response.status_code}: {response.text}\"<br>                    )<br>                return response.json()[\"data\"][0][\"embedding\"]<br>    def _get_text_embedding(self, text: str) -> Embedding:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._generate_embedding(text, self.model_name)<br>    def _get_query_embedding(self, query: str) -> Embedding:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self._generate_embedding(query, self.model_name)<br>    def _get_text_embeddings(self, texts: List[str]) -> List[Embedding]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        return [self._generate_embedding(text, self.model_name) for text in texts]<br>    async def _aget_text_embedding(self, text: str) -> Embedding:<br>        \"\"\"Async get text embedding.\"\"\"<br>        return await self._agenerate_embedding(text, self.model_name)<br>    async def _aget_query_embedding(self, query: str) -> Embedding:<br>        \"\"\"Async get query embedding.\"\"\"<br>        return await self._agenerate_embedding(query, self.model_name)<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[Embedding]:<br>        \"\"\"Async get text embeddings.\"\"\"<br>        return await asyncio.gather(<br>            *[self._agenerate_embedding(text, self.model_name) for text in texts]<br>        )<br></code>`` |\n"
    },
    {
      "id": "94c406b9-0bd0-4e3d-8978-6eaa0a88aeab",
      "size": 1482,
      "headers": {
        "h1": "Cloudflare workersai",
        "h2": "CloudflareEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br></code>`<code> | </code>`<code><br>class CloudflareEmbedding(BaseEmbedding):<br>    \"\"\"<br>    Cloudflare Workers AI class for generating text embeddings.<br>    This class allows for the generation of text embeddings using Cloudflare Workers AI with the BAAI general embedding models.<br>    Args:<br>    account_id (str): The Cloudflare Account ID.<br>    auth_token (str, Optional): The Cloudflare Auth Token. Alternatively, set up environment variable </code>CLOUDFLARE_AUTH_TOKEN<code>.<br>    model (str): The model ID for the embedding service. Cloudflare provides different models for embeddings, check https://developers.cloudflare.com/workers-ai/models/#text-embeddings. Defaults to \"@cf/baai/bge-base-en-v1.5\".<br>    embed_batch_size (int): The batch size for embedding generation. Cloudflare's current limit is 100 at max. Defaults to llama_index's default.<br>    Note:<br>    Ensure you have a valid Cloudflare account and have access to the necessary AI services and models. The account ID and authorization token are sensitive details; secure them appropriately.<br>    \"\"\"<br>    account_id: str = Field(default=None, description=\"The Cloudflare Account ID.\")<br>    auth_token: str = Field(default=None, description=\"The Cloudflare Auth Token.\")<br>    model: str = Field(<br>        default=\"@cf/baai/bge-base-en-v1.5\",<br>        description=\"The model to use when calling Cloudflare AI API\",<br>    )<br>    _session: Any = PrivateAttr()<br>    def __init__(<br>        self,<br>        account_id: str,<br>        auth_token: Optional[str] = None,<br>        model: str = \"@cf/baai/bge-base-en-v1.5\",<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>        **kwargs: Any,<br>    ) -> None:<br>        super().__init__(<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            model=model,<br>            **kwargs,<br>        )<br>        self.account_id = account_id<br>        self.auth_token = get_from_param_or_env(<br>            \"auth_token\", auth_token, \"CLOUDFLARE_AUTH_TOKEN\", \"\"<br>        )<br>        self.model = model<br>        self._session = requests.Session()<br>        self._session.headers.update({\"Authorization\": f\"Bearer {self.auth_token}\"})<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"CloudflareEmbedding\"<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self._get_text_embedding(query)<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        return await self._aget_text_embedding(query)<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._get_text_embeddings([text])[0]<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously get text embedding.\"\"\"<br>        result = await self._aget_text_embeddings([text])<br>        return result[0]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        response = self._session.post(<br>            API_URL_TEMPLATE.format(self.account_id, self.model), json={\"text\": texts}<br>        ).json()<br>        if \"result\" not in response:<br>            print(response)<br>            raise RuntimeError(\"Failed to fetch embeddings\")<br>        return response[\"result\"][\"data\"]<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Asynchronously get text embeddings.\"\"\"<br>        import aiohttp<br>        async with aiohttp.ClientSession(trust_env=True) as session:<br>            headers = {<br>                \"Authorization\": f\"Bearer {self.auth_token}\",<br>                \"Accept-Encoding\": \"identity\",<br>            }<br>            async with session.post(<br>                API_URL_TEMPLATE.format(self.account_id, self.model),<br>                json={\"text\": texts},<br>                headers=headers,<br>            ) as response:<br>                resp = await response.json()<br>                if \"result\" not in resp:<br>                    raise RuntimeError(\"Failed to fetch embeddings asynchronously\")<br>                return resp[\"result\"][\"data\"]<br></code>`` |\n"
    },
    {
      "id": "bbbe37ee-dffd-447a-a5f0-3aeea6326d10",
      "size": 1440,
      "headers": {
        "h1": "Openai",
        "h2": "OpenAIAgent \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br></code>`<code> | </code>``<code><br>class OpenAIAgent(AgentRunner):<br>    \"\"\"OpenAI agent.<br>    Subclasses AgentRunner with a OpenAIAgentWorker.<br>    For the legacy implementation see:<br>    </code>`<code>python<br>    from llama_index..agent.legacy.openai.base import OpenAIAgent<br>    </code>`<code><br>    \"\"\"<br>    def __init__(<br>        self,<br>        tools: List[BaseTool],<br>        llm: OpenAI,<br>        memory: BaseMemory,<br>        prefix_messages: List[ChatMessage],<br>        verbose: bool = False,<br>        max_function_calls: int = DEFAULT_MAX_FUNCTION_CALLS,<br>        default_tool_choice: str = \"auto\",<br>        callback_manager: Optional[CallbackManager] = None,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        tool_call_parser: Optional[Callable[[OpenAIToolCall], Dict]] = None,<br>    ) -> None:<br>        \"\"\"Init params.\"\"\"<br>        callback_manager = callback_manager or llm.callback_manager<br>        step_engine = OpenAIAgentWorker.from_tools(<br>            tools=tools,<br>            tool_retriever=tool_retriever,<br>            llm=llm,<br>            verbose=verbose,<br>            max_function_calls=max_function_calls,<br>            callback_manager=callback_manager,<br>            prefix_messages=prefix_messages,<br>            tool_call_parser=tool_call_parser,<br>        )<br>        super().__init__(<br>            step_engine,<br>            memory=memory,<br>            llm=llm,<br>            callback_manager=callback_manager,<br>            default_tool_choice=default_tool_choice,<br>        )<br>    @classmethod<br>    def from_tools(<br>        cls,<br>        tools: Optional[List[BaseTool]] = None,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        llm: Optional[LLM] = None,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        memory: Optional[BaseMemory] = None,<br>        memory_cls: Type[BaseMemory] = ChatMemoryBuffer,<br>        verbose: bool = False,<br>        max_function_calls: int = DEFAULT_MAX_FUNCTION_CALLS,<br>        default_tool_choice: str = \"auto\",<br>        callback_manager: Optional[CallbackManager] = None,<br>        system_prompt: Optional[str] = None,<br>        prefix_messages: Optional[List[ChatMessage]] = None,<br>        tool_call_parser: Optional[Callable[[OpenAIToolCall], Dict]] = None,<br>        **kwargs: Any,<br>    ) -> \"OpenAIAgent\":<br>        \"\"\"Create an OpenAIAgent from a list of tools.<br>        Similar to </code>from_defaults<code> in other classes, this method will<br>        infer defaults for a variety of parameters, including the LLM,<br>        if they are not specified.<br>        \"\"\"<br>        tools = tools or []<br>        chat_history = chat_history or []<br>        llm = llm or Settings.llm<br>        if not isinstance(llm, OpenAI):<br>            raise ValueError(\"llm must be a OpenAI instance\")<br>        if callback_manager is not None:<br>            llm.callback_manager = callback_manager<br>        memory = memory or memory_cls.from_defaults(chat_history, llm=llm)<br>        if not llm.metadata.is_function_calling_model:<br>            raise ValueError(<br>                f\"Model name {llm.model} does not support function calling API. \"<br>            )<br>        if system_prompt is not None:<br>            if prefix_messages is not None:<br>                raise ValueError(<br>                    \"Cannot specify both system_prompt and prefix_messages\"<br>                )<br>            prefix_messages = [ChatMessage(content=system_prompt, role=\"system\")]<br>        prefix_messages = prefix_messages or []<br>        return cls(<br>            tools=tools,<br>            tool_retriever=tool_retriever,<br>            llm=llm,<br>            memory=memory,<br>            prefix_messages=prefix_messages,<br>            verbose=verbose,<br>            max_function_calls=max_function_calls,<br>            callback_manager=callback_manager,<br>            default_tool_choice=default_tool_choice,<br>            tool_call_parser=tool_call_parser,<br>        )<br></code>``` |\n"
    },
    {
      "id": "d63af0a1-fa86-4a13-a5f3-ffeaeb8c419e",
      "size": 5139,
      "headers": {
        "h1": "Openai",
        "h2": "OpenAIAssistantAgent \\#",
        "h3": ""
      },
      "text": "| ``<code><br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>472<br>473<br>474<br>475<br>476<br>477<br>478<br>479<br>480<br>481<br>482<br>483<br>484<br>485<br>486<br>487<br>488<br>489<br>490<br>491<br>492<br>493<br>494<br>495<br>496<br>497<br>498<br>499<br>500<br>501<br>502<br>503<br>504<br>505<br>506<br>507<br>508<br>509<br>510<br>511<br>512<br>513<br>514<br>515<br>516<br>517<br>518<br>519<br>520<br>521<br>522<br>523<br>524<br>525<br>526<br>527<br>528<br>529<br>530<br>531<br>532<br>533<br>534<br>535<br>536<br>537<br>538<br>539<br>540<br>541<br>542<br>543<br>544<br>545<br>546<br>547<br>548<br>549<br>550<br>551<br>552<br>553<br>554<br>555<br>556<br>557<br>558<br>559<br>560<br>561<br>562<br>563<br>564<br>565<br>566<br>567<br>568<br>569<br>570<br>571<br>572<br>573<br>574<br>575<br>576<br>577<br>578<br>579<br>580<br>581<br>582<br>583<br>584<br>585<br>586<br>587<br>588<br>589<br>590<br>591<br>592<br>593<br>594<br></code>`<code> | </code>`<code><br>class OpenAIAssistantAgent(BaseAgent):<br>    \"\"\"OpenAIAssistant agent.<br>    Wrapper around OpenAI assistant API: https://platform.openai.com/docs/assistants/overview<br>    \"\"\"<br>    def __init__(<br>        self,<br>        client: Any,<br>        assistant: Any,<br>        tools: Optional[List[BaseTool]],<br>        callback_manager: Optional[CallbackManager] = None,<br>        thread_id: Optional[str] = None,<br>        instructions_prefix: Optional[str] = None,<br>        run_retrieve_sleep_time: float = 0.1,<br>        file_dict: Dict[str, str] = {},<br>        verbose: bool = False,<br>    ) -> None:<br>        \"\"\"Init params.\"\"\"<br>        from openai import OpenAI<br>        from openai.types.beta.assistant import Assistant<br>        self._client = cast(OpenAI, client)<br>        self._assistant = cast(Assistant, assistant)<br>        self._tools = tools or []<br>        if thread_id is None:<br>            thread = self._client.beta.threads.create()<br>            thread_id = thread.id<br>        self._thread_id = thread_id<br>        self._instructions_prefix = instructions_prefix<br>        self._run_retrieve_sleep_time = run_retrieve_sleep_time<br>        self._verbose = verbose<br>        self.file_dict = file_dict<br>        self.callback_manager = callback_manager or CallbackManager([])<br>    @classmethod<br>    def from_new(<br>        cls,<br>        name: str,<br>        instructions: str,<br>        tools: Optional[List[BaseTool]] = None,<br>        openai_tools: Optional[List[Dict]] = None,<br>        thread_id: Optional[str] = None,<br>        model: str = \"gpt-4-1106-preview\",<br>        instructions_prefix: Optional[str] = None,<br>        run_retrieve_sleep_time: float = 0.1,<br>        files: Optional[List[str]] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        file_ids: Optional[List[str]] = None,<br>        api_key: Optional[str] = None,<br>    ) -> \"OpenAIAssistantAgent\":<br>        \"\"\"From new assistant.<br>        Args:<br>            name: name of assistant<br>            instructions: instructions for assistant<br>            tools: list of tools<br>            openai_tools: list of openai tools<br>            thread_id: thread id<br>            model: model<br>            run_retrieve_sleep_time: run retrieve sleep time<br>            files: files<br>            instructions_prefix: instructions prefix<br>            callback_manager: callback manager<br>            verbose: verbose<br>            file_ids: list of file ids<br>            api_key: OpenAI API key<br>        \"\"\"<br>        from openai import OpenAI<br>        # this is the set of openai tools<br>        # not to be confused with the tools we pass in for function calling<br>        openai_tools = openai_tools or []<br>        tools = tools or []<br>        tool_fns = [t.metadata.to_openai_tool() for t in tools]<br>        all_openai_tools = openai_tools + tool_fns<br>        # initialize client<br>        client = OpenAI(api_key=api_key)<br>        # process files<br>        files = files or []<br>        file_ids = file_ids or []<br>        file_dict = _process_files(client, files)<br>        # TODO: openai's typing is a bit sus<br>        all_openai_tools = cast(List[Any], all_openai_tools)<br>        assistant = client.beta.assistants.create(<br>            name=name,<br>            instructions=instructions,<br>            tools=cast(List[Any], all_openai_tools),<br>            model=model,<br>        )<br>        return cls(<br>            client,<br>            assistant,<br>            tools,<br>            callback_manager=callback_manager,<br>            thread_id=thread_id,<br>            instructions_prefix=instructions_prefix,<br>            file_dict=file_dict,<br>            run_retrieve_sleep_time=run_retrieve_sleep_time,<br>            verbose=verbose,<br>        )<br>    @classmethod<br>    def from_existing(<br>        cls,<br>        assistant_id: str,<br>        tools: Optional[List[BaseTool]] = None,<br>        thread_id: Optional[str] = None,<br>        instructions_prefix: Optional[str] = None,<br>        run_retrieve_sleep_time: float = 0.1,<br>        callback_manager: Optional[CallbackManager] = None,<br>        api_key: Optional[str] = None,<br>        verbose: bool = False,<br>    ) -> \"OpenAIAssistantAgent\":<br>        \"\"\"From existing assistant id.<br>        Args:<br>            assistant_id: id of assistant<br>            tools: list of BaseTools Assistant can use<br>            thread_id: thread id<br>            run_retrieve_sleep_time: run retrieve sleep time<br>            instructions_prefix: instructions prefix<br>            callback_manager: callback manager<br>            api_key: OpenAI API key<br>            verbose: verbose<br>        \"\"\"<br>        from openai import OpenAI<br>        # initialize client<br>        client = OpenAI(api_key=api_key)<br>        # get assistant<br>        assistant = client.beta.assistants.retrieve(assistant_id)<br>        # assistant.tools is incompatible with BaseTools so have to pass from params<br>        return cls(<br>            client,<br>            assistant,<br>            tools=tools,<br>            callback_manager=callback_manager,<br>            thread_id=thread_id,<br>            instructions_prefix=instructions_prefix,<br>            run_retrieve_sleep_time=run_retrieve_sleep_time,<br>            verbose=verbose,<br>        )<br>    @property<br>    def assistant(self) -> Any:<br>        \"\"\"Get assistant.\"\"\"<br>        return self._assistant<br>    @property<br>    def client(self) -> Any:<br>        \"\"\"Get client.\"\"\"<br>        return self._client<br>    @property<br>    def thread_id(self) -> str:<br>        \"\"\"Get thread id.\"\"\"<br>        return self._thread_id<br>    @property<br>    def files_dict(self) -> Dict[str, str]:<br>        \"\"\"Get files dict.\"\"\"<br>        return self.file_dict<br>    @property<br>    def chat_history(self) -> List[ChatMessage]:<br>        raw_messages = self._client.beta.threads.messages.list(<br>            thread_id=self._thread_id, order=\"asc\"<br>        )<br>        return from_openai_thread_messages(list(raw_messages))<br>    def reset(self) -> None:<br>        \"\"\"Delete and create a new thread.\"\"\"<br>        self._client.beta.threads.delete(self._thread_id)<br>        thread = self._client.beta.threads.create()<br>        thread_id = thread.id<br>        self._thread_id = thread_id<br>    def get_tools(self, message: str) -> List[BaseTool]:<br>        \"\"\"Get tools.\"\"\"<br>        return self._tools<br>    def upload_files(self, files: List[str]) -> Dict[str, Any]:<br>        \"\"\"Upload files.\"\"\"<br>        return _process_files(self._client, files)<br>    def add_message(<br>        self,<br>        message: str,<br>        file_ids: Optional[List[str]] = None,<br>        tools: Optional[List[Dict[str, Any]]] = None,<br>    ) -> Any:<br>        \"\"\"Add message to assistant.\"\"\"<br>        attachments = format_attachments(file_ids=file_ids, tools=tools)<br>        return self._client.beta.threads.messages.create(<br>            thread_id=self._thread_id,<br>            role=\"user\",<br>            content=message,<br>            attachments=attachments,<br>        )<br>    def _run_function_calling(self, run: Any) -> List[ToolOutput]:<br>        \"\"\"Run function calling.\"\"\"<br>        tool_calls = run.required_action.submit_tool_outputs.tool_calls<br>        tool_output_dicts = []<br>        tool_output_objs: List[ToolOutput] = []<br>        for tool_call in tool_calls:<br>            fn_obj = tool_call.function<br>            _, tool_output = call_function(self._tools, fn_obj, verbose=self._verbose)<br>            tool_output_dicts.append(<br>                {\"tool_call_id\": tool_call.id, \"output\": str(tool_output)}<br>            )<br>            tool_output_objs.append(tool_output)<br>        # submit tool outputs<br>        # TODO: openai's typing is a bit sus<br>        self._client.beta.threads.runs.submit_tool_outputs(<br>            thread_id=self._thread_id,<br>            run_id=run.id,<br>            tool_outputs=cast(List[Any], tool_output_dicts),<br>        )<br>        return tool_output_objs<br>    async def _arun_function_calling(self, run: Any) -> List[ToolOutput]:<br>        \"\"\"Run function calling.\"\"\"<br>        tool_calls = run.required_action.submit_tool_outputs.tool_calls<br>        tool_output_dicts = []<br>        tool_output_objs: List[ToolOutput] = []<br>        for tool_call in tool_calls:<br>            fn_obj = tool_call.function<br>            _, tool_output = await acall_function(<br>                self._tools, fn_obj, verbose=self._verbose<br>            )<br>            tool_output_dicts.append(<br>                {\"tool_call_id\": tool_call.id, \"output\": str(tool_output)}<br>            )<br>            tool_output_objs.append(tool_output)<br>        # submit tool outputs<br>        self._client.beta.threads.runs.submit_tool_outputs(<br>            thread_id=self._thread_id,<br>            run_id=run.id,<br>            tool_outputs=cast(List[Any], tool_output_dicts),<br>        )<br>        return tool_output_objs<br>    def run_assistant(<br>        self, instructions_prefix: Optional[str] = None<br>    ) -> Tuple[Any, Dict]:<br>        \"\"\"Run assistant.\"\"\"<br>        instructions_prefix = instructions_prefix or self._instructions_prefix<br>        run = self._client.beta.threads.runs.create(<br>            thread_id=self._thread_id,<br>            assistant_id=self._assistant.id,<br>            instructions=instructions_prefix,<br>        )<br>        from openai.types.beta.threads import Run<br>        run = cast(Run, run)<br>        sources = []<br>        while run.status in [\"queued\", \"in_progress\", \"requires_action\"]:<br>            run = self._client.beta.threads.runs.retrieve(<br>                thread_id=self._thread_id, run_id=run.id<br>            )<br>            if run.status == \"requires_action\":<br>                cur_tool_outputs = self._run_function_calling(run)<br>                sources.extend(cur_tool_outputs)<br>            time.sleep(self._run_retrieve_sleep_time)<br>        if run.status == \"failed\":<br>            raise ValueError(<br>                f\"Run failed with status {run.status}.\\n\" f\"Error: {run.last_error}\"<br>            )<br>        return run, {\"sources\": sources}<br>    async def arun_assistant(<br>        self, instructions_prefix: Optional[str] = None<br>    ) -> Tuple[Any, Dict]:<br>        \"\"\"Run assistant.\"\"\"<br>        instructions_prefix = instructions_prefix or self._instructions_prefix<br>        run = self._client.beta.threads.runs.create(<br>            thread_id=self._thread_id,<br>            assistant_id=self._assistant.id,<br>            instructions=instructions_prefix,<br>        )<br>        from openai.types.beta.threads import Run<br>        run = cast(Run, run)<br>        sources = []<br>        while run.status in [\"queued\", \"in_progress\", \"requires_action\"]:<br>            run = self._client.beta.threads.runs.retrieve(<br>                thread_id=self._thread_id, run_id=run.id<br>            )<br>            if run.status == \"requires_action\":<br>                cur_tool_outputs = await self._arun_function_calling(run)<br>                sources.extend(cur_tool_outputs)<br>            await asyncio.sleep(self._run_retrieve_sleep_time)<br>        if run.status == \"failed\":<br>            raise ValueError(<br>                f\"Run failed with status {run.status}.\\n\" f\"Error: {run.last_error}\"<br>            )<br>        return run, {\"sources\": sources}<br>    @property<br>    def latest_message(self) -> ChatMessage:<br>        \"\"\"Get latest message.\"\"\"<br>        raw_messages = self._client.beta.threads.messages.list(<br>            thread_id=self._thread_id, order=\"desc\"<br>        )<br>        messages = from_openai_thread_messages(list(raw_messages))<br>        return messages[0]<br>    def _chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        function_call: Union[str, dict] = \"auto\",<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>    ) -> AGENT_CHAT_RESPONSE_TYPE:<br>        \"\"\"Main chat interface.\"\"\"<br>        # TODO: since chat interface doesn't expose additional kwargs<br>        # we can't pass in file_ids per message<br>        _added_message_obj = self.add_message(message)<br>        _run, metadata = self.run_assistant(<br>            instructions_prefix=self._instructions_prefix,<br>        )<br>        latest_message = self.latest_message<br>        # get most recent message content<br>        return AgentChatResponse(<br>            response=str(latest_message.content),<br>            sources=metadata[\"sources\"],<br>        )<br>    async def _achat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        function_call: Union[str, dict] = \"auto\",<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>    ) -> AGENT_CHAT_RESPONSE_TYPE:<br>        \"\"\"Asynchronous main chat interface.\"\"\"<br>        self.add_message(message)<br>        run, metadata = await self.arun_assistant(<br>            instructions_prefix=self._instructions_prefix,<br>        )<br>        latest_message = self.latest_message<br>        # get most recent message content<br>        return AgentChatResponse(<br>            response=str(latest_message.content),<br>            sources=metadata[\"sources\"],<br>        )<br>    @trace_method(\"chat\")<br>    def chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        function_call: Union[str, dict] = \"auto\",<br>    ) -> AgentChatResponse:<br>        with self.callback_manager.event(<br>            CBEventType.AGENT_STEP,<br>            payload={EventPayload.MESSAGES: [message]},<br>        ) as e:<br>            chat_response = self._chat(<br>                message, chat_history, function_call, mode=ChatResponseMode.WAIT<br>            )<br>            assert isinstance(chat_response, AgentChatResponse)<br>            e.on_end(payload={EventPayload.RESPONSE: chat_response})<br>        return chat_response<br>    @trace_method(\"chat\")<br>    async def achat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        function_call: Union[str, dict] = \"auto\",<br>    ) -> AgentChatResponse:<br>        with self.callback_manager.event(<br>            CBEventType.AGENT_STEP,<br>            payload={EventPayload.MESSAGES: [message]},<br>        ) as e:<br>            chat_response = await self._achat(<br>                message, chat_history, function_call, mode=ChatResponseMode.WAIT<br>            )<br>            assert isinstance(chat_response, AgentChatResponse)<br>            e.on_end(payload={EventPayload.RESPONSE: chat_response})<br>        return chat_response<br>    @trace_method(\"chat\")<br>    def stream_chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        function_call: Union[str, dict] = \"auto\",<br>    ) -> StreamingAgentChatResponse:<br>        raise NotImplementedError(\"stream_chat not implemented\")<br>    @trace_method(\"chat\")<br>    async def astream_chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        function_call: Union[str, dict] = \"auto\",<br>    ) -> StreamingAgentChatResponse:<br>        raise NotImplementedError(\"astream_chat not implemented\")<br></code>`` |\n"
    },
    {
      "id": "3afa6dff-5898-4382-82e3-94ae3d145a3d",
      "size": 4046,
      "headers": {
        "h1": "Condense plus context",
        "h2": "CondensePlusContextChatEngine \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br></code>`<code> | </code>`<code><br>class CondensePlusContextChatEngine(BaseChatEngine):<br>    \"\"\"<br>    Condensed Conversation & Context Chat Engine.<br>    First condense a conversation and latest user message to a standalone question<br>    Then build a context for the standalone question from a retriever,<br>    Then pass the context along with prompt and user message to LLM to generate a response.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        retriever: BaseRetriever,<br>        llm: LLM,<br>        memory: BaseMemory,<br>        context_prompt: Optional[str] = None,<br>        condense_prompt: Optional[str] = None,<br>        system_prompt: Optional[str] = None,<br>        skip_condense: bool = False,<br>        node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>    ):<br>        self._retriever = retriever<br>        self._llm = llm<br>        self._memory = memory<br>        self._context_prompt_template = (<br>            context_prompt or DEFAULT_CONTEXT_PROMPT_TEMPLATE<br>        )<br>        condense_prompt_str = condense_prompt or DEFAULT_CONDENSE_PROMPT_TEMPLATE<br>        self._condense_prompt_template = PromptTemplate(condense_prompt_str)<br>        self._system_prompt = system_prompt<br>        self._skip_condense = skip_condense<br>        self._node_postprocessors = node_postprocessors or []<br>        self.callback_manager = callback_manager or CallbackManager([])<br>        for node_postprocessor in self._node_postprocessors:<br>            node_postprocessor.callback_manager = self.callback_manager<br>        self._token_counter = TokenCounter()<br>        self._verbose = verbose<br>    @classmethod<br>    def from_defaults(<br>        cls,<br>        retriever: BaseRetriever,<br>        llm: Optional[LLM] = None,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        memory: Optional[BaseMemory] = None,<br>        system_prompt: Optional[str] = None,<br>        context_prompt: Optional[str] = None,<br>        condense_prompt: Optional[str] = None,<br>        skip_condense: bool = False,<br>        node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,<br>        verbose: bool = False,<br>        **kwargs: Any,<br>    ) -> \"CondensePlusContextChatEngine\":<br>        \"\"\"Initialize a CondensePlusContextChatEngine from default parameters.\"\"\"<br>        llm = llm or Settings.llm<br>        chat_history = chat_history or []<br>        memory = memory or ChatMemoryBuffer.from_defaults(<br>            chat_history=chat_history, token_limit=llm.metadata.context_window - 256<br>        )<br>        return cls(<br>            retriever=retriever,<br>            llm=llm,<br>            memory=memory,<br>            context_prompt=context_prompt,<br>            condense_prompt=condense_prompt,<br>            skip_condense=skip_condense,<br>            callback_manager=Settings.callback_manager,<br>            node_postprocessors=node_postprocessors,<br>            system_prompt=system_prompt,<br>            verbose=verbose,<br>        )<br>    def _condense_question(<br>        self, chat_history: List[ChatMessage], latest_message: str<br>    ) -> str:<br>        \"\"\"Condense a conversation history and latest user message to a standalone question.\"\"\"<br>        if self._skip_condense or len(chat_history) == 0:<br>            return latest_message<br>        chat_history_str = messages_to_history_str(chat_history)<br>        logger.debug(chat_history_str)<br>        return self._llm.predict(<br>            self._condense_prompt_template,<br>            question=latest_message,<br>            chat_history=chat_history_str,<br>        )<br>    async def _acondense_question(<br>        self, chat_history: List[ChatMessage], latest_message: str<br>    ) -> str:<br>        \"\"\"Condense a conversation history and latest user message to a standalone question.\"\"\"<br>        if self._skip_condense or len(chat_history) == 0:<br>            return latest_message<br>        chat_history_str = messages_to_history_str(chat_history)<br>        logger.debug(chat_history_str)<br>        return await self._llm.apredict(<br>            self._condense_prompt_template,<br>            question=latest_message,<br>            chat_history=chat_history_str,<br>        )<br>    def _retrieve_context(self, message: str) -> Tuple[str, List[NodeWithScore]]:<br>        \"\"\"Build context for a message from retriever.\"\"\"<br>        nodes = self._retriever.retrieve(message)<br>        for postprocessor in self._node_postprocessors:<br>            nodes = postprocessor.postprocess_nodes(<br>                nodes, query_bundle=QueryBundle(message)<br>            )<br>        context_str = \"\\n\\n\".join(<br>            [n.node.get_content(metadata_mode=MetadataMode.LLM).strip() for n in nodes]<br>        )<br>        return context_str, nodes<br>    async def _aretrieve_context(self, message: str) -> Tuple[str, List[NodeWithScore]]:<br>        \"\"\"Build context for a message from retriever.\"\"\"<br>        nodes = await self._retriever.aretrieve(message)<br>        for postprocessor in self._node_postprocessors:<br>            nodes = postprocessor.postprocess_nodes(<br>                nodes, query_bundle=QueryBundle(message)<br>            )<br>        context_str = \"\\n\\n\".join(<br>            [n.node.get_content(metadata_mode=MetadataMode.LLM).strip() for n in nodes]<br>        )<br>        return context_str, nodes<br>    def _run_c3(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> Tuple[List[ChatMessage], ToolOutput, List[NodeWithScore]]:<br>        if chat_history is not None:<br>            self._memory.set(chat_history)<br>        chat_history = self._memory.get(input=message)<br>        # Condense conversation history and latest message to a standalone question<br>        condensed_question = self._condense_question(chat_history, message)  # type: ignore<br>        logger.info(f\"Condensed question: {condensed_question}\")<br>        if self._verbose:<br>            print(f\"Condensed question: {condensed_question}\")<br>        # Build context for the standalone question from a retriever<br>        context_str, context_nodes = self._retrieve_context(condensed_question)<br>        context_source = ToolOutput(<br>            tool_name=\"retriever\",<br>            content=context_str,<br>            raw_input={\"message\": condensed_question},<br>            raw_output=context_str,<br>        )<br>        logger.debug(f\"Context: {context_str}\")<br>        if self._verbose:<br>            print(f\"Context: {context_str}\")<br>        system_message_content = self._context_prompt_template.format(<br>            context_str=context_str<br>        )<br>        if self._system_prompt:<br>            system_message_content = self._system_prompt + \"\\n\" + system_message_content<br>        system_message = ChatMessage(<br>            content=system_message_content, role=self._llm.metadata.system_role<br>        )<br>        initial_token_count = self._token_counter.estimate_tokens_in_messages(<br>            [system_message]<br>        )<br>        self._memory.put(ChatMessage(content=message, role=MessageRole.USER))<br>        chat_messages = [<br>            system_message,<br>            *self._memory.get(initial_token_count=initial_token_count),<br>        ]<br>        return chat_messages, context_source, context_nodes<br>    async def _arun_c3(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> Tuple[List[ChatMessage], ToolOutput, List[NodeWithScore]]:<br>        if chat_history is not None:<br>            self._memory.set(chat_history)<br>        chat_history = self._memory.get(input=message)<br>        # Condense conversation history and latest message to a standalone question<br>        condensed_question = await self._acondense_question(chat_history, message)  # type: ignore<br>        logger.info(f\"Condensed question: {condensed_question}\")<br>        if self._verbose:<br>            print(f\"Condensed question: {condensed_question}\")<br>        # Build context for the standalone question from a retriever<br>        context_str, context_nodes = await self._aretrieve_context(condensed_question)<br>        context_source = ToolOutput(<br>            tool_name=\"retriever\",<br>            content=context_str,<br>            raw_input={\"message\": condensed_question},<br>            raw_output=context_str,<br>        )<br>        logger.debug(f\"Context: {context_str}\")<br>        if self._verbose:<br>            print(f\"Context: {context_str}\")<br>        system_message_content = self._context_prompt_template.format(<br>            context_str=context_str<br>        )<br>        if self._system_prompt:<br>            system_message_content = self._system_prompt + \"\\n\" + system_message_content<br>        system_message = ChatMessage(<br>            content=system_message_content, role=self._llm.metadata.system_role<br>        )<br>        initial_token_count = self._token_counter.estimate_tokens_in_messages(<br>            [system_message]<br>        )<br>        self._memory.put(ChatMessage(content=message, role=MessageRole.USER))<br>        chat_messages = [<br>            system_message,<br>            *self._memory.get(initial_token_count=initial_token_count),<br>        ]<br>        return chat_messages, context_source, context_nodes<br>    @trace_method(\"chat\")<br>    def chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> AgentChatResponse:<br>        chat_messages, context_source, context_nodes = self._run_c3(<br>            message, chat_history<br>        )<br>        # pass the context, system prompt and user message as chat to LLM to generate a response<br>        chat_response = self._llm.chat(chat_messages)<br>        assistant_message = chat_response.message<br>        self._memory.put(assistant_message)<br>        return AgentChatResponse(<br>            response=str(assistant_message.content),<br>            sources=[context_source],<br>            source_nodes=context_nodes,<br>        )<br>    @trace_method(\"chat\")<br>    def stream_chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> StreamingAgentChatResponse:<br>        chat_messages, context_source, context_nodes = self._run_c3(<br>            message, chat_history<br>        )<br>        # pass the context, system prompt and user message as chat to LLM to generate a response<br>        chat_response = StreamingAgentChatResponse(<br>            chat_stream=self._llm.stream_chat(chat_messages),<br>            sources=[context_source],<br>            source_nodes=context_nodes,<br>        )<br>        thread = Thread(<br>            target=chat_response.write_response_to_history, args=(self._memory,)<br>        )<br>        thread.start()<br>        return chat_response<br>    @trace_method(\"chat\")<br>    async def achat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> AgentChatResponse:<br>        chat_messages, context_source, context_nodes = await self._arun_c3(<br>            message, chat_history<br>        )<br>        # pass the context, system prompt and user message as chat to LLM to generate a response<br>        chat_response = await self._llm.achat(chat_messages)<br>        assistant_message = chat_response.message<br>        self._memory.put(assistant_message)<br>        return AgentChatResponse(<br>            response=str(assistant_message.content),<br>            sources=[context_source],<br>            source_nodes=context_nodes,<br>        )<br>    @trace_method(\"chat\")<br>    async def astream_chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> StreamingAgentChatResponse:<br>        chat_messages, context_source, context_nodes = await self._arun_c3(<br>            message, chat_history<br>        )<br>        # pass the context, system prompt and user message as chat to LLM to generate a response<br>        chat_response = StreamingAgentChatResponse(<br>            achat_stream=await self._llm.astream_chat(chat_messages),<br>            sources=[context_source],<br>            source_nodes=context_nodes,<br>        )<br>        asyncio.create_task(chat_response.awrite_response_to_history(self._memory))<br>        return chat_response<br>    def reset(self) -> None:<br>        # Clear chat history<br>        self._memory.reset()<br>    @property<br>    def chat_history(self) -> List[ChatMessage]:<br>        \"\"\"Get chat history.\"\"\"<br>        return self._memory.get_all()<br></code>`` |\n"
    },
    {
      "id": "0dd5f3e8-d3b6-47be-99c1-3b812eaef0ae",
      "size": 3057,
      "headers": {
        "h1": "Oci genai",
        "h2": "OCIGenAIEmbeddings \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br></code>`<code> | </code>`<code><br>class OCIGenAIEmbeddings(BaseEmbedding):<br>    \"\"\"OCI embedding models.<br>    To authenticate, the OCI client uses the methods described in<br>    https://docs.oracle.com/en-us/iaas/Content/API/Concepts/sdk_authentication_methods.htm<br>    The authentifcation method is passed through auth_type and should be one of:<br>    API_KEY (default), SECURITY_TOKEN, INSTANCE_PRINCIPAL, RESOURCE_PRINCIPAL<br>    Make sure you have the required policies (profile/roles) to<br>    access the OCI Generative AI service. If a specific config profile is used,<br>    you must pass the name of the profile (~/.oci/config) through auth_profile.<br>    To use, you must provide the compartment id<br>    along with the endpoint url, and model id<br>    as named parameters to the constructor.<br>    Example:<br>        .. code-block:: python<br>            from llama_index.embeddings.oci_genai import OCIGenAIEmbeddings<br>            embeddings = OCIGenAIEmbeddings(<br>                model_name=\"MY_EMBEDDING_MODEL\",<br>                service_endpoint=\"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\",<br>                compartment_id=\"MY_OCID\"<br>            )<br>    \"\"\"<br>    model_name: str = Field(<br>        description=\"ID or Name of the OCI Generative AI embedding model to use.\"<br>    )<br>    truncate: str = Field(<br>        description=\"Truncate embeddings that are too long from start or end, values START/ END/ NONE\",<br>        default=\"END\",<br>    )<br>    input_type: Optional[str] = Field(<br>        description=\"Model Input type. If not provided, search_document and search_query are used when needed.\",<br>        default=None,<br>    )<br>    service_endpoint: Optional[str] = Field(<br>        description=\"service endpoint url.\",<br>        default=None,<br>    )<br>    compartment_id: Optional[str] = Field(<br>        description=\"OCID of compartment.\",<br>        default=None,<br>    )<br>    auth_type: Optional[str] = Field(<br>        description=\"Authentication type, can be: API_KEY, SECURITY_TOKEN, INSTANCE_PRINCIPAL, RESOURCE_PRINCIPAL. If not specified, API_KEY will be used\",<br>        default=\"API_KEY\",<br>    )<br>    auth_profile: Optional[str] = Field(<br>        description=\"The name of the profile in ~/.oci/config. If not specified , DEFAULT will be used\",<br>        default=\"DEFAULT\",<br>    )<br>    _client: Any = PrivateAttr()<br>    def __init__(<br>        self,<br>        model_name: str,<br>        truncate: str = \"END\",<br>        input_type: Optional[str] = None,<br>        service_endpoint: Optional[str] = None,<br>        compartment_id: Optional[str] = None,<br>        auth_type: Optional[str] = \"API_KEY\",<br>        auth_profile: Optional[str] = \"DEFAULT\",<br>        client: Optional[Any] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>    ):<br>        \"\"\"<br>        Initializes the OCIGenAIEmbeddings class.<br>        Args:<br>            model_name (str): The name or ID of the model to be used for generating embeddings, e.g., \"cohere.embed-english-light-v3.0\".<br>            truncate (str): A string indicating the truncation strategy for long input text. Possible values<br>                            are 'START', 'END', or 'NONE'.<br>            input_type (Optional[str]): An optional string that specifies the type of input provided to the model.<br>                                        This is model-dependent and could be one of the following: \"search_query\",<br>                                        \"search_document\", \"classification\", or \"clustering\".<br>            service_endpoint (str): service endpoint url, e.g., \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\"<br>            compartment_id (str): OCID of the compartment.<br>            auth_type (Optional[str]): Authentication type, can be: API_KEY (default), SECURITY_TOKEN, INSTANCEAL, RESOURCE_PRINCIPAL.<br>                                    If not specified, API_KEY will be used<br>            auth_profile (Optional[str]): The name of the profile in ~/.oci/config. If not specified , DEFAULT will be used<br>            client (Optional[Any]): An optional OCI client object. If not provided, the client will be created using the<br>                                    provided service endpoint and authentifcation method.<br>        \"\"\"<br>        super().__init__(<br>            model_name=model_name,<br>            truncate=truncate,<br>            input_type=input_type,<br>            service_endpoint=service_endpoint,<br>            compartment_id=compartment_id,<br>            auth_type=auth_type,<br>            auth_profile=auth_profile,<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>        )<br>        if client is not None:<br>            self._client = client<br>        else:<br>            try:<br>                import oci<br>                client_kwargs = {<br>                    \"config\": {},<br>                    \"signer\": None,<br>                    \"service_endpoint\": service_endpoint,<br>                    \"retry_strategy\": oci.retry.DEFAULT_RETRY_STRATEGY,<br>                    \"timeout\": (<br>                        10,<br>                        240,<br>                    ),  # default timeout config for OCI Gen AI service<br>                }<br>                if auth_type == OCIAuthType(1).name:<br>                    client_kwargs[\"config\"] = oci.config.from_file(<br>                        profile_name=auth_profile<br>                    )<br>                    client_kwargs.pop(\"signer\", None)<br>                elif auth_type == OCIAuthType(2).name:<br>                    def make_security_token_signer(oci_config):  # type: ignore[no-untyped-def]<br>                        pk = oci.signer.load_private_key_from_file(<br>                            oci_config.get(\"key_file\"), None<br>                        )<br>                        with open(<br>                            oci_config.get(\"security_token_file\"), encoding=\"utf-8\"<br>                        ) as f:<br>                            st_string = f.read()<br>                        return oci.auth.signers.SecurityTokenSigner(st_string, pk)<br>                    client_kwargs[\"config\"] = oci.config.from_file(<br>                        profile_name=auth_profile<br>                    )<br>                    client_kwargs[\"signer\"] = make_security_token_signer(<br>                        oci_config=client_kwargs[\"config\"]<br>                    )<br>                elif auth_type == OCIAuthType(3).name:<br>                    client_kwargs[<br>                        \"signer\"<br>                    ] = oci.auth.signers.InstancePrincipalsSecurityTokenSigner()<br>                elif auth_type == OCIAuthType(4).name:<br>                    client_kwargs[<br>                        \"signer\"<br>                    ] = oci.auth.signers.get_resource_principals_signer()<br>                else:<br>                    raise ValueError(<br>                        f\"Please provide valid value to auth_type, {auth_type} is not valid.\"<br>                    )<br>                self._client = oci.generative_ai_inference.GenerativeAiInferenceClient(<br>                    **client_kwargs<br>                )<br>            except ImportError as ex:<br>                raise ModuleNotFoundError(<br>                    \"Could not import oci python package. \"<br>                    \"Please make sure you have the oci package installed.\"<br>                ) from ex<br>            except Exception as e:<br>                raise ValueError(<br>                    \"\"\"Could not authenticate with OCI client. Please check if ~/.oci/config exists.<br>                    If INSTANCE_PRINCIPAL or RESOURCE_PRINCIPAL is used, please check the specified<br>                    auth_profile and auth_type are valid.\"\"\",<br>                    e,<br>                ) from e<br>    @classmethod<br>    def class_name(self) -> str:<br>        return \"OCIGenAIEmbeddings\"<br>    @staticmethod<br>    def list_supported_models() -> List[str]:<br>        return list(SUPPORTED_MODELS)<br>    def _embed(self, texts: List[str], input_type: str) -> List[List[float]]:<br>        try:<br>            from oci.generative_ai_inference import models<br>        except ImportError as ex:<br>            raise ModuleNotFoundError(<br>                \"Could not import oci python package. \"<br>                \"Please make sure you have the oci package installed.\"<br>            ) from ex<br>        if self.model_name.startswith(CUSTOM_ENDPOINT_PREFIX):<br>            serving_mode = models.DedicatedServingMode(endpoint_id=self.model_name)<br>        else:<br>            serving_mode = models.OnDemandServingMode(model_id=self.model_name)<br>        request = models.EmbedTextDetails(<br>            serving_mode=serving_mode,<br>            compartment_id=self.compartment_id,<br>            input_type=self.input_type or input_type,<br>            truncate=self.truncate,<br>            inputs=texts,<br>        )<br>        response = self._client.embed_text(request)<br>        return response.data.embeddings<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        return self._embed([query], input_type=\"SEARCH_QUERY\")[0]<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        return self._embed([text], input_type=\"SEARCH_DOCUMENT\")[0]<br>    def _get_text_embeddings(self, text: str) -> List[List[float]]:<br>        return self._embed(text, input_type=\"SEARCH_DOCUMENT\")<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        return self._get_text_embedding(text)<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        return self._get_query_embedding(query)<br></code>`` |\n"
    },
    {
      "id": "2f17bdb9-8ecf-4e1c-8a27-67a9e098e2ff",
      "size": 2948,
      "headers": {
        "h1": "Mixedbreadai",
        "h2": "MixedbreadAIEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br></code>`<code> | </code>`<code><br>class MixedbreadAIEmbedding(BaseEmbedding):<br>    \"\"\"<br>    Class to get embeddings using the mixedbread ai embedding API with models such as 'mixedbread-ai/mxbai-embed-large-v1'.<br>    Args:<br>        api_key (Optional[str]): mixedbread ai API key. Defaults to None.<br>        model_name (str): Model for embedding. Defaults to \"mixedbread-ai/mxbai-embed-large-v1\".<br>        encoding_format (EncodingFormat): Encoding format for embeddings. Defaults to EncodingFormat.FLOAT.<br>        truncation_strategy (TruncationStrategy): Truncation strategy. Defaults to TruncationStrategy.START.<br>        normalized (bool): Whether to normalize the embeddings. Defaults to True.<br>        dimensions (Optional[int]): Number of dimensions for embeddings. Only applicable for models with matryoshka support.<br>        prompt (Optional[str]): An optional prompt to provide context to the model.<br>        embed_batch_size (Optional[int]): The batch size for embedding calls. Defaults to 128.<br>        callback_manager (Optional[CallbackManager]): Manager for handling callbacks.<br>        timeout (Optional[float]): Timeout for API calls.<br>        max_retries (Optional[int]): Maximum number of retries for API calls.<br>        httpx_client (Optional[httpx.Client]): Custom HTTPX client.<br>        httpx_async_client (Optional[httpx.AsyncClient]): Custom asynchronous HTTPX client.<br>    \"\"\"<br>    api_key: str = Field(description=\"The mixedbread ai API key.\", min_length=1)<br>    model_name: str = Field(<br>        default=\"mixedbread-ai/mxbai-embed-large-v1\",<br>        description=\"Model to use for embeddings.\",<br>        min_length=1,<br>    )<br>    encoding_format: EncodingFormat = Field(<br>        default=EncodingFormat.FLOAT, description=\"Encoding format for the embeddings.\"<br>    )<br>    truncation_strategy: TruncationStrategy = Field(<br>        default=TruncationStrategy.START,<br>        description=\"Truncation strategy for input text.\",<br>    )<br>    normalized: bool = Field(<br>        default=True, description=\"Whether to normalize the embeddings.\"<br>    )<br>    dimensions: Optional[int] = Field(<br>        default=None,<br>        description=\"Number of dimensions for embeddings. Only applicable for models with matryoshka support.\",<br>        gt=0,<br>    )<br>    prompt: Optional[str] = Field(<br>        default=None,<br>        description=\"An optional prompt to provide context to the model.\",<br>        min_length=1,<br>    )<br>    embed_batch_size: int = Field(<br>        default=128, description=\"The batch size for embedding calls.\", gt=0, lte=256<br>    )<br>    _client: MixedbreadAI = PrivateAttr()<br>    _async_client: AsyncMixedbreadAI = PrivateAttr()<br>    _request_options: Optional[RequestOptions] = PrivateAttr()<br>    def __init__(<br>        self,<br>        api_key: Optional[str] = None,<br>        model_name: str = \"mixedbread-ai/mxbai-embed-large-v1\",<br>        encoding_format: EncodingFormat = EncodingFormat.FLOAT,<br>        truncation_strategy: TruncationStrategy = TruncationStrategy.START,<br>        normalized: bool = True,<br>        dimensions: Optional[int] = None,<br>        prompt: Optional[str] = None,<br>        embed_batch_size: Optional[int] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        timeout: Optional[float] = None,<br>        max_retries: Optional[int] = None,<br>        httpx_client: Optional[httpx.Client] = None,<br>        httpx_async_client: Optional[httpx.AsyncClient] = None,<br>        **kwargs: Any,<br>    ):<br>        if embed_batch_size is None:<br>            embed_batch_size = 128  # Default batch size for mixedbread ai<br>        try:<br>            api_key = api_key or os.environ[\"MXBAI_API_KEY\"]<br>        except KeyError:<br>            raise ValueError(<br>                \"Must pass in mixedbread ai API key or \"<br>                \"specify via MXBAI_API_KEY environment variable \"<br>            )<br>        super().__init__(<br>            api_key=api_key,<br>            model_name=model_name,<br>            encoding_format=encoding_format,<br>            truncation_strategy=truncation_strategy,<br>            normalized=normalized,<br>            dimensions=dimensions,<br>            prompt=prompt,<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            **kwargs,<br>        )<br>        self._client = MixedbreadAI(<br>            api_key=api_key, timeout=timeout, httpx_client=httpx_client<br>        )<br>        self._async_client = AsyncMixedbreadAI(<br>            api_key=api_key, timeout=timeout, httpx_client=httpx_async_client<br>        )<br>        self._request_options = (<br>            RequestOptions(max_retries=max_retries) if max_retries is not None else None<br>        )<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"MixedbreadAIEmbedding\"<br>    def _get_embedding(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"<br>        Get embeddings for a list of texts using the mixedbread ai API.<br>        Args:<br>            texts (List[str]): List of texts to embed.<br>        Returns:<br>            List[List[float]]: List of embeddings.<br>        \"\"\"<br>        response = self._client.embeddings(<br>            model=self.model_name,<br>            input=texts,<br>            encoding_format=self.encoding_format,<br>            normalized=self.normalized,<br>            truncation_strategy=self.truncation_strategy,<br>            dimensions=self.dimensions,<br>            prompt=self.prompt,<br>            request_options=self._request_options,<br>        )<br>        return [item.embedding for item in response.data]<br>    async def _aget_embedding(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"<br>        Asynchronously get embeddings for a list of texts using the mixedbread ai API.<br>        Args:<br>            texts (List[str]): List of texts to embed.<br>        Returns:<br>            List[List[float]]: List of embeddings.<br>        \"\"\"<br>        response = await self._async_client.embeddings(<br>            model=self.model_name,<br>            input=texts,<br>            encoding_format=self.encoding_format,<br>            normalized=self.normalized,<br>            truncation_strategy=self.truncation_strategy,<br>            dimensions=self.dimensions,<br>            prompt=self.prompt,<br>            request_options=self._request_options,<br>        )<br>        return [item.embedding for item in response.data]<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"<br>        Get embedding for a query using the mixedbread ai API.<br>        Args:<br>            query (str): Query text.<br>        Returns:<br>            List[float]: Embedding for the query.<br>        \"\"\"<br>        return self._get_embedding([query])[0]<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"<br>        Asynchronously get embedding for a query using the mixedbread ai API.<br>        Args:<br>            query (str): Query text.<br>        Returns:<br>            List[float]: Embedding for the query.<br>        \"\"\"<br>        r = await self._aget_embedding([query])<br>        return r[0]<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"<br>        Get embedding for a text using the mixedbread ai API.<br>        Args:<br>            text (str): Text to embed.<br>        Returns:<br>            List[float]: Embedding for the text.<br>        \"\"\"<br>        return self._get_embedding([text])[0]<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"<br>        Asynchronously get embedding for a text using the mixedbread ai API.<br>        Args:<br>            text (str): Text to embed.<br>        Returns:<br>            List[float]: Embedding for the text.<br>        \"\"\"<br>        r = await self._aget_embedding([text])<br>        return r[0]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"<br>        Get embeddings for multiple texts using the mixedbread ai API.<br>        Args:<br>            texts (List[str]): List of texts to embed.<br>        Returns:<br>            List[List[float]]: List of embeddings.<br>        \"\"\"<br>        return self._get_embedding(texts)<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"<br>        Asynchronously get embeddings for multiple texts using the mixedbread ai API.<br>        Args:<br>            texts (List[str]): List of texts to embed.<br>        Returns:<br>            List[List[float]]: List of embeddings.<br>        \"\"\"<br>        return await self._aget_embedding(texts)<br></code>`` |\n"
    },
    {
      "id": "af570927-dcce-43fb-aa3c-e77e14315324",
      "size": 1618,
      "headers": {
        "h1": "Jinaai",
        "h2": "JinaEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br></code>`<code> | </code>`<code><br>class JinaEmbedding(MultiModalEmbedding):<br>    \"\"\"<br>    JinaAI class for embeddings.<br>    Args:<br>        model (str): Model for embedding.<br>            Defaults to </code>jina-embeddings-v2-base-en<code><br>    \"\"\"<br>    api_key: Optional[str] = Field(default=None, description=\"The JinaAI API key.\")<br>    model: str = Field(<br>        default=\"jina-embeddings-v2-base-en\",<br>        description=\"The model to use when calling Jina AI API\",<br>    )<br>    _encoding_queries: str = PrivateAttr()<br>    _encoding_documents: str = PrivateAttr()<br>    _api: Any = PrivateAttr()<br>    def __init__(<br>        self,<br>        model: str = \"jina-embeddings-v2-base-en\",<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        api_key: Optional[str] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        encoding_queries: Optional[str] = None,<br>        encoding_documents: Optional[str] = None,<br>        **kwargs: Any,<br>    ) -> None:<br>        super().__init__(<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            model=model,<br>            api_key=api_key,<br>            **kwargs,<br>        )<br>        self._encoding_queries = encoding_queries or \"float\"<br>        self._encoding_documents = encoding_documents or \"float\"<br>        assert (<br>            self._encoding_documents in VALID_ENCODING<br>        ), f\"Encoding Documents parameter {self._encoding_documents} not supported. Please choose one of {VALID_ENCODING}\"<br>        assert (<br>            self._encoding_queries in VALID_ENCODING<br>        ), f\"Encoding Queries parameter {self._encoding_documents} not supported. Please choose one of {VALID_ENCODING}\"<br>        self._api = _JinaAPICaller(model=model, api_key=api_key)<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"JinaAIEmbedding\"<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self._api.get_embeddings(<br>            input=[query], encoding_type=self._encoding_queries<br>        )[0]<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        result = await self._api.aget_embeddings(<br>            input=[query], encoding_type=self._encoding_queries<br>        )<br>        return result[0]<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._get_text_embeddings([text])[0]<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously get text embedding.\"\"\"<br>        result = await self._aget_text_embeddings([text])<br>        return result[0]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        return self._api.get_embeddings(<br>            input=texts, encoding_type=self._encoding_documents<br>        )<br>    async def _aget_text_embeddings(<br>        self,<br>        texts: List[str],<br>    ) -> List[List[float]]:<br>        return await self._api.aget_embeddings(<br>            input=texts, encoding_type=self._encoding_documents<br>        )<br>    def _get_image_embedding(self, img_file_path: ImageType) -> List[float]:<br>        if is_local(img_file_path):<br>            input = [{\"bytes\": get_bytes_str(img_file_path)}]<br>        else:<br>            input = [{\"url\": img_file_path}]<br>        return self._api.get_embeddings(input=input)[0]<br>    async def _aget_image_embedding(self, img_file_path: ImageType) -> List[float]:<br>        if is_local(img_file_path):<br>            input = [{\"bytes\": get_bytes_str(img_file_path)}]<br>        else:<br>            input = [{\"url\": img_file_path}]<br>        return await self._api.aget_embeddings(input=input)[0]<br>    def _get_image_embeddings(<br>        self, img_file_paths: List[ImageType]<br>    ) -> List[List[float]]:<br>        input = []<br>        for img_file_path in img_file_paths:<br>            if is_local(img_file_path):<br>                input.append({\"bytes\": get_bytes_str(img_file_path)})<br>            else:<br>                input.append({\"url\": img_file_path})<br>        return self._api.get_embeddings(input=input)<br>    async def _aget_image_embeddings(<br>        self, img_file_paths: List[ImageType]<br>    ) -> List[List[float]]:<br>        input = []<br>        for img_file_path in img_file_paths:<br>            if is_local(img_file_path):<br>                input.append({\"bytes\": get_bytes_str(img_file_path)})<br>            else:<br>                input.append({\"url\": img_file_path})<br>        return await self._api.aget_embeddings(input=input)<br></code>`` |\n"
    },
    {
      "id": "5002c547-c406-4f95-9465-e9e3f9e8336e",
      "size": 2027,
      "headers": {
        "h1": "Huggingface optimum intel",
        "h2": "IntelEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 14<br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br></code>`<code> | </code>`<code><br>class IntelEmbedding(BaseEmbedding):<br>    folder_name: str = Field(description=\"Folder name to load from.\")<br>    max_length: int = Field(description=\"Maximum length of input.\")<br>    pooling: str = Field(description=\"Pooling strategy. One of ['cls', 'mean'].\")<br>    normalize: str = Field(default=True, description=\"Normalize embeddings or not.\")<br>    query_instruction: Optional[str] = Field(<br>        description=\"Instruction to prepend to query text.\"<br>    )<br>    text_instruction: Optional[str] = Field(<br>        description=\"Instruction to prepend to text.\"<br>    )<br>    cache_folder: Optional[str] = Field(<br>        description=\"Cache folder for huggingface files.\"<br>    )<br>    _model: Any = PrivateAttr()<br>    _tokenizer: Any = PrivateAttr()<br>    _device: Any = PrivateAttr()<br>    def __init__(<br>        self,<br>        folder_name: str,<br>        pooling: str = \"cls\",<br>        max_length: Optional[int] = None,<br>        normalize: bool = True,<br>        query_instruction: Optional[str] = None,<br>        text_instruction: Optional[str] = None,<br>        model: Optional[Any] = None,<br>        tokenizer: Optional[Any] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>        device: Optional[str] = None,<br>    ):<br>        try:<br>            from optimum.intel import IPEXModel<br>        except ImportError:<br>            raise ImportError(<br>                \"Optimum-Intel requires the following dependencies; please install with \"<br>                \"</code>pip install optimum[exporters] \"<br>                \"optimum-intel neural-compressor intel_extension_for_pytorch<code>\"<br>            )<br>        model = model or IPEXModel.from_pretrained(folder_name)<br>        tokenizer = tokenizer or AutoTokenizer.from_pretrained(folder_name)<br>        device = device or infer_torch_device()<br>        if max_length is None:<br>            try:<br>                max_length = int(model.config.max_position_embeddings)<br>            except Exception:<br>                raise ValueError(<br>                    \"Unable to find max_length from model config. \"<br>                    \"Please provide max_length.\"<br>                )<br>            try:<br>                max_length = min(max_length, int(tokenizer.model_max_length))<br>            except Exception as exc:<br>                print(f\"An error occurred while retrieving tokenizer max length: {exc}\")<br>        if pooling not in [\"cls\", \"mean\"]:<br>            raise ValueError(f\"Pooling {pooling} not supported.\")<br>        super().__init__(<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            folder_name=folder_name,<br>            max_length=max_length,<br>            pooling=pooling,<br>            normalize=normalize,<br>            query_instruction=query_instruction,<br>            text_instruction=text_instruction,<br>        )<br>        self._model = model<br>        self._tokenizer = tokenizer<br>        self._device = device<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"IntelEmbedding\"<br>    def _mean_pooling(self, model_output: Any, attention_mask: Any) -> Any:<br>        \"\"\"Mean Pooling - Take attention mask into account for correct averaging.\"\"\"<br>        import torch<br>        # First element of model_output contains all token embeddings<br>        token_embeddings = model_output[0]<br>        input_mask_expanded = (<br>            attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()<br>        )<br>        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(<br>            input_mask_expanded.sum(1), min=1e-9<br>        )<br>    def _cls_pooling(self, model_output: list) -> Any:<br>        \"\"\"Use the CLS token as the pooling token.\"\"\"<br>        if isinstance(model_output, dict):<br>            token_embeddings = model_output[\"last_hidden_state\"]<br>        else:<br>            token_embeddings = model_output[0]<br>        return token_embeddings[:, 0]<br>    def _embed(self, sentences: List[str]) -> List[List[float]]:<br>        \"\"\"Embed sentences.\"\"\"<br>        encoded_input = self._tokenizer(<br>            sentences,<br>            padding=True,<br>            max_length=self.max_length,<br>            truncation=True,<br>            return_tensors=\"pt\",<br>        )<br>        import torch<br>        with torch.inference_mode(), torch.cpu.amp.autocast():<br>            model_output = self._model(**encoded_input)<br>        if self.pooling == \"cls\":<br>            embeddings = self._cls_pooling(model_output)<br>        else:<br>            embeddings = self._mean_pooling(<br>                model_output, encoded_input[\"attention_mask\"].to(self._device)<br>            )<br>        if self.normalize:<br>            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)<br>        return embeddings.tolist()<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        query = format_query(query, self.model_name, self.query_instruction)<br>        return self._embed([query])[0]<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding async.\"\"\"<br>        return self._get_query_embedding(query)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding async.\"\"\"<br>        return self._get_text_embedding(text)<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        text = format_text(text, self.model_name, self.text_instruction)<br>        return self._embed([text])[0]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        texts = [<br>            format_text(text, self.model_name, self.text_instruction) for text in texts<br>        ]<br>        return self._embed(texts)<br></code>`` |\n"
    },
    {
      "id": "6fa25acc-67c1-4e58-a222-07d94b773ecd",
      "size": 1512,
      "headers": {
        "h1": "Gemini",
        "h2": "GeminiEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br></code>`<code> | </code>`<code><br>class GeminiEmbedding(BaseEmbedding):<br>    \"\"\"Google Gemini embeddings.<br>    Args:<br>        model_name (str): Model for embedding.<br>            Defaults to \"models/embedding-001\".<br>        api_key (Optional[str]): API key to access the model. Defaults to None.<br>        api_base (Optional[str]): API base to access the model. Defaults to Official Base.<br>        transport (Optional[str]): Transport to access the model.<br>    \"\"\"<br>    _model: Any = PrivateAttr()<br>    title: Optional[str] = Field(<br>        default=\"\",<br>        description=\"Title is only applicable for retrieval_document tasks, and is used to represent a document title. For other tasks, title is invalid.\",<br>    )<br>    task_type: Optional[str] = Field(<br>        default=\"retrieval_document\",<br>        description=\"The task for embedding model.\",<br>    )<br>    api_key: Optional[str] = Field(<br>        default=None,<br>        description=\"API key to access the model. Defaults to None.\",<br>    )<br>    def __init__(<br>        self,<br>        model_name: str = \"models/embedding-001\",<br>        task_type: Optional[str] = \"retrieval_document\",<br>        api_key: Optional[str] = None,<br>        api_base: Optional[str] = None,<br>        transport: Optional[str] = None,<br>        title: Optional[str] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>        **kwargs: Any,<br>    ):<br>        # API keys are optional. The API can be authorised via OAuth (detected<br>        # environmentally) or by the GOOGLE_API_KEY environment variable.<br>        config_params: Dict[str, Any] = {<br>            \"api_key\": api_key or os.getenv(\"GOOGLE_API_KEY\"),<br>        }<br>        if api_base:<br>            config_params[\"client_options\"] = {\"api_endpoint\": api_base}<br>        if transport:<br>            config_params[\"transport\"] = transport<br>        # transport: A string, one of: [</code>rest<code>, </code>grpc<code>, </code>grpc_asyncio<code>].<br>        super().__init__(<br>            api_key=api_key,<br>            model_name=model_name,<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            title=title,<br>            task_type=task_type,<br>            **kwargs,<br>        )<br>        gemini.configure(**config_params)<br>        self._model = gemini<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"GeminiEmbedding\"<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self._model.embed_content(<br>            model=self.model_name,<br>            content=query,<br>            title=self.title,<br>            task_type=self.task_type,<br>        )[\"embedding\"]<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._model.embed_content(<br>            model=self.model_name,<br>            content=text,<br>            title=self.title,<br>            task_type=self.task_type,<br>        )[\"embedding\"]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        return [<br>            self._model.embed_content(<br>                model=self.model_name,<br>                content=text,<br>                title=self.title,<br>                task_type=self.task_type,<br>            )[\"embedding\"]<br>            for text in texts<br>        ]<br>    ### Async methods ###<br>    # need to wait async calls from Gemini side to be implemented.<br>    # Issue: https://github.com/google/generative-ai-python/issues/125<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        return self._get_query_embedding(query)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously get text embedding.\"\"\"<br>        return self._get_text_embedding(text)<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Asynchronously get text embeddings.\"\"\"<br>        return self._get_text_embeddings(texts)<br></code>`` |\n"
    },
    {
      "id": "70fd2e06-a116-4b8b-b0bb-b98fa17ce1f0",
      "size": 1542,
      "headers": {
        "h1": "Llamafile",
        "h2": "LlamafileEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 13<br> 14<br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br></code>`<code> | </code>`<code><br>class LlamafileEmbedding(BaseEmbedding):<br>    \"\"\"Class for llamafile embeddings.<br>    llamafile lets you distribute and run large language models with a<br>    single file.<br>    To get started, see: https://github.com/Mozilla-Ocho/llamafile<br>    To use this class, you will need to first:<br>    1. Download a llamafile.<br>    2. Make the downloaded file executable: </code>chmod +x path/to/model.llamafile<code><br>    3. Start the llamafile in server mode with embeddings enabled:<br>        </code>./path/to/model.llamafile --server --nobrowser --embedding<code><br>    \"\"\"<br>    base_url: str = Field(<br>        description=\"base url of the llamafile server\", default=\"http://localhost:8080\"<br>    )<br>    request_timeout: float = Field(<br>        default=DEFAULT_REQUEST_TIMEOUT,<br>        description=\"The timeout for making http request to llamafile API server\",<br>    )<br>    def __init__(<br>        self,<br>        base_url: str = \"http://localhost:8080\",<br>        callback_manager: Optional[CallbackManager] = None,<br>        **kwargs,<br>    ) -> None:<br>        super().__init__(<br>            base_url=base_url,<br>            callback_manager=callback_manager or CallbackManager([]),<br>            **kwargs,<br>        )<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"LlamafileEmbedding\"<br>    def _get_query_embedding(self, query: str) -> Embedding:<br>        return self._get_text_embedding(query)<br>    async def _aget_query_embedding(self, query: str) -> Embedding:<br>        return await self._aget_text_embedding(query)<br>    def _get_text_embedding(self, text: str) -> Embedding:<br>        \"\"\"<br>        Embed the input text synchronously.<br>        \"\"\"<br>        request_body = {<br>            \"content\": text,<br>        }<br>        with httpx.Client(timeout=Timeout(self.request_timeout)) as client:<br>            response = client.post(<br>                url=f\"{self.base_url}/embedding\",<br>                headers={\"Content-Type\": \"application/json\"},<br>                json=request_body,<br>            )<br>            response.encoding = \"utf-8\"<br>            response.raise_for_status()<br>            return response.json()[\"embedding\"]<br>    async def _aget_text_embedding(self, text: str) -> Embedding:<br>        \"\"\"<br>        Embed the input text asynchronously.<br>        \"\"\"<br>        request_body = {<br>            \"content\": text,<br>        }<br>        async with httpx.AsyncClient(timeout=Timeout(self.request_timeout)) as client:<br>            response = await client.post(<br>                url=f\"{self.base_url}/embedding\",<br>                headers={\"Content-Type\": \"application/json\"},<br>                json=request_body,<br>            )<br>            response.encoding = \"utf-8\"<br>            response.raise_for_status()<br>            return response.json()[\"embedding\"]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[Embedding]:<br>        \"\"\"<br>        Embed the input texts synchronously.<br>        \"\"\"<br>        request_body = {<br>            \"content\": texts,<br>        }<br>        with httpx.Client(timeout=Timeout(self.request_timeout)) as client:<br>            response = client.post(<br>                url=f\"{self.base_url}/embedding\",<br>                headers={\"Content-Type\": \"application/json\"},<br>                json=request_body,<br>            )<br>            response.encoding = \"utf-8\"<br>            response.raise_for_status()<br>            return [output[\"embedding\"] for output in response.json()[\"results\"]]<br>    async def _aget_text_embeddings(self, texts: List[str]) -> Embedding:<br>        \"\"\"<br>        Embed the input text asynchronously.<br>        \"\"\"<br>        request_body = {<br>            \"content\": texts,<br>        }<br>        async with httpx.AsyncClient(timeout=Timeout(self.request_timeout)) as client:<br>            response = await client.post(<br>                url=f\"{self.base_url}/embedding\",<br>                headers={\"Content-Type\": \"application/json\"},<br>                json=request_body,<br>            )<br>            response.encoding = \"utf-8\"<br>            response.raise_for_status()<br>            return [output[\"embedding\"] for output in response.json()[\"results\"]]<br></code>`` |\n"
    },
    {
      "id": "b96c0403-4284-4fba-9243-feccdb415d0f",
      "size": 1273,
      "headers": {
        "h1": "Query response",
        "h2": "RelevancyEvaluator \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br></code>`<code> | </code>`<code><br>class RelevancyEvaluator(BaseEvaluator):<br>    \"\"\"Relenvancy evaluator.<br>    Evaluates the relevancy of retrieved contexts and response to a query.<br>    This evaluator considers the query string, retrieved contexts, and response string.<br>    Args:<br>        raise_error(Optional[bool]):<br>            Whether to raise an error if the response is invalid.<br>            Defaults to False.<br>        eval_template(Optional[Union[str, BasePromptTemplate]]):<br>            The template to use for evaluation.<br>        refine_template(Optional[Union[str, BasePromptTemplate]]):<br>            The template to use for refinement.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        llm: Optional[LLM] = None,<br>        raise_error: bool = False,<br>        eval_template: Optional[Union[str, BasePromptTemplate]] = None,<br>        refine_template: Optional[Union[str, BasePromptTemplate]] = None,<br>    ) -> None:<br>        \"\"\"Init params.\"\"\"<br>        self._llm = llm or Settings.llm<br>        self._raise_error = raise_error<br>        self._eval_template: BasePromptTemplate<br>        if isinstance(eval_template, str):<br>            self._eval_template = PromptTemplate(eval_template)<br>        else:<br>            self._eval_template = eval_template or DEFAULT_EVAL_TEMPLATE<br>        self._refine_template: BasePromptTemplate<br>        if isinstance(refine_template, str):<br>            self._refine_template = PromptTemplate(refine_template)<br>        else:<br>            self._refine_template = refine_template or DEFAULT_REFINE_TEMPLATE<br>    def _get_prompts(self) -> PromptDictType:<br>        \"\"\"Get prompts.\"\"\"<br>        return {<br>            \"eval_template\": self._eval_template,<br>            \"refine_template\": self._refine_template,<br>        }<br>    def _update_prompts(self, prompts: PromptDictType) -> None:<br>        \"\"\"Update prompts.\"\"\"<br>        if \"eval_template\" in prompts:<br>            self._eval_template = prompts[\"eval_template\"]<br>        if \"refine_template\" in prompts:<br>            self._refine_template = prompts[\"refine_template\"]<br>    async def aevaluate(<br>        self,<br>        query: str | None = None,<br>        response: str | None = None,<br>        contexts: Sequence[str] | None = None,<br>        sleep_time_in_seconds: int = 0,<br>        **kwargs: Any,<br>    ) -> EvaluationResult:<br>        \"\"\"Evaluate whether the contexts and response are relevant to the query.\"\"\"<br>        del kwargs  # Unused<br>        if query is None or contexts is None or response is None:<br>            raise ValueError(\"query, contexts, and response must be provided\")<br>        docs = [Document(text=context) for context in contexts]<br>        index = SummaryIndex.from_documents(docs)<br>        query_response = f\"Question: {query}\\nResponse: {response}\"<br>        await asyncio.sleep(sleep_time_in_seconds)<br>        query_engine = index.as_query_engine(<br>            llm=self._llm,<br>            text_qa_template=self._eval_template,<br>            refine_template=self._refine_template,<br>        )<br>        response_obj = await query_engine.aquery(query_response)<br>        raw_response_txt = str(response_obj)<br>        if \"yes\" in raw_response_txt.lower():<br>            passing = True<br>        else:<br>            if self._raise_error:<br>                raise ValueError(\"The response is invalid\")<br>            passing = False<br>        return EvaluationResult(<br>            query=query,<br>            response=response,<br>            passing=passing,<br>            score=1.0 if passing else 0.0,<br>            feedback=raw_response_txt,<br>            contexts=contexts,<br>        )<br></code>`` |\n"
    },
    {
      "id": "1e8334a4-636c-4b43-a31f-88f9a989d62c",
      "size": 3028,
      "headers": {
        "h1": "Huggingface openvino",
        "h2": "OpenVINOEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br></code>`<code> | </code>`<code><br>class OpenVINOEmbedding(BaseEmbedding):<br>    model_id_or_path: str = Field(description=\"Huggingface model id or local path.\")<br>    max_length: int = Field(description=\"Maximum length of input.\")<br>    pooling: str = Field(description=\"Pooling strategy. One of ['cls', 'mean'].\")<br>    normalize: bool = Field(default=True, description=\"Normalize embeddings or not.\")<br>    query_instruction: Optional[str] = Field(<br>        description=\"Instruction to prepend to query text.\"<br>    )<br>    text_instruction: Optional[str] = Field(<br>        description=\"Instruction to prepend to text.\"<br>    )<br>    cache_folder: Optional[str] = Field(<br>        description=\"Cache folder for huggingface files.\", default=None<br>    )<br>    _model: Any = PrivateAttr()<br>    _tokenizer: Any = PrivateAttr()<br>    _device: Any = PrivateAttr()<br>    def __init__(<br>        self,<br>        model_id_or_path: str = \"BAAI/bge-m3\",<br>        pooling: str = \"cls\",<br>        max_length: Optional[int] = None,<br>        normalize: bool = True,<br>        query_instruction: Optional[str] = None,<br>        text_instruction: Optional[str] = None,<br>        model: Optional[Any] = None,<br>        tokenizer: Optional[Any] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>        model_kwargs: Dict[str, Any] = {},<br>        device: Optional[str] = \"auto\",<br>    ):<br>        try:<br>            from huggingface_hub import HfApi<br>        except ImportError as e:<br>            raise ValueError(<br>                \"Could not import huggingface_hub python package. \"<br>                \"Please install it with: \"<br>                \"</code>pip install -U huggingface_hub<code>.\"<br>            ) from e<br>        def require_model_export(<br>            model_id: str, revision: Any = None, subfolder: Any = None<br>        ) -> bool:<br>            model_dir = Path(model_id)<br>            if subfolder is not None:<br>                model_dir = model_dir / subfolder<br>            if model_dir.is_dir():<br>                return (<br>                    not (model_dir / \"openvino_model.xml\").exists()<br>                    or not (model_dir / \"openvino_model.bin\").exists()<br>                )<br>            hf_api = HfApi()<br>            try:<br>                model_info = hf_api.model_info(model_id, revision=revision or \"main\")<br>                normalized_subfolder = (<br>                    None if subfolder is None else Path(subfolder).as_posix()<br>                )<br>                model_files = [<br>                    file.rfilename<br>                    for file in model_info.siblings<br>                    if normalized_subfolder is None<br>                    or file.rfilename.startswith(normalized_subfolder)<br>                ]<br>                ov_model_path = (<br>                    \"openvino_model.xml\"<br>                    if subfolder is None<br>                    else f\"{normalized_subfolder}/openvino_model.xml\"<br>                )<br>                return (<br>                    ov_model_path not in model_files<br>                    or ov_model_path.replace(\".xml\", \".bin\") not in model_files<br>                )<br>            except Exception:<br>                return True<br>        if require_model_export(model_id_or_path):<br>            # use remote model<br>            model = model or OVModelForFeatureExtraction.from_pretrained(<br>                model_id_or_path, export=True, device=device, **model_kwargs<br>            )<br>        else:<br>            # use local model<br>            model = model or OVModelForFeatureExtraction.from_pretrained(<br>                model_id_or_path, device=device, **model_kwargs<br>            )<br>        tokenizer = tokenizer or AutoTokenizer.from_pretrained(model_id_or_path)<br>        if max_length is None:<br>            try:<br>                max_length = int(model.config.max_position_embeddings)<br>            except Exception:<br>                raise ValueError(<br>                    \"Unable to find max_length from model config. \"<br>                    \"Please provide max_length.\"<br>                )<br>            try:<br>                max_length = min(max_length, int(tokenizer.model_max_length))<br>            except Exception as exc:<br>                print(f\"An error occurred while retrieving tokenizer max length: {exc}\")<br>        if pooling not in [\"cls\", \"mean\"]:<br>            raise ValueError(f\"Pooling {pooling} not supported.\")<br>        super().__init__(<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager or CallbackManager([]),<br>            model_id_or_path=model_id_or_path,<br>            max_length=max_length,<br>            pooling=pooling,<br>            normalize=normalize,<br>            query_instruction=query_instruction,<br>            text_instruction=text_instruction,<br>        )<br>        self._device = device<br>        self._model = model<br>        self._tokenizer = tokenizer<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"OpenVINOEmbedding\"<br>    @staticmethod<br>    def create_and_save_openvino_model(<br>        model_name_or_path: str,<br>        output_path: str,<br>        export_kwargs: Optional[dict] = None,<br>    ) -> None:<br>        try:<br>            from optimum.intel.openvino import OVModelForFeatureExtraction<br>            from transformers import AutoTokenizer<br>        except ImportError:<br>            raise ImportError(<br>                \"OptimumEmbedding requires transformers to be installed.\\n\"<br>                \"Please install transformers with \"<br>                \"</code>pip install transformers optimum[openvino]<code>.\"<br>            )<br>        export_kwargs = export_kwargs or {}<br>        model = OVModelForFeatureExtraction.from_pretrained(<br>            model_name_or_path, export=True, compile=False, **export_kwargs<br>        )<br>        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)<br>        model.save_pretrained(output_path)<br>        tokenizer.save_pretrained(output_path)<br>        print(<br>            f\"Saved OpenVINO model to {output_path}. Use it with \"<br>            f\"</code>embed_model = OpenVINOEmbedding(folder_name='{output_path}')<code>.\"<br>        )<br>    def _mean_pooling(self, model_output: Any, attention_mask: Any) -> Any:<br>        \"\"\"Mean Pooling - Take attention mask into account for correct averaging.\"\"\"<br>        import torch<br>        # First element of model_output contains all token embeddings<br>        token_embeddings = model_output[0]<br>        input_mask_expanded = (<br>            attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()<br>        )<br>        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(<br>            input_mask_expanded.sum(1), min=1e-9<br>        )<br>    def _cls_pooling(self, model_output: list) -> Any:<br>        \"\"\"Use the CLS token as the pooling token.\"\"\"<br>        return model_output[0][:, 0]<br>    def _embed(self, sentences: List[str]) -> List[List[float]]:<br>        \"\"\"Embed sentences.\"\"\"<br>        length = self._model.request.inputs[0].get_partial_shape()[1]<br>        if length.is_dynamic:<br>            encoded_input = self._tokenizer(<br>                sentences,<br>                padding=True,<br>                max_length=self.max_length,<br>                truncation=True,<br>                return_tensors=\"pt\",<br>            )<br>        else:<br>            encoded_input = self._tokenizer(<br>                sentences,<br>                padding=\"max_length\",<br>                max_length=length.get_length(),<br>                truncation=True,<br>                return_tensors=\"pt\",<br>            )<br>        model_output = self._model(**encoded_input)<br>        if self.pooling == \"cls\":<br>            embeddings = self._cls_pooling(model_output)<br>        else:<br>            embeddings = self._mean_pooling(<br>                model_output, encoded_input[\"attention_mask\"]<br>            )<br>        if self.normalize:<br>            import torch<br>            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)<br>        return embeddings.tolist()<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        query = format_query(query, self.model_name, self.query_instruction)<br>        return self._embed([query])[0]<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding async.\"\"\"<br>        return self._get_query_embedding(query)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding async.\"\"\"<br>        return self._get_text_embedding(text)<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        text = format_text(text, self.model_name, self.text_instruction)<br>        return self._embed([text])[0]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        texts = [<br>            format_text(text, self.model_name, self.text_instruction) for text in texts<br>        ]<br>        return self._embed(texts)<br></code>`` |\n"
    },
    {
      "id": "87ccf587-f58f-4b7c-9846-42b308ec5de0",
      "size": 2474,
      "headers": {
        "h1": "Yandexgpt",
        "h2": "YandexGPTEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br></code>`<code> | </code>`<code><br>class YandexGPTEmbedding(BaseEmbedding):<br>    \"\"\"<br>    A class representation for generating embeddings using the Yandex Cloud API.<br>    Args:<br>      api_key (Optional[str]): An API key for Yandex Cloud.<br>      model_name (str): The name of the model to be used for generating embeddings.<br>                         The class ensures that this model is supported. Defaults to \"general:embedding\".<br>      embed_batch_size (int): The batch size for embedding. Defaults to DEFAULT_EMBED_BATCH_SIZE.<br>      callback_manager (Optional[CallbackManager]): Callback manager for hooks.<br>    Example:<br>        . code-block:: python<br>            from llama_index.embeddings.yandexgpt import YandexGPTEmbedding<br>            embeddings = YandexGPTEmbedding(<br>                api_key=\"your-api-key\",<br>                folder_id=\"your-folder-id\",<br>            )<br>    \"\"\"<br>    api_key: str = Field(description=\"The YandexGPT API key.\")<br>    folder_id: str = Field(description=\"The folder id for YandexGPT API.\")<br>    retries: int = 6<br>    sleep_interval: float = 0.1<br>    def __init__(<br>        self,<br>        api_key: Optional[str] = None,<br>        folder_id: Optional[str] = None,<br>        model_name: str = \"general:embedding\",<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>        **kwargs: Any,<br>    ) -> None:<br>        if not api_key:<br>            raise ValueError(<br>                \"You must provide an API key or IAM token to use YandexGPT. \"<br>                \"You can either pass it in as an argument or set it </code>YANDEXGPT_API_KEY<code>.\"<br>            )<br>        if not folder_id:<br>            raise ValueError(<br>                \"You must provide catalog_id to use YandexGPT. \"<br>                \"You can either pass it in as an argument or set it </code>YANDEXGPT_CATALOG_ID<code>.\"<br>            )<br>        api_key = get_from_param_or_env(\"api_key\", api_key, \"YANDEXGPT_KEY\")<br>        folder_id = get_from_param_or_env(<br>            \"folder_id\", folder_id, \"YANDEXGPT_CATALOG_ID\"<br>        )<br>        super().__init__(<br>            model_name=model_name,<br>            api_key=api_key,<br>            folder_id=folder_id,<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            **kwargs,<br>        )<br>    def _getModelUri(self, is_document: bool = False) -> str:<br>        \"\"\"Construct the model URI based on whether the text is a document or a query.\"\"\"<br>        return f\"emb://{self.folder_id}/text-search-{'doc' if is_document else 'query'}/latest\"<br>    @classmethod<br>    def class_name(cls) -> str:<br>        \"\"\"Return the class name.\"\"\"<br>        return \"YandexGPTEmbedding\"<br>    def _embed(self, text: str, is_document: bool = False) -> List[float]:<br>        \"\"\"<br>        Embeds text using the YandexGPT Cloud API synchronously.<br>        Args:<br>          text: The text to embed.<br>          is_document: Whether the text is a document (True) or a query (False).<br>        Returns:<br>          A list of floats representing the embedding.<br>        Raises:<br>          YException: If an error occurs during embedding.<br>        \"\"\"<br>        payload = {\"modelUri\": self._getModelUri(is_document), \"text\": text}<br>        header = {<br>            \"Content-Type\": \"application/json\",<br>            \"Authorization\": f\"Api-Key {self.api_key}\",<br>            \"x-data-logging-enabled\": \"false\",<br>        }<br>        try:<br>            for attempt in Retrying(<br>                stop=stop_after_attempt(self.retries),<br>                wait=wait_fixed(self.sleep_interval),<br>            ):<br>                with attempt:<br>                    response = requests.post(<br>                        DEFAULT_YANDEXGPT_API_BASE, json=payload, headers=header<br>                    )<br>                    response = response.json()<br>                    if \"embedding\" in response:<br>                        return response[\"embedding\"]<br>                    raise YException(f\"No embedding found, result returned: {response}\")<br>        except RetryError:<br>            raise YException(<br>                f\"Error computing embeddings after {self.retries} retries. Result returned:\\n{response}\"<br>            )<br>    async def _aembed(self, text: str, is_document: bool = False) -> List[float]:<br>        \"\"\"<br>        Embeds text using the YandexGPT Cloud API asynchronously.<br>        Args:<br>          text: The text to embed.<br>          is_document: Whether the text is a document (True) or a query (False).<br>        Returns:<br>          A list of floats representing the embedding.<br>        Raises:<br>          YException: If an error occurs during embedding.<br>        \"\"\"<br>        payload = {\"modelUri\": self._getModelUri(is_document), \"text\": text}<br>        header = {<br>            \"Content-Type\": \"application/json\",<br>            \"Authorization\": f\"Api-Key {self.api_key}\",<br>            \"x-data-logging-enabled\": \"false\",<br>        }<br>        try:<br>            for attempt in Retrying(<br>                stop=stop_after_attempt(self.retries),<br>                wait=wait_fixed(self.sleep_interval),<br>            ):<br>                with attempt:<br>                    async with aiohttp.ClientSession() as session:<br>                        async with session.post(<br>                            DEFAULT_YANDEXGPT_API_BASE, json=payload, headers=header<br>                        ) as response:<br>                            result = await response.json()<br>                            if \"embedding\" in result:<br>                                return result[\"embedding\"]<br>                            raise YException(<br>                                f\"No embedding found, result returned: {result}\"<br>                            )<br>        except RetryError:<br>            raise YException(<br>                f\"Error computing embeddings after {self.retries} retries. Result returned:\\n{result}\"<br>            )<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding sync.\"\"\"<br>        return self._embed(text, is_document=True)<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get list of texts embeddings sync.\"\"\"<br>        embeddings = []<br>        for text in texts:<br>            embeddings.append(self._embed(text, is_document=True))<br>            time.sleep(self.sleep_interval)<br>        return embeddings<br>    def _get_query_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get query embedding sync.\"\"\"<br>        return self._embed(text, is_document=False)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get query text async.\"\"\"<br>        return await self._aembed(text, is_document=True)<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get list of texts embeddings async.\"\"\"<br>        embeddings = []<br>        for text in texts:<br>            embeddings.append(await self._aembed(text, is_document=True))<br>            await asyncio.sleep(self.sleep_interval)<br>        return embeddings<br>    async def _aget_query_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get query embedding async.\"\"\"<br>        return await self._aembed(text, is_document=False)<br></code>`` |\n"
    },
    {
      "id": "d57c736c-78f4-4282-998e-b2b92b8c8e3a",
      "size": 1520,
      "headers": {
        "h1": "Nomic",
        "h2": "NomicEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br></code>`<code> | </code>`<code><br>class NomicEmbedding(MultiModalEmbedding):<br>    \"\"\"NomicEmbedding uses the Nomic API to generate embeddings.\"\"\"<br>    query_task_type: Optional[NomicTaskType] = Field(<br>        description=\"Task type for queries\",<br>    )<br>    document_task_type: Optional[NomicTaskType] = Field(<br>        description=\"Task type for documents\",<br>    )<br>    dimensionality: Optional[int] = Field(<br>        description=\"Embedding dimension, for use with Matryoshka-capable models\",<br>    )<br>    model_name: str = Field(description=\"Embedding model name\")<br>    vision_model_name: Optional[str] = Field(<br>        description=\"Vision model name for multimodal embeddings\",<br>    )<br>    inference_mode: NomicInferenceMode = Field(<br>        description=\"Whether to generate embeddings locally\",<br>    )<br>    device: Optional[str] = Field(description=\"Device to use for local embeddings\")<br>    def __init__(<br>        self,<br>        model_name: str = \"nomic-embed-text-v1\",<br>        vision_model_name: Optional[str] = \"nomic-embed-vision-v1\",<br>        embed_batch_size: int = 32,<br>        api_key: Optional[str] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        query_task_type: Optional[str] = \"search_query\",<br>        document_task_type: Optional[str] = \"search_document\",<br>        dimensionality: Optional[int] = 768,<br>        inference_mode: str = \"remote\",<br>        device: Optional[str] = None,<br>    ):<br>        if api_key is not None:<br>            nomic.login(api_key)<br>        super().__init__(<br>            model_name=model_name,<br>            vision_model_name=vision_model_name,<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            query_task_type=query_task_type,<br>            document_task_type=document_task_type,<br>            dimensionality=dimensionality,<br>            inference_mode=inference_mode,<br>            device=device,<br>        )<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"NomicEmbedding\"<br>    def load_images(self, image_paths: List[ImageType]) -> List[Image.Image]:<br>        \"\"\"Load images from the specified paths.\"\"\"<br>        return [Image.open(image_path).convert(\"RGB\") for image_path in image_paths]<br>    def _embed_text(<br>        self, texts: List[str], task_type: Optional[str] = None<br>    ) -> List[List[float]]:<br>        result = nomic.embed.text(<br>            texts,<br>            model=self.model_name,<br>            task_type=task_type,<br>            dimensionality=self.dimensionality,<br>            inference_mode=self.inference_mode,<br>            device=self.device,<br>        )<br>        return result[\"embeddings\"]<br>    def _embed_image(self, images_paths: List[ImageType]) -> List[List[float]]:<br>        images = self.load_images(images_paths)<br>        result = nomic.embed.image(images, model=self.vision_model_name)<br>        return result[\"embeddings\"]<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        return self._embed_text([query], task_type=self.query_task_type)[0]<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        self._warn_async()<br>        return self._get_query_embedding(query)<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        return self._embed_text([text], task_type=self.document_task_type)[0]<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        self._warn_async()<br>        return self._get_text_embedding(text)<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        return self._embed_text(texts, task_type=self.document_task_type)<br>    def _get_image_embedding(self, image: ImageType) -> List[float]:<br>        return self._embed_image([image])[0]<br>    async def _aget_image_embedding(self, image: ImageType) -> List[float]:<br>        self._warn_async()<br>        return self._get_image_embedding(image)<br>    def _get_image_embeddings(self, images: List[ImageType]) -> List[List[float]]:<br>        return self._embed_image(images)<br>    def _warn_async(self) -> None:<br>        warnings.warn(<br>            f\"{self.class_name()} does not implement async embeddings, falling back to sync method.\",<br>        )<br></code>`` |\n"
    },
    {
      "id": "2cefcebe-e717-4e11-952a-ec895b08e0c1",
      "size": 1646,
      "headers": {
        "h1": "Entity",
        "h2": "EntityExtractor \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br></code>`<code> | </code>`<code><br>class EntityExtractor(BaseExtractor):<br>    \"\"\"<br>    Entity extractor. Extracts </code>entities<code> into a metadata field using a default model<br>    </code>tomaarsen/span-marker-mbert-base-multinerd<code> and the SpanMarker library.<br>    Install SpanMarker with </code>pip install span-marker<code>.<br>    \"\"\"<br>    model_name: str = Field(<br>        default=DEFAULT_ENTITY_MODEL,<br>        description=\"The model name of the SpanMarker model to use.\",<br>    )<br>    prediction_threshold: float = Field(<br>        default=0.5,<br>        description=\"The confidence threshold for accepting predictions.\",<br>        gte=0.0,<br>        lte=1.0,<br>    )<br>    span_joiner: str = Field(<br>        default=\" \", description=\"The separator between entity names.\"<br>    )<br>    label_entities: bool = Field(<br>        default=False, description=\"Include entity class labels or not.\"<br>    )<br>    device: Optional[str] = Field(<br>        default=None, description=\"Device to run model on, i.e. 'cuda', 'cpu'\"<br>    )<br>    entity_map: Dict[str, str] = Field(<br>        default_factory=dict,<br>        description=\"Mapping of entity class names to usable names.\",<br>    )<br>    _tokenizer: Callable = PrivateAttr()<br>    _model: Any = PrivateAttr()<br>    def __init__(<br>        self,<br>        model_name: str = DEFAULT_ENTITY_MODEL,<br>        prediction_threshold: float = 0.5,<br>        span_joiner: str = \" \",<br>        label_entities: bool = False,<br>        device: Optional[str] = None,<br>        entity_map: Optional[Dict[str, str]] = None,<br>        tokenizer: Optional[Callable[[str], List[str]]] = None,<br>        **kwargs: Any,<br>    ):<br>        \"\"\"<br>        Entity extractor for extracting entities from text and inserting<br>        into node metadata.<br>        Args:<br>            model_name (str):<br>                Name of the SpanMarker model to use.<br>            prediction_threshold (float):<br>                Minimum prediction threshold for entities. Defaults to 0.5.<br>            span_joiner (str):<br>                String to join spans with. Defaults to \" \".<br>            label_entities (bool):<br>                Whether to label entities with their type. Setting to true can be<br>                slightly error prone, but can be useful for downstream tasks.<br>                Defaults to False.<br>            device (Optional[str]):<br>                Device to use for SpanMarker model, i.e. \"cpu\" or \"cuda\".<br>                Loads onto \"cpu\" by default.<br>            entity_map (Optional[Dict[str, str]]):<br>                Mapping from entity class name to label.<br>            tokenizer (Optional[Callable[[str], List[str]]]):<br>                Tokenizer to use for splitting text into words.<br>                Defaults to NLTK word_tokenize.<br>        \"\"\"<br>        base_entity_map = DEFAULT_ENTITY_MAP<br>        if entity_map is not None:<br>            base_entity_map.update(entity_map)<br>        super().__init__(<br>            model_name=model_name,<br>            prediction_threshold=prediction_threshold,<br>            span_joiner=span_joiner,<br>            label_entities=label_entities,<br>            device=device,<br>            entity_map=base_entity_map,<br>            **kwargs,<br>        )<br>        self._model = SpanMarkerModel.from_pretrained(model_name)<br>        if device is not None:<br>            self._model = self._model.to(device)<br>        self._tokenizer = tokenizer or word_tokenize<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"EntityExtractor\"<br>    async def aextract(self, nodes: Sequence[BaseNode]) -> List[Dict]:<br>        # Extract node-level entity metadata<br>        metadata_list: List[Dict] = [{} for _ in nodes]<br>        metadata_queue: Iterable[int] = get_tqdm_iterable(<br>            range(len(nodes)), self.show_progress, \"Extracting entities\"<br>        )<br>        for i in metadata_queue:<br>            metadata = metadata_list[i]<br>            node_text = nodes[i].get_content(metadata_mode=self.metadata_mode)<br>            words = self._tokenizer(node_text)<br>            spans = self._model.predict(words)<br>            for span in spans:<br>                if span[\"score\"] > self.prediction_threshold:<br>                    ent_label = self.entity_map.get(span[\"label\"], span[\"label\"])<br>                    metadata_label = ent_label if self.label_entities else \"entities\"<br>                    if metadata_label not in metadata:<br>                        metadata[metadata_label] = set()<br>                    metadata[metadata_label].add(self.span_joiner.join(span[\"span\"]))<br>        # convert metadata from set to list<br>        for metadata in metadata_list:<br>            for key, val in metadata.items():<br>                metadata[key] = list(val)<br>        return metadata_list<br></code>`` |\n"
    },
    {
      "id": "4e2e86cc-921c-4687-9535-25c83e111555",
      "size": 1376,
      "headers": {
        "h1": "Xinference",
        "h2": "XinferenceEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br>  9<br> 10<br> 11<br> 12<br> 13<br> 14<br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br></code>`<code> | </code>`<code><br>class XinferenceEmbedding(BaseEmbedding):<br>    \"\"\"Class for Xinference embeddings.\"\"\"<br>    model_uid: str = Field(<br>        default=\"unknown\",<br>        description=\"The Xinference model uid to use.\",<br>    )<br>    base_url: str = Field(<br>        default=\"http://localhost:9997\",<br>        description=\"The Xinference base url to use.\",<br>    )<br>    timeout: float = Field(<br>        default=60.0,<br>        description=\"Timeout in seconds for the request.\",<br>    )<br>    def __init__(<br>        self,<br>        model_uid: str,<br>        base_url: str = \"http://localhost:9997\",<br>        timeout: float = 60.0,<br>        **kwargs: Any,<br>    ) -> None:<br>        super().__init__(<br>            model_uid=model_uid,<br>            base_url=base_url,<br>            timeout=timeout,<br>            **kwargs,<br>        )<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"XinferenceEmbedding\"<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self.get_general_text_embedding(query)<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        return await self.aget_general_text_embedding(query)<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self.get_general_text_embedding(text)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously get text embedding.\"\"\"<br>        return await self.aget_general_text_embedding(text)<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        embeddings_list: List[List[float]] = []<br>        for text in texts:<br>            embeddings = self.get_general_text_embedding(text)<br>            embeddings_list.append(embeddings)<br>        return embeddings_list<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Asynchronously get text embeddings.\"\"\"<br>        return await asyncio.gather(<br>            *[self.aget_general_text_embedding(text) for text in texts]<br>        )<br>    def get_general_text_embedding(self, prompt: str) -> List[float]:<br>        \"\"\"Get Xinference embeddings.\"\"\"<br>        headers = {\"Content-Type\": \"application/json\"}<br>        json_data = {\"input\": prompt, \"model\": self.model_uid}<br>        response = requests.post(<br>            url=f\"{self.base_url}/v1/embeddings\",<br>            headers=headers,<br>            json=json_data,<br>            timeout=self.timeout,<br>        )<br>        response.encoding = \"utf-8\"<br>        if response.status_code != 200:<br>            raise Exception(<br>                f\"Xinference call failed with status code {response.status_code}.\"<br>                f\"Details: {response.text}\"<br>            )<br>        return response.json()[\"data\"][0][\"embedding\"]<br>    async def aget_general_text_embedding(self, prompt: str) -> List[float]:<br>        \"\"\"Asynchronously get Xinference embeddings.\"\"\"<br>        headers = {\"Content-Type\": \"application/json\"}<br>        json_data = {\"input\": prompt, \"model\": self.model_uid}<br>        async with aiohttp.ClientSession() as session:<br>            async with session.post(<br>                url=f\"{self.base_url}/v1/embeddings\",<br>                headers=headers,<br>                json=json_data,<br>                timeout=self.timeout,<br>            ) as response:<br>                if response.status != 200:<br>                    raise Exception(<br>                        f\"Xinference call failed with status code {response.status}.\"<br>                    )<br>                data = await response.json()<br>                return data[\"data\"][0][\"embedding\"]<br></code>`` |\n"
    },
    {
      "id": "14b5f8e6-f9c8-4e95-b2af-14a6656661a8",
      "size": 1657,
      "headers": {
        "h1": "Ipex llm",
        "h2": "IpexLLMEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br></code>`<code> | </code>`<code><br>class IpexLLMEmbedding(BaseEmbedding):<br>    max_length: int = Field(<br>        default=DEFAULT_HUGGINGFACE_LENGTH, description=\"Maximum length of input.\", gt=0<br>    )<br>    normalize: bool = Field(default=True, description=\"Normalize embeddings or not.\")<br>    query_instruction: Optional[str] = Field(<br>        description=\"Instruction to prepend to query text.\"<br>    )<br>    text_instruction: Optional[str] = Field(<br>        description=\"Instruction to prepend to text.\"<br>    )<br>    cache_folder: Optional[str] = Field(<br>        description=\"Cache folder for Hugging Face files.\"<br>    )<br>    _model: Any = PrivateAttr()<br>    _device: str = PrivateAttr()<br>    def __init__(<br>        self,<br>        model_name: str = DEFAULT_HUGGINGFACE_EMBEDDING_MODEL,<br>        max_length: Optional[int] = None,<br>        query_instruction: Optional[str] = None,<br>        text_instruction: Optional[str] = None,<br>        normalize: bool = True,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        cache_folder: Optional[str] = None,<br>        trust_remote_code: bool = False,<br>        device: str = \"cpu\",<br>        callback_manager: Optional[CallbackManager] = None,<br>        **model_kwargs,<br>    ):<br>        if device not in [\"cpu\", \"xpu\"] and not device.startswith(\"xpu:\"):<br>            raise ValueError(<br>                \"IpexLLMEmbedding currently only supports device to be 'cpu', 'xpu', \"<br>                f\"or 'xpu:<device_id>', but you have: {device}.\"<br>            )<br>        device = device<br>        cache_folder = cache_folder or get_cache_dir()<br>        if model_name is None:<br>            raise ValueError(\"The </code>model_name<code> argument must be provided.\")<br>        if not is_listed_model(model_name, BGE_MODELS):<br>            bge_model_list_str = \", \".join(BGE_MODELS)<br>            logger.warning(<br>                \"IpexLLMEmbedding currently only provides optimization for \"<br>                f\"Hugging Face BGE models, which are: {bge_model_list_str}\"<br>            )<br>        model = SentenceTransformer(<br>            model_name,<br>            device=device,<br>            cache_folder=cache_folder,<br>            trust_remote_code=trust_remote_code,<br>            prompts={<br>                \"query\": query_instruction<br>                or get_query_instruct_for_model_name(model_name),<br>                \"text\": text_instruction<br>                or get_text_instruct_for_model_name(model_name),<br>            },<br>            **model_kwargs,<br>        )<br>        # Apply ipex-llm optimizations<br>        model = _optimize_pre(self._model)<br>        model = _optimize_post(self._model)<br>        if device == \"xpu\":<br>            # TODO: apply </code>ipex_llm.optimize_model<code><br>            model = model.half().to(device)<br>        if max_length:<br>            model.max_seq_length = max_length<br>        else:<br>            max_length = model.max_seq_length<br>        super().__init__(<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            model_name=model_name,<br>            max_length=max_length,<br>            normalize=normalize,<br>            query_instruction=query_instruction,<br>            text_instruction=text_instruction,<br>        )<br>        self._model = model<br>        self._device = device<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"IpexLLMEmbedding\"<br>    def _embed(<br>        self,<br>        sentences: List[str],<br>        prompt_name: Optional[str] = None,<br>    ) -> List[List[float]]:<br>        \"\"\"Embed sentences.\"\"\"<br>        return self._model.encode(<br>            sentences,<br>            batch_size=self.embed_batch_size,<br>            prompt_name=prompt_name,<br>            normalize_embeddings=self.normalize,<br>        ).tolist()<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self._embed(query, prompt_name=\"query\")<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding async.\"\"\"<br>        return self._get_query_embedding(query)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding async.\"\"\"<br>        return self._get_text_embedding(text)<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._embed(text, prompt_name=\"text\")<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        return self._embed(texts, prompt_name=\"text\")<br></code>`` |\n"
    },
    {
      "id": "30ecfa54-cde0-4ad0-87ce-4fe8a1becd78",
      "size": 1658,
      "headers": {
        "h1": "Promptlayer",
        "h2": "PromptLayerHandler \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 13<br> 14<br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br></code>`<code> | </code>`<code><br>class PromptLayerHandler(BaseCallbackHandler):<br>    \"\"\"Callback handler for sending to promptlayer.com.\"\"\"<br>    pl_tags: Optional[List[str]]<br>    return_pl_id: bool = False<br>    def __init__(self, pl_tags: List[str] = [], return_pl_id: bool = False) -> None:<br>        try:<br>            from promptlayer.utils import get_api_key, promptlayer_api_request<br>            self._promptlayer_api_request = promptlayer_api_request<br>            self._promptlayer_api_key = get_api_key()<br>        except ImportError:<br>            raise ImportError(<br>                \"Please install PromptLAyer with </code>pip install promptlayer<code>\"<br>            )<br>        self.pl_tags = pl_tags<br>        self.return_pl_id = return_pl_id<br>        super().__init__(event_starts_to_ignore=[], event_ends_to_ignore=[])<br>    def start_trace(self, trace_id: Optional[str] = None) -> None:<br>        return<br>    def end_trace(<br>        self,<br>        trace_id: Optional[str] = None,<br>        trace_map: Optional[Dict[str, List[str]]] = None,<br>    ) -> None:<br>        return<br>    event_map: Dict[str, Dict[str, Any]] = {}<br>    def add_event(self, event_id: str, **kwargs: Any) -> None:<br>        self.event_map[event_id] = {<br>            \"kwargs\": kwargs,<br>            \"request_start_time\": datetime.datetime.now().timestamp(),<br>        }<br>    def get_event(<br>        self,<br>        event_id: str,<br>    ) -> Dict[str, Any]:<br>        return self.event_map[event_id] or {}<br>    def on_event_start(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        parent_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> str:<br>        if event_type == CBEventType.LLM and payload is not None:<br>            self.add_event(<br>                event_id=event_id, **payload.get(EventPayload.SERIALIZED, {})<br>            )<br>        return event_id<br>    def on_event_end(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> None:<br>        if event_type != CBEventType.LLM or payload is None:<br>            return<br>        request_end_time = datetime.datetime.now().timestamp()<br>        prompt = str(payload.get(EventPayload.PROMPT))<br>        completion = payload.get(EventPayload.COMPLETION)<br>        response = payload.get(EventPayload.RESPONSE)<br>        function_name = PROMPT_LAYER_CHAT_FUNCTION_NAME<br>        event_data = self.get_event(event_id=event_id)<br>        resp: Union[str, Dict]<br>        extra_args = {}<br>        resp = None<br>        if response:<br>            messages = cast(List[ChatMessage], payload.get(EventPayload.MESSAGES, []))<br>            resp = response.message.dict()<br>            assert isinstance(resp, dict)<br>            usage_dict: Dict[str, int] = {}<br>            try:<br>                usage = response.raw.get(\"usage\", None)  # type: ignore<br>                if isinstance(usage, dict):<br>                    usage_dict = {<br>                        \"prompt_tokens\": usage.get(\"prompt_tokens\", 0),<br>                        \"completion_tokens\": usage.get(\"completion_tokens\", 0),<br>                        \"total_tokens\": usage.get(\"total_tokens\", 0),<br>                    }<br>                elif isinstance(usage, BaseModel):<br>                    usage_dict = usage.dict()<br>            except Exception:<br>                pass<br>            extra_args = {<br>                \"messages\": [message.dict() for message in messages],<br>                \"usage\": usage_dict,<br>            }<br>            ## promptlayer needs tool_calls toplevel.<br>            if \"tool_calls\" in response.message.additional_kwargs:<br>                resp[\"tool_calls\"] = [<br>                    tool_call.dict()<br>                    for tool_call in resp[\"additional_kwargs\"][\"tool_calls\"]<br>                ]<br>                del resp[\"additional_kwargs\"][\"tool_calls\"]<br>        if completion:<br>            function_name = PROMPT_LAYER_COMPLETION_FUNCTION_NAME<br>            resp = str(completion)<br>        if resp:<br>            _pl_request_id = self._promptlayer_api_request(<br>                function_name,<br>                \"openai\",<br>                [prompt],<br>                {<br>                    **extra_args,<br>                    **event_data[\"kwargs\"],<br>                },<br>                self.pl_tags,<br>                [resp],<br>                event_data[\"request_start_time\"],<br>                request_end_time,<br>                self._promptlayer_api_key,<br>                return_pl_id=self.return_pl_id,<br>            )<br></code>`` |\n"
    },
    {
      "id": "243c89e5-c9cb-40b1-ac80-220907bf49c7",
      "size": 1881,
      "headers": {
        "h1": "Elasticsearch",
        "h2": "ElasticsearchEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 10<br> 11<br> 12<br> 13<br> 14<br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br></code>`<code> | </code>`<code><br>class ElasticsearchEmbedding(BaseEmbedding):<br>    \"\"\"Elasticsearch embedding models.<br>    This class provides an interface to generate embeddings using a model deployed<br>    in an Elasticsearch cluster. It requires an Elasticsearch connection object<br>    and the model_id of the model deployed in the cluster.<br>    In Elasticsearch you need to have an embedding model loaded and deployed.<br>    - https://www.elastic.co<br>        /guide/en/elasticsearch/reference/current/infer-trained-model.html<br>    - https://www.elastic.co<br>        /guide/en/machine-learning/current/ml-nlp-deploy-models.html<br>    \"\"\"  #<br>    _client: Any = PrivateAttr()<br>    model_id: str<br>    input_field: str<br>    def class_name(self) -> str:<br>        return \"ElasticsearchEmbedding\"<br>    def __init__(<br>        self,<br>        client: Any,<br>        model_id: str,<br>        input_field: str = \"text_field\",<br>        **kwargs: Any,<br>    ):<br>        super().__init__(model_id=model_id, input_field=input_field, **kwargs)<br>        self._client = client<br>    @classmethod<br>    def from_es_connection(<br>        cls,<br>        model_id: str,<br>        es_connection: Any,<br>        input_field: str = \"text_field\",<br>    ) -> BaseEmbedding:<br>        \"\"\"<br>        Instantiate embeddings from an existing Elasticsearch connection.<br>        This method provides a way to create an instance of the ElasticsearchEmbedding<br>        class using an existing Elasticsearch connection. The connection object is used<br>        to create an MlClient, which is then used to initialize the<br>        ElasticsearchEmbedding instance.<br>        Args:<br>        model_id (str): The model_id of the model deployed in the Elasticsearch cluster.<br>        es_connection (elasticsearch.Elasticsearch): An existing Elasticsearch<br>            connection object.<br>        input_field (str, optional): The name of the key for the input text field<br>            in the document. Defaults to 'text_field'.<br>        Returns:<br>        ElasticsearchEmbedding: An instance of the ElasticsearchEmbedding class.<br>        Example:<br>            .. code-block:: python<br>                from elasticsearch import Elasticsearch<br>                from llama_index.embeddings.elasticsearch import ElasticsearchEmbedding<br>                # Define the model ID and input field name (if different from default)<br>                model_id = \"your_model_id\"<br>                # Optional, only if different from 'text_field'<br>                input_field = \"your_input_field\"<br>                # Create Elasticsearch connection<br>                es_connection = Elasticsearch(hosts=[\"localhost:9200\"], basic_auth=(\"user\", \"password\"))<br>                # Instantiate ElasticsearchEmbedding using the existing connection<br>                embeddings = ElasticsearchEmbedding.from_es_connection(<br>                    model_id,<br>                    es_connection,<br>                    input_field=input_field,<br>                )<br>        \"\"\"<br>        client = MlClient(es_connection)<br>        return cls(client, model_id, input_field=input_field)<br>    @classmethod<br>    def from_credentials(<br>        cls,<br>        model_id: str,<br>        es_url: str,<br>        es_username: str,<br>        es_password: str,<br>        input_field: str = \"text_field\",<br>    ) -> BaseEmbedding:<br>        \"\"\"Instantiate embeddings from Elasticsearch credentials.<br>        Args:<br>            model_id (str): The model_id of the model deployed in the Elasticsearch<br>                cluster.<br>            input_field (str): The name of the key for the input text field in the<br>                document. Defaults to 'text_field'.<br>            es_url: (str): The Elasticsearch url to connect to.<br>            es_username: (str): Elasticsearch username.<br>            es_password: (str): Elasticsearch password.<br>        Example:<br>            .. code-block:: python<br>                from llama_index.embeddings.bedrock import ElasticsearchEmbedding<br>                # Define the model ID and input field name (if different from default)<br>                model_id = \"your_model_id\"<br>                # Optional, only if different from 'text_field'<br>                input_field = \"your_input_field\"<br>                embeddings = ElasticsearchEmbedding.from_credentials(<br>                    model_id,<br>                    input_field=input_field,<br>                    es_url=\"foo\",<br>                    es_username=\"bar\",<br>                    es_password=\"baz\",<br>                )<br>        \"\"\"<br>        es_connection = Elasticsearch(<br>            hosts=[es_url],<br>            basic_auth=(es_username, es_password),<br>        )<br>        client = MlClient(es_connection)<br>        return cls(client, model_id, input_field=input_field)<br>    def _get_embedding(self, text: str) -> List[float]:<br>        \"\"\"<br>        Generate an embedding for a single query text.<br>        Args:<br>            text (str): The query text to generate an embedding for.<br>        Returns:<br>            List[float]: The embedding for the input query text.<br>        \"\"\"<br>        response = self._client.infer_trained_model(<br>            model_id=self.model_id,<br>            docs=[{self.input_field: text}],<br>        )<br>        return response[\"inference_results\"][0][\"predicted_value\"]<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        return self._get_embedding(text)<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        return self._get_embedding(query)<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        return self._get_query_embedding(query)<br></code>`` |\n"
    },
    {
      "id": "606bac0b-ff92-455c-8c14-266fee0c55b6",
      "size": 1334,
      "headers": {
        "h1": "Adapter",
        "h2": "AdapterEmbeddingModel \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br></code>`<code> | </code>`<code><br>class AdapterEmbeddingModel(BaseEmbedding):<br>    \"\"\"Adapter for any embedding model.<br>    This is a wrapper around any embedding model that adds an adapter layer \\<br>        on top of it.<br>    This is useful for finetuning an embedding model on a downstream task.<br>    The embedding model can be any model - it does not need to expose gradients.<br>    Args:<br>        base_embed_model (BaseEmbedding): Base embedding model.<br>        adapter_path (str): Path to adapter.<br>        adapter_cls (Optional[Type[Any]]): Adapter class. Defaults to None, in which \\<br>            case a linear adapter is used.<br>        transform_query (bool): Whether to transform query embeddings. Defaults to True.<br>        device (Optional[str]): Device to use. Defaults to None.<br>        embed_batch_size (int): Batch size for embedding. Defaults to 10.<br>        callback_manager (Optional[CallbackManager]): Callback manager. \\<br>            Defaults to None.<br>    \"\"\"<br>    _base_embed_model: BaseEmbedding = PrivateAttr()<br>    _adapter: Any = PrivateAttr()<br>    _transform_query: bool = PrivateAttr()<br>    _device: Optional[str] = PrivateAttr()<br>    _target_device: Any = PrivateAttr()<br>    def __init__(<br>        self,<br>        base_embed_model: BaseEmbedding,<br>        adapter_path: str,<br>        adapter_cls: Optional[Type[Any]] = None,<br>        transform_query: bool = True,<br>        device: Optional[str] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>    ) -> None:<br>        \"\"\"Init params.\"\"\"<br>        import torch<br>        from llama_index.embeddings.adapter.utils import BaseAdapter, LinearLayer<br>        super().__init__(<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            model_name=f\"Adapter for {base_embed_model.model_name}\",<br>        )<br>        if device is None:<br>            device = infer_torch_device()<br>            logger.info(f\"Use pytorch device: {device}\")<br>        self._target_device = torch.device(device)<br>        self._base_embed_model = base_embed_model<br>        if adapter_cls is None:<br>            adapter_cls = LinearLayer<br>        else:<br>            adapter_cls = cast(Type[BaseAdapter], adapter_cls)<br>        adapter = adapter_cls.load(adapter_path)<br>        self._adapter = cast(BaseAdapter, adapter)<br>        self._adapter.to(self._target_device)<br>        self._transform_query = transform_query<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"AdapterEmbeddingModel\"<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        import torch<br>        query_embedding = self._base_embed_model._get_query_embedding(query)<br>        if self._transform_query:<br>            query_embedding_t = torch.tensor(query_embedding).to(self._target_device)<br>            query_embedding_t = self._adapter.forward(query_embedding_t)<br>            query_embedding = query_embedding_t.tolist()<br>        return query_embedding<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        import torch<br>        query_embedding = await self._base_embed_model._aget_query_embedding(query)<br>        if self._transform_query:<br>            query_embedding_t = torch.tensor(query_embedding).to(self._target_device)<br>            query_embedding_t = self._adapter.forward(query_embedding_t)<br>            query_embedding = query_embedding_t.tolist()<br>        return query_embedding<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        return self._base_embed_model._get_text_embedding(text)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        return await self._base_embed_model._aget_text_embedding(text)<br></code>`` |\n"
    },
    {
      "id": "52f6e632-b991-44a6-9b3f-cc6704167fce",
      "size": 2683,
      "headers": {
        "h1": "Llama debug",
        "h2": "LlamaDebugHandler \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br></code>`<code> | </code>`<code><br>class LlamaDebugHandler(PythonicallyPrintingBaseHandler):<br>    \"\"\"Callback handler that keeps track of debug info.<br>    NOTE: this is a beta feature. The usage within our codebase, and the interface<br>    may change.<br>    This handler simply keeps track of event starts/ends, separated by event types.<br>    You can use this callback handler to keep track of and debug events.<br>    Args:<br>        event_starts_to_ignore (Optional[List[CBEventType]]): list of event types to<br>            ignore when tracking event starts.<br>        event_ends_to_ignore (Optional[List[CBEventType]]): list of event types to<br>            ignore when tracking event ends.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        event_starts_to_ignore: Optional[List[CBEventType]] = None,<br>        event_ends_to_ignore: Optional[List[CBEventType]] = None,<br>        print_trace_on_end: bool = True,<br>        logger: Optional[logging.Logger] = None,<br>    ) -> None:<br>        \"\"\"Initialize the llama debug handler.\"\"\"<br>        self._event_pairs_by_type: Dict[CBEventType, List[CBEvent]] = defaultdict(list)<br>        self._event_pairs_by_id: Dict[str, List[CBEvent]] = defaultdict(list)<br>        self._sequential_events: List[CBEvent] = []<br>        self._cur_trace_id: Optional[str] = None<br>        self._trace_map: Dict[str, List[str]] = defaultdict(list)<br>        self.print_trace_on_end = print_trace_on_end<br>        event_starts_to_ignore = (<br>            event_starts_to_ignore if event_starts_to_ignore else []<br>        )<br>        event_ends_to_ignore = event_ends_to_ignore if event_ends_to_ignore else []<br>        super().__init__(<br>            event_starts_to_ignore=event_starts_to_ignore,<br>            event_ends_to_ignore=event_ends_to_ignore,<br>            logger=logger,<br>        )<br>    def on_event_start(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        parent_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> str:<br>        \"\"\"Store event start data by event type.<br>        Args:<br>            event_type (CBEventType): event type to store.<br>            payload (Optional[Dict[str, Any]]): payload to store.<br>            event_id (str): event id to store.<br>            parent_id (str): parent event id.<br>        \"\"\"<br>        event = CBEvent(event_type, payload=payload, id_=event_id)<br>        self._event_pairs_by_type[event.event_type].append(event)<br>        self._event_pairs_by_id[event.id_].append(event)<br>        self._sequential_events.append(event)<br>        return event.id_<br>    def on_event_end(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> None:<br>        \"\"\"Store event end data by event type.<br>        Args:<br>            event_type (CBEventType): event type to store.<br>            payload (Optional[Dict[str, Any]]): payload to store.<br>            event_id (str): event id to store.<br>        \"\"\"<br>        event = CBEvent(event_type, payload=payload, id_=event_id)<br>        self._event_pairs_by_type[event.event_type].append(event)<br>        self._event_pairs_by_id[event.id_].append(event)<br>        self._sequential_events.append(event)<br>        self._trace_map = defaultdict(list)<br>    def get_events(self, event_type: Optional[CBEventType] = None) -> List[CBEvent]:<br>        \"\"\"Get all events for a specific event type.\"\"\"<br>        if event_type is not None:<br>            return self._event_pairs_by_type[event_type]<br>        return self._sequential_events<br>    def _get_event_pairs(self, events: List[CBEvent]) -> List[List[CBEvent]]:<br>        \"\"\"Helper function to pair events according to their ID.\"\"\"<br>        event_pairs: Dict[str, List[CBEvent]] = defaultdict(list)<br>        for event in events:<br>            event_pairs[event.id_].append(event)<br>        return sorted(<br>            event_pairs.values(),<br>            key=lambda x: datetime.strptime(x[0].time, TIMESTAMP_FORMAT),<br>        )<br>    def _get_time_stats_from_event_pairs(<br>        self, event_pairs: List[List[CBEvent]]<br>    ) -> EventStats:<br>        \"\"\"Calculate time-based stats for a set of event pairs.\"\"\"<br>        total_secs = 0.0<br>        for event_pair in event_pairs:<br>            start_time = datetime.strptime(event_pair[0].time, TIMESTAMP_FORMAT)<br>            end_time = datetime.strptime(event_pair[-1].time, TIMESTAMP_FORMAT)<br>            total_secs += (end_time - start_time).total_seconds()<br>        return EventStats(<br>            total_secs=total_secs,<br>            average_secs=total_secs / len(event_pairs),<br>            total_count=len(event_pairs),<br>        )<br>    def get_event_pairs(<br>        self, event_type: Optional[CBEventType] = None<br>    ) -> List[List[CBEvent]]:<br>        \"\"\"Pair events by ID, either all events or a specific type.\"\"\"<br>        if event_type is not None:<br>            return self._get_event_pairs(self._event_pairs_by_type[event_type])<br>        return self._get_event_pairs(self._sequential_events)<br>    def get_llm_inputs_outputs(self) -> List[List[CBEvent]]:<br>        \"\"\"Get the exact LLM inputs and outputs.\"\"\"<br>        return self._get_event_pairs(self._event_pairs_by_type[CBEventType.LLM])<br>    def get_event_time_info(<br>        self, event_type: Optional[CBEventType] = None<br>    ) -> EventStats:<br>        event_pairs = self.get_event_pairs(event_type)<br>        return self._get_time_stats_from_event_pairs(event_pairs)<br>    def flush_event_logs(self) -> None:<br>        \"\"\"Clear all events from memory.\"\"\"<br>        self._event_pairs_by_type = defaultdict(list)<br>        self._event_pairs_by_id = defaultdict(list)<br>        self._sequential_events = []<br>    def start_trace(self, trace_id: Optional[str] = None) -> None:<br>        \"\"\"Launch a trace.\"\"\"<br>        self._trace_map = defaultdict(list)<br>        self._cur_trace_id = trace_id<br>    def end_trace(<br>        self,<br>        trace_id: Optional[str] = None,<br>        trace_map: Optional[Dict[str, List[str]]] = None,<br>    ) -> None:<br>        \"\"\"Shutdown the current trace.\"\"\"<br>        self._trace_map = trace_map or defaultdict(list)<br>        if self.print_trace_on_end:<br>            self.print_trace_map()<br>    def _print_trace_map(self, cur_event_id: str, level: int = 0) -> None:<br>        \"\"\"Recursively print trace map to terminal for debugging.\"\"\"<br>        event_pair = self._event_pairs_by_id[cur_event_id]<br>        if event_pair:<br>            time_stats = self._get_time_stats_from_event_pairs([event_pair])<br>            indent = \" \" * level * 2<br>            self._print(<br>                f\"{indent}|_{event_pair[0].event_type} -> {time_stats.total_secs} seconds\",<br>            )<br>        child_event_ids = self._trace_map[cur_event_id]<br>        for child_event_id in child_event_ids:<br>            self._print_trace_map(child_event_id, level=level + 1)<br>    def print_trace_map(self) -> None:<br>        \"\"\"Print simple trace map to terminal for debugging of the most recent trace.\"\"\"<br>        self._print(\"*\" * 10)<br>        self._print(f\"Trace: {self._cur_trace_id}\")<br>        self._print_trace_map(BASE_TRACE_EVENT, level=1)<br>        self._print(\"*\" * 10)<br>    @property<br>    def event_pairs_by_type(self) -> Dict[CBEventType, List[CBEvent]]:<br>        return self._event_pairs_by_type<br>    @property<br>    def events_pairs_by_id(self) -> Dict[str, List[CBEvent]]:<br>        return self._event_pairs_by_id<br>    @property<br>    def sequential_events(self) -> List[CBEvent]:<br>        return self._sequential_events<br></code>`` |\n"
    },
    {
      "id": "c00dc6a9-29b6-48a8-803d-2addbd103c99",
      "size": 1520,
      "headers": {
        "h1": "Voyageai",
        "h2": "VoyageEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br></code>`<code> | </code>`<code><br>class VoyageEmbedding(BaseEmbedding):<br>    \"\"\"Class for Voyage embeddings.<br>    Args:<br>        model_name (str): Model for embedding.<br>            Defaults to \"voyage-01\".<br>        voyage_api_key (Optional[str]): Voyage API key. Defaults to None.<br>            You can either specify the key here or store it as an environment variable.<br>    \"\"\"<br>    _client: voyageai.Client = PrivateAttr(None)<br>    _aclient: voyageai.client_async.AsyncClient = PrivateAttr()<br>    truncation: Optional[bool] = None<br>    def __init__(<br>        self,<br>        model_name: str,<br>        voyage_api_key: Optional[str] = None,<br>        embed_batch_size: Optional[int] = None,<br>        truncation: Optional[bool] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        **kwargs: Any,<br>    ):<br>        if model_name == \"voyage-01\":<br>            logger.warning(<br>                \"voyage-01 is not the latest model by Voyage AI. Please note that </code>model_name<code> \"<br>                \"will be a required argument in the future. We recommend setting it explicitly. Please see \"<br>                \"https://docs.voyageai.com/docs/embeddings for the latest models offered by Voyage AI.\"<br>            )<br>        if embed_batch_size is None:<br>            embed_batch_size = 72 if model_name in [\"voyage-2\", \"voyage-02\"] else 7<br>        super().__init__(<br>            model_name=model_name,<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            **kwargs,<br>        )<br>        self._client = voyageai.Client(api_key=voyage_api_key)<br>        self._aclient = voyageai.AsyncClient(api_key=voyage_api_key)<br>        self.truncation = truncation<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"VoyageEmbedding\"<br>    def _get_embedding(self, texts: List[str], input_type: str) -> List[List[float]]:<br>        return self._client.embed(<br>            texts,<br>            model=self.model_name,<br>            input_type=input_type,<br>            truncation=self.truncation,<br>        ).embeddings<br>    async def _aget_embedding(<br>        self, texts: List[str], input_type: str<br>    ) -> List[List[float]]:<br>        r = await self._aclient.embed(<br>            texts,<br>            model=self.model_name,<br>            input_type=input_type,<br>            truncation=self.truncation,<br>        )<br>        return r.embeddings<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self._get_embedding([query], input_type=\"query\")[0]<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        r = await self._aget_embedding([query], input_type=\"query\")<br>        return r[0]<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._get_embedding([text], input_type=\"document\")[0]<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously get text embedding.\"\"\"<br>        r = await self._aget_embedding([text], input_type=\"document\")<br>        return r[0]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        return self._get_embedding(texts, input_type=\"document\")<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Asynchronously get text embeddings.\"\"\"<br>        return await self._aget_embedding(texts, input_type=\"document\")<br>    def get_general_text_embedding(<br>        self, text: str, input_type: Optional[str] = None<br>    ) -> List[float]:<br>        \"\"\"Get general text embedding with input_type.\"\"\"<br>        return self._get_embedding([text], input_type=input_type)[0]<br>    async def aget_general_text_embedding(<br>        self, text: str, input_type: Optional[str] = None<br>    ) -> List[float]:<br>        \"\"\"Asynchronously get general text embedding with input_type.\"\"\"<br>        r = await self._aget_embedding([text], input_type=input_type)<br>        return r[0]<br></code>`` |\n"
    },
    {
      "id": "63ba946d-8d67-47f1-ae2a-4accd08b8cb3",
      "size": 2460,
      "headers": {
        "h1": "Cohere",
        "h2": "CohereEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br></code>`<code> | </code>`<code><br>class CohereEmbedding(BaseEmbedding):<br>    \"\"\"CohereEmbedding uses the Cohere API to generate embeddings for text.\"\"\"<br>    # Instance variables initialized via Pydantic's mechanism<br>    api_key: str = Field(description=\"The Cohere API key.\")<br>    truncate: str = Field(description=\"Truncation type - START/ END/ NONE\")<br>    input_type: Optional[str] = Field(<br>        description=\"Model Input type. If not provided, search_document and search_query are used when needed.\"<br>    )<br>    embedding_type: str = Field(<br>        description=\"Embedding type. If not provided float embedding_type is used when needed.\"<br>    )<br>    _client: cohere.Client = PrivateAttr()<br>    _async_client: cohere.AsyncClient = PrivateAttr()<br>    _base_url: Optional[str] = PrivateAttr()<br>    _timeout: Optional[float] = PrivateAttr()<br>    _httpx_client: Optional[httpx.Client] = PrivateAttr()<br>    _httpx_async_client: Optional[httpx.AsyncClient] = PrivateAttr()<br>    def __init__(<br>        self,<br>        # deprecated<br>        cohere_api_key: Optional[str] = None,<br>        api_key: Optional[str] = None,<br>        model_name: str = \"embed-english-v3.0\",<br>        truncate: str = \"END\",<br>        input_type: Optional[str] = None,<br>        embedding_type: str = \"float\",<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>        base_url: Optional[str] = None,<br>        timeout: Optional[float] = None,<br>        httpx_client: Optional[httpx.Client] = None,<br>        httpx_async_client: Optional[httpx.AsyncClient] = None,<br>        num_workers: Optional[int] = None,<br>        **kwargs: Any,<br>    ):<br>        \"\"\"<br>        A class representation for generating embeddings using the Cohere API.<br>        Args:<br>            truncate (str): A string indicating the truncation strategy to be applied to input text. Possible values<br>                        are 'START', 'END', or 'NONE'.<br>            input_type (Optional[str]): An optional string that specifies the type of input provided to the model.<br>                                    This is model-dependent and could be one of the following: 'search_query',<br>                                    'search_document', 'classification', or 'clustering'.<br>            model_name (str): The name of the model to be used for generating embeddings. The class ensures that<br>                          this model is supported and that the input type provided is compatible with the model.<br>        \"\"\"<br>        # Validate model_name and input_type<br>        if model_name not in VALID_MODEL_INPUT_TYPES:<br>            raise ValueError(f\"{model_name} is not a valid model name\")<br>        if input_type not in VALID_MODEL_INPUT_TYPES[model_name]:<br>            raise ValueError(<br>                f\"{input_type} is not a valid input type for the provided model.\"<br>            )<br>        if embedding_type not in VALID_MODEL_EMBEDDING_TYPES[model_name]:<br>            raise ValueError(<br>                f\"{embedding_type} is not a embedding type for the provided model.\"<br>            )<br>        if truncate not in VALID_TRUNCATE_OPTIONS:<br>            raise ValueError(f\"truncate must be one of {VALID_TRUNCATE_OPTIONS}\")<br>        super().__init__(<br>            api_key=api_key or cohere_api_key,<br>            model_name=model_name,<br>            input_type=input_type,<br>            embedding_type=embedding_type,<br>            truncate=truncate,<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            num_workers=num_workers,<br>            **kwargs,<br>        )<br>        self._client = None<br>        self._async_client = None<br>        self._base_url = base_url<br>        self._timeout = timeout<br>        self._httpx_client = httpx_client<br>        self._httpx_async_client = httpx_async_client<br>    def _get_client(self) -> cohere.Client:<br>        if self._client is None:<br>            self._client = cohere.Client(<br>                api_key=self.api_key,<br>                client_name=\"llama_index\",<br>                base_url=self._base_url,<br>                timeout=self._timeout,<br>                httpx_client=self._httpx_client,<br>            )<br>        return self._client<br>    def _get_async_client(self) -> cohere.AsyncClient:<br>        if self._async_client is None:<br>            self._async_client = cohere.AsyncClient(<br>                api_key=self.api_key,<br>                client_name=\"llama_index\",<br>                base_url=self._base_url,<br>                timeout=self._timeout,<br>                httpx_client=self._httpx_async_client,<br>            )<br>        return self._async_client<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"CohereEmbedding\"<br>    def _embed(self, texts: List[str], input_type: str) -> List[List[float]]:<br>        \"\"\"Embed sentences using Cohere.\"\"\"<br>        client = self._get_client()<br>        if self.model_name in V3_MODELS:<br>            result = client.embed(<br>                texts=texts,<br>                input_type=self.input_type or input_type,<br>                embedding_types=[self.embedding_type],<br>                model=self.model_name,<br>                truncate=self.truncate,<br>            ).embeddings<br>        else:<br>            result = client.embed(<br>                texts=texts,<br>                model=self.model_name,<br>                embedding_types=[self.embedding_type],<br>                truncate=self.truncate,<br>            ).embeddings<br>        return getattr(result, self.embedding_type, None)<br>    async def _aembed(self, texts: List[str], input_type: str) -> List[List[float]]:<br>        \"\"\"Embed sentences using Cohere.\"\"\"<br>        async_client = self._get_async_client()<br>        if self.model_name in V3_MODELS:<br>            result = (<br>                await async_client.embed(<br>                    texts=texts,<br>                    input_type=self.input_type or input_type,<br>                    embedding_types=[self.embedding_type],<br>                    model=self.model_name,<br>                    truncate=self.truncate,<br>                )<br>            ).embeddings<br>        else:<br>            result = (<br>                await async_client.embed(<br>                    texts=texts,<br>                    model=self.model_name,<br>                    embedding_types=[self.embedding_type],<br>                    truncate=self.truncate,<br>                )<br>            ).embeddings<br>        return getattr(result, self.embedding_type, None)<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding. For query embeddings, input_type='search_query'.\"\"\"<br>        return self._embed([query], input_type=\"search_query\")[0]<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding async. For query embeddings, input_type='search_query'.\"\"\"<br>        return (await self._aembed([query], input_type=\"search_query\"))[0]<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._embed([text], input_type=\"search_document\")[0]<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding async.\"\"\"<br>        return (await self._aembed([text], input_type=\"search_document\"))[0]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        return self._embed(texts, input_type=\"search_document\")<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        return await self._aembed(texts, input_type=\"search_document\")<br></code>`` |\n"
    },
    {
      "id": "fe8b76b5-d4e3-4090-be19-4335c4a6f8f9",
      "size": 1600,
      "headers": {
        "h1": "React",
        "h2": "ReActAgent \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br></code>`<code> | </code>``<code><br>class ReActAgent(AgentRunner):<br>    \"\"\"ReAct agent.<br>    Subclasses AgentRunner with a ReActAgentWorker.<br>    For the legacy implementation see:<br>    </code>`<code>python<br>    from llama_index.core.agent.legacy.react.base import ReActAgent<br>    </code>`<code><br>    \"\"\"<br>    def __init__(<br>        self,<br>        tools: Sequence[BaseTool],<br>        llm: LLM,<br>        memory: BaseMemory,<br>        max_iterations: int = 10,<br>        react_chat_formatter: Optional[ReActChatFormatter] = None,<br>        output_parser: Optional[ReActOutputParser] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        context: Optional[str] = None,<br>        handle_reasoning_failure_fn: Optional[<br>            Callable[[CallbackManager, Exception], ToolOutput]<br>        ] = None,<br>    ) -> None:<br>        \"\"\"Init params.\"\"\"<br>        callback_manager = callback_manager or llm.callback_manager<br>        if context and react_chat_formatter:<br>            raise ValueError(\"Cannot provide both context and react_chat_formatter\")<br>        if context:<br>            react_chat_formatter = ReActChatFormatter.from_context(context)<br>        step_engine = ReActAgentWorker.from_tools(<br>            tools=tools,<br>            tool_retriever=tool_retriever,<br>            llm=llm,<br>            max_iterations=max_iterations,<br>            react_chat_formatter=react_chat_formatter,<br>            output_parser=output_parser,<br>            callback_manager=callback_manager,<br>            verbose=verbose,<br>            handle_reasoning_failure_fn=handle_reasoning_failure_fn,<br>        )<br>        super().__init__(<br>            step_engine,<br>            memory=memory,<br>            llm=llm,<br>            callback_manager=callback_manager,<br>            verbose=verbose,<br>        )<br>    @classmethod<br>    def from_tools(<br>        cls,<br>        tools: Optional[List[BaseTool]] = None,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        llm: Optional[LLM] = None,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        memory: Optional[BaseMemory] = None,<br>        memory_cls: Type[BaseMemory] = ChatMemoryBuffer,<br>        max_iterations: int = 10,<br>        react_chat_formatter: Optional[ReActChatFormatter] = None,<br>        output_parser: Optional[ReActOutputParser] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        context: Optional[str] = None,<br>        handle_reasoning_failure_fn: Optional[<br>            Callable[[CallbackManager, Exception], ToolOutput]<br>        ] = None,<br>        **kwargs: Any,<br>    ) -> \"ReActAgent\":<br>        \"\"\"Convenience constructor method from set of BaseTools (Optional).<br>        NOTE: kwargs should have been exhausted by this point. In other words<br>        the various upstream components such as BaseSynthesizer (response synthesizer)<br>        or BaseRetriever should have picked up off their respective kwargs in their<br>        constructions.<br>        If </code>handle_reasoning_failure_fn<code> is provided, when LLM fails to follow the response templates specified in<br>        the System Prompt, this function will be called. This function should provide to the Agent, so that the Agent<br>        can have a second chance to fix its mistakes.<br>        To handle the exception yourself, you can provide a function that raises the </code>Exception<code>.<br>        Note: If you modified any response template in the System Prompt, you should override the method<br>        </code>_extract_reasoning_step<code> in </code>ReActAgentWorker<code>.<br>        Returns:<br>            ReActAgent<br>        \"\"\"<br>        llm = llm or Settings.llm<br>        if callback_manager is not None:<br>            llm.callback_manager = callback_manager<br>        memory = memory or memory_cls.from_defaults(<br>            chat_history=chat_history or [], llm=llm<br>        )<br>        return cls(<br>            tools=tools or [],<br>            tool_retriever=tool_retriever,<br>            llm=llm,<br>            memory=memory,<br>            max_iterations=max_iterations,<br>            react_chat_formatter=react_chat_formatter,<br>            output_parser=output_parser,<br>            callback_manager=callback_manager,<br>            verbose=verbose,<br>            context=context,<br>            handle_reasoning_failure_fn=handle_reasoning_failure_fn,<br>        )<br>    def _get_prompt_modules(self) -> PromptMixinType:<br>        \"\"\"Get prompt modules.\"\"\"<br>        return {\"agent_worker\": self.agent_worker}<br></code>``` |\n"
    },
    {
      "id": "2ab3a6c5-0bd5-4518-88af-0bad4b6eecbd",
      "size": 9084,
      "headers": {
        "h1": "React",
        "h2": "ReActAgentWorker \\#",
        "h3": ""
      },
      "text": "| ``<code><br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>472<br>473<br>474<br>475<br>476<br>477<br>478<br>479<br>480<br>481<br>482<br>483<br>484<br>485<br>486<br>487<br>488<br>489<br>490<br>491<br>492<br>493<br>494<br>495<br>496<br>497<br>498<br>499<br>500<br>501<br>502<br>503<br>504<br>505<br>506<br>507<br>508<br>509<br>510<br>511<br>512<br>513<br>514<br>515<br>516<br>517<br>518<br>519<br>520<br>521<br>522<br>523<br>524<br>525<br>526<br>527<br>528<br>529<br>530<br>531<br>532<br>533<br>534<br>535<br>536<br>537<br>538<br>539<br>540<br>541<br>542<br>543<br>544<br>545<br>546<br>547<br>548<br>549<br>550<br>551<br>552<br>553<br>554<br>555<br>556<br>557<br>558<br>559<br>560<br>561<br>562<br>563<br>564<br>565<br>566<br>567<br>568<br>569<br>570<br>571<br>572<br>573<br>574<br>575<br>576<br>577<br>578<br>579<br>580<br>581<br>582<br>583<br>584<br>585<br>586<br>587<br>588<br>589<br>590<br>591<br>592<br>593<br>594<br>595<br>596<br>597<br>598<br>599<br>600<br>601<br>602<br>603<br>604<br>605<br>606<br>607<br>608<br>609<br>610<br>611<br>612<br>613<br>614<br>615<br>616<br>617<br>618<br>619<br>620<br>621<br>622<br>623<br>624<br>625<br>626<br>627<br>628<br>629<br>630<br>631<br>632<br>633<br>634<br>635<br>636<br>637<br>638<br>639<br>640<br>641<br>642<br>643<br>644<br>645<br>646<br>647<br>648<br>649<br>650<br>651<br>652<br>653<br>654<br>655<br>656<br>657<br>658<br>659<br>660<br>661<br>662<br>663<br>664<br>665<br>666<br>667<br>668<br>669<br>670<br>671<br>672<br>673<br>674<br>675<br>676<br>677<br>678<br>679<br>680<br>681<br>682<br>683<br>684<br>685<br>686<br>687<br>688<br>689<br>690<br>691<br>692<br>693<br>694<br>695<br>696<br>697<br>698<br>699<br>700<br>701<br>702<br>703<br>704<br>705<br>706<br>707<br>708<br>709<br>710<br>711<br>712<br>713<br>714<br>715<br>716<br>717<br>718<br>719<br>720<br>721<br>722<br>723<br>724<br>725<br>726<br>727<br>728<br>729<br>730<br>731<br>732<br>733<br>734<br>735<br>736<br>737<br>738<br>739<br>740<br>741<br>742<br>743<br>744<br>745<br>746<br>747<br>748<br>749<br>750<br>751<br>752<br>753<br>754<br>755<br>756<br>757<br>758<br>759<br>760<br>761<br>762<br>763<br>764<br>765<br>766<br>767<br>768<br>769<br>770<br>771<br>772<br>773<br>774<br>775<br>776<br>777<br>778<br>779<br>780<br>781<br>782<br>783<br>784<br>785<br>786<br>787<br>788<br>789<br>790<br>791<br>792<br>793<br>794<br>795<br>796<br>797<br>798<br>799<br>800<br>801<br>802<br>803<br>804<br>805<br>806<br>807<br>808<br>809<br>810<br>811<br>812<br>813<br>814<br>815<br>816<br>817<br>818<br>819<br>820<br>821<br>822<br>823<br>824<br>825<br>826<br>827<br>828<br>829<br>830<br></code>`<code> | </code>`<code><br>class ReActAgentWorker(BaseAgentWorker):<br>    \"\"\"OpenAI Agent worker.\"\"\"<br>    def __init__(<br>        self,<br>        tools: Sequence[BaseTool],<br>        llm: LLM,<br>        max_iterations: int = 10,<br>        react_chat_formatter: Optional[ReActChatFormatter] = None,<br>        output_parser: Optional[ReActOutputParser] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        handle_reasoning_failure_fn: Optional[<br>            Callable[[CallbackManager, Exception], ToolOutput]<br>        ] = None,<br>    ) -> None:<br>        self._llm = llm<br>        self.callback_manager = callback_manager or llm.callback_manager<br>        self._max_iterations = max_iterations<br>        self._react_chat_formatter = react_chat_formatter or ReActChatFormatter()<br>        self._output_parser = output_parser or ReActOutputParser()<br>        self._verbose = verbose<br>        self._handle_reasoning_failure_fn = (<br>            handle_reasoning_failure_fn<br>            or tell_llm_about_failure_in_extract_reasoning_step<br>        )<br>        if len(tools) > 0 and tool_retriever is not None:<br>            raise ValueError(\"Cannot specify both tools and tool_retriever\")<br>        elif len(tools) > 0:<br>            self._get_tools = lambda _: tools<br>        elif tool_retriever is not None:<br>            tool_retriever_c = cast(ObjectRetriever[BaseTool], tool_retriever)<br>            self._get_tools = lambda message: tool_retriever_c.retrieve(message)<br>        else:<br>            self._get_tools = lambda _: []<br>    @classmethod<br>    def from_tools(<br>        cls,<br>        tools: Optional[Sequence[BaseTool]] = None,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        llm: Optional[LLM] = None,<br>        max_iterations: int = 10,<br>        react_chat_formatter: Optional[ReActChatFormatter] = None,<br>        output_parser: Optional[ReActOutputParser] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        handle_reasoning_failure_fn: Optional[<br>            Callable[[CallbackManager, Exception], ToolOutput]<br>        ] = None,<br>        **kwargs: Any,<br>    ) -> \"ReActAgentWorker\":<br>        \"\"\"Convenience constructor method from set of BaseTools (Optional).<br>        NOTE: kwargs should have been exhausted by this point. In other words<br>        the various upstream components such as BaseSynthesizer (response synthesizer)<br>        or BaseRetriever should have picked up off their respective kwargs in their<br>        constructions.<br>        Returns:<br>            ReActAgentWorker<br>        \"\"\"<br>        llm = llm or Settings.llm<br>        if callback_manager is not None:<br>            llm.callback_manager = callback_manager<br>        return cls(<br>            tools=tools or [],<br>            tool_retriever=tool_retriever,<br>            llm=llm,<br>            max_iterations=max_iterations,<br>            react_chat_formatter=react_chat_formatter,<br>            output_parser=output_parser,<br>            callback_manager=callback_manager,<br>            verbose=verbose,<br>            handle_reasoning_failure_fn=handle_reasoning_failure_fn,<br>        )<br>    def _get_prompts(self) -> PromptDictType:<br>        \"\"\"Get prompts.\"\"\"<br>        # TODO: the ReAct formatter does not explicitly specify PromptTemplate<br>        # objects, but wrap it in this to obey the interface<br>        sys_header = self._react_chat_formatter.system_header<br>        return {\"system_prompt\": PromptTemplate(sys_header)}<br>    def _update_prompts(self, prompts: PromptDictType) -> None:<br>        \"\"\"Update prompts.\"\"\"<br>        if \"system_prompt\" in prompts:<br>            sys_prompt = cast(PromptTemplate, prompts[\"system_prompt\"])<br>            self._react_chat_formatter.system_header = sys_prompt.template<br>    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>        \"\"\"Initialize step from task.\"\"\"<br>        sources: List[ToolOutput] = []<br>        current_reasoning: List[BaseReasoningStep] = []<br>        # temporary memory for new messages<br>        new_memory = ChatMemoryBuffer.from_defaults()<br>        # initialize task state<br>        task_state = {<br>            \"sources\": sources,<br>            \"current_reasoning\": current_reasoning,<br>            \"new_memory\": new_memory,<br>        }<br>        task.extra_state.update(task_state)<br>        return TaskStep(<br>            task_id=task.task_id,<br>            step_id=str(uuid.uuid4()),<br>            input=task.input,<br>            step_state={\"is_first\": True},<br>        )<br>    def get_tools(self, input: str) -> List[AsyncBaseTool]:<br>        \"\"\"Get tools.\"\"\"<br>        return [adapt_to_async_tool(t) for t in self._get_tools(input)]<br>    def _extract_reasoning_step(<br>        self, output: ChatResponse, is_streaming: bool = False<br>    ) -> Tuple[str, List[BaseReasoningStep], bool]:<br>        \"\"\"<br>        Extracts the reasoning step from the given output.<br>        This method parses the message content from the output,<br>        extracts the reasoning step, and determines whether the processing is<br>        complete. It also performs validation checks on the output and<br>        handles possible errors.<br>        \"\"\"<br>        if output.message.content is None:<br>            raise ValueError(\"Got empty message.\")<br>        message_content = output.message.content<br>        current_reasoning = []<br>        try:<br>            reasoning_step = self._output_parser.parse(message_content, is_streaming)<br>        except BaseException as exc:<br>            raise ValueError(f\"Could not parse output: {message_content}\") from exc<br>        if self._verbose:<br>            print_text(f\"{reasoning_step.get_content()}\\n\", color=\"pink\")<br>        current_reasoning.append(reasoning_step)<br>        if reasoning_step.is_done:<br>            return message_content, current_reasoning, True<br>        reasoning_step = cast(ActionReasoningStep, reasoning_step)<br>        if not isinstance(reasoning_step, ActionReasoningStep):<br>            raise ValueError(f\"Expected ActionReasoningStep, got {reasoning_step}\")<br>        return message_content, current_reasoning, False<br>    def _process_actions(<br>        self,<br>        task: Task,<br>        tools: Sequence[AsyncBaseTool],<br>        output: ChatResponse,<br>        is_streaming: bool = False,<br>    ) -> Tuple[List[BaseReasoningStep], bool]:<br>        tools_dict: Dict[str, AsyncBaseTool] = {<br>            tool.metadata.get_name(): tool for tool in tools<br>        }<br>        tool = None<br>        try:<br>            _, current_reasoning, is_done = self._extract_reasoning_step(<br>                output, is_streaming<br>            )<br>        except ValueError as exp:<br>            current_reasoning = []<br>            tool_output = self._handle_reasoning_failure_fn(self.callback_manager, exp)<br>        else:<br>            if is_done:<br>                return current_reasoning, True<br>            # call tool with input<br>            reasoning_step = cast(ActionReasoningStep, current_reasoning[-1])<br>            if reasoning_step.action in tools_dict:<br>                tool = tools_dict[reasoning_step.action]<br>                with self.callback_manager.event(<br>                    CBEventType.FUNCTION_CALL,<br>                    payload={<br>                        EventPayload.FUNCTION_CALL: reasoning_step.action_input,<br>                        EventPayload.TOOL: tool.metadata,<br>                    },<br>                ) as event:<br>                    try:<br>                        dispatcher.event(<br>                            AgentToolCallEvent(<br>                                arguments=json.dumps({**reasoning_step.action_input}),<br>                                tool=tool.metadata,<br>                            )<br>                        )<br>                        tool_output = tool.call(**reasoning_step.action_input)<br>                    except Exception as e:<br>                        tool_output = ToolOutput(<br>                            content=f\"Error: {e!s}\",<br>                            tool_name=tool.metadata.name,<br>                            raw_input={\"kwargs\": reasoning_step.action_input},<br>                            raw_output=e,<br>                            is_error=True,<br>                        )<br>                    event.on_end(<br>                        payload={EventPayload.FUNCTION_OUTPUT: str(tool_output)}<br>                    )<br>            else:<br>                tool_output = self._handle_nonexistent_tool_name(reasoning_step)<br>        task.extra_state[\"sources\"].append(tool_output)<br>        observation_step = ObservationReasoningStep(<br>            observation=str(tool_output),<br>            return_direct=(<br>                tool.metadata.return_direct and not tool_output.is_error<br>                if tool<br>                else False<br>            ),<br>        )<br>        current_reasoning.append(observation_step)<br>        if self._verbose:<br>            print_text(f\"{observation_step.get_content()}\\n\", color=\"blue\")<br>        return (<br>            current_reasoning,<br>            tool.metadata.return_direct and not tool_output.is_error if tool else False,<br>        )<br>    async def _aprocess_actions(<br>        self,<br>        task: Task,<br>        tools: Sequence[AsyncBaseTool],<br>        output: ChatResponse,<br>        is_streaming: bool = False,<br>    ) -> Tuple[List[BaseReasoningStep], bool]:<br>        tools_dict = {tool.metadata.name: tool for tool in tools}<br>        tool = None<br>        try:<br>            _, current_reasoning, is_done = self._extract_reasoning_step(<br>                output, is_streaming<br>            )<br>        except ValueError as exp:<br>            current_reasoning = []<br>            tool_output = self._handle_reasoning_failure_fn(self.callback_manager, exp)<br>        else:<br>            if is_done:<br>                return current_reasoning, True<br>            # call tool with input<br>            reasoning_step = cast(ActionReasoningStep, current_reasoning[-1])<br>            if reasoning_step.action in tools_dict:<br>                tool = tools_dict[reasoning_step.action]<br>                with self.callback_manager.event(<br>                    CBEventType.FUNCTION_CALL,<br>                    payload={<br>                        EventPayload.FUNCTION_CALL: reasoning_step.action_input,<br>                        EventPayload.TOOL: tool.metadata,<br>                    },<br>                ) as event:<br>                    try:<br>                        dispatcher.event(<br>                            AgentToolCallEvent(<br>                                arguments=json.dumps({**reasoning_step.action_input}),<br>                                tool=tool.metadata,<br>                            )<br>                        )<br>                        tool_output = await tool.acall(**reasoning_step.action_input)<br>                    except Exception as e:<br>                        tool_output = ToolOutput(<br>                            content=f\"Error: {e!s}\",<br>                            tool_name=tool.metadata.name,<br>                            raw_input={\"kwargs\": reasoning_step.action_input},<br>                            raw_output=e,<br>                            is_error=True,<br>                        )<br>                    event.on_end(<br>                        payload={EventPayload.FUNCTION_OUTPUT: str(tool_output)}<br>                    )<br>            else:<br>                tool_output = self._handle_nonexistent_tool_name(reasoning_step)<br>        task.extra_state[\"sources\"].append(tool_output)<br>        observation_step = ObservationReasoningStep(<br>            observation=str(tool_output),<br>            return_direct=(<br>                tool.metadata.return_direct and not tool_output.is_error<br>                if tool<br>                else False<br>            ),<br>        )<br>        current_reasoning.append(observation_step)<br>        if self._verbose:<br>            print_text(f\"{observation_step.get_content()}\\n\", color=\"blue\")<br>        return (<br>            current_reasoning,<br>            tool.metadata.return_direct and not tool_output.is_error if tool else False,<br>        )<br>    def _handle_nonexistent_tool_name(<br>        self, reasoning_step: ActionReasoningStep<br>    ) -> ToolOutput:<br>        # We still emit a </code>tool_output<code> object to the task, so that the LLM can know<br>        # it has hallucinated in the next reasoning step.<br>        with self.callback_manager.event(<br>            CBEventType.FUNCTION_CALL,<br>            payload={<br>                EventPayload.FUNCTION_CALL: reasoning_step.action_input,<br>            },<br>        ) as event:<br>            # TODO(L10N): This should be localized.<br>            content = f\"Error: No such tool named </code>{reasoning_step.action}<code>.\"<br>            tool_output = ToolOutput(<br>                content=content,<br>                tool_name=reasoning_step.action,<br>                raw_input={\"kwargs\": reasoning_step.action_input},<br>                raw_output=content,<br>                is_error=True,<br>            )<br>            event.on_end(payload={EventPayload.FUNCTION_OUTPUT: str(tool_output)})<br>        return tool_output<br>    def _get_response(<br>        self,<br>        current_reasoning: List[BaseReasoningStep],<br>        sources: List[ToolOutput],<br>    ) -> AgentChatResponse:<br>        \"\"\"Get response from reasoning steps.\"\"\"<br>        if len(current_reasoning) == 0:<br>            raise ValueError(\"No reasoning steps were taken.\")<br>        elif len(current_reasoning) == self._max_iterations:<br>            raise ValueError(\"Reached max iterations.\")<br>        if isinstance(current_reasoning[-1], ResponseReasoningStep):<br>            response_step = cast(ResponseReasoningStep, current_reasoning[-1])<br>            response_str = response_step.response<br>        elif (<br>            isinstance(current_reasoning[-1], ObservationReasoningStep)<br>            and current_reasoning[-1].return_direct<br>        ):<br>            response_str = current_reasoning[-1].observation<br>        else:<br>            response_str = current_reasoning[-1].get_content()<br>        # TODO: add sources from reasoning steps<br>        return AgentChatResponse(response=response_str, sources=sources)<br>    def _get_task_step_response(<br>        self, agent_response: AGENT_CHAT_RESPONSE_TYPE, step: TaskStep, is_done: bool<br>    ) -> TaskStepOutput:<br>        \"\"\"Get task step response.\"\"\"<br>        if is_done:<br>            new_steps = []<br>        else:<br>            new_steps = [<br>                step.get_next_step(<br>                    step_id=str(uuid.uuid4()),<br>                    # NOTE: input is unused<br>                    input=None,<br>                )<br>            ]<br>        return TaskStepOutput(<br>            output=agent_response,<br>            task_step=step,<br>            is_last=is_done,<br>            next_steps=new_steps,<br>        )<br>    def _infer_stream_chunk_is_final(<br>        self, chunk: ChatResponse, missed_chunks_storage: list<br>    ) -> bool:<br>        \"\"\"Infers if a chunk from a live stream is the start of the final<br>        reasoning step. (i.e., and should eventually become<br>        ResponseReasoningStep â€” not part of this function's logic tho.).<br>        Args:<br>            chunk (ChatResponse): the current chunk stream to check<br>            missed_chunks_storage (list): list to store missed chunks<br>        Returns:<br>            bool: Boolean on whether the chunk is the start of the final response<br>        \"\"\"<br>        latest_content = chunk.message.content<br>        if latest_content:<br>            # doesn't follow thought-action format<br>            # keep first chunks<br>            if len(latest_content) < len(\"Thought\"):<br>                missed_chunks_storage.append(chunk)<br>            elif not latest_content.startswith(\"Thought\"):<br>                return True<br>            elif \"Answer: \" in latest_content:<br>                missed_chunks_storage.clear()<br>                return True<br>        return False<br>    def _add_back_chunk_to_stream(<br>        self,<br>        chunks: List[ChatResponse],<br>        chat_stream: Generator[ChatResponse, None, None],<br>    ) -> Generator[ChatResponse, None, None]:<br>        \"\"\"Helper method for adding back initial chunk stream of final response<br>        back to the rest of the chat_stream.<br>        Args:<br>            chunks List[ChatResponse]: the chunks to add back to the beginning of the<br>                                    chat_stream.<br>        Return:<br>            Generator[ChatResponse, None, None]: the updated chat_stream<br>        \"\"\"<br>        def gen() -> Generator[ChatResponse, None, None]:<br>            yield from chunks<br>            yield from chat_stream<br>        return gen()<br>    async def _async_add_back_chunk_to_stream(<br>        self,<br>        chunks: List[ChatResponse],<br>        chat_stream: AsyncGenerator[ChatResponse, None],<br>    ) -> AsyncGenerator[ChatResponse, None]:<br>        \"\"\"Helper method for adding back initial chunk stream of final response<br>        back to the rest of the chat_stream.<br>        NOTE: this itself is not an async function.<br>        Args:<br>            chunks List[ChatResponse]: the chunks to add back to the beginning of the<br>                                    chat_stream.<br>        Return:<br>            AsyncGenerator[ChatResponse, None]: the updated async chat_stream<br>        \"\"\"<br>        for chunk in chunks:<br>            yield chunk<br>        async for item in chat_stream:<br>            yield item<br>    def _run_step(<br>        self,<br>        step: TaskStep,<br>        task: Task,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        if step.input is not None:<br>            add_user_step_to_reasoning(<br>                step,<br>                task.extra_state[\"new_memory\"],<br>                task.extra_state[\"current_reasoning\"],<br>                verbose=self._verbose,<br>            )<br>        # TODO: see if we want to do step-based inputs<br>        tools = self.get_tools(task.input)<br>        input_chat = self._react_chat_formatter.format(<br>            tools,<br>            chat_history=task.memory.get(input=task.input)<br>            + task.extra_state[\"new_memory\"].get_all(),<br>            current_reasoning=task.extra_state[\"current_reasoning\"],<br>        )<br>        # send prompt<br>        chat_response = self._llm.chat(input_chat)<br>        # given react prompt outputs, call tools or return response<br>        reasoning_steps, is_done = self._process_actions(<br>            task, tools, output=chat_response<br>        )<br>        task.extra_state[\"current_reasoning\"].extend(reasoning_steps)<br>        agent_response = self._get_response(<br>            task.extra_state[\"current_reasoning\"], task.extra_state[\"sources\"]<br>        )<br>        if is_done:<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(content=agent_response.response, role=MessageRole.ASSISTANT)<br>            )<br>        return self._get_task_step_response(agent_response, step, is_done)<br>    async def _arun_step(<br>        self,<br>        step: TaskStep,<br>        task: Task,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        if step.input is not None:<br>            add_user_step_to_reasoning(<br>                step,<br>                task.extra_state[\"new_memory\"],<br>                task.extra_state[\"current_reasoning\"],<br>                verbose=self._verbose,<br>            )<br>        # TODO: see if we want to do step-based inputs<br>        tools = self.get_tools(task.input)<br>        input_chat = self._react_chat_formatter.format(<br>            tools,<br>            chat_history=task.memory.get(input=task.input)<br>            + task.extra_state[\"new_memory\"].get_all(),<br>            current_reasoning=task.extra_state[\"current_reasoning\"],<br>        )<br>        # send prompt<br>        chat_response = await self._llm.achat(input_chat)<br>        # given react prompt outputs, call tools or return response<br>        reasoning_steps, is_done = await self._aprocess_actions(<br>            task, tools, output=chat_response<br>        )<br>        task.extra_state[\"current_reasoning\"].extend(reasoning_steps)<br>        agent_response = self._get_response(<br>            task.extra_state[\"current_reasoning\"], task.extra_state[\"sources\"]<br>        )<br>        if is_done:<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(content=agent_response.response, role=MessageRole.ASSISTANT)<br>            )<br>        return self._get_task_step_response(agent_response, step, is_done)<br>    def _run_step_stream(<br>        self,<br>        step: TaskStep,<br>        task: Task,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        if step.input is not None:<br>            add_user_step_to_reasoning(<br>                step,<br>                task.extra_state[\"new_memory\"],<br>                task.extra_state[\"current_reasoning\"],<br>                verbose=self._verbose,<br>            )<br>        # TODO: see if we want to do step-based inputs<br>        tools = self.get_tools(task.input)<br>        input_chat = self._react_chat_formatter.format(<br>            tools,<br>            chat_history=task.memory.get(input=task.input)<br>            + task.extra_state[\"new_memory\"].get_all(),<br>            current_reasoning=task.extra_state[\"current_reasoning\"],<br>        )<br>        chat_stream = self._llm.stream_chat(input_chat)<br>        # iterate over stream, break out if is final answer after the \"Answer: \"<br>        full_response = ChatResponse(<br>            message=ChatMessage(content=None, role=\"assistant\")<br>        )<br>        missed_chunks_storage: List[ChatResponse] = []<br>        is_done = False<br>        for latest_chunk in chat_stream:<br>            full_response = latest_chunk<br>            is_done = self._infer_stream_chunk_is_final(<br>                latest_chunk, missed_chunks_storage<br>            )<br>            if is_done:<br>                break<br>        non_streaming_agent_response = None<br>        agent_response_stream = None<br>        if not is_done:<br>            # given react prompt outputs, call tools or return response<br>            reasoning_steps, is_done = self._process_actions(<br>                task, tools=tools, output=full_response, is_streaming=True<br>            )<br>            task.extra_state[\"current_reasoning\"].extend(reasoning_steps)<br>            # use _get_response to return intermediate response<br>            non_streaming_agent_response = self._get_response(<br>                task.extra_state[\"current_reasoning\"], task.extra_state[\"sources\"]<br>            )<br>            if is_done:<br>                non_streaming_agent_response.is_dummy_stream = True<br>                task.extra_state[\"new_memory\"].put(<br>                    ChatMessage(<br>                        content=non_streaming_agent_response.response,<br>                        role=MessageRole.ASSISTANT,<br>                    )<br>                )<br>        else:<br>            # Get the response in a separate thread so we can yield the response<br>            response_stream = self._add_back_chunk_to_stream(<br>                chunks=[*missed_chunks_storage, latest_chunk], chat_stream=chat_stream<br>            )<br>            agent_response_stream = StreamingAgentChatResponse(<br>                chat_stream=response_stream,<br>                sources=task.extra_state[\"sources\"],<br>            )<br>            thread = Thread(<br>                target=agent_response_stream.write_response_to_history,<br>                args=(task.extra_state[\"new_memory\"],),<br>                kwargs={\"on_stream_end_fn\": partial(self.finalize_task, task)},<br>            )<br>            thread.start()<br>        response = agent_response_stream or non_streaming_agent_response<br>        assert response is not None<br>        return self._get_task_step_response(response, step, is_done)<br>    async def _arun_step_stream(<br>        self,<br>        step: TaskStep,<br>        task: Task,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        if step.input is not None:<br>            add_user_step_to_reasoning(<br>                step,<br>                task.extra_state[\"new_memory\"],<br>                task.extra_state[\"current_reasoning\"],<br>                verbose=self._verbose,<br>            )<br>        # TODO: see if we want to do step-based inputs<br>        tools = self.get_tools(task.input)<br>        input_chat = self._react_chat_formatter.format(<br>            tools,<br>            chat_history=task.memory.get(input=task.input)<br>            + task.extra_state[\"new_memory\"].get_all(),<br>            current_reasoning=task.extra_state[\"current_reasoning\"],<br>        )<br>        chat_stream = await self._llm.astream_chat(input_chat)<br>        # iterate over stream, break out if is final answer after the \"Answer: \"<br>        full_response = ChatResponse(<br>            message=ChatMessage(content=None, role=\"assistant\")<br>        )<br>        missed_chunks_storage: List[ChatResponse] = []<br>        is_done = False<br>        async for latest_chunk in chat_stream:<br>            full_response = latest_chunk<br>            is_done = self._infer_stream_chunk_is_final(<br>                latest_chunk, missed_chunks_storage<br>            )<br>            if is_done:<br>                break<br>        non_streaming_agent_response = None<br>        agent_response_stream = None<br>        if not is_done:<br>            # given react prompt outputs, call tools or return response<br>            reasoning_steps, is_done = await self._aprocess_actions(<br>                task, tools=tools, output=full_response, is_streaming=True<br>            )<br>            task.extra_state[\"current_reasoning\"].extend(reasoning_steps)<br>            # use _get_response to return intermediate response<br>            non_streaming_agent_response = self._get_response(<br>                task.extra_state[\"current_reasoning\"], task.extra_state[\"sources\"]<br>            )<br>            if is_done:<br>                non_streaming_agent_response.is_dummy_stream = True<br>                task.extra_state[\"new_memory\"].put(<br>                    ChatMessage(<br>                        content=non_streaming_agent_response.response,<br>                        role=MessageRole.ASSISTANT,<br>                    )<br>                )<br>        else:<br>            # Get the response in a separate thread so we can yield the response<br>            response_stream = self._async_add_back_chunk_to_stream(<br>                chunks=[*missed_chunks_storage, latest_chunk], chat_stream=chat_stream<br>            )<br>            agent_response_stream = StreamingAgentChatResponse(<br>                achat_stream=response_stream,<br>                sources=task.extra_state[\"sources\"],<br>            )<br>            # create task to write chat response to history<br>            asyncio.create_task(<br>                agent_response_stream.awrite_response_to_history(<br>                    task.extra_state[\"new_memory\"],<br>                    on_stream_end_fn=partial(self.finalize_task, task),<br>                )<br>            )<br>            # wait until response writing is done<br>            agent_response_stream._ensure_async_setup()<br>            assert agent_response_stream.is_function_false_event is not None<br>            await agent_response_stream.is_function_false_event.wait()<br>        response = agent_response_stream or non_streaming_agent_response<br>        assert response is not None<br>        return self._get_task_step_response(response, step, is_done)<br>    @trace_method(\"run_step\")<br>    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        return self._run_step(step, task)<br>    @trace_method(\"run_step\")<br>    async def arun_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        return await self._arun_step(step, task)<br>    @trace_method(\"run_step\")<br>    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        # TODO: figure out if we need a different type for TaskStepOutput<br>        return self._run_step_stream(step, task)<br>    @trace_method(\"run_step\")<br>    async def astream_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async stream).\"\"\"<br>        return await self._arun_step_stream(step, task)<br>    def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>        \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>        # add new messages to memory<br>        task.memory.set(<br>            task.memory.get_all() + task.extra_state[\"new_memory\"].get_all()<br>        )<br>        # reset new memory<br>        task.extra_state[\"new_memory\"].reset()<br>    def set_callback_manager(self, callback_manager: CallbackManager) -> None:<br>        \"\"\"Set callback manager.\"\"\"<br>        # TODO: make this abstractmethod (right now will break some agent impls)<br>        self.callback_manager = callback_manager<br></code>`` |\n"
    },
    {
      "id": "b927a0b1-3cbc-4e95-ac49-9d2a04855a3e",
      "size": 3041,
      "headers": {
        "h1": "Core Callback Classes \\#",
        "h2": "CallbackManager \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br></code>`<code> | </code>`<code><br>class CallbackManager(BaseCallbackHandler, ABC):<br>    \"\"\"<br>    Callback manager that handles callbacks for events within LlamaIndex.<br>    The callback manager provides a way to call handlers on event starts/ends.<br>    Additionally, the callback manager traces the current stack of events.<br>    It does this by using a few key attributes.<br>    - trace_stack - The current stack of events that have not ended yet.<br>                    When an event ends, it's removed from the stack.<br>                    Since this is a contextvar, it is unique to each<br>                    thread/task.<br>    - trace_map - A mapping of event ids to their children events.<br>                  On the start of events, the bottom of the trace stack<br>                  is used as the current parent event for the trace map.<br>    - trace_id - A simple name for the current trace, usually denoting the<br>                 entrypoint (query, index_construction, insert, etc.)<br>    Args:<br>        handlers (List[BaseCallbackHandler]): list of handlers to use.<br>    Usage:<br>        with callback_manager.event(CBEventType.QUERY) as event:<br>            event.on_start(payload={key, val})<br>            ...<br>            event.on_end(payload={key, val})<br>    \"\"\"<br>    def __init__(self, handlers: Optional[List[BaseCallbackHandler]] = None):<br>        \"\"\"Initialize the manager with a list of handlers.\"\"\"<br>        from llama_index.core import global_handler<br>        handlers = handlers or []<br>        # add eval handlers based on global defaults<br>        if global_handler is not None:<br>            new_handler = global_handler<br>            # go through existing handlers, check if any are same type as new handler<br>            # if so, error<br>            for existing_handler in handlers:<br>                if isinstance(existing_handler, type(new_handler)):<br>                    raise ValueError(<br>                        \"Cannot add two handlers of the same type \"<br>                        f\"{type(new_handler)} to the callback manager.\"<br>                    )<br>            handlers.append(new_handler)<br>        # if we passed in no handlers, use the global default<br>        if len(handlers) == 0:<br>            from llama_index.core.settings import Settings<br>            # hidden var access to prevent recursion in getter<br>            cb_manager = Settings._callback_manager<br>            if cb_manager is not None:<br>                handlers = cb_manager.handlers<br>        self.handlers: List[BaseCallbackHandler] = handlers<br>        self._trace_map: Dict[str, List[str]] = defaultdict(list)<br>    def on_event_start(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: Optional[str] = None,<br>        parent_id: Optional[str] = None,<br>        **kwargs: Any,<br>    ) -> str:<br>        \"\"\"Run handlers when an event starts and return id of event.\"\"\"<br>        event_id = event_id or str(uuid.uuid4())<br>        # if no trace is running, start a default trace<br>        try:<br>            parent_id = parent_id or global_stack_trace.get()[-1]<br>        except IndexError:<br>            self.start_trace(\"llama-index\")<br>            parent_id = global_stack_trace.get()[-1]<br>        parent_id = cast(str, parent_id)<br>        self._trace_map[parent_id].append(event_id)<br>        for handler in self.handlers:<br>            if event_type not in handler.event_starts_to_ignore:<br>                handler.on_event_start(<br>                    event_type,<br>                    payload,<br>                    event_id=event_id,<br>                    parent_id=parent_id,<br>                    **kwargs,<br>                )<br>        if event_type not in LEAF_EVENTS:<br>            # copy the stack trace to prevent conflicts with threads/coroutines<br>            current_trace_stack = global_stack_trace.get().copy()<br>            current_trace_stack.append(event_id)<br>            global_stack_trace.set(current_trace_stack)<br>        return event_id<br>    def on_event_end(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: Optional[str] = None,<br>        **kwargs: Any,<br>    ) -> None:<br>        \"\"\"Run handlers when an event ends.\"\"\"<br>        event_id = event_id or str(uuid.uuid4())<br>        for handler in self.handlers:<br>            if event_type not in handler.event_ends_to_ignore:<br>                handler.on_event_end(event_type, payload, event_id=event_id, **kwargs)<br>        if event_type not in LEAF_EVENTS:<br>            # copy the stack trace to prevent conflicts with threads/coroutines<br>            current_trace_stack = global_stack_trace.get().copy()<br>            current_trace_stack.pop()<br>            global_stack_trace.set(current_trace_stack)<br>    def add_handler(self, handler: BaseCallbackHandler) -> None:<br>        \"\"\"Add a handler to the callback manager.\"\"\"<br>        self.handlers.append(handler)<br>    def remove_handler(self, handler: BaseCallbackHandler) -> None:<br>        \"\"\"Remove a handler from the callback manager.\"\"\"<br>        self.handlers.remove(handler)<br>    def set_handlers(self, handlers: List[BaseCallbackHandler]) -> None:<br>        \"\"\"Set handlers as the only handlers on the callback manager.\"\"\"<br>        self.handlers = handlers<br>    @contextmanager<br>    def event(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: Optional[str] = None,<br>    ) -> Generator[\"EventContext\", None, None]:<br>        \"\"\"Context manager for lanching and shutdown of events.<br>        Handles sending on_evnt_start and on_event_end to handlers for specified event.<br>        Usage:<br>            with callback_manager.event(CBEventType.QUERY, payload={key, val}) as event:<br>                ...<br>                event.on_end(payload={key, val})  # optional<br>        \"\"\"<br>        # create event context wrapper<br>        event = EventContext(self, event_type, event_id=event_id)<br>        event.on_start(payload=payload)<br>        payload = None<br>        try:<br>            yield event<br>        except Exception as e:<br>            # data already logged to trace?<br>            if not hasattr(e, \"event_added\"):<br>                payload = {EventPayload.EXCEPTION: e}<br>                e.event_added = True  # type: ignore<br>                if not event.finished:<br>                    event.on_end(payload=payload)<br>            raise<br>        finally:<br>            # ensure event is ended<br>            if not event.finished:<br>                event.on_end(payload=payload)<br>    @contextmanager<br>    def as_trace(self, trace_id: str) -> Generator[None, None, None]:<br>        \"\"\"Context manager tracer for lanching and shutdown of traces.\"\"\"<br>        self.start_trace(trace_id=trace_id)<br>        try:<br>            yield<br>        except Exception as e:<br>            # event already added to trace?<br>            if not hasattr(e, \"event_added\"):<br>                self.on_event_start(<br>                    CBEventType.EXCEPTION, payload={EventPayload.EXCEPTION: e}<br>                )<br>                e.event_added = True  # type: ignore<br>            raise<br>        finally:<br>            # ensure trace is ended<br>            self.end_trace(trace_id=trace_id)<br>    def start_trace(self, trace_id: Optional[str] = None) -> None:<br>        \"\"\"Run when an overall trace is launched.\"\"\"<br>        current_trace_stack_ids = global_stack_trace_ids.get().copy()<br>        if trace_id is not None:<br>            if len(current_trace_stack_ids) == 0:<br>                self._reset_trace_events()<br>                for handler in self.handlers:<br>                    handler.start_trace(trace_id=trace_id)<br>                current_trace_stack_ids = [trace_id]<br>            else:<br>                current_trace_stack_ids.append(trace_id)<br>        global_stack_trace_ids.set(current_trace_stack_ids)<br>    def end_trace(<br>        self,<br>        trace_id: Optional[str] = None,<br>        trace_map: Optional[Dict[str, List[str]]] = None,<br>    ) -> None:<br>        \"\"\"Run when an overall trace is exited.\"\"\"<br>        current_trace_stack_ids = global_stack_trace_ids.get().copy()<br>        if trace_id is not None and len(current_trace_stack_ids) > 0:<br>            current_trace_stack_ids.pop()<br>            if len(current_trace_stack_ids) == 0:<br>                for handler in self.handlers:<br>                    handler.end_trace(trace_id=trace_id, trace_map=self._trace_map)<br>                current_trace_stack_ids = []<br>        global_stack_trace_ids.set(current_trace_stack_ids)<br>    def _reset_trace_events(self) -> None:<br>        \"\"\"Helper function to reset the current trace.\"\"\"<br>        self._trace_map = defaultdict(list)<br>        global_stack_trace.set([BASE_TRACE_EVENT])<br>    @property<br>    def trace_map(self) -> Dict[str, List[str]]:<br>        return self._trace_map<br>    @classmethod<br>    def __get_pydantic_core_schema__(<br>        cls, source: Type[Any], handler: GetCoreSchemaHandler<br>    ) -> CoreSchema:<br>        return core_schema.any_schema()<br>    @classmethod<br>    def __get_pydantic_json_schema__(<br>        cls, core_schema: CoreSchema, handler: GetJsonSchemaHandler<br>    ) -> Dict[str, Any]:<br>        json_schema = handler(core_schema)<br>        return handler.resolve_ref_schema(json_schema)<br></code>`` |\n"
    },
    {
      "id": "185698c7-885a-42a0-9480-cb9a96fc60bc",
      "size": 2876,
      "headers": {
        "h1": "Coa",
        "h2": "CoAAgentWorker \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br></code>`<code> | </code>`<code><br>class CoAAgentWorker(BaseAgentWorker):<br>    \"\"\"Chain-of-abstraction Agent Worker.\"\"\"<br>    def __init__(<br>        self,<br>        llm: LLM,<br>        reasoning_prompt_template: str,<br>        refine_reasoning_prompt_template: str,<br>        output_parser: BaseOutputParser,<br>        tools: Optional[Sequence[BaseTool]] = None,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>    ) -> None:<br>        self.llm = llm<br>        self.callback_manager = callback_manager or llm.callback_manager<br>        if tools is None and tool_retriever is None:<br>            raise ValueError(\"Either tools or tool_retriever must be provided.\")<br>        self.tools = tools<br>        self.tool_retriever = tool_retriever<br>        self.reasoning_prompt_template = reasoning_prompt_template<br>        self.refine_reasoning_prompt_template = refine_reasoning_prompt_template<br>        self.output_parser = output_parser<br>        self.verbose = verbose<br>    @classmethod<br>    def from_tools(<br>        cls,<br>        tools: Optional[Sequence[BaseTool]] = None,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        llm: Optional[LLM] = None,<br>        reasoning_prompt_template: Optional[str] = None,<br>        refine_reasoning_prompt_template: Optional[str] = None,<br>        output_parser: Optional[BaseOutputParser] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        **kwargs: Any,<br>    ) -> \"CoAAgentWorker\":<br>        \"\"\"Convenience constructor method from set of of BaseTools (Optional).<br>        Returns:<br>            LLMCompilerAgentWorker: the LLMCompilerAgentWorker instance<br>        \"\"\"<br>        llm = llm or Settings.llm<br>        if callback_manager is not None:<br>            llm.callback_manager = callback_manager<br>        reasoning_prompt_template = (<br>            reasoning_prompt_template or REASONING_PROMPT_TEMPALTE<br>        )<br>        refine_reasoning_prompt_template = (<br>            refine_reasoning_prompt_template or REFINE_REASONING_PROMPT_TEMPALTE<br>        )<br>        output_parser = output_parser or ChainOfAbstractionParser(verbose=verbose)<br>        return cls(<br>            llm,<br>            reasoning_prompt_template,<br>            refine_reasoning_prompt_template,<br>            output_parser,<br>            tools=tools,<br>            tool_retriever=tool_retriever,<br>            callback_manager=callback_manager,<br>            verbose=verbose,<br>        )<br>    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>        \"\"\"Initialize step from task.\"\"\"<br>        sources: List[ToolOutput] = []<br>        # temporary memory for new messages<br>        new_memory = ChatMemoryBuffer.from_defaults()<br>        # put current history in new memory<br>        messages = task.memory.get(input=task.input)<br>        for message in messages:<br>            new_memory.put(message)<br>        # initialize task state<br>        task_state = {<br>            \"sources\": sources,<br>            \"new_memory\": new_memory,<br>        }<br>        task.extra_state.update(task_state)<br>        return TaskStep(<br>            task_id=task.task_id,<br>            step_id=str(uuid.uuid4()),<br>            input=task.input,<br>            step_state={\"prev_reasoning\": \"\"},<br>        )<br>    def get_tools(self, query_str: str) -> List[AsyncBaseTool]:<br>        \"\"\"Get tools.\"\"\"<br>        if self.tool_retriever:<br>            tools = self.tool_retriever.retrieve(query_str)<br>        else:<br>            tools = self.tools<br>        return [adapt_to_async_tool(t) for t in tools]<br>    async def _arun_step(<br>        self,<br>        step: TaskStep,<br>        task: Task,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        tools = self.get_tools(task.input)<br>        tools_by_name = {tool.metadata.name: tool for tool in tools}<br>        tools_strs = []<br>        for tool in tools:<br>            if isinstance(tool, FunctionTool):<br>                description = tool.metadata.description<br>                # remove function def, we will make our own<br>                if \"def \" in description:<br>                    description = \"\\n\".join(description.split(\"\\n\")[1:])<br>            else:<br>                description = tool.metadata.description<br>            tool_str = json_schema_to_python(<br>                tool.metadata.fn_schema_str, tool.metadata.name, description=description<br>            )<br>            tools_strs.append(tool_str)<br>        prev_reasoning = step.step_state.get(\"prev_reasoning\", \"\")<br>        # show available functions if first step<br>        if self.verbose and not prev_reasoning:<br>            print(f\"==== Available Parsed Functions ====\")<br>            for tool_str in tools_strs:<br>                print(tool_str)<br>        if not prev_reasoning:<br>            # get the reasoning prompt<br>            reasoning_prompt = self.reasoning_prompt_template.format(<br>                functions=\"\\n\".join(tools_strs), question=step.input<br>            )<br>        else:<br>            # get the refine reasoning prompt<br>            reasoning_prompt = self.refine_reasoning_prompt_template.format(<br>                question=step.input, prev_reasoning=prev_reasoning<br>            )<br>        messages = task.extra_state[\"new_memory\"].get()<br>        reasoning_message = ChatMessage(role=\"user\", content=reasoning_prompt)<br>        messages.append(reasoning_message)<br>        # run the reasoning prompt<br>        response = await self.llm.achat(messages)<br>        # print the chain of abstraction if first step<br>        if self.verbose and not prev_reasoning:<br>            print(f\"==== Generated Chain of Abstraction ====\")<br>            print(str(response.message.content))<br>        # parse the output, run functions<br>        parsed_response, tool_sources = await self.output_parser.aparse(<br>            response.message.content, tools_by_name<br>        )<br>        if len(tool_sources) == 0 or prev_reasoning:<br>            is_done = True<br>            new_steps = []<br>            # only add to memory when we are done<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(role=\"user\", content=task.input)<br>            )<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(role=\"assistant\", content=parsed_response)<br>            )<br>        else:<br>            is_done = False<br>            new_steps = [<br>                TaskStep(<br>                    task_id=task.task_id,<br>                    step_id=str(uuid.uuid4()),<br>                    input=task.input,<br>                    step_state={<br>                        \"prev_reasoning\": parsed_response,<br>                    },<br>                )<br>            ]<br>        agent_response = AgentChatResponse(<br>            response=parsed_response, sources=tool_sources<br>        )<br>        return TaskStepOutput(<br>            output=agent_response,<br>            task_step=step,<br>            is_last=is_done,<br>            next_steps=new_steps,<br>        )<br>    @trace_method(\"run_step\")<br>    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        return asyncio.run(self.arun_step(step=step, task=task, **kwargs))<br>    @trace_method(\"run_step\")<br>    async def arun_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        return await self._arun_step(step, task)<br>    @trace_method(\"run_step\")<br>    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        # Streaming isn't really possible, because we need the full response to know if we are done<br>        raise NotImplementedError<br>    @trace_method(\"run_step\")<br>    async def astream_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async stream).\"\"\"<br>        # Streaming isn't really possible, because we need the full response to know if we are done<br>        raise NotImplementedError<br>    def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>        \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>        # add new messages to memory<br>        task.memory.put_messages(task.extra_state[\"new_memory\"].get_all())<br>        # reset new memory<br>        task.extra_state[\"new_memory\"].reset()<br></code>`` |\n"
    },
    {
      "id": "c1b72681-ad75-4271-b23d-1fd180b073cd",
      "size": 1094,
      "headers": {
        "h1": "Guideline",
        "h2": "GuidelineEvaluator \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br></code>`<code> | </code>`<code><br>class GuidelineEvaluator(BaseEvaluator):<br>    \"\"\"Guideline evaluator.<br>    Evaluates whether a query and response pair passes the given guidelines.<br>    This evaluator only considers the query string and the response string.<br>    Args:<br>        guidelines(Optional[str]): User-added guidelines to use for evaluation.<br>            Defaults to None, which uses the default guidelines.<br>        eval_template(Optional[Union[str, BasePromptTemplate]] ):<br>            The template to use for evaluation.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        llm: Optional[LLM] = None,<br>        guidelines: Optional[str] = None,<br>        eval_template: Optional[Union[str, BasePromptTemplate]] = None,<br>        output_parser: Optional[PydanticOutputParser] = None,<br>    ) -> None:<br>        self._llm = llm or Settings.llm<br>        self._guidelines = guidelines or DEFAULT_GUIDELINES<br>        self._eval_template: BasePromptTemplate<br>        if isinstance(eval_template, str):<br>            self._eval_template = PromptTemplate(eval_template)<br>        else:<br>            self._eval_template = eval_template or DEFAULT_EVAL_TEMPLATE<br>        self._output_parser = output_parser or PydanticOutputParser(<br>            output_cls=EvaluationData<br>        )<br>        self._eval_template.output_parser = self._output_parser<br>    def _get_prompts(self) -> PromptDictType:<br>        \"\"\"Get prompts.\"\"\"<br>        return {<br>            \"eval_template\": self._eval_template,<br>        }<br>    def _update_prompts(self, prompts: PromptDictType) -> None:<br>        \"\"\"Update prompts.\"\"\"<br>        if \"eval_template\" in prompts:<br>            self._eval_template = prompts[\"eval_template\"]<br>    async def aevaluate(<br>        self,<br>        query: Optional[str] = None,<br>        response: Optional[str] = None,<br>        contexts: Optional[Sequence[str]] = None,<br>        sleep_time_in_seconds: int = 0,<br>        **kwargs: Any,<br>    ) -> EvaluationResult:<br>        \"\"\"Evaluate whether the query and response pair passes the guidelines.\"\"\"<br>        del contexts  # Unused<br>        del kwargs  # Unused<br>        if query is None or response is None:<br>            raise ValueError(\"query and response must be provided\")<br>        logger.debug(\"prompt: %s\", self._eval_template)<br>        logger.debug(\"query: %s\", query)<br>        logger.debug(\"response: %s\", response)<br>        logger.debug(\"guidelines: %s\", self._guidelines)<br>        await asyncio.sleep(sleep_time_in_seconds)<br>        eval_response = await self._llm.apredict(<br>            self._eval_template,<br>            query=query,<br>            response=response,<br>            guidelines=self._guidelines,<br>        )<br>        eval_data = self._output_parser.parse(eval_response)<br>        eval_data = cast(EvaluationData, eval_data)<br>        return EvaluationResult(<br>            query=query,<br>            response=response,<br>            passing=eval_data.passing,<br>            score=1.0 if eval_data.passing else 0.0,<br>            feedback=eval_data.feedback,<br>        )<br></code>`` |\n"
    },
    {
      "id": "3879eab2-2d1a-4556-9226-082c2d9e0d08",
      "size": 1192,
      "headers": {
        "h1": "Answer relevancy",
        "h2": "AnswerRelevancyEvaluator \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br></code>`<code> | </code>`<code><br>class AnswerRelevancyEvaluator(BaseEvaluator):<br>    \"\"\"Answer relevancy evaluator.<br>    Evaluates the relevancy of response to a query.<br>    This evaluator considers the query string and response string.<br>    Args:<br>        raise_error(Optional[bool]):<br>            Whether to raise an error if the response is invalid.<br>            Defaults to False.<br>        eval_template(Optional[Union[str, BasePromptTemplate]]):<br>            The template to use for evaluation.<br>        refine_template(Optional[Union[str, BasePromptTemplate]]):<br>            The template to use for refinement.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        llm: Optional[LLM] = None,<br>        raise_error: bool = False,<br>        eval_template: str | BasePromptTemplate | None = None,<br>        score_threshold: float = _DEFAULT_SCORE_THRESHOLD,<br>        parser_function: Callable[<br>            [str], Tuple[Optional[float], Optional[str]]<br>        ] = _default_parser_function,<br>    ) -> None:<br>        \"\"\"Init params.\"\"\"<br>        self._llm = llm or Settings.llm<br>        self._raise_error = raise_error<br>        self._eval_template: BasePromptTemplate<br>        if isinstance(eval_template, str):<br>            self._eval_template = PromptTemplate(eval_template)<br>        else:<br>            self._eval_template = eval_template or DEFAULT_EVAL_TEMPLATE<br>        self.parser_function = parser_function<br>        self.score_threshold = score_threshold<br>    def _get_prompts(self) -> PromptDictType:<br>        \"\"\"Get prompts.\"\"\"<br>        return {<br>            \"eval_template\": self._eval_template,<br>            \"refine_template\": self._refine_template,<br>        }<br>    def _update_prompts(self, prompts: PromptDictType) -> None:<br>        \"\"\"Update prompts.\"\"\"<br>        if \"eval_template\" in prompts:<br>            self._eval_template = prompts[\"eval_template\"]<br>        if \"refine_template\" in prompts:<br>            self._refine_template = prompts[\"refine_template\"]<br>    async def aevaluate(<br>        self,<br>        query: str | None = None,<br>        response: str | None = None,<br>        contexts: Sequence[str] | None = None,<br>        sleep_time_in_seconds: int = 0,<br>        **kwargs: Any,<br>    ) -> EvaluationResult:<br>        \"\"\"Evaluate whether the response is relevant to the query.\"\"\"<br>        del kwargs  # Unused<br>        del contexts  # Unused<br>        if query is None or response is None:<br>            raise ValueError(\"query and response must be provided\")<br>        await asyncio.sleep(sleep_time_in_seconds)<br>        eval_response = await self._llm.apredict(<br>            prompt=self._eval_template,<br>            query=query,<br>            response=response,<br>        )<br>        score, reasoning = self.parser_function(eval_response)<br>        invalid_result, invalid_reason = False, None<br>        if score is None and reasoning is None:<br>            if self._raise_error:<br>                raise ValueError(\"The response is invalid\")<br>            invalid_result = True<br>            invalid_reason = \"Unable to parse the output string.\"<br>        if score:<br>            score /= self.score_threshold<br>        return EvaluationResult(<br>            query=query,<br>            response=response,<br>            score=score,<br>            feedback=eval_response,<br>            invalid_result=invalid_result,<br>            invalid_reason=invalid_reason,<br>        )<br></code>`` |\n"
    },
    {
      "id": "2ba51f8e-c963-415d-825a-ecf4563e7792",
      "size": 2925,
      "headers": {
        "h1": "Introspective",
        "h2": "IntrospectiveAgentWorker \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br></code>`<code> | </code>`<code><br>class IntrospectiveAgentWorker(BaseAgentWorker):<br>    \"\"\"Introspective Agent Worker.<br>    This agent worker implements the Reflection AI agentic pattern. It does<br>    so by merely delegating the work to two other agents in a purely<br>    deterministic fashion.<br>    The task this agent performs (again via delegation) is to generate a response<br>    to a query and perform reflection and correction on the response. This<br>    agent delegates the task to (optionally) first a </code>main_agent_worker<code> that<br>    generates the initial response to the query. This initial response is then<br>    passed to the </code>reflective_agent_worker<code> to perform the reflection and<br>    correction of the initial response. Optionally, the </code>main_agent_worker<code><br>    can be skipped if none is provided. In this case, the users input query<br>    will be assumed to contain the original response that needs to go thru<br>    reflection and correction.<br>    Attributes:<br>        reflective_agent_worker (BaseAgentWorker): Reflective agent responsible for<br>            performing reflection and correction of the initial response.<br>        main_agent_worker (Optional[BaseAgentWorker], optional): Main agent responsible<br>            for generating an initial response to the user query. Defaults to None.<br>            If None, the user input is assumed as the initial response.<br>        verbose (bool, optional): Whether execution should be verbose. Defaults to False.<br>        callback_manager (Optional[CallbackManager], optional): Callback manager. Defaults to None.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        reflective_agent_worker: BaseAgentWorker,<br>        main_agent_worker: Optional[BaseAgentWorker] = None,<br>        verbose: bool = False,<br>        callback_manager: Optional[CallbackManager] = None,<br>    ) -> None:<br>        \"\"\"Init params.\"\"\"<br>        self._verbose = verbose<br>        self._main_agent_worker = main_agent_worker<br>        self._reflective_agent_worker = reflective_agent_worker<br>        self.callback_manager = callback_manager or CallbackManager([])<br>    @classmethod<br>    def from_defaults(<br>        cls,<br>        reflective_agent_worker: BaseAgentWorker,<br>        main_agent_worker: Optional[BaseAgentWorker] = None,<br>        verbose: bool = False,<br>        callback_manager: Optional[CallbackManager] = None,<br>        **kwargs: Any,<br>    ) -> \"IntrospectiveAgentWorker\":<br>        \"\"\"Create an IntrospectiveAgentWorker from args.<br>        Similar to </code>from_defaults<code> in other classes, this method will<br>        infer defaults for a variety of parameters, including the LLM,<br>        if they are not specified.<br>        \"\"\"<br>        return cls(<br>            main_agent_worker=main_agent_worker,<br>            reflective_agent_worker=reflective_agent_worker,<br>            verbose=verbose,<br>            callback_manager=callback_manager,<br>            **kwargs,<br>        )<br>    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>        \"\"\"Initialize step from task.\"\"\"<br>        # temporary memory for new messages<br>        main_memory = ChatMemoryBuffer.from_defaults()<br>        reflective_memory = ChatMemoryBuffer.from_defaults()<br>        # put current history in new memory<br>        messages = task.memory.get()<br>        for message in messages:<br>            main_memory.put(message)<br>        # initialize task state<br>        task_state = {<br>            \"main\": {<br>                \"memory\": main_memory,<br>                \"sources\": [],<br>            },<br>            \"reflection\": {\"memory\": reflective_memory, \"sources\": []},<br>        }<br>        task.extra_state.update(task_state)<br>        return TaskStep(<br>            task_id=task.task_id,<br>            step_id=str(uuid.uuid4()),<br>            input=task.input,<br>        )<br>    def get_all_messages(self, task: Task) -> List[ChatMessage]:<br>        return (<br>            +task.memory.get()<br>            + task.extra_state[\"main\"][\"memory\"].get_all()<br>            + task.extra_state[\"reflection\"][\"memory\"].get_all()<br>        )<br>    @trace_method(\"run_step\")<br>    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        # run main agent<br>        if self._main_agent_worker is not None:<br>            main_agent_messages = task.extra_state[\"main\"][\"memory\"].get()<br>            main_agent = self._main_agent_worker.as_agent(<br>                chat_history=main_agent_messages<br>            )<br>            main_agent_response = main_agent.chat(task.input)<br>            original_response = main_agent_response.response<br>            task.extra_state[\"main\"][\"sources\"] = main_agent_response.sources<br>            task.extra_state[\"main\"][\"memory\"] = main_agent.memory<br>        else:<br>            pass<br>        # run reflective agent<br>        reflective_agent_messages = task.extra_state[\"main\"][\"memory\"].get()<br>        reflective_agent = self._reflective_agent_worker.as_agent(<br>            chat_history=reflective_agent_messages<br>        )<br>        # NOTE: atm you *need* to pass an input string to </code>chat<code>, even if the memory is already<br>        # preloaded. Input will be concatenated on top of chat history from memory<br>        # which will be used to generate the response.<br>        # TODO: make agent interface more flexible<br>        reflective_agent_response = reflective_agent.chat(original_response)<br>        task.extra_state[\"reflection\"][\"sources\"] = reflective_agent_response.sources<br>        task.extra_state[\"reflection\"][\"memory\"] = reflective_agent.memory<br>        agent_response = AgentChatResponse(<br>            response=str(reflective_agent_response.response),<br>            sources=task.extra_state[\"main\"][\"sources\"]<br>            + task.extra_state[\"reflection\"][\"sources\"],<br>        )<br>        return TaskStepOutput(<br>            output=agent_response,<br>            task_step=step,<br>            is_last=True,<br>            next_steps=[],<br>        )<br>    @trace_method(\"run_step\")<br>    async def arun_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        # run main agent if one is supplied otherwise assume user input<br>        # is the original response to be reflected on and subsequently corrected<br>        if self._main_agent_worker is not None:<br>            main_agent_messages = task.extra_state[\"main\"][\"memory\"].get()<br>            main_agent = self._main_agent_worker.as_agent(<br>                chat_history=main_agent_messages, verbose=self._verbose<br>            )<br>            main_agent_response = await main_agent.achat(task.input)<br>            original_response = main_agent_response.response<br>            task.extra_state[\"main\"][\"sources\"] = main_agent_response.sources<br>            task.extra_state[\"main\"][\"memory\"] = main_agent.memory<br>        else:<br>            add_user_step_to_memory(<br>                step, task.extra_state[\"main\"][\"memory\"], verbose=self._verbose<br>            )<br>            original_response = step.input<br>            # fictitious agent's initial response (to get reflection/correction cycle started)<br>            task.extra_state[\"main\"][\"memory\"].put(<br>                ChatMessage(content=original_response, role=\"assistant\")<br>            )<br>        # run reflective agent<br>        reflective_agent_messages = task.extra_state[\"main\"][\"memory\"].get()<br>        reflective_agent = self._reflective_agent_worker.as_agent(<br>            chat_history=reflective_agent_messages, verbose=self._verbose<br>        )<br>        reflective_agent_response = await reflective_agent.achat(original_response)<br>        task.extra_state[\"reflection\"][\"sources\"] = reflective_agent_response.sources<br>        task.extra_state[\"reflection\"][\"memory\"] = reflective_agent.memory<br>        agent_response = AgentChatResponse(<br>            response=str(reflective_agent_response.response),<br>            sources=task.extra_state[\"main\"][\"sources\"]<br>            + task.extra_state[\"reflection\"][\"sources\"],<br>        )<br>        return TaskStepOutput(<br>            output=agent_response,<br>            task_step=step,<br>            is_last=True,<br>            next_steps=[],<br>        )<br>    @trace_method(\"run_step\")<br>    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        raise NotImplementedError(\"Stream not supported for introspective agent\")<br>    @trace_method(\"run_step\")<br>    async def astream_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async stream).\"\"\"<br>        raise NotImplementedError(\"Stream not supported for introspective agent\")<br>    def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>        \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>        # add new messages to memory<br>        main_memory = task.extra_state[\"main\"][<br>            \"memory\"<br>        ].get_all()  # contains initial response as final message<br>        final_corrected_message = task.extra_state[\"reflection\"][\"memory\"].get_all()[-1]<br>        # swap main workers response with the reflected/corrected one<br>        finalized_task_memory = main_memory[:-1] + [final_corrected_message]<br>        task.memory.set(finalized_task_memory)<br></code>`` |\n"
    },
    {
      "id": "079eebb0-37b3-4bbf-b6ad-ef5b965dbc7b",
      "size": 4645,
      "headers": {
        "h1": "Introspective",
        "h2": "SelfReflectionAgentWorker \\#",
        "h3": ""
      },
      "text": "| ``<code><br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>472<br>473<br></code>`<code> | </code>`<code><br>class SelfReflectionAgentWorker(BaseModel, BaseAgentWorker):<br>    \"\"\"Self Reflection Agent Worker.<br>    This agent performs a reflection without any tools on a given response<br>    and subsequently performs correction. It should be noted that this reflection<br>    implementation has been inspired by two works:<br>    1. Reflexion: Language Agents with Verbal Reinforcement Learning, by Shinn et al. (2023)<br>        (https://arxiv.org/pdf/2303.11366.pdf)<br>    2. CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing, by Gou et al. (2024)<br>       (https://arxiv.org/pdf/2305.11738.pdf)<br>    This agent performs cycles of reflection and correction on an initial response<br>    until a satisfactory correction has been generated or a max number of cycles<br>    has been reached. To perform reflection, this agent utilizes a user-specified<br>    LLM along with a PydanticProgram (thru structured_predict) to generate a structured<br>    output that contains an LLM generated reflection of the current response. After reflection,<br>    the same user-specified LLM is used again but this time with another PydanticProgram<br>    to generate a structured output that contains an LLM generated corrected<br>    version of the current response against the priorly generated reflection.<br>    Attr:<br>        max_iterations (int, optional): The max number of reflection & correction.<br>            Defaults to DEFAULT_MAX_ITERATIONS.<br>        callback_manager (Optional[CallbackManager], optional): Callback manager.<br>            Defaults to None.<br>        llm (Optional[LLM], optional): The LLM used to perform reflection and correction.<br>            Must be an OpenAI LLM at this time. Defaults to None.<br>        verbose (bool, optional): Whether execution should be verbose. Defaults to False.<br>    \"\"\"<br>    callback_manager: CallbackManager = Field(default=CallbackManager([]))<br>    max_iterations: int = Field(default=DEFAULT_MAX_ITERATIONS)<br>    _llm: LLM = PrivateAttr()<br>    _verbose: bool = PrivateAttr()<br>    class Config:<br>        arbitrary_types_allowed = True<br>    def __init__(<br>        self,<br>        max_iterations: int = DEFAULT_MAX_ITERATIONS,<br>        callback_manager: Optional[CallbackManager] = None,<br>        llm: Optional[LLM] = None,<br>        verbose: bool = False,<br>        **kwargs: Any,<br>    ) -> None:<br>        \"\"\"__init__.\"\"\"<br>        super().__init__(<br>            callback_manager=callback_manager or CallbackManager([]),<br>            max_iterations=max_iterations,<br>            **kwargs,<br>        )<br>        self._llm = llm<br>        self._verbose = verbose<br>    @classmethod<br>    def from_defaults(<br>        cls,<br>        llm: Optional[LLM] = None,<br>        max_iterations: int = DEFAULT_MAX_ITERATIONS,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        **kwargs: Any,<br>    ) -> \"SelfReflectionAgentWorker\":<br>        \"\"\"Convenience constructor.\"\"\"<br>        if llm is None:<br>            try:<br>                from llama_index.llms.openai import OpenAI<br>            except ImportError:<br>                raise ImportError(<br>                    \"Missing OpenAI LLMs. Please run </code>pip install llama-index-llms-openai<code>.\"<br>                )<br>            llm = OpenAI(model=\"gpt-4-turbo-preview\", temperature=0)<br>        return cls(<br>            llm=llm,<br>            max_iterations=max_iterations,<br>            callback_manager=callback_manager,<br>            verbose=verbose,<br>            **kwargs,<br>        )<br>    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>        \"\"\"Initialize step from task.\"\"\"<br>        # temporary memory for new messages<br>        new_memory = ChatMemoryBuffer.from_defaults()<br>        # put current history in new memory<br>        messages = task.memory.get()<br>        for message in messages:<br>            new_memory.put(message)<br>        # inject new input into memory<br>        new_memory.put(ChatMessage(content=task.input, role=MessageRole.USER))<br>        # initialize task state<br>        task_state = {<br>            \"new_memory\": new_memory,<br>            \"sources\": [],<br>        }<br>        task.extra_state.update(task_state)<br>        return TaskStep(<br>            task_id=task.task_id,<br>            step_id=str(uuid.uuid4()),<br>            input=task.input,<br>            step_state={\"count\": 0},<br>        )<br>    def _remove_correction_str_prefix(self, correct_msg: str) -> str:<br>        \"\"\"Helper function to format correction message for final response.\"\"\"<br>        return correct_msg.replace(CORRECT_RESPONSE_PREFIX, \"\")<br>    @dispatcher.span<br>    def _reflect(<br>        self, chat_history: List[ChatMessage]<br>    ) -> Tuple[Reflection, ChatMessage]:<br>        \"\"\"Reflect on the trajectory.\"\"\"<br>        reflection = self._llm.structured_predict(<br>            Reflection,<br>            PromptTemplate(REFLECTION_PROMPT_TEMPLATE),<br>            chat_history=messages_to_prompt(chat_history),<br>        )<br>        if self._verbose:<br>            print(f\"> Reflection: {reflection.model_dump()}\")<br>        # end state: return user message<br>        reflection_output_str = (<br>            f\"Is Done: {reflection.is_done}\\nCritique: {reflection.feedback}\"<br>        )<br>        critique = REFLECTION_RESPONSE_TEMPLATE.format(<br>            reflection_output=reflection_output_str<br>        )<br>        return reflection, ChatMessage.from_str(critique, role=\"user\")<br>    @dispatcher.span<br>    def _correct(self, input_str: str, critique: str) -> ChatMessage:<br>        correction = self._llm.structured_predict(<br>            Correction,<br>            PromptTemplate(CORRECT_PROMPT_TEMPLATE),<br>            input_str=input_str,<br>            feedback=critique,<br>        )<br>        correct_response_str = CORRECT_RESPONSE_FSTRING.format(<br>            correction=correction.correction<br>        )<br>        if self._verbose:<br>            print(f\"Correction: {correction.correction}\", flush=True)<br>        return ChatMessage.from_str(correct_response_str, role=\"assistant\")<br>    @dispatcher.span<br>    @trace_method(\"run_step\")<br>    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        state = step.step_state<br>        state[\"count\"] += 1<br>        # new_memory should at the very least contain the user input<br>        messages = task.extra_state[\"new_memory\"].get()<br>        prev_correct_str = messages[-1].content<br>        prev_correct_str_without_prefix = self._remove_correction_str_prefix(<br>            prev_correct_str<br>        )<br>        # reflect phase<br>        reflection, reflection_msg = self._reflect(chat_history=messages)<br>        is_done = reflection.is_done<br>        critique_msg = ChatMessage(role=MessageRole.USER, content=reflection_msg)<br>        task.extra_state[\"new_memory\"].put(critique_msg)<br>        # correction phase<br>        if is_done:<br>            # no correction to be made prev correction is sufficient<br>            agent_response = AgentChatResponse(<br>                response=prev_correct_str_without_prefix,<br>                sources=task.extra_state[\"sources\"],<br>            )<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(<br>                    role=MessageRole.ASSISTANT,<br>                    content=prev_correct_str_without_prefix,<br>                )<br>            )<br>            new_steps = []<br>        else:<br>            # generate a new correction<br>            correct_msg = self._correct(<br>                input_str=prev_correct_str_without_prefix,<br>                critique=reflection_msg.content,<br>            )<br>            correct_str_without_prefix = self._remove_correction_str_prefix(<br>                correct_msg.content<br>            )<br>            if self.max_iterations == state[\"count\"]:<br>                # this will be the last iteration<br>                task.extra_state[\"new_memory\"].put(<br>                    ChatMessage(<br>                        role=MessageRole.ASSISTANT,<br>                        content=correct_str_without_prefix,<br>                    )<br>                )<br>                agent_response = AgentChatResponse(response=correct_str_without_prefix)<br>                new_steps = []<br>            else:<br>                # another round of reflection/correction will take place<br>                task.extra_state[\"new_memory\"].put(correct_msg)<br>                agent_response = AgentChatResponse(response=str(correct_msg))<br>                new_steps = [<br>                    step.get_next_step(<br>                        step_id=str(uuid.uuid4()),<br>                        # NOTE: input is unused<br>                        input=None,<br>                        step_state=state,<br>                    )<br>                ]<br>        return TaskStepOutput(<br>            output=agent_response,<br>            task_step=step,<br>            is_last=is_done | (self.max_iterations == state[\"count\"]),<br>            next_steps=new_steps,<br>        )<br>    # Async methods<br>    @dispatcher.span<br>    async def _areflect(<br>        self, chat_history: List[ChatMessage]<br>    ) -> Tuple[Reflection, ChatMessage]:<br>        \"\"\"Reflect on the trajectory.\"\"\"<br>        reflection = await self._llm.astructured_predict(<br>            Reflection,<br>            PromptTemplate(REFLECTION_PROMPT_TEMPLATE),<br>            chat_history=messages_to_prompt(chat_history),<br>        )<br>        if self._verbose:<br>            print(f\"> Reflection: {reflection.model_dump()}\")<br>        # end state: return user message<br>        reflection_output_str = (<br>            f\"Is Done: {reflection.is_done}\\nCritique: {reflection.feedback}\"<br>        )<br>        critique = REFLECTION_RESPONSE_TEMPLATE.format(<br>            reflection_output=reflection_output_str<br>        )<br>        return reflection, ChatMessage.from_str(critique, role=\"user\")<br>    @dispatcher.span<br>    async def _acorrect(self, input_str: str, critique: str) -> ChatMessage:<br>        correction = await self._llm.astructured_predict(<br>            Correction,<br>            PromptTemplate(CORRECT_PROMPT_TEMPLATE),<br>            input_str=input_str,<br>            feedback=critique,<br>        )<br>        correct_response_str = CORRECT_RESPONSE_FSTRING.format(<br>            correction=correction.correction<br>        )<br>        if self._verbose:<br>            print(f\"Correction: {correction.correction}\", flush=True)<br>        return ChatMessage.from_str(correct_response_str, role=\"assistant\")<br>    @dispatcher.span<br>    @trace_method(\"run_step\")<br>    async def arun_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        state = step.step_state<br>        state[\"count\"] += 1<br>        messages = task.extra_state[\"new_memory\"].get()<br>        prev_correct_str = messages[-1].content<br>        prev_correct_str_without_prefix = self._remove_correction_str_prefix(<br>            prev_correct_str<br>        )<br>        # reflect<br>        reflection, reflection_msg = await self._areflect(chat_history=messages)<br>        is_done = reflection.is_done<br>        critique_msg = ChatMessage(role=MessageRole.USER, content=reflection_msg)<br>        task.extra_state[\"new_memory\"].put(critique_msg)<br>        # correction phase<br>        if is_done:<br>            # no correction to be made prev correction is sufficient<br>            agent_response = AgentChatResponse(<br>                response=prev_correct_str_without_prefix,<br>                sources=task.extra_state[\"sources\"],<br>            )<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(<br>                    role=MessageRole.ASSISTANT,<br>                    content=prev_correct_str_without_prefix,<br>                )<br>            )<br>            new_steps = []<br>        else:<br>            # generate a new correction<br>            correct_msg = await self._acorrect(<br>                input_str=prev_correct_str_without_prefix,<br>                critique=reflection_msg.content,<br>            )<br>            correct_str_without_prefix = self._remove_correction_str_prefix(<br>                correct_msg.content<br>            )<br>            if self.max_iterations == state[\"count\"]:<br>                # this will be the last iteration<br>                task.extra_state[\"new_memory\"].put(<br>                    ChatMessage(<br>                        role=MessageRole.ASSISTANT,<br>                        content=correct_str_without_prefix,<br>                    )<br>                )<br>                agent_response = AgentChatResponse(response=correct_str_without_prefix)<br>                new_steps = []<br>            else:<br>                # another round of reflection/correction will take place<br>                task.extra_state[\"new_memory\"].put(correct_msg)<br>                agent_response = AgentChatResponse(response=str(correct_msg))<br>                new_steps = [<br>                    step.get_next_step(<br>                        step_id=str(uuid.uuid4()),<br>                        # NOTE: input is unused<br>                        input=None,<br>                        step_state=state,<br>                    )<br>                ]<br>        return TaskStepOutput(<br>            output=agent_response,<br>            task_step=step,<br>            is_last=is_done | (self.max_iterations == state[\"count\"]),<br>            next_steps=new_steps,<br>        )<br>    # Stream methods<br>    @dispatcher.span<br>    @trace_method(\"run_step\")<br>    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        raise NotImplementedError(\"Stream not supported for self reflection agent\")<br>    @dispatcher.span<br>    @trace_method(\"run_step\")<br>    async def astream_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async stream).\"\"\"<br>        raise NotImplementedError(\"Stream not supported for self reflection agent\")<br>    def get_all_messages(self, task: Task) -> List[ChatMessage]:<br>        return (<br>            self.prefix_messages<br>            + task.memory.get()<br>            + task.extra_state[\"new_memory\"].get_all()<br>        )<br>    def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>        \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>        # add new messages to memory<br>        task.memory.set(task.extra_state[\"new_memory\"].get_all())<br>        # reset new memory<br>        task.extra_state[\"new_memory\"].reset()<br></code>`` |\n"
    },
    {
      "id": "5d33549b-a552-4856-a4b4-b18bb248ed0b",
      "size": 4967,
      "headers": {
        "h1": "Introspective",
        "h2": "ToolInteractiveReflectionAgentWorker \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br></code>`<code> | </code>`<code><br>class ToolInteractiveReflectionAgentWorker(BaseModel, BaseAgentWorker):<br>    \"\"\"Tool-Interactive Reflection Agent Worker.<br>    This agent worker implements the CRITIC reflection framework introduced<br>    by Gou, Zhibin, et al. (2024) ICLR. (source: https://arxiv.org/pdf/2305.11738)<br>    CRITIC stands for </code>Correcting with tool-interactive critiquing<code>. It works<br>    by performing a reflection on a response to a task/query using external tools<br>    (e.g., fact checking using a Google search tool) and subsequently using<br>    the critique to generate a corrected response. It cycles thru tool-interactive<br>    reflection and correction until a specific stopping criteria has been met<br>    or a max number of iterations has been reached.<br>    This agent delegates the critique subtask to a user-supplied </code>critique_agent_worker<code><br>    that is of </code>FunctionCallingAgentWorker<code> type i.e. it uses tools to perform<br>    tasks. For correction, it uses a user-specified </code>correction_llm<code> with a<br>    PydanticProgram (determined dynamically with llm.structured_predict)<br>    in order to produce a structured output, namely </code>Correction<code> that<br>    contains the correction generated by the </code>correction_llm<code>.<br>    Attributes:<br>        critique_agent_worker (FunctionCallingAgentWorker): Critique agent responsible<br>            for performing the critique reflection.<br>        critique_template (str): The template containing instructions for how the<br>            Critique agent should perform the reflection.<br>        max_iterations (int, optional): The max number of reflection & correction<br>            cycles permitted. Defaults to DEFAULT_MAX_ITERATIONS = 5.<br>        stopping_callable (Optional[StoppingCallable], optional): An optional stopping<br>            condition that operates over the critique reflection string and returns<br>            a boolean to determine if the latest correction is sufficient. Defaults to None.<br>        correction_llm (Optional[LLM], optional): The LLM used for producing corrected<br>            responses against a critique or reflection. Defaults to None.<br>        callback_manager (Optional[CallbackManager], optional): Callback manager. Defaults to None.<br>        verbose (bool, optional): Whether execution should be verbose. Defaults to False.<br>    \"\"\"<br>    callback_manager: CallbackManager = Field(default=CallbackManager([]))<br>    max_iterations: int = Field(default=DEFAULT_MAX_ITERATIONS)<br>    stopping_callable: Optional[StoppingCallable] = Field(<br>        default=None,<br>        description=\"Optional function that operates on critique string to see if no more corrections are needed.\",<br>    )<br>    _critique_agent_worker: FunctionCallingAgentWorker = PrivateAttr()<br>    _critique_template: str = PrivateAttr()<br>    _correction_llm: LLM = PrivateAttr()<br>    _verbose: bool = PrivateAttr()<br>    class Config:<br>        arbitrary_types_allowed = True<br>    def __init__(<br>        self,<br>        critique_agent_worker: FunctionCallingAgentWorker,<br>        critique_template: str,<br>        max_iterations: int = DEFAULT_MAX_ITERATIONS,<br>        stopping_callable: Optional[StoppingCallable] = None,<br>        correction_llm: Optional[LLM] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        **kwargs: Any,<br>    ) -> None:<br>        \"\"\"__init__.\"\"\"<br>        super().__init__(<br>            callback_manager=callback_manager,<br>            max_iterations=max_iterations,<br>            stopping_callable=stopping_callable,<br>            **kwargs,<br>        )<br>        self._critique_agent_worker = critique_agent_worker<br>        self._critique_template = critique_template<br>        self._verbose = verbose<br>        self._correction_llm = correction_llm<br>    @classmethod<br>    def from_defaults(<br>        cls,<br>        critique_agent_worker: FunctionCallingAgentWorker,<br>        critique_template: str,<br>        correction_llm: Optional[LLM] = None,<br>        max_iterations: int = DEFAULT_MAX_ITERATIONS,<br>        stopping_callable: Optional[StoppingCallable] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        **kwargs: Any,<br>    ) -> \"ToolInteractiveReflectionAgentWorker\":<br>        \"\"\"Convenience constructor method from set of of BaseTools (Optional).\"\"\"<br>        if correction_llm is None:<br>            try:<br>                from llama_index.llms.openai import OpenAI<br>            except ImportError:<br>                raise ImportError(<br>                    \"Missing OpenAI LLMs. Please run </code>pip install llama-index-llms-openai<code>.\"<br>                )<br>            correction_llm = OpenAI(model=\"gpt-4-turbo-preview\", temperature=0)<br>        return cls(<br>            critique_agent_worker=critique_agent_worker,<br>            critique_template=critique_template,<br>            correction_llm=correction_llm,<br>            max_iterations=max_iterations,<br>            stopping_callable=stopping_callable,<br>            callback_manager=callback_manager or CallbackManager([]),<br>            verbose=verbose,<br>            **kwargs,<br>        )<br>    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>        \"\"\"Initialize step from task.\"\"\"<br>        # temporary memory for new messages<br>        new_memory = ChatMemoryBuffer.from_defaults()<br>        # put current history in new memory<br>        messages = task.memory.get()<br>        for message in messages:<br>            new_memory.put(message)<br>        # inject new input into memory<br>        new_memory.put(ChatMessage(content=task.input, role=MessageRole.USER))<br>        # initialize task state<br>        task_state = {<br>            \"new_memory\": new_memory,<br>            \"sources\": [],<br>        }<br>        task.extra_state.update(task_state)<br>        return TaskStep(<br>            task_id=task.task_id,<br>            step_id=str(uuid.uuid4()),<br>            input=task.input,<br>            step_state={\"count\": 0},<br>        )<br>    def _remove_correction_str_prefix(self, correct_msg: str) -> str:<br>        \"\"\"Helper function to format correction message for final response.\"\"\"<br>        return correct_msg.replace(CORRECT_RESPONSE_PREFIX, \"\")<br>    @dispatcher.span<br>    def _critique(self, input_str: str) -> AgentChatResponse:<br>        agent = self._critique_agent_worker.as_agent(verbose=self._verbose)<br>        critique = agent.chat(self._critique_template.format(input_str=input_str))<br>        if self._verbose:<br>            print(f\"Critique: {critique.response}\", flush=True)<br>        return critique<br>    @dispatcher.span<br>    def _correct(self, input_str: str, critique: str) -> ChatMessage:<br>        correction = self._correction_llm.structured_predict(<br>            Correction,<br>            PromptTemplate(CORRECT_PROMPT_TEMPLATE),<br>            input_str=input_str,<br>            critique=critique,<br>        )<br>        correct_response_str = CORRECT_RESPONSE_FSTRING.format(<br>            correction=correction.correction<br>        )<br>        if self._verbose:<br>            print(f\"Correction: {correction.correction}\", flush=True)<br>        return ChatMessage.from_str(correct_response_str, role=\"assistant\")<br>    @dispatcher.span<br>    @trace_method(\"run_step\")<br>    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        state = step.step_state<br>        state[\"count\"] += 1<br>        messages = task.extra_state[\"new_memory\"].get()<br>        prev_correct_str = messages[-1].content<br>        prev_correct_str_without_prefix = self._remove_correction_str_prefix(<br>            prev_correct_str<br>        )<br>        # critique phase<br>        critique_response = self._critique(input_str=prev_correct_str_without_prefix)<br>        task.extra_state[\"sources\"].extend(critique_response.sources)<br>        is_done = False<br>        if self.stopping_callable:<br>            is_done = self.stopping_callable(critique_str=critique_response.response)<br>        critique_msg = ChatMessage(<br>            role=MessageRole.USER, content=critique_response.response<br>        )<br>        task.extra_state[\"new_memory\"].put(critique_msg)<br>        # correction phase<br>        if is_done:<br>            # no correction to be made prev correction is sufficient<br>            agent_response = AgentChatResponse(<br>                response=prev_correct_str_without_prefix,<br>                sources=task.extra_state[\"sources\"],<br>            )<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(<br>                    role=MessageRole.ASSISTANT, content=prev_correct_str_without_prefix<br>                )<br>            )<br>            new_steps = []<br>        else:<br>            # generate a new correction<br>            correct_msg = self._correct(<br>                input_str=prev_correct_str_without_prefix,<br>                critique=critique_response.response,<br>            )<br>            correct_str_without_prefix = self._remove_correction_str_prefix(<br>                correct_msg.content<br>            )<br>            # reached max iterations, no further reflection/correction cycles<br>            if self.max_iterations == state[\"count\"]:<br>                task.extra_state[\"new_memory\"].put(<br>                    ChatMessage(<br>                        role=MessageRole.ASSISTANT, content=correct_str_without_prefix<br>                    )<br>                )<br>                agent_response = AgentChatResponse(response=correct_str_without_prefix)<br>                new_steps = []<br>            else:<br>                # another round of reflection/correction will take place<br>                task.extra_state[\"new_memory\"].put(correct_msg)<br>                agent_response = AgentChatResponse(<br>                    response=str(correct_msg), sources=critique_response.sources<br>                )<br>                new_steps = [<br>                    step.get_next_step(<br>                        step_id=str(uuid.uuid4()),<br>                        # NOTE: input is unused<br>                        input=None,<br>                        step_state=state,<br>                    )<br>                ]<br>        return TaskStepOutput(<br>            output=agent_response,<br>            task_step=step,<br>            is_last=is_done | (self.max_iterations == state[\"count\"]),<br>            next_steps=new_steps,<br>        )<br>    # Async Methods<br>    @dispatcher.span<br>    async def _acritique(self, input_str: str) -> AgentChatResponse:<br>        agent = self._critique_agent_worker.as_agent(verbose=self._verbose)<br>        critique = await agent.achat(<br>            self._critique_template.format(input_str=input_str)<br>        )<br>        if self._verbose:<br>            print(f\"Critique: {critique.response}\", flush=True)<br>        return critique<br>    @dispatcher.span<br>    async def _acorrect(self, input_str: str, critique: str) -> ChatMessage:<br>        correction = await self._correction_llm.astructured_predict(<br>            Correction,<br>            PromptTemplate(CORRECT_PROMPT_TEMPLATE),<br>            input_str=input_str,<br>            critique=critique,<br>        )<br>        correct_response_str = CORRECT_RESPONSE_FSTRING.format(<br>            correction=correction.correction<br>        )<br>        if self._verbose:<br>            print(f\"Correction: {correction.correction}\", flush=True)<br>        return ChatMessage.from_str(correct_response_str, role=\"assistant\")<br>    @dispatcher.span<br>    @trace_method(\"run_step\")<br>    async def arun_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        state = step.step_state<br>        state[\"count\"] += 1<br>        messages = task.extra_state[\"new_memory\"].get()<br>        prev_correct_str = messages[-1].content<br>        prev_correct_str_without_prefix = self._remove_correction_str_prefix(<br>            prev_correct_str<br>        )<br>        # critique phase<br>        critique_response = await self._acritique(<br>            input_str=prev_correct_str_without_prefix<br>        )<br>        task.extra_state[\"sources\"].extend(critique_response.sources)<br>        is_done = False<br>        if self.stopping_callable:<br>            is_done = self.stopping_callable(critique_str=critique_response.response)<br>        critique_msg = ChatMessage(<br>            role=MessageRole.USER, content=critique_response.response<br>        )<br>        task.extra_state[\"new_memory\"].put(critique_msg)<br>        # correction phase<br>        if is_done:<br>            # no correction to be made prev correction is sufficient<br>            agent_response = AgentChatResponse(<br>                response=prev_correct_str_without_prefix,<br>                sources=task.extra_state[\"sources\"],<br>            )<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(<br>                    role=MessageRole.ASSISTANT, content=prev_correct_str_without_prefix<br>                )<br>            )<br>            new_steps = []<br>        else:<br>            # generate a new correction<br>            correct_msg = await self._acorrect(<br>                input_str=prev_correct_str_without_prefix,<br>                critique=critique_response.response,<br>            )<br>            correct_str_without_prefix = self._remove_correction_str_prefix(<br>                correct_msg.content<br>            )<br>            # reached max iterations, no further reflection/correction cycles<br>            if self.max_iterations == state[\"count\"]:<br>                task.extra_state[\"new_memory\"].put(<br>                    ChatMessage(<br>                        role=MessageRole.ASSISTANT, content=correct_str_without_prefix<br>                    )<br>                )<br>                agent_response = AgentChatResponse(response=correct_str_without_prefix)<br>                new_steps = []<br>            else:<br>                # another round of reflection/correction will take place<br>                task.extra_state[\"new_memory\"].put(correct_msg)<br>                agent_response = AgentChatResponse(<br>                    response=str(correct_msg), sources=critique_response.sources<br>                )<br>                new_steps = [<br>                    step.get_next_step(<br>                        step_id=str(uuid.uuid4()),<br>                        # NOTE: input is unused<br>                        input=None,<br>                        step_state=state,<br>                    )<br>                ]<br>        return TaskStepOutput(<br>            output=agent_response,<br>            task_step=step,<br>            is_last=is_done | (self.max_iterations == state[\"count\"]),<br>            next_steps=new_steps,<br>        )<br>    # Steam methods<br>    @dispatcher.span<br>    @trace_method(\"run_step\")<br>    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        raise NotImplementedError(<br>            \"Stream not supported for tool-interactive reflection agent\"<br>        )<br>    @dispatcher.span<br>    @trace_method(\"run_step\")<br>    async def astream_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async stream).\"\"\"<br>        raise NotImplementedError(<br>            \"Stream not supported for tool-interactive reflection agent\"<br>        )<br>    def get_all_messages(self, task: Task) -> List[ChatMessage]:<br>        return (<br>            self.prefix_messages<br>            + task.memory.get()<br>            + task.extra_state[\"new_memory\"].get_all()<br>        )<br>    def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>        \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>        # add new messages to memory<br>        task.memory.set(task.extra_state[\"new_memory\"].get_all())<br>        # reset new memory<br>        task.extra_state[\"new_memory\"].reset()<br></code>`` |\n"
    },
    {
      "id": "caf36119-c053-4a68-9148-474881a82549",
      "size": 1005,
      "headers": {
        "h1": "Introspective",
        "h2": "ToolInteractiveReflectionAgentWorker \\#",
        "h3": "arun\\_step<code>async</code>\\#"
      },
      "text": "| ``<code><br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br></code>`<code> | </code>`<code><br>@dispatcher.span<br>@trace_method(\"run_step\")<br>async def arun_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async).\"\"\"<br>    state = step.step_state<br>    state[\"count\"] += 1<br>    messages = task.extra_state[\"new_memory\"].get()<br>    prev_correct_str = messages[-1].content<br>    prev_correct_str_without_prefix = self._remove_correction_str_prefix(<br>        prev_correct_str<br>    )<br>    # critique phase<br>    critique_response = await self._acritique(<br>        input_str=prev_correct_str_without_prefix<br>    )<br>    task.extra_state[\"sources\"].extend(critique_response.sources)<br>    is_done = False<br>    if self.stopping_callable:<br>        is_done = self.stopping_callable(critique_str=critique_response.response)<br>    critique_msg = ChatMessage(<br>        role=MessageRole.USER, content=critique_response.response<br>    )<br>    task.extra_state[\"new_memory\"].put(critique_msg)<br>    # correction phase<br>    if is_done:<br>        # no correction to be made prev correction is sufficient<br>        agent_response = AgentChatResponse(<br>            response=prev_correct_str_without_prefix,<br>            sources=task.extra_state[\"sources\"],<br>        )<br>        task.extra_state[\"new_memory\"].put(<br>            ChatMessage(<br>                role=MessageRole.ASSISTANT, content=prev_correct_str_without_prefix<br>            )<br>        )<br>        new_steps = []<br>    else:<br>        # generate a new correction<br>        correct_msg = await self._acorrect(<br>            input_str=prev_correct_str_without_prefix,<br>            critique=critique_response.response,<br>        )<br>        correct_str_without_prefix = self._remove_correction_str_prefix(<br>            correct_msg.content<br>        )<br>        # reached max iterations, no further reflection/correction cycles<br>        if self.max_iterations == state[\"count\"]:<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(<br>                    role=MessageRole.ASSISTANT, content=correct_str_without_prefix<br>                )<br>            )<br>            agent_response = AgentChatResponse(response=correct_str_without_prefix)<br>            new_steps = []<br>        else:<br>            # another round of reflection/correction will take place<br>            task.extra_state[\"new_memory\"].put(correct_msg)<br>            agent_response = AgentChatResponse(<br>                response=str(correct_msg), sources=critique_response.sources<br>            )<br>            new_steps = [<br>                step.get_next_step(<br>                    step_id=str(uuid.uuid4()),<br>                    # NOTE: input is unused<br>                    input=None,<br>                    step_state=state,<br>                )<br>            ]<br>    return TaskStepOutput(<br>        output=agent_response,<br>        task_step=step,<br>        is_last=is_done | (self.max_iterations == state[\"count\"]),<br>        next_steps=new_steps,<br>    )<br></code>`` |\n"
    },
    {
      "id": "62d80edf-fdfa-4e58-93cd-8a5ab35b1c26",
      "size": 1636,
      "headers": {
        "h1": "Alibabacloud aisearch",
        "h2": "AlibabaCloudAISearchEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br></code>`<code> | </code>`<code><br>class AlibabaCloudAISearchEmbedding(BaseEmbedding):<br>    \"\"\"<br>    For further details, please visit </code>https://help.aliyun.com/zh/open-search/search-platform/developer-reference/text-embedding-api-details<code>.<br>    \"\"\"<br>    _client: Client = PrivateAttr()<br>    aisearch_api_key: str = Field(default=None, exclude=True)<br>    endpoint: str = None<br>    service_id: str = \"ops-text-embedding-002\"<br>    workspace_name: str = \"default\"<br>    def __init__(<br>        self, endpoint: str = None, aisearch_api_key: str = None, **kwargs: Any<br>    ) -> None:<br>        super().__init__(**kwargs)<br>        self.aisearch_api_key = get_from_param_or_env(<br>            \"aisearch_api_key\", aisearch_api_key, \"AISEARCH_API_KEY\"<br>        )<br>        self.endpoint = get_from_param_or_env(\"endpoint\", endpoint, \"AISEARCH_ENDPOINT\")<br>        config = AISearchConfig(<br>            bearer_token=self.aisearch_api_key,<br>            endpoint=self.endpoint,<br>            protocol=\"http\",<br>        )<br>        self._client = Client(config=config)<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"AlibabaCloudAISearchEmbedding\"<br>    @retry_decorator<br>    def _get_embedding(self, text: str, input_type: str) -> List[float]:<br>        request = GetTextEmbeddingRequest(input=text, input_type=input_type)<br>        response: GetTextEmbeddingResponse = self._client.get_text_embedding(<br>            workspace_name=self.workspace_name,<br>            service_id=self.service_id,<br>            request=request,<br>        )<br>        embeddings = response.body.result.embeddings<br>        return embeddings[0].embedding<br>    @aretry_decorator<br>    async def _aget_embedding(self, text: str, input_type: str) -> List[float]:<br>        request = GetTextEmbeddingRequest(input=text, input_type=input_type)<br>        response: GetTextEmbeddingResponse = (<br>            await self._client.get_text_embedding_async(<br>                workspace_name=self.workspace_name,<br>                service_id=self.service_id,<br>                request=request,<br>            )<br>        )<br>        embeddings = response.body.result.embeddings<br>        return embeddings[0].embedding<br>    @retry_decorator<br>    def _get_embeddings(self, texts: List[str], input_type: str) -> List[List[float]]:<br>        request = GetTextEmbeddingRequest(input=texts, input_type=input_type)<br>        response: GetTextEmbeddingResponse = self._client.get_text_embedding(<br>            workspace_name=self.workspace_name,<br>            service_id=self.service_id,<br>            request=request,<br>        )<br>        embeddings = response.body.result.embeddings<br>        return [emb.embedding for emb in embeddings]<br>    @aretry_decorator<br>    async def _aget_embeddings(<br>        self,<br>        texts: List[str],<br>        input_type: str,<br>    ) -> List[List[float]]:<br>        request = GetTextEmbeddingRequest(input=texts, input_type=input_type)<br>        response: GetTextEmbeddingResponse = (<br>            await self._client.get_text_embedding_async(<br>                workspace_name=self.workspace_name,<br>                service_id=self.service_id,<br>                request=request,<br>            )<br>        )<br>        embeddings = response.body.result.embeddings<br>        return [emb.embedding for emb in embeddings]<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self._get_embedding(<br>            query,<br>            input_type=\"query\",<br>        )<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        return await self._aget_embedding(<br>            query,<br>            input_type=\"query\",<br>        )<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._get_embedding(<br>            text,<br>            input_type=\"document\",<br>        )<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_text_embedding.\"\"\"<br>        return await self._aget_embedding(<br>            text,<br>            input_type=\"document\",<br>        )<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        return self._get_embeddings(<br>            texts,<br>            input_type=\"document\",<br>        )<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"The asynchronous version of _get_text_embeddings.\"\"\"<br>        return await self._aget_embeddings(<br>            texts,<br>            input_type=\"document\",<br>        )<br></code>`` |\n"
    },
    {
      "id": "27cba1e4-7e55-4913-a187-1fe6043dadfd",
      "size": 2443,
      "headers": {
        "h1": "Deepinfra",
        "h2": "DeepInfraEmbeddingModel \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br></code>`<code> | </code>`<code><br>class DeepInfraEmbeddingModel(BaseEmbedding):<br>    \"\"\"<br>    A wrapper class for accessing embedding models available via the DeepInfra API. This class allows for easy integration<br>    of DeepInfra embeddings into your projects, supporting both synchronous and asynchronous retrieval of text embeddings.<br>    Args:<br>        model_id (str): Identifier for the model to be used for embeddings. Defaults to 'sentence-transformers/clip-ViT-B-32'.<br>        normalize (bool): Flag to normalize embeddings post retrieval. Defaults to False.<br>        api_token (str): DeepInfra API token. If not provided,<br>        the token is fetched from the environment variable 'DEEPINFRA_API_TOKEN'.<br>    Examples:<br>        >>> from llama_index.embeddings.deepinfra import DeepInfraEmbeddingModel<br>        >>> model = DeepInfraEmbeddingModel()<br>        >>> print(model.get_text_embedding(\"Hello, world!\"))<br>        [0.1, 0.2, 0.3, ...]<br>    \"\"\"<br>    \"\"\"model_id can be obtained from the DeepInfra website.\"\"\"<br>    _model_id: str = PrivateAttr()<br>    \"\"\"normalize flag to normalize embeddings post retrieval.\"\"\"<br>    _normalize: bool = PrivateAttr()<br>    \"\"\"api_token should be obtained from the DeepInfra website.\"\"\"<br>    _api_token: str = PrivateAttr()<br>    \"\"\"query_prefix is used to add a prefix to queries.\"\"\"<br>    _query_prefix: str = PrivateAttr()<br>    \"\"\"text_prefix is used to add a prefix to texts.\"\"\"<br>    _text_prefix: str = PrivateAttr()<br>    def __init__(<br>        self,<br>        model_id: str = DEFAULT_MODEL_ID,<br>        normalize: bool = False,<br>        api_token: str = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        query_prefix: str = \"\",<br>        text_prefix: str = \"\",<br>        embed_batch_size: int = MAX_BATCH_SIZE,<br>    ) -> None:<br>        \"\"\"<br>        Init params.<br>        \"\"\"<br>        super().__init__(<br>            callback_manager=callback_manager, embed_batch_size=embed_batch_size<br>        )<br>        self._model_id = model_id<br>        self._normalize = normalize<br>        self._api_token = api_token or os.getenv(ENV_VARIABLE, None)<br>        self._query_prefix = query_prefix<br>        self._text_prefix = text_prefix<br>    def _post(self, data: List[str]) -> List[List[float]]:<br>        \"\"\"<br>        Sends a POST request to the DeepInfra Inference API with the given data and returns the API response.<br>        Input data is chunked into batches to avoid exceeding the maximum batch size (1024).<br>        Args:<br>            data (List[str]): A list of strings to be embedded.<br>        Returns:<br>            dict: A dictionary containing embeddings from the API.<br>        \"\"\"<br>        url = self.get_url()<br>        chunked_data = _chunk(data, self.embed_batch_size)<br>        embeddings = []<br>        for chunk in chunked_data:<br>            response = requests.post(<br>                url,<br>                json={<br>                    \"inputs\": chunk,<br>                },<br>                headers=self._get_headers(),<br>            )<br>            response.raise_for_status()<br>            embeddings.extend(response.json()[\"embeddings\"])<br>        return embeddings<br>    def get_url(self):<br>        \"\"\"<br>        Get DeepInfra API URL.<br>        \"\"\"<br>        return f\"{INFERENCE_URL}/{self._model_id}\"<br>    async def _apost(self, data: List[str]) -> List[List[float]]:<br>        \"\"\"<br>        Sends a POST request to the DeepInfra Inference API with the given data and returns the API response.<br>        Input data is chunked into batches to avoid exceeding the maximum batch size (1024).<br>        Args:<br>            data (List[str]): A list of strings to be embedded.<br>        Output:<br>            List[float]: A list of embeddings from the API.<br>        \"\"\"<br>        url = self.get_url()<br>        chunked_data = _chunk(data, self.embed_batch_size)<br>        embeddings = []<br>        for chunk in chunked_data:<br>            async with aiohttp.ClientSession() as session:<br>                async with session.post(<br>                    url,<br>                    json={<br>                        \"inputs\": chunk,<br>                    },<br>                    headers=self._get_headers(),<br>                ) as resp:<br>                    response = await resp.json()<br>                    embeddings.extend(response[\"embeddings\"])<br>        return embeddings<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"<br>        Get query embedding.<br>        \"\"\"<br>        return self._post(self._add_query_prefix([query]))[0]<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"<br>        Async get query embedding.<br>        \"\"\"<br>        response = await self._apost(self._add_query_prefix([query]))<br>        return response[0]<br>    def _get_query_embeddings(self, queries: List[str]) -> List[List[float]]:<br>        \"\"\"<br>        Get query embeddings.<br>        \"\"\"<br>        return self._post(self._add_query_prefix(queries))<br>    async def _aget_query_embeddings(self, queries: List[str]) -> List[List[float]]:<br>        \"\"\"<br>        Async get query embeddings.<br>        \"\"\"<br>        return await self._apost(self._add_query_prefix(queries))<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"<br>        Get text embedding.<br>        \"\"\"<br>        return self._post(self._add_text_prefix([text]))[0]<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"<br>        Async get text embedding.<br>        \"\"\"<br>        response = await self._apost(self._add_text_prefix([text]))<br>        return response[0]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"<br>        Get text embedding.<br>        \"\"\"<br>        return self._post(self._add_text_prefix(texts))<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"<br>        Async get text embeddings.<br>        \"\"\"<br>        return await self._apost(self._add_text_prefix(texts))<br>    def _add_query_prefix(self, queries: List[str]) -> List[str]:<br>        \"\"\"<br>        Add query prefix to queries.<br>        \"\"\"<br>        return (<br>            [self._query_prefix + query for query in queries]<br>            if self._query_prefix<br>            else queries<br>        )<br>    def _add_text_prefix(self, texts: List[str]) -> List[str]:<br>        \"\"\"<br>        Add text prefix to texts.<br>        \"\"\"<br>        return (<br>            [self._text_prefix + text for text in texts] if self._text_prefix else texts<br>        )<br>    def _get_headers(self) -> dict:<br>        \"\"\"<br>        Get headers.<br>        \"\"\"<br>        return {<br>            \"Authorization\": f\"Bearer {self._api_token}\",<br>            \"Content-Type\": \"application/json\",<br>            \"User-Agent\": USER_AGENT,<br>        }<br></code>`` |\n"
    },
    {
      "id": "9d94887e-be08-44c6-8135-94bf6921d7d1",
      "size": 2327,
      "headers": {
        "h1": "Aim",
        "h2": "AimCallback \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 13<br> 14<br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br></code>`<code> | </code>`<code><br>class AimCallback(BaseCallbackHandler):<br>    \"\"\"<br>    AimCallback callback class.<br>    Args:<br>        repo (:obj:</code>str<code>, optional):<br>            Aim repository path or Repo object to which Run object is bound.<br>            If skipped, default Repo is used.<br>        experiment_name (:obj:</code>str<code>, optional):<br>            Sets Run's </code>experiment<code> property. 'default' if not specified.<br>            Can be used later to query runs/sequences.<br>        system_tracking_interval (:obj:</code>int<code>, optional):<br>            Sets the tracking interval in seconds for system usage<br>            metrics (CPU, Memory, etc.). Set to </code>None<code> to disable<br>            system metrics tracking.<br>        log_system_params (:obj:</code>bool<code>, optional):<br>            Enable/Disable logging of system params such as installed packages,<br>            git info, environment variables, etc.<br>        capture_terminal_logs (:obj:</code>bool<code>, optional):<br>            Enable/Disable terminal stdout logging.<br>        event_starts_to_ignore (Optional[List[CBEventType]]):<br>            list of event types to ignore when tracking event starts.<br>        event_ends_to_ignore (Optional[List[CBEventType]]):<br>            list of event types to ignore when tracking event ends.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        repo: Optional[str] = None,<br>        experiment_name: Optional[str] = None,<br>        system_tracking_interval: Optional[int] = 1,<br>        log_system_params: Optional[bool] = True,<br>        capture_terminal_logs: Optional[bool] = True,<br>        event_starts_to_ignore: Optional[List[CBEventType]] = None,<br>        event_ends_to_ignore: Optional[List[CBEventType]] = None,<br>        run_params: Optional[Dict[str, Any]] = None,<br>    ) -> None:<br>        if Run is None:<br>            raise ModuleNotFoundError(<br>                \"Please install aim to use the AimCallback: 'pip install aim'\"<br>            )<br>        event_starts_to_ignore = (<br>            event_starts_to_ignore if event_starts_to_ignore else []<br>        )<br>        event_ends_to_ignore = event_ends_to_ignore if event_ends_to_ignore else []<br>        super().__init__(<br>            event_starts_to_ignore=event_starts_to_ignore,<br>            event_ends_to_ignore=event_ends_to_ignore,<br>        )<br>        self.repo = repo<br>        self.experiment_name = experiment_name<br>        self.system_tracking_interval = system_tracking_interval<br>        self.log_system_params = log_system_params<br>        self.capture_terminal_logs = capture_terminal_logs<br>        self._run: Optional[Any] = None<br>        self._run_hash = None<br>        self._llm_response_step = 0<br>        self.setup(run_params)<br>    def on_event_start(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        parent_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> str:<br>        \"\"\"<br>        Args:<br>            event_type (CBEventType): event type to store.<br>            payload (Optional[Dict[str, Any]]): payload to store.<br>            event_id (str): event id to store.<br>            parent_id (str): parent event id.<br>        \"\"\"<br>        return \"\"<br>    def on_event_end(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> None:<br>        \"\"\"<br>        Args:<br>            event_type (CBEventType): event type to store.<br>            payload (Optional[Dict[str, Any]]): payload to store.<br>            event_id (str): event id to store.<br>        \"\"\"<br>        if not self._run:<br>            raise ValueError(\"AimCallback failed to init properly.\")<br>        if event_type is CBEventType.LLM and payload:<br>            if EventPayload.PROMPT in payload:<br>                llm_input = str(payload[EventPayload.PROMPT])<br>                llm_output = str(payload[EventPayload.COMPLETION])<br>            else:<br>                message = payload.get(EventPayload.MESSAGES, [])<br>                llm_input = \"\\n\".join([str(x) for x in message])<br>                llm_output = str(payload[EventPayload.RESPONSE])<br>            self._run.track(<br>                Text(llm_input),<br>                name=\"prompt\",<br>                step=self._llm_response_step,<br>                context={\"event_id\": event_id},<br>            )<br>            self._run.track(<br>                Text(llm_output),<br>                name=\"response\",<br>                step=self._llm_response_step,<br>                context={\"event_id\": event_id},<br>            )<br>            self._llm_response_step += 1<br>        elif event_type is CBEventType.CHUNKING and payload:<br>            for chunk_id, chunk in enumerate(payload[EventPayload.CHUNKS]):<br>                self._run.track(<br>                    Text(chunk),<br>                    name=\"chunk\",<br>                    step=self._llm_response_step,<br>                    context={\"chunk_id\": chunk_id, \"event_id\": event_id},<br>                )<br>    @property<br>    def experiment(self) -> Run:<br>        if not self._run:<br>            self.setup()<br>        return self._run<br>    def setup(self, args: Optional[Dict[str, Any]] = None) -> None:<br>        if not self._run:<br>            if self._run_hash:<br>                self._run = Run(<br>                    self._run_hash,<br>                    repo=self.repo,<br>                    system_tracking_interval=self.system_tracking_interval,<br>                    log_system_params=self.log_system_params,<br>                    capture_terminal_logs=self.capture_terminal_logs,<br>                )<br>            else:<br>                self._run = Run(<br>                    repo=self.repo,<br>                    experiment=self.experiment_name,<br>                    system_tracking_interval=self.system_tracking_interval,<br>                    log_system_params=self.log_system_params,<br>                    capture_terminal_logs=self.capture_terminal_logs,<br>                )<br>                self._run_hash = self._run.hash<br>        # Log config parameters<br>        if args:<br>            try:<br>                for key in args:<br>                    self._run.set(key, args[key], strict=False)<br>            except Exception as e:<br>                logger.warning(f\"Aim could not log config parameters -> {e}\")<br>    def __del__(self) -> None:<br>        if self._run and self._run.active:<br>            self._run.close()<br>    def start_trace(self, trace_id: Optional[str] = None) -> None:<br>        pass<br>    def end_trace(<br>        self,<br>        trace_id: Optional[str] = None,<br>        trace_map: Optional[Dict[str, List[str]]] = None,<br>    ) -> None:<br>        pass<br></code>`` |\n"
    },
    {
      "id": "b6f4b2db-87ea-4e54-9b49-4b6b99604aa3",
      "size": 4407,
      "headers": {
        "h1": "Context",
        "h2": "ContextChatEngine \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br></code>`<code> | </code>`<code><br>class ContextChatEngine(BaseChatEngine):<br>    \"\"\"<br>    Context Chat Engine.<br>    Uses a retriever to retrieve a context, set the context in the system prompt,<br>    and then uses an LLM to generate a response, for a fluid chat experience.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        retriever: BaseRetriever,<br>        llm: LLM,<br>        memory: BaseMemory,<br>        prefix_messages: List[ChatMessage],<br>        node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,<br>        context_template: Optional[str] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>    ) -> None:<br>        self._retriever = retriever<br>        self._llm = llm<br>        self._memory = memory<br>        self._prefix_messages = prefix_messages<br>        self._node_postprocessors = node_postprocessors or []<br>        self._context_template = context_template or DEFAULT_CONTEXT_TEMPLATE<br>        self.callback_manager = callback_manager or CallbackManager([])<br>        for node_postprocessor in self._node_postprocessors:<br>            node_postprocessor.callback_manager = self.callback_manager<br>    @classmethod<br>    def from_defaults(<br>        cls,<br>        retriever: BaseRetriever,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        memory: Optional[BaseMemory] = None,<br>        system_prompt: Optional[str] = None,<br>        prefix_messages: Optional[List[ChatMessage]] = None,<br>        node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,<br>        context_template: Optional[str] = None,<br>        llm: Optional[LLM] = None,<br>        **kwargs: Any,<br>    ) -> \"ContextChatEngine\":<br>        \"\"\"Initialize a ContextChatEngine from default parameters.\"\"\"<br>        llm = llm or Settings.llm<br>        chat_history = chat_history or []<br>        memory = memory or ChatMemoryBuffer.from_defaults(<br>            chat_history=chat_history, token_limit=llm.metadata.context_window - 256<br>        )<br>        if system_prompt is not None:<br>            if prefix_messages is not None:<br>                raise ValueError(<br>                    \"Cannot specify both system_prompt and prefix_messages\"<br>                )<br>            prefix_messages = [<br>                ChatMessage(content=system_prompt, role=llm.metadata.system_role)<br>            ]<br>        prefix_messages = prefix_messages or []<br>        node_postprocessors = node_postprocessors or []<br>        return cls(<br>            retriever,<br>            llm=llm,<br>            memory=memory,<br>            prefix_messages=prefix_messages,<br>            node_postprocessors=node_postprocessors,<br>            callback_manager=Settings.callback_manager,<br>            context_template=context_template,<br>        )<br>    def _generate_context(self, message: str) -> Tuple[str, List[NodeWithScore]]:<br>        \"\"\"Generate context information from a message.\"\"\"<br>        nodes = self._retriever.retrieve(message)<br>        for postprocessor in self._node_postprocessors:<br>            nodes = postprocessor.postprocess_nodes(<br>                nodes, query_bundle=QueryBundle(message)<br>            )<br>        context_str = \"\\n\\n\".join(<br>            [n.node.get_content(metadata_mode=MetadataMode.LLM).strip() for n in nodes]<br>        )<br>        return self._context_template.format(context_str=context_str), nodes<br>    async def _agenerate_context(self, message: str) -> Tuple[str, List[NodeWithScore]]:<br>        \"\"\"Generate context information from a message.\"\"\"<br>        nodes = await self._retriever.aretrieve(message)<br>        for postprocessor in self._node_postprocessors:<br>            nodes = postprocessor.postprocess_nodes(<br>                nodes, query_bundle=QueryBundle(message)<br>            )<br>        context_str = \"\\n\\n\".join(<br>            [n.node.get_content(metadata_mode=MetadataMode.LLM).strip() for n in nodes]<br>        )<br>        return self._context_template.format(context_str=context_str), nodes<br>    def _get_prefix_messages_with_context(self, context_str: str) -> List[ChatMessage]:<br>        \"\"\"Get the prefix messages with context.\"\"\"<br>        # ensure we grab the user-configured system prompt<br>        system_prompt = \"\"<br>        prefix_messages = self._prefix_messages<br>        if (<br>            len(self._prefix_messages) != 0<br>            and self._prefix_messages[0].role == MessageRole.SYSTEM<br>        ):<br>            system_prompt = str(self._prefix_messages[0].content)<br>            prefix_messages = self._prefix_messages[1:]<br>        context_str_w_sys_prompt = system_prompt.strip() + \"\\n\" + context_str<br>        return [<br>            ChatMessage(<br>                content=context_str_w_sys_prompt, role=self._llm.metadata.system_role<br>            ),<br>            *prefix_messages,<br>        ]<br>    @trace_method(\"chat\")<br>    def chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        prev_chunks: Optional[List[NodeWithScore]] = None,<br>    ) -> AgentChatResponse:<br>        if chat_history is not None:<br>            self._memory.set(chat_history)<br>        self._memory.put(ChatMessage(content=message, role=\"user\"))<br>        context_str_template, nodes = self._generate_context(message)<br>        # If the fetched context is completely empty<br>        if len(nodes) == 0 and prev_chunks is not None:<br>            context_str = \"\\n\\n\".join(<br>                [<br>                    n.node.get_content(metadata_mode=MetadataMode.LLM).strip()<br>                    for n in prev_chunks<br>                ]<br>            )<br>            # Create a new context string template by using previous nodes<br>            context_str_template = self._context_template.format(<br>                context_str=context_str<br>            )<br>        prefix_messages = self._get_prefix_messages_with_context(context_str_template)<br>        if hasattr(self._memory, \"tokenizer_fn\"):<br>            prefix_messages_token_count = len(<br>                self._memory.tokenizer_fn(<br>                    \" \".join([(m.content or \"\") for m in prefix_messages])<br>                )<br>            )<br>        else:<br>            prefix_messages_token_count = 0<br>        all_messages = prefix_messages + self._memory.get(<br>            initial_token_count=prefix_messages_token_count<br>        )<br>        chat_response = self._llm.chat(all_messages)<br>        ai_message = chat_response.message<br>        self._memory.put(ai_message)<br>        return AgentChatResponse(<br>            response=str(chat_response.message.content),<br>            sources=[<br>                ToolOutput(<br>                    tool_name=\"retriever\",<br>                    content=str(prefix_messages[0]),<br>                    raw_input={\"message\": message},<br>                    raw_output=prefix_messages[0],<br>                )<br>            ],<br>            source_nodes=nodes,<br>        )<br>    @trace_method(\"chat\")<br>    def stream_chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        prev_chunks: Optional[List[NodeWithScore]] = None,<br>    ) -> StreamingAgentChatResponse:<br>        if chat_history is not None:<br>            self._memory.set(chat_history)<br>        self._memory.put(ChatMessage(content=message, role=\"user\"))<br>        context_str_template, nodes = self._generate_context(message)<br>        # If the fetched context is completely empty<br>        if len(nodes) == 0 and prev_chunks is not None:<br>            context_str = \"\\n\\n\".join(<br>                [<br>                    n.node.get_content(metadata_mode=MetadataMode.LLM).strip()<br>                    for n in prev_chunks<br>                ]<br>            )<br>            # Create a new context string template by using previous nodes<br>            context_str_template = self._context_template.format(<br>                context_str=context_str<br>            )<br>        prefix_messages = self._get_prefix_messages_with_context(context_str_template)<br>        if hasattr(self._memory, \"tokenizer_fn\"):<br>            initial_token_count = len(<br>                self._memory.tokenizer_fn(<br>                    \" \".join([(m.content or \"\") for m in prefix_messages])<br>                )<br>            )<br>        else:<br>            initial_token_count = 0<br>        all_messages = prefix_messages + self._memory.get(<br>            initial_token_count=initial_token_count<br>        )<br>        chat_response = StreamingAgentChatResponse(<br>            chat_stream=self._llm.stream_chat(all_messages),<br>            sources=[<br>                ToolOutput(<br>                    tool_name=\"retriever\",<br>                    content=str(prefix_messages[0]),<br>                    raw_input={\"message\": message},<br>                    raw_output=prefix_messages[0],<br>                )<br>            ],<br>            source_nodes=nodes,<br>        )<br>        thread = Thread(<br>            target=chat_response.write_response_to_history, args=(self._memory,)<br>        )<br>        thread.start()<br>        return chat_response<br>    @trace_method(\"chat\")<br>    async def achat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        prev_chunks: Optional[Sequence[NodeWithScore]] = None,<br>    ) -> AgentChatResponse:<br>        if chat_history is not None:<br>            self._memory.set(chat_history)<br>        self._memory.put(ChatMessage(content=message, role=\"user\"))<br>        context_str_template, nodes = await self._agenerate_context(message)<br>        # If the fetched context is completely empty<br>        if len(nodes) == 0 and prev_chunks is not None:<br>            context_str = \"\\n\\n\".join(<br>                [<br>                    n.node.get_content(metadata_mode=MetadataMode.LLM).strip()<br>                    for n in prev_chunks<br>                ]<br>            )<br>            # Create a new context string template by using previous nodes<br>            context_str_template = self._context_template.format(<br>                context_str=context_str<br>            )<br>        prefix_messages = self._get_prefix_messages_with_context(context_str_template)<br>        if hasattr(self._memory, \"tokenizer_fn\"):<br>            initial_token_count = len(<br>                self._memory.tokenizer_fn(<br>                    \" \".join([(m.content or \"\") for m in prefix_messages])<br>                )<br>            )<br>        else:<br>            initial_token_count = 0<br>        all_messages = prefix_messages + self._memory.get(<br>            initial_token_count=initial_token_count<br>        )<br>        chat_response = await self._llm.achat(all_messages)<br>        ai_message = chat_response.message<br>        self._memory.put(ai_message)<br>        return AgentChatResponse(<br>            response=str(chat_response.message.content),<br>            sources=[<br>                ToolOutput(<br>                    tool_name=\"retriever\",<br>                    content=str(prefix_messages[0]),<br>                    raw_input={\"message\": message},<br>                    raw_output=prefix_messages[0],<br>                )<br>            ],<br>            source_nodes=nodes,<br>        )<br>    @trace_method(\"chat\")<br>    async def astream_chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        prev_chunks: Optional[Sequence[NodeWithScore]] = None,<br>    ) -> StreamingAgentChatResponse:<br>        if chat_history is not None:<br>            self._memory.set(chat_history)<br>        self._memory.put(ChatMessage(content=message, role=\"user\"))<br>        context_str_template, nodes = await self._agenerate_context(message)<br>        # If the fetched context is completely empty<br>        if len(nodes) == 0 and prev_chunks is not None:<br>            context_str = \"\\n\\n\".join(<br>                [<br>                    n.node.get_content(metadata_mode=MetadataMode.LLM).strip()<br>                    for n in prev_chunks<br>                ]<br>            )<br>            # Create a new context string template by using previous nodes<br>            context_str_template = self._context_template.format(<br>                context_str=context_str<br>            )<br>        prefix_messages = self._get_prefix_messages_with_context(context_str_template)<br>        if hasattr(self._memory, \"tokenizer_fn\"):<br>            initial_token_count = len(<br>                self._memory.tokenizer_fn(<br>                    \" \".join([(m.content or \"\") for m in prefix_messages])<br>                )<br>            )<br>        else:<br>            initial_token_count = 0<br>        all_messages = prefix_messages + self._memory.get(<br>            initial_token_count=initial_token_count<br>        )<br>        chat_response = StreamingAgentChatResponse(<br>            achat_stream=await self._llm.astream_chat(all_messages),<br>            sources=[<br>                ToolOutput(<br>                    tool_name=\"retriever\",<br>                    content=str(prefix_messages[0]),<br>                    raw_input={\"message\": message},<br>                    raw_output=prefix_messages[0],<br>                )<br>            ],<br>            source_nodes=nodes,<br>        )<br>        asyncio.create_task(chat_response.awrite_response_to_history(self._memory))<br>        return chat_response<br>    def reset(self) -> None:<br>        self._memory.reset()<br>    @property<br>    def chat_history(self) -> List[ChatMessage]:<br>        \"\"\"Get chat history.\"\"\"<br>        return self._memory.get_all()<br></code>`` |\n"
    },
    {
      "id": "dd4fbedb-60d9-4a12-930c-a9d7d1ca3f58",
      "size": 2648,
      "headers": {
        "h1": "Huggingface api",
        "h2": "HuggingFaceInferenceAPIEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br></code>`<code> | </code>`<code><br>class HuggingFaceInferenceAPIEmbedding(BaseEmbedding):  # type: ignore[misc]<br>    \"\"\"<br>    Wrapper on the Hugging Face's Inference API for embeddings.<br>    Overview of the design:<br>    - Uses the feature extraction task: https://huggingface.co/tasks/feature-extraction<br>    \"\"\"<br>    pooling: Optional[Pooling] = Field(<br>        default=Pooling.CLS,<br>        description=\"Pooling strategy. If None, the model's default pooling is used.\",<br>    )<br>    query_instruction: Optional[str] = Field(<br>        default=None, description=\"Instruction to prepend during query embedding.\"<br>    )<br>    text_instruction: Optional[str] = Field(<br>        default=None, description=\"Instruction to prepend during text embedding.\"<br>    )<br>    # Corresponds with huggingface_hub.InferenceClient<br>    model_name: Optional[str] = Field(<br>        default=None,<br>        description=\"Hugging Face model name. If None, the task will be used.\",<br>    )<br>    token: Union[str, bool, None] = Field(<br>        default=None,<br>        description=(<br>            \"Hugging Face token. Will default to the locally saved token. Pass \"<br>            \"token=False if you donâ€™t want to send your token to the server.\"<br>        ),<br>    )<br>    timeout: Optional[float] = Field(<br>        default=None,<br>        description=(<br>            \"The maximum number of seconds to wait for a response from the server.\"<br>            \" Loading a new model in Inference API can take up to several minutes.\"<br>            \" Defaults to None, meaning it will loop until the server is available.\"<br>        ),<br>    )<br>    headers: Dict[str, str] = Field(<br>        default=None,<br>        description=(<br>            \"Additional headers to send to the server. By default only the\"<br>            \" authorization and user-agent headers are sent. Values in this dictionary\"<br>            \" will override the default values.\"<br>        ),<br>    )<br>    cookies: Dict[str, str] = Field(<br>        default=None, description=\"Additional cookies to send to the server.\"<br>    )<br>    task: Optional[str] = Field(<br>        default=None,<br>        description=(<br>            \"Optional task to pick Hugging Face's recommended model, used when\"<br>            \" model_name is left as default of None.\"<br>        ),<br>    )<br>    _sync_client: \"InferenceClient\" = PrivateAttr()<br>    _async_client: \"AsyncInferenceClient\" = PrivateAttr()<br>    _get_model_info: \"Callable[..., ModelInfo]\" = PrivateAttr()<br>    def _get_inference_client_kwargs(self) -> Dict[str, Any]:<br>        \"\"\"Extract the Hugging Face InferenceClient construction parameters.\"\"\"<br>        return {<br>            \"model\": self.model_name,<br>            \"token\": self.token,<br>            \"timeout\": self.timeout,<br>            \"headers\": self.headers,<br>            \"cookies\": self.cookies,<br>        }<br>    def __init__(self, **kwargs: Any) -> None:<br>        \"\"\"Initialize.<br>        Args:<br>            kwargs: See the class-level Fields.<br>        \"\"\"<br>        if kwargs.get(\"model_name\") is None:<br>            task = kwargs.get(\"task\", \"\")<br>            # NOTE: task being None or empty string leads to ValueError,<br>            # which ensures model is present<br>            kwargs[\"model_name\"] = InferenceClient.get_recommended_model(task=task)<br>            logger.debug(<br>                f\"Using Hugging Face's recommended model {kwargs['model_name']}\"<br>                f\" given task {task}.\"<br>            )<br>            print(kwargs[\"model_name\"], flush=True)<br>        super().__init__(**kwargs)  # Populate pydantic Fields<br>        self._sync_client = InferenceClient(**self._get_inference_client_kwargs())<br>        self._async_client = AsyncInferenceClient(**self._get_inference_client_kwargs())<br>        self._get_model_info = model_info<br>    def validate_supported(self, task: str) -> None:<br>        \"\"\"<br>        Confirm the contained model_name is deployed on the Inference API service.<br>        Args:<br>            task: Hugging Face task to check within. A list of all tasks can be<br>                found here: https://huggingface.co/tasks<br>        \"\"\"<br>        all_models = self._sync_client.list_deployed_models(frameworks=\"all\")<br>        try:<br>            if self.model_name not in all_models[task]:<br>                raise ValueError(<br>                    \"The Inference API service doesn't have the model\"<br>                    f\" {self.model_name!r} deployed.\"<br>                )<br>        except KeyError as exc:<br>            raise KeyError(<br>                f\"Input task {task!r} not in possible tasks {list(all_models.keys())}.\"<br>            ) from exc<br>    def get_model_info(self, **kwargs: Any) -> \"ModelInfo\":<br>        \"\"\"Get metadata on the current model from Hugging Face.\"\"\"<br>        return self._get_model_info(self.model_name, **kwargs)<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"HuggingFaceInferenceAPIEmbedding\"<br>    async def _async_embed_single(self, text: str) -> Embedding:<br>        embedding = await self._async_client.feature_extraction(text)<br>        if len(embedding.shape) == 1:<br>            return embedding.tolist()<br>        embedding = embedding.squeeze(axis=0)<br>        if len(embedding.shape) == 1:  # Some models pool internally<br>            return embedding.tolist()<br>        try:<br>            return self.pooling(embedding).tolist()  # type: ignore[misc]<br>        except TypeError as exc:<br>            raise ValueError(<br>                f\"Pooling is required for {self.model_name} because it returned\"<br>                \" a > 1-D value, please specify pooling as not None.\"<br>            ) from exc<br>    async def _async_embed_bulk(self, texts: Sequence[str]) -> List[Embedding]:<br>        \"\"\"<br>        Embed a sequence of text, in parallel and asynchronously.<br>        NOTE: this uses an externally created asyncio event loop.<br>        \"\"\"<br>        tasks = [self._async_embed_single(text) for text in texts]<br>        return await asyncio.gather(*tasks)<br>    def _get_query_embedding(self, query: str) -> Embedding:<br>        \"\"\"<br>        Embed the input query synchronously.<br>        NOTE: a new asyncio event loop is created internally for this.<br>        \"\"\"<br>        return asyncio.run(self._aget_query_embedding(query))<br>    def _get_text_embedding(self, text: str) -> Embedding:<br>        \"\"\"<br>        Embed the text query synchronously.<br>        NOTE: a new asyncio event loop is created internally for this.<br>        \"\"\"<br>        return asyncio.run(self._aget_text_embedding(text))<br>    def _get_text_embeddings(self, texts: List[str]) -> List[Embedding]:<br>        \"\"\"<br>        Embed the input sequence of text synchronously and in parallel.<br>        NOTE: a new asyncio event loop is created internally for this.<br>        \"\"\"<br>        loop = asyncio.new_event_loop()<br>        try:<br>            tasks = [<br>                loop.create_task(self._aget_text_embedding(text)) for text in texts<br>            ]<br>            loop.run_until_complete(asyncio.wait(tasks))<br>        finally:<br>            loop.close()<br>        return [task.result() for task in tasks]<br>    async def _aget_query_embedding(self, query: str) -> Embedding:<br>        return await self._async_embed_single(<br>            text=format_query(query, self.model_name, self.query_instruction)<br>        )<br>    async def _aget_text_embedding(self, text: str) -> Embedding:<br>        return await self._async_embed_single(<br>            text=format_text(text, self.model_name, self.text_instruction)<br>        )<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[Embedding]:<br>        return await self._async_embed_bulk(<br>            texts=[<br>                format_text(text, self.model_name, self.text_instruction)<br>                for text in texts<br>            ]<br>        )<br></code>`` |\n"
    },
    {
      "id": "589d0a67-ed4a-4762-9e4d-5bf1752357e7",
      "size": 1844,
      "headers": {
        "h1": "Index",
        "h2": "BaseExtractor \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br></code>`<code> | </code>`<code><br>class BaseExtractor(TransformComponent):<br>    \"\"\"Metadata extractor.\"\"\"<br>    is_text_node_only: bool = True<br>    show_progress: bool = Field(default=True, description=\"Whether to show progress.\")<br>    metadata_mode: MetadataMode = Field(<br>        default=MetadataMode.ALL, description=\"Metadata mode to use when reading nodes.\"<br>    )<br>    node_text_template: str = Field(<br>        default=DEFAULT_NODE_TEXT_TEMPLATE,<br>        description=\"Template to represent how node text is mixed with metadata text.\",<br>    )<br>    disable_template_rewrite: bool = Field(<br>        default=False, description=\"Disable the node template rewrite.\"<br>    )<br>    in_place: bool = Field(<br>        default=True, description=\"Whether to process nodes in place.\"<br>    )<br>    num_workers: int = Field(<br>        default=4,<br>        description=\"Number of workers to use for concurrent async processing.\",<br>    )<br>    @classmethod<br>    def from_dict(cls, data: Dict[str, Any], **kwargs: Any) -> Self:  # type: ignore<br>        if isinstance(kwargs, dict):<br>            data.update(kwargs)<br>        data.pop(\"class_name\", None)<br>        llm_predictor = data.get(\"llm_predictor\", None)<br>        if llm_predictor:<br>            from llama_index.core.llm_predictor.loading import load_predictor<br>            llm_predictor = load_predictor(llm_predictor)<br>            data[\"llm_predictor\"] = llm_predictor<br>        llm = data.get(\"llm\", None)<br>        if llm:<br>            from llama_index.core.llms.loading import load_llm<br>            llm = load_llm(llm)<br>            data[\"llm\"] = llm<br>        return cls(**data)<br>    @classmethod<br>    def class_name(cls) -> str:<br>        \"\"\"Get class name.\"\"\"<br>        return \"MetadataExtractor\"<br>    @abstractmethod<br>    async def aextract(self, nodes: Sequence[BaseNode]) -> List[Dict]:<br>        \"\"\"Extracts metadata for a sequence of nodes, returning a list of<br>        metadata dictionaries corresponding to each node.<br>        Args:<br>            nodes (Sequence[Document]): nodes to extract metadata from<br>        \"\"\"<br>    def extract(self, nodes: Sequence[BaseNode]) -> List[Dict]:<br>        \"\"\"Extracts metadata for a sequence of nodes, returning a list of<br>        metadata dictionaries corresponding to each node.<br>        Args:<br>            nodes (Sequence[Document]): nodes to extract metadata from<br>        \"\"\"<br>        return asyncio_run(self.aextract(nodes))<br>    async def aprocess_nodes(<br>        self,<br>        nodes: Sequence[BaseNode],<br>        excluded_embed_metadata_keys: Optional[List[str]] = None,<br>        excluded_llm_metadata_keys: Optional[List[str]] = None,<br>        **kwargs: Any,<br>    ) -> List[BaseNode]:<br>        \"\"\"Post process nodes parsed from documents.<br>        Allows extractors to be chained.<br>        Args:<br>            nodes (List[BaseNode]): nodes to post-process<br>            excluded_embed_metadata_keys (Optional[List[str]]):<br>                keys to exclude from embed metadata<br>            excluded_llm_metadata_keys (Optional[List[str]]):<br>                keys to exclude from llm metadata<br>        \"\"\"<br>        if self.in_place:<br>            new_nodes = nodes<br>        else:<br>            new_nodes = [deepcopy(node) for node in nodes]<br>        cur_metadata_list = await self.aextract(new_nodes)<br>        for idx, node in enumerate(new_nodes):<br>            node.metadata.update(cur_metadata_list[idx])<br>        for idx, node in enumerate(new_nodes):<br>            if excluded_embed_metadata_keys is not None:<br>                node.excluded_embed_metadata_keys.extend(excluded_embed_metadata_keys)<br>            if excluded_llm_metadata_keys is not None:<br>                node.excluded_llm_metadata_keys.extend(excluded_llm_metadata_keys)<br>            if not self.disable_template_rewrite:<br>                if isinstance(node, TextNode):<br>                    cast(TextNode, node).text_template = self.node_text_template<br>        return new_nodes  # type: ignore<br>    def process_nodes(<br>        self,<br>        nodes: Sequence[BaseNode],<br>        excluded_embed_metadata_keys: Optional[List[str]] = None,<br>        excluded_llm_metadata_keys: Optional[List[str]] = None,<br>        **kwargs: Any,<br>    ) -> List[BaseNode]:<br>        return asyncio_run(<br>            self.aprocess_nodes(<br>                nodes,<br>                excluded_embed_metadata_keys=excluded_embed_metadata_keys,<br>                excluded_llm_metadata_keys=excluded_llm_metadata_keys,<br>                **kwargs,<br>            )<br>        )<br>    def __call__(self, nodes: Sequence[BaseNode], **kwargs: Any) -> List[BaseNode]:<br>        \"\"\"Post process nodes parsed from documents.<br>        Allows extractors to be chained.<br>        Args:<br>            nodes (List[BaseNode]): nodes to post-process<br>        \"\"\"<br>        return self.process_nodes(nodes, **kwargs)<br>    async def acall(self, nodes: Sequence[BaseNode], **kwargs: Any) -> List[BaseNode]:<br>        \"\"\"Post process nodes parsed from documents.<br>        Allows extractors to be chained.<br>        Args:<br>            nodes (List[BaseNode]): nodes to post-process<br>        \"\"\"<br>        return await self.aprocess_nodes(nodes, **kwargs)<br></code>`` |\n"
    },
    {
      "id": "ff85e3b6-15d7-4f8b-b620-82b26c0866d6",
      "size": 1702,
      "headers": {
        "h1": "Text embeddings inference",
        "h2": "TextEmbeddingsInference \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br></code>`<code> | </code>`<code><br>class TextEmbeddingsInference(BaseEmbedding):<br>    base_url: str = Field(<br>        default=DEFAULT_URL,<br>        description=\"Base URL for the text embeddings service.\",<br>    )<br>    query_instruction: Optional[str] = Field(<br>        description=\"Instruction to prepend to query text.\"<br>    )<br>    text_instruction: Optional[str] = Field(<br>        description=\"Instruction to prepend to text.\"<br>    )<br>    timeout: float = Field(<br>        default=60.0,<br>        description=\"Timeout in seconds for the request.\",<br>    )<br>    truncate_text: bool = Field(<br>        default=True,<br>        description=\"Whether to truncate text or not when generating embeddings.\",<br>    )<br>    auth_token: Optional[Union[str, Callable[[str], str]]] = Field(<br>        default=None,<br>        description=\"Authentication token or authentication token generating function for authenticated requests\",<br>    )<br>    def __init__(<br>        self,<br>        model_name: str,<br>        base_url: str = DEFAULT_URL,<br>        text_instruction: Optional[str] = None,<br>        query_instruction: Optional[str] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        timeout: float = 60.0,<br>        truncate_text: bool = True,<br>        callback_manager: Optional[CallbackManager] = None,<br>        auth_token: Optional[Union[str, Callable[[str], str]]] = None,<br>    ):<br>        super().__init__(<br>            base_url=base_url,<br>            model_name=model_name,<br>            text_instruction=text_instruction,<br>            query_instruction=query_instruction,<br>            embed_batch_size=embed_batch_size,<br>            timeout=timeout,<br>            truncate_text=truncate_text,<br>            callback_manager=callback_manager,<br>            auth_token=auth_token,<br>        )<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"TextEmbeddingsInference\"<br>    def _call_api(self, texts: List[str]) -> List[List[float]]:<br>        import httpx<br>        headers = {\"Content-Type\": \"application/json\"}<br>        if self.auth_token is not None:<br>            if callable(self.auth_token):<br>                headers[\"Authorization\"] = self.auth_token(self.base_url)<br>            else:<br>                headers[\"Authorization\"] = self.auth_token<br>        json_data = {\"inputs\": texts, \"truncate\": self.truncate_text}<br>        with httpx.Client() as client:<br>            response = client.post(<br>                f\"{self.base_url}/embed\",<br>                headers=headers,<br>                json=json_data,<br>                timeout=self.timeout,<br>            )<br>        return response.json()<br>    async def _acall_api(self, texts: List[str]) -> List[List[float]]:<br>        import httpx<br>        headers = {\"Content-Type\": \"application/json\"}<br>        if self.auth_token is not None:<br>            if callable(self.auth_token):<br>                headers[\"Authorization\"] = self.auth_token(self.base_url)<br>            else:<br>                headers[\"Authorization\"] = self.auth_token<br>        json_data = {\"inputs\": texts, \"truncate\": self.truncate_text}<br>        async with httpx.AsyncClient() as client:<br>            response = await client.post(<br>                f\"{self.base_url}/embed\",<br>                headers=headers,<br>                json=json_data,<br>                timeout=self.timeout,<br>            )<br>        return response.json()<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        query = format_query(query, self.model_name, self.query_instruction)<br>        return self._call_api([query])[0]<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        text = format_text(text, self.model_name, self.text_instruction)<br>        return self._call_api([text])[0]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        texts = [<br>            format_text(text, self.model_name, self.text_instruction) for text in texts<br>        ]<br>        return self._call_api(texts)<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding async.\"\"\"<br>        query = format_query(query, self.model_name, self.query_instruction)<br>        return (await self._acall_api([query]))[0]<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding async.\"\"\"<br>        text = format_text(text, self.model_name, self.text_instruction)<br>        return (await self._acall_api([text]))[0]<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[Embedding]:<br>        texts = [<br>            format_text(text, self.model_name, self.text_instruction) for text in texts<br>        ]<br>        return await self._acall_api(texts)<br></code>`` |\n"
    },
    {
      "id": "6d39505e-622e-4cb5-b36c-4ed50bef5a09",
      "size": 4079,
      "headers": {
        "h1": "Condense question",
        "h2": "CondenseQuestionChatEngine \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br></code>`<code> | </code>`<code><br>class CondenseQuestionChatEngine(BaseChatEngine):<br>    \"\"\"<br>    Condense Question Chat Engine.<br>    First generate a standalone question from conversation context and last message,<br>    then query the query engine for a response.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        query_engine: BaseQueryEngine,<br>        condense_question_prompt: BasePromptTemplate,<br>        memory: BaseMemory,<br>        llm: LLM,<br>        verbose: bool = False,<br>        callback_manager: Optional[CallbackManager] = None,<br>    ) -> None:<br>        self._query_engine = query_engine<br>        self._condense_question_prompt = condense_question_prompt<br>        self._memory = memory<br>        self._llm = llm<br>        self._verbose = verbose<br>        self.callback_manager = callback_manager or CallbackManager([])<br>    @classmethod<br>    def from_defaults(<br>        cls,<br>        query_engine: BaseQueryEngine,<br>        condense_question_prompt: Optional[BasePromptTemplate] = None,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        memory: Optional[BaseMemory] = None,<br>        memory_cls: Type[BaseMemory] = ChatMemoryBuffer,<br>        verbose: bool = False,<br>        system_prompt: Optional[str] = None,<br>        prefix_messages: Optional[List[ChatMessage]] = None,<br>        llm: Optional[LLM] = None,<br>        **kwargs: Any,<br>    ) -> \"CondenseQuestionChatEngine\":<br>        \"\"\"Initialize a CondenseQuestionChatEngine from default parameters.\"\"\"<br>        condense_question_prompt = condense_question_prompt or DEFAULT_PROMPT<br>        llm = llm or Settings.llm<br>        chat_history = chat_history or []<br>        memory = memory or memory_cls.from_defaults(chat_history=chat_history, llm=llm)<br>        if system_prompt is not None:<br>            raise NotImplementedError(<br>                \"system_prompt is not supported for CondenseQuestionChatEngine.\"<br>            )<br>        if prefix_messages is not None:<br>            raise NotImplementedError(<br>                \"prefix_messages is not supported for CondenseQuestionChatEngine.\"<br>            )<br>        return cls(<br>            query_engine,<br>            condense_question_prompt,<br>            memory,<br>            llm,<br>            verbose=verbose,<br>            callback_manager=Settings.callback_manager,<br>        )<br>    def _condense_question(<br>        self, chat_history: List[ChatMessage], last_message: str<br>    ) -> str:<br>        \"\"\"<br>        Generate standalone question from conversation context and last message.<br>        \"\"\"<br>        if not chat_history:<br>            # Keep the question as is if there's no conversation context.<br>            return last_message<br>        chat_history_str = messages_to_history_str(chat_history)<br>        logger.debug(chat_history_str)<br>        return self._llm.predict(<br>            self._condense_question_prompt,<br>            question=last_message,<br>            chat_history=chat_history_str,<br>        )<br>    async def _acondense_question(<br>        self, chat_history: List[ChatMessage], last_message: str<br>    ) -> str:<br>        \"\"\"<br>        Generate standalone question from conversation context and last message.<br>        \"\"\"<br>        if not chat_history:<br>            # Keep the question as is if there's no conversation context.<br>            return last_message<br>        chat_history_str = messages_to_history_str(chat_history)<br>        logger.debug(chat_history_str)<br>        return await self._llm.apredict(<br>            self._condense_question_prompt,<br>            question=last_message,<br>            chat_history=chat_history_str,<br>        )<br>    def _get_tool_output_from_response(<br>        self, query: str, response: RESPONSE_TYPE<br>    ) -> ToolOutput:<br>        if isinstance(response, (StreamingResponse, AsyncStreamingResponse)):<br>            return ToolOutput(<br>                content=\"\",<br>                tool_name=\"query_engine\",<br>                raw_input={\"query\": query},<br>                raw_output=response,<br>            )<br>        else:<br>            return ToolOutput(<br>                content=str(response),<br>                tool_name=\"query_engine\",<br>                raw_input={\"query\": query},<br>                raw_output=response,<br>            )<br>    @trace_method(\"chat\")<br>    def chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> AgentChatResponse:<br>        chat_history = chat_history or self._memory.get(input=message)<br>        # Generate standalone question from conversation context and last message<br>        condensed_question = self._condense_question(chat_history, message)<br>        log_str = f\"Querying with: {condensed_question}\"<br>        logger.info(log_str)<br>        if self._verbose:<br>            print(log_str)<br>        # TODO: right now, query engine uses class attribute to configure streaming,<br>        #       we are moving towards separate streaming and non-streaming methods.<br>        #       In the meanwhile, use this hack to toggle streaming.<br>        from llama_index.core.query_engine.retriever_query_engine import (<br>            RetrieverQueryEngine,<br>        )<br>        if isinstance(self._query_engine, RetrieverQueryEngine):<br>            is_streaming = self._query_engine._response_synthesizer._streaming<br>            self._query_engine._response_synthesizer._streaming = False<br>        # Query with standalone question<br>        query_response = self._query_engine.query(condensed_question)<br>        # NOTE: reset streaming flag<br>        if isinstance(self._query_engine, RetrieverQueryEngine):<br>            self._query_engine._response_synthesizer._streaming = is_streaming<br>        tool_output = self._get_tool_output_from_response(<br>            condensed_question, query_response<br>        )<br>        # Record response<br>        self._memory.put(ChatMessage(role=MessageRole.USER, content=message))<br>        self._memory.put(<br>            ChatMessage(role=MessageRole.ASSISTANT, content=str(query_response))<br>        )<br>        return AgentChatResponse(response=str(query_response), sources=[tool_output])<br>    @trace_method(\"chat\")<br>    def stream_chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> StreamingAgentChatResponse:<br>        chat_history = chat_history or self._memory.get(input=message)<br>        # Generate standalone question from conversation context and last message<br>        condensed_question = self._condense_question(chat_history, message)<br>        log_str = f\"Querying with: {condensed_question}\"<br>        logger.info(log_str)<br>        if self._verbose:<br>            print(log_str)<br>        # TODO: right now, query engine uses class attribute to configure streaming,<br>        #       we are moving towards separate streaming and non-streaming methods.<br>        #       In the meanwhile, use this hack to toggle streaming.<br>        from llama_index.core.query_engine.retriever_query_engine import (<br>            RetrieverQueryEngine,<br>        )<br>        if isinstance(self._query_engine, RetrieverQueryEngine):<br>            is_streaming = self._query_engine._response_synthesizer._streaming<br>            self._query_engine._response_synthesizer._streaming = True<br>        # Query with standalone question<br>        query_response = self._query_engine.query(condensed_question)<br>        # NOTE: reset streaming flag<br>        if isinstance(self._query_engine, RetrieverQueryEngine):<br>            self._query_engine._response_synthesizer._streaming = is_streaming<br>        tool_output = self._get_tool_output_from_response(<br>            condensed_question, query_response<br>        )<br>        # Record response<br>        if (<br>            isinstance(query_response, StreamingResponse)<br>            and query_response.response_gen is not None<br>        ):<br>            # override the generator to include writing to chat history<br>            self._memory.put(ChatMessage(role=MessageRole.USER, content=message))<br>            response = StreamingAgentChatResponse(<br>                chat_stream=response_gen_from_query_engine(query_response.response_gen),<br>                sources=[tool_output],<br>            )<br>            thread = Thread(<br>                target=response.write_response_to_history,<br>                args=(self._memory,),<br>            )<br>            thread.start()<br>        else:<br>            raise ValueError(\"Streaming is not enabled. Please use chat() instead.\")<br>        return response<br>    @trace_method(\"chat\")<br>    async def achat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> AgentChatResponse:<br>        chat_history = chat_history or self._memory.get(input=message)<br>        # Generate standalone question from conversation context and last message<br>        condensed_question = await self._acondense_question(chat_history, message)<br>        log_str = f\"Querying with: {condensed_question}\"<br>        logger.info(log_str)<br>        if self._verbose:<br>            print(log_str)<br>        # TODO: right now, query engine uses class attribute to configure streaming,<br>        #       we are moving towards separate streaming and non-streaming methods.<br>        #       In the meanwhile, use this hack to toggle streaming.<br>        from llama_index.core.query_engine.retriever_query_engine import (<br>            RetrieverQueryEngine,<br>        )<br>        if isinstance(self._query_engine, RetrieverQueryEngine):<br>            is_streaming = self._query_engine._response_synthesizer._streaming<br>            self._query_engine._response_synthesizer._streaming = False<br>        # Query with standalone question<br>        query_response = await self._query_engine.aquery(condensed_question)<br>        # NOTE: reset streaming flag<br>        if isinstance(self._query_engine, RetrieverQueryEngine):<br>            self._query_engine._response_synthesizer._streaming = is_streaming<br>        tool_output = self._get_tool_output_from_response(<br>            condensed_question, query_response<br>        )<br>        # Record response<br>        self._memory.put(ChatMessage(role=MessageRole.USER, content=message))<br>        self._memory.put(<br>            ChatMessage(role=MessageRole.ASSISTANT, content=str(query_response))<br>        )<br>        return AgentChatResponse(response=str(query_response), sources=[tool_output])<br>    @trace_method(\"chat\")<br>    async def astream_chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> StreamingAgentChatResponse:<br>        chat_history = chat_history or self._memory.get(input=message)<br>        # Generate standalone question from conversation context and last message<br>        condensed_question = await self._acondense_question(chat_history, message)<br>        log_str = f\"Querying with: {condensed_question}\"<br>        logger.info(log_str)<br>        if self._verbose:<br>            print(log_str)<br>        # TODO: right now, query engine uses class attribute to configure streaming,<br>        #       we are moving towards separate streaming and non-streaming methods.<br>        #       In the meanwhile, use this hack to toggle streaming.<br>        from llama_index.core.query_engine.retriever_query_engine import (<br>            RetrieverQueryEngine,<br>        )<br>        if isinstance(self._query_engine, RetrieverQueryEngine):<br>            is_streaming = self._query_engine._response_synthesizer._streaming<br>            self._query_engine._response_synthesizer._streaming = True<br>        # Query with standalone question<br>        query_response = await self._query_engine.aquery(condensed_question)<br>        # NOTE: reset streaming flag<br>        if isinstance(self._query_engine, RetrieverQueryEngine):<br>            self._query_engine._response_synthesizer._streaming = is_streaming<br>        tool_output = self._get_tool_output_from_response(<br>            condensed_question, query_response<br>        )<br>        # Record response<br>        if isinstance(query_response, AsyncStreamingResponse):<br>            # override the generator to include writing to chat history<br>            # TODO: query engine does not support async generator yet<br>            self._memory.put(ChatMessage(role=MessageRole.USER, content=message))<br>            response = StreamingAgentChatResponse(<br>                achat_stream=aresponse_gen_from_query_engine(<br>                    query_response.async_response_gen()<br>                ),<br>                sources=[tool_output],<br>            )<br>            asyncio.create_task(response.awrite_response_to_history(self._memory))<br>        else:<br>            raise ValueError(\"Streaming is not enabled. Please use achat() instead.\")<br>        return response<br>    def reset(self) -> None:<br>        # Clear chat history<br>        self._memory.reset()<br>    @property<br>    def chat_history(self) -> List[ChatMessage]:<br>        \"\"\"Get chat history.\"\"\"<br>        return self._memory.get_all()<br></code>`` |\n"
    },
    {
      "id": "a14e5e7d-45a6-47de-a2d3-585af213e608",
      "size": 3330,
      "headers": {
        "h1": "Huggingface",
        "h2": "HuggingFaceEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br></code>`<code> | </code>``<code><br>class HuggingFaceEmbedding(BaseEmbedding):<br>    \"\"\"HuggingFace class for text embeddings.<br>    Args:<br>        model_name (str, optional): If it is a filepath on disc, it loads the model from that path.<br>            If it is not a path, it first tries to download a pre-trained SentenceTransformer model.<br>            If that fails, tries to construct a model from the Hugging Face Hub with that name.<br>            Defaults to DEFAULT_HUGGINGFACE_EMBEDDING_MODEL.<br>        max_length (Optional[int], optional): Max sequence length to set in Model's config. If None,<br>            it will use the Model's default max_seq_length. Defaults to None.<br>        query_instruction (Optional[str], optional): Instruction to prepend to query text.<br>            Defaults to None.<br>        text_instruction (Optional[str], optional): Instruction to prepend to text.<br>            Defaults to None.<br>        normalize (bool, optional): Whether to normalize returned vectors.<br>            Defaults to True.<br>        embed_batch_size (int, optional): The batch size used for the computation.<br>            Defaults to DEFAULT_EMBED_BATCH_SIZE.<br>        cache_folder (Optional[str], optional): Path to store models. Defaults to None.<br>        trust_remote_code (bool, optional): Whether or not to allow for custom models defined on the<br>            Hub in their own modeling files. This option should only be set to True for repositories<br>            you trust and in which you have read the code, as it will execute code present on the Hub<br>            on your local machine. Defaults to False.<br>        device (Optional[str], optional): Device (like \"cuda\", \"cpu\", \"mps\", \"npu\", ...) that should<br>            be used for computation. If None, checks if a GPU can be used. Defaults to None.<br>        callback_manager (Optional[CallbackManager], optional): Callback Manager. Defaults to None.<br>        parallel_process (bool, optional): If True it will start a multi-process pool to process the<br>            encoding with several independent processes. Great for vast amount of texts.<br>            Defaults to False.<br>        target_devices (Optional[List[str]], optional): PyTorch target devices, e.g.<br>            [\"cuda:0\", \"cuda:1\", ...], [\"npu:0\", \"npu:1\", ...], or [\"cpu\", \"cpu\", \"cpu\", \"cpu\"].<br>            If target_devices is None and CUDA/NPU is available, then all available CUDA/NPU devices<br>            will be used. If target_devices is None and CUDA/NPU is not available, then 4 CPU devices<br>            will be used. This parameter will only be used if </code>parallel_process = True<code>.<br>            Defaults to None.<br>        num_workers (int, optional): The number of workers to use for async embedding calls.<br>            Defaults to None.<br>        **model_kwargs: Other model kwargs to use<br>        tokenizer_name (Optional[str], optional): \"Deprecated\"<br>        pooling (str, optional): \"Deprecated\"<br>        model (Optional[Any], optional): \"Deprecated\"<br>        tokenizer (Optional[Any], optional): \"Deprecated\"<br>    Examples:<br>        </code>pip install llama-index-embeddings-huggingface<code><br>        </code>`<code>python<br>        from llama_index.core import Settings<br>        from llama_index.embeddings.huggingface import HuggingFaceEmbedding<br>        # Set up the HuggingFaceEmbedding class with the required model to use with llamaindex core.<br>        embed_model  = HuggingFaceEmbedding(model_name = \"BAAI/bge-small-en\")<br>        Settings.embed_model = embed_model<br>        # Or if you want to Embed some text separately<br>        embeddings = embed_model.get_text_embedding(\"I want to Embed this text!\")<br>        </code>`<code><br>    \"\"\"<br>    max_length: int = Field(<br>        default=DEFAULT_HUGGINGFACE_LENGTH, description=\"Maximum length of input.\", gt=0<br>    )<br>    normalize: bool = Field(default=True, description=\"Normalize embeddings or not.\")<br>    query_instruction: Optional[str] = Field(<br>        description=\"Instruction to prepend to query text.\", default=None<br>    )<br>    text_instruction: Optional[str] = Field(<br>        description=\"Instruction to prepend to text.\", default=None<br>    )<br>    cache_folder: Optional[str] = Field(<br>        description=\"Cache folder for Hugging Face files.\", default=None<br>    )<br>    _model: Any = PrivateAttr()<br>    _device: str = PrivateAttr()<br>    _parallel_process: bool = PrivateAttr()<br>    _target_devices: Optional[List[str]] = PrivateAttr()<br>    def __init__(<br>        self,<br>        model_name: str = DEFAULT_HUGGINGFACE_EMBEDDING_MODEL,<br>        tokenizer_name: Optional[str] = \"deprecated\",<br>        pooling: str = \"deprecated\",<br>        max_length: Optional[int] = None,<br>        query_instruction: Optional[str] = None,<br>        text_instruction: Optional[str] = None,<br>        normalize: bool = True,<br>        model: Optional[Any] = \"deprecated\",<br>        tokenizer: Optional[Any] = \"deprecated\",<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        cache_folder: Optional[str] = None,<br>        trust_remote_code: bool = False,<br>        device: Optional[str] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        parallel_process: bool = False,<br>        target_devices: Optional[List[str]] = None,<br>        **model_kwargs,<br>    ):<br>        device = device or infer_torch_device()<br>        cache_folder = cache_folder or get_cache_dir()<br>        for variable, value in [<br>            (\"model\", model),<br>            (\"tokenizer\", tokenizer),<br>            (\"pooling\", pooling),<br>            (\"tokenizer_name\", tokenizer_name),<br>        ]:<br>            if value != \"deprecated\":<br>                raise ValueError(<br>                    f\"{variable} is deprecated. Please remove it from the arguments.\"<br>                )<br>        if model_name is None:<br>            raise ValueError(\"The </code>model_name<code> argument must be provided.\")<br>        model = SentenceTransformer(<br>            model_name,<br>            device=device,<br>            cache_folder=cache_folder,<br>            trust_remote_code=trust_remote_code,<br>            prompts={<br>                \"query\": query_instruction<br>                or get_query_instruct_for_model_name(model_name),<br>                \"text\": text_instruction<br>                or get_text_instruct_for_model_name(model_name),<br>            },<br>            **model_kwargs,<br>        )<br>        if max_length:<br>            model.max_seq_length = max_length<br>        else:<br>            max_length = model.max_seq_length<br>        super().__init__(<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            model_name=model_name,<br>            max_length=max_length,<br>            normalize=normalize,<br>            query_instruction=query_instruction,<br>            text_instruction=text_instruction,<br>        )<br>        self._device = device<br>        self._model = model<br>        self._parallel_process = parallel_process<br>        self._target_devices = target_devices<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"HuggingFaceEmbedding\"<br>    def _embed(<br>        self,<br>        sentences: List[str],<br>        prompt_name: Optional[str] = None,<br>    ) -> List[List[float]]:<br>        \"\"\"Generates Embeddings either multiprocess or single process.<br>        Args:<br>            sentences (List[str]): Texts or Sentences to embed<br>            prompt_name (Optional[str], optional): The name of the prompt to use for encoding. Must be a key in the </code>prompts<code> dictionary i.e. \"query\" or \"text\" If </code><code>prompt</code><code> is also set, this argument is ignored. Defaults to None.<br>        Returns:<br>            List[List[float]]: a 2d numpy array with shape [num_inputs, output_dimension] is returned.<br>            If only one string input is provided, then the output is a 1d array with shape [output_dimension]<br>        \"\"\"<br>        if self._parallel_process:<br>            pool = self._model.start_multi_process_pool(<br>                target_devices=self._target_devices<br>            )<br>            emb = self._model.encode_multi_process(<br>                sentences=sentences,<br>                pool=pool,<br>                batch_size=self.embed_batch_size,<br>                prompt_name=prompt_name,<br>                normalize_embeddings=self.normalize,<br>            )<br>            self._model.stop_multi_process_pool(pool=pool)<br>        else:<br>            emb = self._model.encode(<br>                sentences,<br>                batch_size=self.embed_batch_size,<br>                prompt_name=prompt_name,<br>                normalize_embeddings=self.normalize,<br>            )<br>        return emb.tolist()<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Generates Embeddings for Query.<br>        Args:<br>            query (str): Query text/sentence<br>        Returns:<br>            List[float]: numpy array of embeddings<br>        \"\"\"<br>        return self._embed(query, prompt_name=\"query\")<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Generates Embeddings for Query Asynchronously.<br>        Args:<br>            query (str): Query text/sentence<br>        Returns:<br>            List[float]: numpy array of embeddings<br>        \"\"\"<br>        return self._get_query_embedding(query)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Generates Embeddings for text Asynchronously.<br>        Args:<br>            text (str): Text/Sentence<br>        Returns:<br>            List[float]: numpy array of embeddings<br>        \"\"\"<br>        return self._get_text_embedding(text)<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Generates Embeddings for text.<br>        Args:<br>            text (str): Text/sentences<br>        Returns:<br>            List[float]: numpy array of embeddings<br>        \"\"\"<br>        return self._embed(text, prompt_name=\"text\")<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Generates Embeddings for text.<br>        Args:<br>            texts (List[str]): Texts / Sentences<br>        Returns:<br>            List[List[float]]: numpy array of embeddings<br>        \"\"\"<br>        return self._embed(texts, prompt_name=\"text\")<br></code>``` |\n"
    },
    {
      "id": "5a1c3f89-b854-47b8-9ffd-2e7d01233c9b",
      "size": 2645,
      "headers": {
        "h1": "Huggingface",
        "h2": "HuggingFaceInferenceAPIEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>472<br>473<br>474<br>475<br>476<br></code>`<code> | </code>`<code><br>@deprecated(<br>    \"Deprecated in favor of </code>HuggingFaceInferenceAPIEmbedding<code> from </code>llama-index-embeddings-huggingface-api<code> which should be used instead.\",<br>    action=\"always\",<br>)<br>class HuggingFaceInferenceAPIEmbedding(BaseEmbedding):  # type: ignore[misc]<br>    \"\"\"<br>    Wrapper on the Hugging Face's Inference API for embeddings.<br>    Overview of the design:<br>    - Uses the feature extraction task: https://huggingface.co/tasks/feature-extraction<br>    \"\"\"<br>    pooling: Optional[Pooling] = Field(<br>        default=Pooling.CLS,<br>        description=\"Pooling strategy. If None, the model's default pooling is used.\",<br>    )<br>    query_instruction: Optional[str] = Field(<br>        default=None, description=\"Instruction to prepend during query embedding.\"<br>    )<br>    text_instruction: Optional[str] = Field(<br>        default=None, description=\"Instruction to prepend during text embedding.\"<br>    )<br>    # Corresponds with huggingface_hub.InferenceClient<br>    model_name: Optional[str] = Field(<br>        default=None,<br>        description=\"Hugging Face model name. If None, the task will be used.\",<br>    )<br>    token: Union[str, bool, None] = Field(<br>        default=None,<br>        description=(<br>            \"Hugging Face token. Will default to the locally saved token. Pass \"<br>            \"token=False if you donâ€™t want to send your token to the server.\"<br>        ),<br>    )<br>    timeout: Optional[float] = Field(<br>        default=None,<br>        description=(<br>            \"The maximum number of seconds to wait for a response from the server.\"<br>            \" Loading a new model in Inference API can take up to several minutes.\"<br>            \" Defaults to None, meaning it will loop until the server is available.\"<br>        ),<br>    )<br>    headers: Dict[str, str] = Field(<br>        default=None,<br>        description=(<br>            \"Additional headers to send to the server. By default only the\"<br>            \" authorization and user-agent headers are sent. Values in this dictionary\"<br>            \" will override the default values.\"<br>        ),<br>    )<br>    cookies: Dict[str, str] = Field(<br>        default=None, description=\"Additional cookies to send to the server.\"<br>    )<br>    task: Optional[str] = Field(<br>        default=None,<br>        description=(<br>            \"Optional task to pick Hugging Face's recommended model, used when\"<br>            \" model_name is left as default of None.\"<br>        ),<br>    )<br>    _sync_client: \"InferenceClient\" = PrivateAttr()<br>    _async_client: \"AsyncInferenceClient\" = PrivateAttr()<br>    _get_model_info: \"Callable[..., ModelInfo]\" = PrivateAttr()<br>    def _get_inference_client_kwargs(self) -> Dict[str, Any]:<br>        \"\"\"Extract the Hugging Face InferenceClient construction parameters.\"\"\"<br>        return {<br>            \"model\": self.model_name,<br>            \"token\": self.token,<br>            \"timeout\": self.timeout,<br>            \"headers\": self.headers,<br>            \"cookies\": self.cookies,<br>        }<br>    def __init__(self, **kwargs: Any) -> None:<br>        \"\"\"Initialize.<br>        Args:<br>            kwargs: See the class-level Fields.<br>        \"\"\"<br>        if kwargs.get(\"model_name\") is None:<br>            task = kwargs.get(\"task\", \"\")<br>            # NOTE: task being None or empty string leads to ValueError,<br>            # which ensures model is present<br>            kwargs[\"model_name\"] = InferenceClient.get_recommended_model(task=task)<br>            logger.debug(<br>                f\"Using Hugging Face's recommended model {kwargs['model_name']}\"<br>                f\" given task {task}.\"<br>            )<br>            print(kwargs[\"model_name\"], flush=True)<br>        super().__init__(**kwargs)  # Populate pydantic Fields<br>        self._sync_client = InferenceClient(**self._get_inference_client_kwargs())<br>        self._async_client = AsyncInferenceClient(**self._get_inference_client_kwargs())<br>        self._get_model_info = model_info<br>    def validate_supported(self, task: str) -> None:<br>        \"\"\"<br>        Confirm the contained model_name is deployed on the Inference API service.<br>        Args:<br>            task: Hugging Face task to check within. A list of all tasks can be<br>                found here: https://huggingface.co/tasks<br>        \"\"\"<br>        all_models = self._sync_client.list_deployed_models(frameworks=\"all\")<br>        try:<br>            if self.model_name not in all_models[task]:<br>                raise ValueError(<br>                    \"The Inference API service doesn't have the model\"<br>                    f\" {self.model_name!r} deployed.\"<br>                )<br>        except KeyError as exc:<br>            raise KeyError(<br>                f\"Input task {task!r} not in possible tasks {list(all_models.keys())}.\"<br>            ) from exc<br>    def get_model_info(self, **kwargs: Any) -> \"ModelInfo\":<br>        \"\"\"Get metadata on the current model from Hugging Face.\"\"\"<br>        return self._get_model_info(self.model_name, **kwargs)<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"HuggingFaceInferenceAPIEmbedding\"<br>    async def _async_embed_single(self, text: str) -> Embedding:<br>        embedding = await self._async_client.feature_extraction(text)<br>        if len(embedding.shape) == 1:<br>            return embedding.tolist()<br>        embedding = embedding.squeeze(axis=0)<br>        if len(embedding.shape) == 1:  # Some models pool internally<br>            return embedding.tolist()<br>        try:<br>            return self.pooling(embedding).tolist()  # type: ignore[misc]<br>        except TypeError as exc:<br>            raise ValueError(<br>                f\"Pooling is required for {self.model_name} because it returned\"<br>                \" a > 1-D value, please specify pooling as not None.\"<br>            ) from exc<br>    async def _async_embed_bulk(self, texts: Sequence[str]) -> List[Embedding]:<br>        \"\"\"<br>        Embed a sequence of text, in parallel and asynchronously.<br>        NOTE: this uses an externally created asyncio event loop.<br>        \"\"\"<br>        tasks = [self._async_embed_single(text) for text in texts]<br>        return await asyncio.gather(*tasks)<br>    def _get_query_embedding(self, query: str) -> Embedding:<br>        \"\"\"<br>        Embed the input query synchronously.<br>        NOTE: a new asyncio event loop is created internally for this.<br>        \"\"\"<br>        return asyncio.run(self._aget_query_embedding(query))<br>    def _get_text_embedding(self, text: str) -> Embedding:<br>        \"\"\"<br>        Embed the text query synchronously.<br>        NOTE: a new asyncio event loop is created internally for this.<br>        \"\"\"<br>        return asyncio.run(self._aget_text_embedding(text))<br>    def _get_text_embeddings(self, texts: List[str]) -> List[Embedding]:<br>        \"\"\"<br>        Embed the input sequence of text synchronously and in parallel.<br>        NOTE: a new asyncio event loop is created internally for this.<br>        \"\"\"<br>        loop = asyncio.new_event_loop()<br>        try:<br>            tasks = [<br>                loop.create_task(self._aget_text_embedding(text)) for text in texts<br>            ]<br>            loop.run_until_complete(asyncio.wait(tasks))<br>        finally:<br>            loop.close()<br>        return [task.result() for task in tasks]<br>    async def _aget_query_embedding(self, query: str) -> Embedding:<br>        return await self._async_embed_single(<br>            text=format_query(query, self.model_name, self.query_instruction)<br>        )<br>    async def _aget_text_embedding(self, text: str) -> Embedding:<br>        return await self._async_embed_single(<br>            text=format_text(text, self.model_name, self.text_instruction)<br>        )<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[Embedding]:<br>        return await self._async_embed_bulk(<br>            texts=[<br>                format_text(text, self.model_name, self.text_instruction)<br>                for text in texts<br>            ]<br>        )<br></code>`` |\n"
    },
    {
      "id": "e4c3ca01-ed37-4142-be02-1b5cda00e793",
      "size": 2222,
      "headers": {
        "h1": "Huggingface optimum",
        "h2": "OptimumEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br></code>`<code> | </code>`<code><br>class OptimumEmbedding(BaseEmbedding):<br>    folder_name: str = Field(description=\"Folder name to load from.\")<br>    max_length: int = Field(description=\"Maximum length of input.\")<br>    pooling: str = Field(description=\"Pooling strategy. One of ['cls', 'mean'].\")<br>    normalize: bool = Field(default=True, description=\"Normalize embeddings or not.\")<br>    query_instruction: Optional[str] = Field(<br>        description=\"Instruction to prepend to query text.\"<br>    )<br>    text_instruction: Optional[str] = Field(<br>        description=\"Instruction to prepend to text.\"<br>    )<br>    cache_folder: Optional[str] = Field(<br>        description=\"Cache folder for huggingface files.\", default=None<br>    )<br>    _model: Any = PrivateAttr()<br>    _tokenizer: Any = PrivateAttr()<br>    _device: Any = PrivateAttr()<br>    def __init__(<br>        self,<br>        folder_name: str,<br>        pooling: str = \"cls\",<br>        max_length: Optional[int] = None,<br>        normalize: bool = True,<br>        query_instruction: Optional[str] = None,<br>        text_instruction: Optional[str] = None,<br>        model: Optional[Any] = None,<br>        tokenizer: Optional[Any] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>        device: Optional[str] = None,<br>    ):<br>        model = model or ORTModelForFeatureExtraction.from_pretrained(folder_name)<br>        tokenizer = tokenizer or AutoTokenizer.from_pretrained(folder_name)<br>        device = device or infer_torch_device()<br>        if max_length is None:<br>            try:<br>                max_length = int(model.config.max_position_embeddings)<br>            except Exception:<br>                raise ValueError(<br>                    \"Unable to find max_length from model config. \"<br>                    \"Please provide max_length.\"<br>                )<br>            try:<br>                max_length = min(max_length, int(tokenizer.model_max_length))<br>            except Exception as exc:<br>                print(f\"An error occurred while retrieving tokenizer max length: {exc}\")<br>        if pooling not in [\"cls\", \"mean\"]:<br>            raise ValueError(f\"Pooling {pooling} not supported.\")<br>        super().__init__(<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            folder_name=folder_name,<br>            max_length=max_length,<br>            pooling=pooling,<br>            normalize=normalize,<br>            query_instruction=query_instruction,<br>            text_instruction=text_instruction,<br>        )<br>        self._model = model<br>        self._device = device<br>        self._tokenizer = tokenizer<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"OptimumEmbedding\"<br>    @classmethod<br>    def create_and_save_optimum_model(<br>        cls,<br>        model_name_or_path: str,<br>        output_path: str,<br>        export_kwargs: Optional[dict] = None,<br>    ) -> None:<br>        try:<br>            from optimum.onnxruntime import ORTModelForFeatureExtraction<br>            from transformers import AutoTokenizer<br>        except ImportError:<br>            raise ImportError(<br>                \"OptimumEmbedding requires transformers to be installed.\\n\"<br>                \"Please install transformers with \"<br>                \"</code>pip install transformers optimum[exporters]<code>.\"<br>            )<br>        export_kwargs = export_kwargs or {}<br>        model = ORTModelForFeatureExtraction.from_pretrained(<br>            model_name_or_path, export=True, **export_kwargs<br>        )<br>        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)<br>        model.save_pretrained(output_path)<br>        tokenizer.save_pretrained(output_path)<br>        print(<br>            f\"Saved optimum model to {output_path}. Use it with \"<br>            f\"</code>embed_model = OptimumEmbedding(folder_name='{output_path}')<code>.\"<br>        )<br>    def _mean_pooling(self, model_output: Any, attention_mask: Any) -> Any:<br>        \"\"\"Mean Pooling - Take attention mask into account for correct averaging.\"\"\"<br>        import torch<br>        # First element of model_output contains all token embeddings<br>        token_embeddings = model_output[0]<br>        input_mask_expanded = (<br>            attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()<br>        )<br>        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(<br>            input_mask_expanded.sum(1), min=1e-9<br>        )<br>    def _cls_pooling(self, model_output: list) -> Any:<br>        \"\"\"Use the CLS token as the pooling token.\"\"\"<br>        return model_output[0][:, 0]<br>    def _embed(self, sentences: List[str]) -> List[List[float]]:<br>        \"\"\"Embed sentences.\"\"\"<br>        encoded_input = self._tokenizer(<br>            sentences,<br>            padding=True,<br>            max_length=self.max_length,<br>            truncation=True,<br>            return_tensors=\"pt\",<br>        )<br>        model_output = self._model(**encoded_input)<br>        if self.pooling == \"cls\":<br>            embeddings = self._cls_pooling(model_output)<br>        else:<br>            embeddings = self._mean_pooling(<br>                model_output, encoded_input[\"attention_mask\"].to(self._device)<br>            )<br>        if self.normalize:<br>            import torch<br>            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)<br>        return embeddings.tolist()<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        query = format_query(query, self.model_name, self.query_instruction)<br>        return self._embed([query])[0]<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding async.\"\"\"<br>        return self._get_query_embedding(query)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding async.\"\"\"<br>        return self._get_text_embedding(text)<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        text = format_text(text, self.model_name, self.text_instruction)<br>        return self._embed([text])[0]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        texts = [<br>            format_text(text, self.model_name, self.text_instruction) for text in texts<br>        ]<br>        return self._embed(texts)<br></code>`` |\n"
    },
    {
      "id": "31a7d941-e086-41f5-ab52-de2e1e204966",
      "size": 2222,
      "headers": {
        "h1": "Azure inference",
        "h2": "AzureAIEmbeddingsModel \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br></code>`<code> | </code>``<code><br>class AzureAIEmbeddingsModel(BaseEmbedding):<br>    \"\"\"Azure AI model inference for embeddings.<br>    Examples:<br>        </code>`<code>python<br>        from llama_index.core import Settings<br>        from llama_index.embeddings.azure_inference import AzureAIEmbeddingsModel<br>        llm = AzureAIEmbeddingsModel(<br>            endpoint=\"https://[your-endpoint].inference.ai.azure.com\",<br>            credential=\"your-api-key\",<br>        )<br>        # # If using Microsoft Entra ID authentication, you can create the<br>        # # client as follows<br>        #<br>        # from azure.identity import DefaultAzureCredential<br>        #<br>        # embed_model = AzureAIEmbeddingsModel(<br>        #     endpoint=\"https://[your-endpoint].inference.ai.azure.com\",<br>        #     credential=DefaultAzureCredential()<br>        # )<br>        #<br>        # # If you plan to use asynchronous calling, make sure to use the async<br>        # # credentials as follows<br>        #<br>        # from azure.identity.aio import DefaultAzureCredential as DefaultAzureCredentialAsync<br>        #<br>        # embed_model = AzureAIEmbeddingsModel(<br>        #     endpoint=\"https://[your-endpoint].inference.ai.azure.com\",<br>        #     credential=DefaultAzureCredentialAsync()<br>        # )<br>        # Once the client is instantiated, you can set the context to use the model<br>        Settings.embed_model = embed_model<br>        documents = SimpleDirectoryReader(\"./data\").load_data()<br>        index = VectorStoreIndex.from_documents(documents)<br>        </code>`<code><br>    \"\"\"<br>    model_kwargs: Dict[str, Any] = Field(<br>        default_factory=dict, description=\"Additional kwargs model parameters.\"<br>    )<br>    _client: EmbeddingsClient = PrivateAttr()<br>    _async_client: EmbeddingsClientAsync = PrivateAttr()<br>    def __init__(<br>        self,<br>        endpoint: str = None,<br>        credential: Union[str, AzureKeyCredential, \"TokenCredential\"] = None,<br>        model_name: str = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>        num_workers: Optional[int] = None,<br>        client_kwargs: Optional[Dict[str, Any]] = None,<br>        **kwargs: Any,<br>    ):<br>        client_kwargs = client_kwargs or {}<br>        endpoint = get_from_param_or_env(<br>            \"endpoint\", endpoint, \"AZURE_INFERENCE_ENDPOINT\", None<br>        )<br>        credential = get_from_param_or_env(<br>            \"credential\", credential, \"AZURE_INFERENCE_CREDENTIAL\", None<br>        )<br>        credential = (<br>            AzureKeyCredential(credential)<br>            if isinstance(credential, str)<br>            else credential<br>        )<br>        if not endpoint:<br>            raise ValueError(<br>                \"You must provide an endpoint to use the Azure AI model inference LLM.\"<br>                \"Pass the endpoint as a parameter or set the AZURE_INFERENCE_ENDPOINT\"<br>                \"environment variable.\"<br>            )<br>        if not credential:<br>            raise ValueError(<br>                \"You must provide an credential to use the Azure AI model inference LLM.\"<br>                \"Pass the credential as a parameter or set the AZURE_INFERENCE_CREDENTIAL\"<br>            )<br>        client = EmbeddingsClient(<br>            endpoint=endpoint,<br>            credential=credential,<br>            user_agent=\"llamaindex\",<br>            **client_kwargs,<br>        )<br>        async_client = EmbeddingsClientAsync(<br>            endpoint=endpoint,<br>            credential=credential,<br>            user_agent=\"llamaindex\",<br>            **client_kwargs,<br>        )<br>        if not model_name:<br>            try:<br>                # Get model info from the endpoint. This method may not be supported by all<br>                # endpoints.<br>                model_info = client.get_model_info()<br>                model_name = model_info.get(\"model_name\", None)<br>            except HttpResponseError:<br>                logger.warning(<br>                    f\"Endpoint '{self._client._config.endpoint}' does not support model metadata retrieval. \"<br>                    \"Unable to populate model attributes.\"<br>                )<br>        super().__init__(<br>            model_name=model_name or \"unknown\",<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            num_workers=num_workers,<br>            **kwargs,<br>        )<br>        self._client = client<br>        self._async_client = async_client<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"AzureAIEmbeddingsModel\"<br>    @property<br>    def _model_kwargs(self) -> Dict[str, Any]:<br>        additional_kwargs = {}<br>        if self.model_name and self.model_name != \"unknown\":<br>            additional_kwargs[\"model\"] = self.model_name<br>        if self.model_kwargs:<br>            # pass any extra model parameters<br>            additional_kwargs.update(self.model_kwargs)<br>        return additional_kwargs<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self._client.embed(input=[query], **self._model_kwargs).data[0].embedding<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        return (<br>            (await self._async_client.embed(input=[query], **self._model_kwargs))<br>            .data[0]<br>            .embedding<br>        )<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._client.embed(input=[text], **self._model_kwargs).data[0].embedding<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously get text embedding.\"\"\"<br>        return (<br>            (await self._async_client.embed(input=[text], **self._model_kwargs))<br>            .data[0]<br>            .embedding<br>        )<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        embedding_response = self._client.embed(input=texts, **self._model_kwargs).data<br>        return [embed.embedding for embed in embedding_response]<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Asynchronously get text embeddings.\"\"\"<br>        embedding_response = await self._async_client.embed(<br>            input=texts, **self._model_kwargs<br>        )<br>        return [embed.embedding for embed in embedding_response.data]<br></code>``` |\n"
    },
    {
      "id": "51144cae-d434-46a1-b962-a875ab7053e3",
      "size": 4684,
      "headers": {
        "h1": "Index",
        "h2": "BaseEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br></code>`<code> | </code>`<code><br>class BaseEmbedding(TransformComponent, DispatcherSpanMixin):<br>    \"\"\"Base class for embeddings.\"\"\"<br>    model_config = ConfigDict(<br>        protected_namespaces=(\"pydantic_model_\",), arbitrary_types_allowed=True<br>    )<br>    model_name: str = Field(<br>        default=\"unknown\", description=\"The name of the embedding model.\"<br>    )<br>    embed_batch_size: int = Field(<br>        default=DEFAULT_EMBED_BATCH_SIZE,<br>        description=\"The batch size for embedding calls.\",<br>        gt=0,<br>        le=2048,<br>    )<br>    callback_manager: CallbackManager = Field(<br>        default_factory=lambda: CallbackManager([]), exclude=True<br>    )<br>    num_workers: Optional[int] = Field(<br>        default=None,<br>        description=\"The number of workers to use for async embedding calls.\",<br>    )<br>    @field_validator(\"callback_manager\")<br>    @classmethod<br>    def check_callback_manager(cls, v: CallbackManager) -> CallbackManager:<br>        if v is None:<br>            return CallbackManager([])<br>        return v<br>    @abstractmethod<br>    def _get_query_embedding(self, query: str) -> Embedding:<br>        \"\"\"<br>        Embed the input query synchronously.<br>        Subclasses should implement this method. Reference get_query_embedding's<br>        docstring for more information.<br>        \"\"\"<br>    @abstractmethod<br>    async def _aget_query_embedding(self, query: str) -> Embedding:<br>        \"\"\"<br>        Embed the input query asynchronously.<br>        Subclasses should implement this method. Reference get_query_embedding's<br>        docstring for more information.<br>        \"\"\"<br>    @dispatcher.span<br>    def get_query_embedding(self, query: str) -> Embedding:<br>        \"\"\"<br>        Embed the input query.<br>        When embedding a query, depending on the model, a special instruction<br>        can be prepended to the raw query string. For example, \"Represent the<br>        question for retrieving supporting documents: \". If you're curious,<br>        other examples of predefined instructions can be found in<br>        embeddings/huggingface_utils.py.<br>        \"\"\"<br>        model_dict = self.to_dict()<br>        model_dict.pop(\"api_key\", None)<br>        dispatcher.event(<br>            EmbeddingStartEvent(<br>                model_dict=model_dict,<br>            )<br>        )<br>        with self.callback_manager.event(<br>            CBEventType.EMBEDDING, payload={EventPayload.SERIALIZED: self.to_dict()}<br>        ) as event:<br>            query_embedding = self._get_query_embedding(query)<br>            event.on_end(<br>                payload={<br>                    EventPayload.CHUNKS: [query],<br>                    EventPayload.EMBEDDINGS: [query_embedding],<br>                },<br>            )<br>        dispatcher.event(<br>            EmbeddingEndEvent(<br>                chunks=[query],<br>                embeddings=[query_embedding],<br>            )<br>        )<br>        return query_embedding<br>    @dispatcher.span<br>    async def aget_query_embedding(self, query: str) -> Embedding:<br>        \"\"\"Get query embedding.\"\"\"<br>        model_dict = self.to_dict()<br>        model_dict.pop(\"api_key\", None)<br>        dispatcher.event(<br>            EmbeddingStartEvent(<br>                model_dict=model_dict,<br>            )<br>        )<br>        with self.callback_manager.event(<br>            CBEventType.EMBEDDING, payload={EventPayload.SERIALIZED: self.to_dict()}<br>        ) as event:<br>            query_embedding = await self._aget_query_embedding(query)<br>            event.on_end(<br>                payload={<br>                    EventPayload.CHUNKS: [query],<br>                    EventPayload.EMBEDDINGS: [query_embedding],<br>                },<br>            )<br>        dispatcher.event(<br>            EmbeddingEndEvent(<br>                chunks=[query],<br>                embeddings=[query_embedding],<br>            )<br>        )<br>        return query_embedding<br>    def get_agg_embedding_from_queries(<br>        self,<br>        queries: List[str],<br>        agg_fn: Optional[Callable[..., Embedding]] = None,<br>    ) -> Embedding:<br>        \"\"\"Get aggregated embedding from multiple queries.\"\"\"<br>        query_embeddings = [self.get_query_embedding(query) for query in queries]<br>        agg_fn = agg_fn or mean_agg<br>        return agg_fn(query_embeddings)<br>    async def aget_agg_embedding_from_queries(<br>        self,<br>        queries: List[str],<br>        agg_fn: Optional[Callable[..., Embedding]] = None,<br>    ) -> Embedding:<br>        \"\"\"Async get aggregated embedding from multiple queries.\"\"\"<br>        query_embeddings = [await self.aget_query_embedding(query) for query in queries]<br>        agg_fn = agg_fn or mean_agg<br>        return agg_fn(query_embeddings)<br>    @abstractmethod<br>    def _get_text_embedding(self, text: str) -> Embedding:<br>        \"\"\"<br>        Embed the input text synchronously.<br>        Subclasses should implement this method. Reference get_text_embedding's<br>        docstring for more information.<br>        \"\"\"<br>    async def _aget_text_embedding(self, text: str) -> Embedding:<br>        \"\"\"<br>        Embed the input text asynchronously.<br>        Subclasses can implement this method if there is a true async<br>        implementation. Reference get_text_embedding's docstring for more<br>        information.<br>        \"\"\"<br>        # Default implementation just falls back on _get_text_embedding<br>        return self._get_text_embedding(text)<br>    def _get_text_embeddings(self, texts: List[str]) -> List[Embedding]:<br>        \"\"\"<br>        Embed the input sequence of text synchronously.<br>        Subclasses can implement this method if batch queries are supported.<br>        \"\"\"<br>        # Default implementation just loops over _get_text_embedding<br>        return [self._get_text_embedding(text) for text in texts]<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[Embedding]:<br>        \"\"\"<br>        Embed the input sequence of text asynchronously.<br>        Subclasses can implement this method if batch queries are supported.<br>        \"\"\"<br>        return await asyncio.gather(<br>            *[self._aget_text_embedding(text) for text in texts]<br>        )<br>    @dispatcher.span<br>    def get_text_embedding(self, text: str) -> Embedding:<br>        \"\"\"<br>        Embed the input text.<br>        When embedding text, depending on the model, a special instruction<br>        can be prepended to the raw text string. For example, \"Represent the<br>        document for retrieval: \". If you're curious, other examples of<br>        predefined instructions can be found in embeddings/huggingface_utils.py.<br>        \"\"\"<br>        model_dict = self.to_dict()<br>        model_dict.pop(\"api_key\", None)<br>        dispatcher.event(<br>            EmbeddingStartEvent(<br>                model_dict=model_dict,<br>            )<br>        )<br>        with self.callback_manager.event(<br>            CBEventType.EMBEDDING, payload={EventPayload.SERIALIZED: self.to_dict()}<br>        ) as event:<br>            text_embedding = self._get_text_embedding(text)<br>            event.on_end(<br>                payload={<br>                    EventPayload.CHUNKS: [text],<br>                    EventPayload.EMBEDDINGS: [text_embedding],<br>                }<br>            )<br>        dispatcher.event(<br>            EmbeddingEndEvent(<br>                chunks=[text],<br>                embeddings=[text_embedding],<br>            )<br>        )<br>        return text_embedding<br>    @dispatcher.span<br>    async def aget_text_embedding(self, text: str) -> Embedding:<br>        \"\"\"Async get text embedding.\"\"\"<br>        model_dict = self.to_dict()<br>        model_dict.pop(\"api_key\", None)<br>        dispatcher.event(<br>            EmbeddingStartEvent(<br>                model_dict=model_dict,<br>            )<br>        )<br>        with self.callback_manager.event(<br>            CBEventType.EMBEDDING, payload={EventPayload.SERIALIZED: self.to_dict()}<br>        ) as event:<br>            text_embedding = await self._aget_text_embedding(text)<br>            event.on_end(<br>                payload={<br>                    EventPayload.CHUNKS: [text],<br>                    EventPayload.EMBEDDINGS: [text_embedding],<br>                }<br>            )<br>        dispatcher.event(<br>            EmbeddingEndEvent(<br>                chunks=[text],<br>                embeddings=[text_embedding],<br>            )<br>        )<br>        return text_embedding<br>    @dispatcher.span<br>    def get_text_embedding_batch(<br>        self,<br>        texts: List[str],<br>        show_progress: bool = False,<br>        **kwargs: Any,<br>    ) -> List[Embedding]:<br>        \"\"\"Get a list of text embeddings, with batching.\"\"\"<br>        cur_batch: List[str] = []<br>        result_embeddings: List[Embedding] = []<br>        queue_with_progress = enumerate(<br>            get_tqdm_iterable(texts, show_progress, \"Generating embeddings\")<br>        )<br>        model_dict = self.to_dict()<br>        model_dict.pop(\"api_key\", None)<br>        for idx, text in queue_with_progress:<br>            cur_batch.append(text)<br>            if idx == len(texts) - 1 or len(cur_batch) == self.embed_batch_size:<br>                # flush<br>                dispatcher.event(<br>                    EmbeddingStartEvent(<br>                        model_dict=model_dict,<br>                    )<br>                )<br>                with self.callback_manager.event(<br>                    CBEventType.EMBEDDING,<br>                    payload={EventPayload.SERIALIZED: self.to_dict()},<br>                ) as event:<br>                    embeddings = self._get_text_embeddings(cur_batch)<br>                    result_embeddings.extend(embeddings)<br>                    event.on_end(<br>                        payload={<br>                            EventPayload.CHUNKS: cur_batch,<br>                            EventPayload.EMBEDDINGS: embeddings,<br>                        },<br>                    )<br>                dispatcher.event(<br>                    EmbeddingEndEvent(<br>                        chunks=cur_batch,<br>                        embeddings=embeddings,<br>                    )<br>                )<br>                cur_batch = []<br>        return result_embeddings<br>    @dispatcher.span<br>    async def aget_text_embedding_batch(<br>        self, texts: List[str], show_progress: bool = False<br>    ) -> List[Embedding]:<br>        \"\"\"Asynchronously get a list of text embeddings, with batching.\"\"\"<br>        num_workers = self.num_workers<br>        model_dict = self.to_dict()<br>        model_dict.pop(\"api_key\", None)<br>        cur_batch: List[str] = []<br>        callback_payloads: List[Tuple[str, List[str]]] = []<br>        result_embeddings: List[Embedding] = []<br>        embeddings_coroutines: List[Coroutine] = []<br>        for idx, text in enumerate(texts):<br>            cur_batch.append(text)<br>            if idx == len(texts) - 1 or len(cur_batch) == self.embed_batch_size:<br>                # flush<br>                dispatcher.event(<br>                    EmbeddingStartEvent(<br>                        model_dict=model_dict,<br>                    )<br>                )<br>                event_id = self.callback_manager.on_event_start(<br>                    CBEventType.EMBEDDING,<br>                    payload={EventPayload.SERIALIZED: self.to_dict()},<br>                )<br>                callback_payloads.append((event_id, cur_batch))<br>                embeddings_coroutines.append(self._aget_text_embeddings(cur_batch))<br>                cur_batch = []<br>        # flatten the results of asyncio.gather, which is a list of embeddings lists<br>        nested_embeddings = []<br>        if num_workers and num_workers > 1:<br>            nested_embeddings = await run_jobs(<br>                embeddings_coroutines,<br>                show_progress=show_progress,<br>                workers=self.num_workers,<br>                desc=\"Generating embeddings\",<br>            )<br>        else:<br>            if show_progress:<br>                try:<br>                    from tqdm.asyncio import tqdm_asyncio<br>                    nested_embeddings = await tqdm_asyncio.gather(<br>                        *embeddings_coroutines,<br>                        total=len(embeddings_coroutines),<br>                        desc=\"Generating embeddings\",<br>                    )<br>                except ImportError:<br>                    nested_embeddings = await asyncio.gather(*embeddings_coroutines)<br>            else:<br>                nested_embeddings = await asyncio.gather(*embeddings_coroutines)<br>        result_embeddings = [<br>            embedding for embeddings in nested_embeddings for embedding in embeddings<br>        ]<br>        for (event_id, text_batch), embeddings in zip(<br>            callback_payloads, nested_embeddings<br>        ):<br>            dispatcher.event(<br>                EmbeddingEndEvent(<br>                    chunks=text_batch,<br>                    embeddings=embeddings,<br>                )<br>            )<br>            self.callback_manager.on_event_end(<br>                CBEventType.EMBEDDING,<br>                payload={<br>                    EventPayload.CHUNKS: text_batch,<br>                    EventPayload.EMBEDDINGS: embeddings,<br>                },<br>                event_id=event_id,<br>            )<br>        return result_embeddings<br>    def similarity(<br>        self,<br>        embedding1: Embedding,<br>        embedding2: Embedding,<br>        mode: SimilarityMode = SimilarityMode.DEFAULT,<br>    ) -> float:<br>        \"\"\"Get embedding similarity.\"\"\"<br>        return similarity(embedding1=embedding1, embedding2=embedding2, mode=mode)<br>    def __call__(self, nodes: Sequence[BaseNode], **kwargs: Any) -> Sequence[BaseNode]:<br>        embeddings = self.get_text_embedding_batch(<br>            [node.get_content(metadata_mode=MetadataMode.EMBED) for node in nodes],<br>            **kwargs,<br>        )<br>        for node, embedding in zip(nodes, embeddings):<br>            node.embedding = embedding<br>        return nodes<br>    async def acall(<br>        self, nodes: Sequence[BaseNode], **kwargs: Any<br>    ) -> Sequence[BaseNode]:<br>        embeddings = await self.aget_text_embedding_batch(<br>            [node.get_content(metadata_mode=MetadataMode.EMBED) for node in nodes],<br>            **kwargs,<br>        )<br>        for node, embedding in zip(nodes, embeddings):<br>            node.embedding = embedding<br>        return nodes<br></code>`` |\n"
    },
    {
      "id": "969bcd99-9dc6-4274-80d8-8aaac7367428",
      "size": 1557,
      "headers": {
        "h1": "Index",
        "h2": "resolve\\_embed\\_model \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br></code>`<code> | </code>`<code><br>def resolve_embed_model(<br>    embed_model: Optional[EmbedType] = None,<br>    callback_manager: Optional[CallbackManager] = None,<br>) -> BaseEmbedding:<br>    \"\"\"Resolve embed model.\"\"\"<br>    from llama_index.core.settings import Settings<br>    try:<br>        from llama_index.core.bridge.langchain import Embeddings as LCEmbeddings<br>    except ImportError:<br>        LCEmbeddings = None  # type: ignore<br>    if embed_model == \"default\":<br>        if os.getenv(\"IS_TESTING\"):<br>            embed_model = MockEmbedding(embed_dim=8)<br>            embed_model.callback_manager = callback_manager or Settings.callback_manager<br>            return embed_model<br>        try:<br>            from llama_index.embeddings.openai import (<br>                OpenAIEmbedding,<br>            )  # pants: no-infer-dep<br>            from llama_index.embeddings.openai.utils import (<br>                validate_openai_api_key,<br>            )  # pants: no-infer-dep<br>            embed_model = OpenAIEmbedding()<br>            validate_openai_api_key(embed_model.api_key)  # type: ignore<br>        except ImportError:<br>            raise ImportError(<br>                \"</code>llama-index-embeddings-openai<code> package not found, \"<br>                \"please run </code>pip install llama-index-embeddings-openai<code>\"<br>            )<br>        except ValueError as e:<br>            raise ValueError(<br>                \"\\n******\\n\"<br>                \"Could not load OpenAI embedding model. \"<br>                \"If you intended to use OpenAI, please check your OPENAI_API_KEY.\\n\"<br>                \"Original error:\\n\"<br>                f\"{e!s}\"<br>                \"\\nConsider using embed_model='local'.\\n\"<br>                \"Visit our documentation for more embedding options: \"<br>                \"https://docs.llamaindex.ai/en/stable/module_guides/models/\"<br>                \"embeddings.html#modules\"<br>                \"\\n******\"<br>            )<br>    # for image multi-modal embeddings<br>    elif isinstance(embed_model, str) and embed_model.startswith(\"clip\"):<br>        try:<br>            from llama_index.embeddings.clip import ClipEmbedding  # pants: no-infer-dep<br>            clip_model_name = (<br>                embed_model.split(\":\")[1] if \":\" in embed_model else \"ViT-B/32\"<br>            )<br>            embed_model = ClipEmbedding(model_name=clip_model_name)<br>        except ImportError as e:<br>            raise ImportError(<br>                \"</code>llama-index-embeddings-clip<code> package not found, \"<br>                \"please run </code>pip install llama-index-embeddings-clip<code> and </code>pip install git+https://github.com/openai/CLIP.git<code>\"<br>            )<br>    if isinstance(embed_model, str):<br>        try:<br>            from llama_index.embeddings.huggingface import (<br>                HuggingFaceEmbedding,<br>            )  # pants: no-infer-dep<br>            splits = embed_model.split(\":\", 1)<br>            is_local = splits[0]<br>            model_name = splits[1] if len(splits) > 1 else None<br>            if is_local != \"local\":<br>                raise ValueError(<br>                    \"embed_model must start with str 'local' or of type BaseEmbedding\"<br>                )<br>            cache_folder = os.path.join(get_cache_dir(), \"models\")<br>            os.makedirs(cache_folder, exist_ok=True)<br>            embed_model = HuggingFaceEmbedding(<br>                model_name=model_name, cache_folder=cache_folder<br>            )<br>        except ImportError:<br>            raise ImportError(<br>                \"</code>llama-index-embeddings-huggingface<code> package not found, \"<br>                \"please run </code>pip install llama-index-embeddings-huggingface<code>\"<br>            )<br>    if LCEmbeddings is not None and isinstance(embed_model, LCEmbeddings):<br>        try:<br>            from llama_index.embeddings.langchain import (<br>                LangchainEmbedding,<br>            )  # pants: no-infer-dep<br>            embed_model = LangchainEmbedding(embed_model)<br>        except ImportError as e:<br>            raise ImportError(<br>                \"</code>llama-index-embeddings-langchain<code> package not found, \"<br>                \"please run </code>pip install llama-index-embeddings-langchain<code>\"<br>            )<br>    if embed_model is None:<br>        print(\"Embeddings have been explicitly disabled. Using MockEmbedding.\")<br>        embed_model = MockEmbedding(embed_dim=1)<br>    assert isinstance(embed_model, BaseEmbedding)<br>    embed_model.callback_manager = callback_manager or Settings.callback_manager<br>    return embed_model<br></code>`` |\n"
    },
    {
      "id": "9f34383f-d9eb-4845-8c96-5ea6e9c5d9e9",
      "size": 3863,
      "headers": {
        "h1": "Uptrain",
        "h2": "UpTrainCallbackHandler \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br></code>`<code> | </code>`<code><br>class UpTrainCallbackHandler(BaseCallbackHandler):<br>    \"\"\"<br>    UpTrain callback handler.<br>    This class is responsible for handling the UpTrain API and logging events to UpTrain.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        api_key: str,<br>        key_type: Literal[\"uptrain\", \"openai\"],<br>        project_name: str = \"uptrain_llamaindex\",<br>    ) -> None:<br>        \"\"\"Initialize the UpTrain callback handler.\"\"\"<br>        try:<br>            from uptrain import APIClient, EvalLLM, Settings<br>        except ImportError:<br>            raise ImportError(<br>                \"UpTrainCallbackHandler requires the 'uptrain' package. \"<br>                \"Please install it using 'pip install uptrain'.\"<br>            )<br>        nest_asyncio.apply()<br>        super().__init__(<br>            event_starts_to_ignore=[],<br>            event_ends_to_ignore=[],<br>        )<br>        self.schema = UpTrainDataSchema(project_name=project_name)<br>        self._event_pairs_by_id: Dict[str, List[CBEvent]] = defaultdict(list)<br>        self._trace_map: Dict[str, List[str]] = defaultdict(list)<br>        # Based on whether the user enters an UpTrain API key or an OpenAI API key, the client is initialized<br>        # If both are entered, the UpTrain API key is used<br>        if key_type == \"uptrain\":<br>            settings = Settings(uptrain_access_token=api_key)<br>            self.uptrain_client = APIClient(settings=settings)<br>        elif key_type == \"openai\":<br>            settings = Settings(openai_api_key=api_key)<br>            self.uptrain_client = EvalLLM(settings=settings)<br>        else:<br>            raise ValueError(\"Invalid key type: Must be 'uptrain' or 'openai'\")<br>    def uptrain_evaluate(<br>        self,<br>        evaluation_name: str,<br>        data: List[Dict[str, str]],<br>        checks: List[str],<br>    ) -> None:<br>        \"\"\"Run an evaluation on the UpTrain server using UpTrain client.\"\"\"<br>        if self.uptrain_client.__class__.__name__ == \"APIClient\":<br>            uptrain_result = self.uptrain_client.log_and_evaluate(<br>                project_name=self.schema.project_name,<br>                evaluation_name=evaluation_name,<br>                data=data,<br>                checks=checks,<br>            )<br>        else:<br>            uptrain_result = self.uptrain_client.evaluate(<br>                project_name=self.schema.project_name,<br>                evaluation_name=evaluation_name,<br>                data=data,<br>                checks=checks,<br>            )<br>        self.schema.uptrain_results[self.schema.project_name].append(uptrain_result)<br>        score_name_map = {<br>            \"score_context_relevance\": \"Context Relevance Score\",<br>            \"score_factual_accuracy\": \"Factual Accuracy Score\",<br>            \"score_response_completeness\": \"Response Completeness Score\",<br>            \"score_sub_query_completeness\": \"Sub Query Completeness Score\",<br>            \"score_context_reranking\": \"Context Reranking Score\",<br>            \"score_context_conciseness\": \"Context Conciseness Score\",<br>        }<br>        # Print the results<br>        for row in uptrain_result:<br>            columns = list(row.keys())<br>            for column in columns:<br>                if column == \"question\":<br>                    print(f\"\\nQuestion: {row[column]}\")<br>                elif column == \"response\":<br>                    print(f\"Response: {row[column]}\\n\")<br>                elif column.startswith(\"score\"):<br>                    if column in score_name_map:<br>                        print(f\"{score_name_map[column]}: {row[column]}\")<br>                    else:<br>                        print(f\"{column}: {row[column]}\")<br>            print()<br>    def on_event_start(<br>        self,<br>        event_type: CBEventType,<br>        payload: Any = None,<br>        event_id: str = \"\",<br>        parent_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> str:<br>        \"\"\"Run when an event starts and return id of event.\"\"\"<br>        event = CBEvent(event_type, payload=payload, id_=event_id)<br>        self._event_pairs_by_id[event.id_].append(event)<br>        if event_type is CBEventType.QUERY:<br>            self.schema.question = payload[\"query_str\"]<br>        if event_type is CBEventType.TEMPLATING and \"template_vars\" in payload:<br>            template_vars = payload[\"template_vars\"]<br>            self.schema.context = template_vars.get(\"context_str\", \"\")<br>        elif event_type is CBEventType.RERANKING and \"nodes\" in payload:<br>            self.schema.eval_types.add(\"reranking\")<br>            # Store old context data<br>            self.schema.old_context = [node.text for node in payload[\"nodes\"]]<br>        elif event_type is CBEventType.SUB_QUESTION:<br>            # For the first sub question, store parent question and parent id<br>            if \"sub_question\" not in self.schema.eval_types:<br>                self.schema.parent_question = self.schema.question<br>                self.schema.eval_types.add(\"sub_question\")<br>            # Store sub question data - question and parent id<br>            self.schema.sub_question_parent_id = parent_id<br>        return event_id<br>    def on_event_end(<br>        self,<br>        event_type: CBEventType,<br>        payload: Any = None,<br>        event_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> None:<br>        \"\"\"Run when an event ends.\"\"\"<br>        try:<br>            from uptrain import Evals<br>        except ImportError:<br>            raise ImportError(<br>                \"UpTrainCallbackHandler requires the 'uptrain' package. \"<br>                \"Please install it using 'pip install uptrain'.\"<br>            )<br>        event = CBEvent(event_type, payload=payload, id_=event_id)<br>        self._event_pairs_by_id[event.id_].append(event)<br>        self._trace_map = defaultdict(list)<br>        if event_id == self.schema.sub_question_parent_id:<br>            # Perform individual evaluations for sub questions (but send all sub questions at once)<br>            self.uptrain_evaluate(<br>                evaluation_name=\"sub_question_answering\",<br>                data=list(self.schema.sub_question_map.values()),<br>                checks=[<br>                    Evals.CONTEXT_RELEVANCE,<br>                    Evals.FACTUAL_ACCURACY,<br>                    Evals.RESPONSE_COMPLETENESS,<br>                ],<br>            )<br>            # Perform evaluation for question and all sub questions (as a whole)<br>            sub_questions = [<br>                sub_question[\"question\"]<br>                for sub_question in self.schema.sub_question_map.values()<br>            ]<br>            sub_questions_formatted = \"\\n\".join(<br>                [<br>                    f\"{index}. {string}\"<br>                    for index, string in enumerate(sub_questions, start=1)<br>                ]<br>            )<br>            self.uptrain_evaluate(<br>                evaluation_name=\"sub_query_completeness\",<br>                data=[<br>                    {<br>                        \"question\": self.schema.parent_question,<br>                        \"sub_questions\": sub_questions_formatted,<br>                    }<br>                ],<br>                checks=[Evals.SUB_QUERY_COMPLETENESS],<br>            )<br>            self.schema.eval_types.remove(\"sub_question\")<br>        # Should not be called for sub questions<br>        if (<br>            event_type is CBEventType.SYNTHESIZE<br>            and \"sub_question\" not in self.schema.eval_types<br>        ):<br>            self.schema.response = payload[\"response\"].response<br>            # Perform evaluation for synthesization<br>            if \"reranking\" in self.schema.eval_types:<br>                if self.schema.reranking_type == \"rerank\":<br>                    evaluation_name = \"question_answering_rerank\"<br>                else:<br>                    evaluation_name = \"question_answering_resize\"<br>                self.schema.eval_types.remove(\"reranking\")<br>            else:<br>                evaluation_name = \"question_answering\"<br>            self.uptrain_evaluate(<br>                evaluation_name=evaluation_name,<br>                data=[<br>                    {<br>                        \"question\": self.schema.question,<br>                        \"context\": self.schema.context,<br>                        \"response\": self.schema.response,<br>                    }<br>                ],<br>                checks=[<br>                    Evals.CONTEXT_RELEVANCE,<br>                    Evals.FACTUAL_ACCURACY,<br>                    Evals.RESPONSE_COMPLETENESS,<br>                ],<br>            )<br>        elif event_type is CBEventType.RERANKING:<br>            # Store new context data<br>            self.schema.new_context = [node.text for node in payload[\"nodes\"]]<br>            if len(self.schema.old_context) == len(self.schema.new_context):<br>                self.schema.reranking_type = \"rerank\"<br>                context = \"\\n\".join(<br>                    [<br>                        f\"{index}. {string}\"<br>                        for index, string in enumerate(self.schema.old_context, start=1)<br>                    ]<br>                )<br>                reranked_context = \"\\n\".join(<br>                    [<br>                        f\"{index}. {string}\"<br>                        for index, string in enumerate(self.schema.new_context, start=1)<br>                    ]<br>                )<br>                # Perform evaluation for reranking<br>                self.uptrain_evaluate(<br>                    evaluation_name=\"context_reranking\",<br>                    data=[<br>                        {<br>                            \"question\": self.schema.question,<br>                            \"context\": context,<br>                            \"reranked_context\": reranked_context,<br>                        }<br>                    ],<br>                    checks=[<br>                        Evals.CONTEXT_RERANKING,<br>                    ],<br>                )<br>            else:<br>                self.schema.reranking_type = \"resize\"<br>                context = \"\\n\".join(self.schema.old_context)<br>                concise_context = \"\\n\".join(self.schema.new_context)<br>                # Perform evaluation for resizing<br>                self.uptrain_evaluate(<br>                    evaluation_name=\"context_conciseness\",<br>                    data=[<br>                        {<br>                            \"question\": self.schema.question,<br>                            \"context\": context,<br>                            \"concise_context\": concise_context,<br>                        }<br>                    ],<br>                    checks=[<br>                        Evals.CONTEXT_CONCISENESS,<br>                    ],<br>                )<br>        elif event_type is CBEventType.SUB_QUESTION:<br>            # Store sub question data<br>            self.schema.sub_question_map[event_id][\"question\"] = payload[<br>                \"sub_question\"<br>            ].sub_q.sub_question<br>            self.schema.sub_question_map[event_id][\"context\"] = (<br>                payload[\"sub_question\"].sources[0].node.text<br>            )<br>            self.schema.sub_question_map[event_id][\"response\"] = payload[<br>                \"sub_question\"<br>            ].answer<br>    def start_trace(self, trace_id: Optional[str] = None) -> None:<br>        self._trace_map = defaultdict(list)<br>        return super().start_trace(trace_id)<br>    def end_trace(<br>        self,<br>        trace_id: Optional[str] = None,<br>        trace_map: Optional[Dict[str, List[str]]] = None,<br>    ) -> None:<br>        self._trace_map = trace_map or defaultdict(list)<br>        return super().end_trace(trace_id, trace_map)<br>    def build_trace_map(<br>        self,<br>        cur_event_id: str,<br>        trace_map: Any,<br>    ) -> Dict[str, Any]:<br>        event_pair = self._event_pairs_by_id[cur_event_id]<br>        if event_pair:<br>            event_data = {<br>                \"event_type\": event_pair[0].event_type,<br>                \"event_id\": event_pair[0].id_,<br>                \"children\": {},<br>            }<br>            trace_map[cur_event_id] = event_data<br>        child_event_ids = self._trace_map[cur_event_id]<br>        for child_event_id in child_event_ids:<br>            self.build_trace_map(child_event_id, event_data[\"children\"])<br>        return trace_map<br></code>`` |\n"
    },
    {
      "id": "8b7fb633-3b0d-4d31-a388-ea53e85cb13d",
      "size": 1793,
      "headers": {
        "h1": "Uptrain",
        "h2": "UpTrainCallbackHandler \\#",
        "h3": "on\\_event\\_end \\#"
      },
      "text": "| ``<code><br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br></code>`<code> | </code>`<code><br>def on_event_end(<br>    self,<br>    event_type: CBEventType,<br>    payload: Any = None,<br>    event_id: str = \"\",<br>    **kwargs: Any,<br>) -> None:<br>    \"\"\"Run when an event ends.\"\"\"<br>    try:<br>        from uptrain import Evals<br>    except ImportError:<br>        raise ImportError(<br>            \"UpTrainCallbackHandler requires the 'uptrain' package. \"<br>            \"Please install it using 'pip install uptrain'.\"<br>        )<br>    event = CBEvent(event_type, payload=payload, id_=event_id)<br>    self._event_pairs_by_id[event.id_].append(event)<br>    self._trace_map = defaultdict(list)<br>    if event_id == self.schema.sub_question_parent_id:<br>        # Perform individual evaluations for sub questions (but send all sub questions at once)<br>        self.uptrain_evaluate(<br>            evaluation_name=\"sub_question_answering\",<br>            data=list(self.schema.sub_question_map.values()),<br>            checks=[<br>                Evals.CONTEXT_RELEVANCE,<br>                Evals.FACTUAL_ACCURACY,<br>                Evals.RESPONSE_COMPLETENESS,<br>            ],<br>        )<br>        # Perform evaluation for question and all sub questions (as a whole)<br>        sub_questions = [<br>            sub_question[\"question\"]<br>            for sub_question in self.schema.sub_question_map.values()<br>        ]<br>        sub_questions_formatted = \"\\n\".join(<br>            [<br>                f\"{index}. {string}\"<br>                for index, string in enumerate(sub_questions, start=1)<br>            ]<br>        )<br>        self.uptrain_evaluate(<br>            evaluation_name=\"sub_query_completeness\",<br>            data=[<br>                {<br>                    \"question\": self.schema.parent_question,<br>                    \"sub_questions\": sub_questions_formatted,<br>                }<br>            ],<br>            checks=[Evals.SUB_QUERY_COMPLETENESS],<br>        )<br>        self.schema.eval_types.remove(\"sub_question\")<br>    # Should not be called for sub questions<br>    if (<br>        event_type is CBEventType.SYNTHESIZE<br>        and \"sub_question\" not in self.schema.eval_types<br>    ):<br>        self.schema.response = payload[\"response\"].response<br>        # Perform evaluation for synthesization<br>        if \"reranking\" in self.schema.eval_types:<br>            if self.schema.reranking_type == \"rerank\":<br>                evaluation_name = \"question_answering_rerank\"<br>            else:<br>                evaluation_name = \"question_answering_resize\"<br>            self.schema.eval_types.remove(\"reranking\")<br>        else:<br>            evaluation_name = \"question_answering\"<br>        self.uptrain_evaluate(<br>            evaluation_name=evaluation_name,<br>            data=[<br>                {<br>                    \"question\": self.schema.question,<br>                    \"context\": self.schema.context,<br>                    \"response\": self.schema.response,<br>                }<br>            ],<br>            checks=[<br>                Evals.CONTEXT_RELEVANCE,<br>                Evals.FACTUAL_ACCURACY,<br>                Evals.RESPONSE_COMPLETENESS,<br>            ],<br>        )<br>    elif event_type is CBEventType.RERANKING:<br>        # Store new context data<br>        self.schema.new_context = [node.text for node in payload[\"nodes\"]]<br>        if len(self.schema.old_context) == len(self.schema.new_context):<br>            self.schema.reranking_type = \"rerank\"<br>            context = \"\\n\".join(<br>                [<br>                    f\"{index}. {string}\"<br>                    for index, string in enumerate(self.schema.old_context, start=1)<br>                ]<br>            )<br>            reranked_context = \"\\n\".join(<br>                [<br>                    f\"{index}. {string}\"<br>                    for index, string in enumerate(self.schema.new_context, start=1)<br>                ]<br>            )<br>            # Perform evaluation for reranking<br>            self.uptrain_evaluate(<br>                evaluation_name=\"context_reranking\",<br>                data=[<br>                    {<br>                        \"question\": self.schema.question,<br>                        \"context\": context,<br>                        \"reranked_context\": reranked_context,<br>                    }<br>                ],<br>                checks=[<br>                    Evals.CONTEXT_RERANKING,<br>                ],<br>            )<br>        else:<br>            self.schema.reranking_type = \"resize\"<br>            context = \"\\n\".join(self.schema.old_context)<br>            concise_context = \"\\n\".join(self.schema.new_context)<br>            # Perform evaluation for resizing<br>            self.uptrain_evaluate(<br>                evaluation_name=\"context_conciseness\",<br>                data=[<br>                    {<br>                        \"question\": self.schema.question,<br>                        \"context\": context,<br>                        \"concise_context\": concise_context,<br>                    }<br>                ],<br>                checks=[<br>                    Evals.CONTEXT_CONCISENESS,<br>                ],<br>            )<br>    elif event_type is CBEventType.SUB_QUESTION:<br>        # Store sub question data<br>        self.schema.sub_question_map[event_id][\"question\"] = payload[<br>            \"sub_question\"<br>        ].sub_q.sub_question<br>        self.schema.sub_question_map[event_id][\"context\"] = (<br>            payload[\"sub_question\"].sources[0].node.text<br>        )<br>        self.schema.sub_question_map[event_id][\"response\"] = payload[<br>            \"sub_question\"<br>        ].answer<br></code>`` |\n"
    },
    {
      "id": "2e78a15d-3204-4279-993a-41446851f8a3",
      "size": 2067,
      "headers": {
        "h1": "Dashscope",
        "h2": "DashScopeEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br></code>`<code> | </code>`<code><br>class DashScopeEmbedding(MultiModalEmbedding):<br>    \"\"\"DashScope class for text embedding.<br>    Args:<br>        model_name (str): Model name for embedding.<br>            Defaults to DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V2.<br>                Options are:<br>                - DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V1<br>                - DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V2<br>        text_type (str): The input type, ['query', 'document'],<br>            For asymmetric tasks such as retrieval, in order to achieve better<br>            retrieval results, it is recommended to distinguish between query<br>            text (query) and base text (document) types, clustering Symmetric<br>            tasks such as classification and classification do not need to<br>            be specially specified, and the system default<br>            value \"document\" can be used.<br>        api_key (str): The DashScope api key.<br>    \"\"\"<br>    _api_key: Optional[str] = PrivateAttr()<br>    _text_type: Optional[str] = PrivateAttr()<br>    def __init__(<br>        self,<br>        model_name: str = DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V2,<br>        text_type: str = \"document\",<br>        api_key: Optional[str] = None,<br>        embed_batch_size: int = EMBED_MAX_BATCH_SIZE,<br>        **kwargs: Any,<br>    ) -> None:<br>        super().__init__(<br>            model_name=model_name,<br>            embed_batch_size=embed_batch_size,<br>            **kwargs,<br>        )<br>        self._api_key = api_key<br>        self._text_type = text_type<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"DashScopeEmbedding\"<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        emb = get_text_embedding(<br>            self.model_name,<br>            query,<br>            api_key=self._api_key,<br>            text_type=\"query\",<br>        )<br>        if len(emb) > 0 and emb[0] is not None:<br>            return emb[0]<br>        else:<br>            return []<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        emb = get_text_embedding(<br>            self.model_name,<br>            text,<br>            api_key=self._api_key,<br>            text_type=self._text_type,<br>        )<br>        if len(emb) > 0 and emb[0] is not None:<br>            return emb[0]<br>        else:<br>            return []<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        return get_text_embedding(<br>            self.model_name,<br>            texts,<br>            api_key=self._api_key,<br>            text_type=self._text_type,<br>        )<br>    # TODO: use proper async methods<br>    async def _aget_text_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._get_text_embedding(query)<br>    # TODO: user proper async methods<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self._get_query_embedding(query)<br>    def get_batch_query_embedding(self, embedding_file_url: str) -> Optional[str]:<br>        \"\"\"Get batch query embeddings.<br>        Args:<br>            embedding_file_url (str): The url of the file to embedding which with lines of text to embedding.<br>        Returns:<br>            str: The url of the embedding result, format ref:<br>                 https://help.aliyun.com/zh/dashscope/developer-reference/text-embedding-async-api-details.<br>        \"\"\"<br>        return get_batch_text_embedding(<br>            self.model_name,<br>            embedding_file_url,<br>            api_key=self._api_key,<br>            text_type=self._text_type,<br>        )<br>    def get_batch_text_embedding(self, embedding_file_url: str) -> Optional[str]:<br>        \"\"\"Get batch text embeddings.<br>        Args:<br>            embedding_file_url (str): The url of the file to embedding which with lines of text to embedding.<br>        Returns:<br>            str: The url of the embedding result, format ref:<br>                 https://help.aliyun.com/zh/dashscope/developer-reference/text-embedding-async-api-details.<br>        \"\"\"<br>        return get_batch_text_embedding(<br>            self.model_name,<br>            embedding_file_url,<br>            api_key=self._api_key,<br>            text_type=self._text_type,<br>        )<br>    def _get_image_embedding(self, img_file_path: ImageType) -> List[float]:<br>        \"\"\"<br>        Embed the input image synchronously.<br>        \"\"\"<br>        input = [{\"image\": img_file_path}]<br>        return get_multimodal_embedding(<br>            self.model_name, input=input, api_key=self._api_key<br>        )<br>    async def _aget_image_embedding(self, img_file_path: ImageType) -> List[float]:<br>        \"\"\"<br>        Embed the input image asynchronously.<br>        \"\"\"<br>        return self._get_image_embedding(img_file_path=img_file_path)<br>    def get_multimodal_embedding(<br>        self, input: List[Dict], auto_truncation: bool = False<br>    ) -> List[float]:<br>        \"\"\"Call DashScope multimodal embedding.<br>        ref: https://help.aliyun.com/zh/dashscope/developer-reference/one-peace-multimodal-embedding-api-details.<br>        Args:<br>            input (str): The input of the multimodal embedding, eg:<br>                [{'factor': 1, 'text': 'ä½ å¥½'},<br>                {'factor': 2, 'audio': 'https://dashscope.oss-cn-beijing.aliyuncs.com/audios/cow.flac'},<br>                {'factor': 3, 'image': 'https://dashscope.oss-cn-beijing.aliyuncs.com/images/256_1.png'}]<br>        Raises:<br>            ImportError: Need install dashscope package.<br>        Returns:<br>            List[float]: The embedding result<br>        \"\"\"<br>        return get_multimodal_embedding(<br>            self.model_name,<br>            input=input,<br>            api_key=self._api_key,<br>            auto_truncation=auto_truncation,<br>        )<br></code>`` |\n"
    },
    {
      "id": "35050e37-bd56-4107-a73d-3f77a52823a2",
      "size": 1988,
      "headers": {
        "h1": "Sagemaker endpoint",
        "h2": "SageMakerEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br></code>`<code> | </code>`<code><br>class SageMakerEmbedding(BaseEmbedding):<br>    endpoint_name: str = Field(description=\"SageMaker Embedding endpoint name\")<br>    endpoint_kwargs: Dict[str, Any] = Field(<br>        default={},<br>        description=\"Additional kwargs for the invoke_endpoint request.\",<br>    )<br>    model_kwargs: Dict[str, Any] = Field(<br>        default={},<br>        description=\"kwargs to pass to the model.\",<br>    )<br>    content_handler: BaseIOHandler = Field(<br>        default=DEFAULT_IO_HANDLER,<br>        description=\"used to serialize input, deserialize output, and remove a prefix.\",<br>    )<br>    profile_name: Optional[str] = Field(<br>        description=\"The name of aws profile to use. If not given, then the default profile is used.\"<br>    )<br>    aws_access_key_id: Optional[str] = Field(description=\"AWS Access Key ID to use\")<br>    aws_secret_access_key: Optional[str] = Field(<br>        description=\"AWS Secret Access Key to use\"<br>    )<br>    aws_session_token: Optional[str] = Field(description=\"AWS Session Token to use\")<br>    region_name: Optional[str] = Field(<br>        description=\"AWS region name to use. Uses region configured in AWS CLI if not passed\"<br>    )<br>    max_retries: Optional[int] = Field(<br>        default=3,<br>        description=\"The maximum number of API retries.\",<br>        gte=0,<br>    )<br>    timeout: Optional[float] = Field(<br>        default=60.0,<br>        description=\"The timeout, in seconds, for API requests.\",<br>        gte=0,<br>    )<br>    _client: Any = PrivateAttr()<br>    _verbose: bool = PrivateAttr()<br>    def __init__(<br>        self,<br>        endpoint_name: str,<br>        endpoint_kwargs: Optional[Dict[str, Any]] = {},<br>        model_kwargs: Optional[Dict[str, Any]] = {},<br>        content_handler: BaseIOHandler = DEFAULT_IO_HANDLER,<br>        profile_name: Optional[str] = None,<br>        aws_access_key_id: Optional[str] = None,<br>        aws_secret_access_key: Optional[str] = None,<br>        aws_session_token: Optional[str] = None,<br>        region_name: Optional[str] = None,<br>        max_retries: Optional[int] = 3,<br>        timeout: Optional[float] = 60.0,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>        pydantic_program_mode: PydanticProgramMode = PydanticProgramMode.DEFAULT,<br>        verbose: bool = False,<br>    ):<br>        if not endpoint_name:<br>            raise ValueError(<br>                \"Missing required argument:</code>endpoint_name<code>\"<br>                \" Please specify the endpoint_name\"<br>            )<br>        endpoint_kwargs = endpoint_kwargs or {}<br>        model_kwargs = model_kwargs or {}<br>        content_handler = content_handler<br>        super().__init__(<br>            endpoint_name=endpoint_name,<br>            endpoint_kwargs=endpoint_kwargs,<br>            model_kwargs=model_kwargs,<br>            content_handler=content_handler,<br>            embed_batch_size=embed_batch_size,<br>            profile_name=profile_name,<br>            region_name=region_name,<br>            aws_access_key_id=aws_access_key_id,<br>            aws_secret_access_key=aws_secret_access_key,<br>            aws_session_token=aws_session_token,<br>            pydantic_program_mode=pydantic_program_mode,<br>            callback_manager=callback_manager,<br>        )<br>        self._client = get_aws_service_client(<br>            service_name=\"sagemaker-runtime\",<br>            profile_name=profile_name,<br>            region_name=region_name,<br>            aws_access_key_id=aws_access_key_id,<br>            aws_secret_access_key=aws_secret_access_key,<br>            aws_session_token=aws_session_token,<br>            max_retries=max_retries,<br>            timeout=timeout,<br>        )<br>        self._verbose = verbose<br>    @classmethod<br>    def class_name(self) -> str:<br>        return \"SageMakerEmbedding\"<br>    def _get_embedding(self, payload: List[str], **kwargs: Any) -> List[Embedding]:<br>        model_kwargs = {**self.model_kwargs, **kwargs}<br>        request_body = self.content_handler.serialize_input(<br>            request=payload, model_kwargs=model_kwargs<br>        )<br>        response = self._client.invoke_endpoint(<br>            EndpointName=self.endpoint_name,<br>            Body=request_body,<br>            ContentType=self.content_handler.content_type,<br>            Accept=self.content_handler.accept,<br>            **self.endpoint_kwargs,<br>        )[\"Body\"]<br>        return self.content_handler.deserialize_output(response=response)<br>    def _get_query_embedding(self, query: str, **kwargs: Any) -> Embedding:<br>        query = query.replace(\"\\n\", \" \")<br>        return self._get_embedding([query], **kwargs)[0]<br>    def _get_text_embedding(self, text: str, **kwargs: Any) -> Embedding:<br>        text = text.replace(\"\\n\", \" \")<br>        return self._get_embedding([text], **kwargs)[0]<br>    def _get_text_embeddings(self, texts: List[str], **kwargs: Any) -> List[Embedding]:<br>        \"\"\"<br>        Embed the input sequence of text synchronously.<br>        Subclasses can implement this method if batch queries are supported.<br>        \"\"\"<br>        texts = [text.replace(\"\\n\", \" \") for text in texts]<br>        # Default implementation just loops over _get_text_embedding<br>        return self._get_embedding(texts, **kwargs)<br>    async def _aget_query_embedding(self, query: str, **kwargs: Any) -> Embedding:<br>        raise NotImplementedError<br>    async def _aget_text_embedding(self, text: str, **kwargs: Any) -> Embedding:<br>        raise NotImplementedError<br>    async def _aget_text_embeddings(<br>        self, texts: List[str], **kwargs: Any<br>    ) -> List[Embedding]:<br>        raise NotImplementedError<br></code>`` |\n"
    },
    {
      "id": "33ccbea2-1bfa-4f03-9309-7fc2e5c2079b",
      "size": 2923,
      "headers": {
        "h1": "Dataset generation",
        "h2": "DatasetGenerator \\#",
        "h3": ""
      },
      "text": "| ``<code><br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br></code>`<code> | </code>`<code><br>@deprecated(<br>    \"Deprecated in favor of </code>RagDatasetGenerator<code> which should be used instead.\",<br>    action=\"always\",<br>)<br>class DatasetGenerator(PromptMixin):<br>    \"\"\"Generate dataset (question/ question-answer pairs) \\<br>    based on the given documents.<br>    NOTE: this is a beta feature, subject to change!<br>    Args:<br>        nodes (List[Node]): List of nodes. (Optional)<br>        llm (LLM): Language model.<br>        callback_manager (CallbackManager): Callback manager.<br>        num_questions_per_chunk: number of question to be \\<br>        generated per chunk. Each document is chunked of size 512 words.<br>        text_question_template: Question generation template.<br>        question_gen_query: Question generation query.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        nodes: List[BaseNode],<br>        llm: Optional[LLM] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        num_questions_per_chunk: int = 10,<br>        text_question_template: BasePromptTemplate | None = None,<br>        text_qa_template: BasePromptTemplate | None = None,<br>        question_gen_query: str | None = None,<br>        metadata_mode: MetadataMode = MetadataMode.NONE,<br>        show_progress: bool = False,<br>    ) -> None:<br>        \"\"\"Init params.\"\"\"<br>        self.llm = llm or Settings.llm<br>        self.callback_manager = callback_manager or Settings.callback_manager<br>        self.text_question_template = text_question_template or PromptTemplate(<br>            DEFAULT_QUESTION_GENERATION_PROMPT<br>        )<br>        self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT<br>        self.question_gen_query = (<br>            question_gen_query<br>            or f\"You are a Teacher/Professor. Your task is to setup \\<br>                        {num_questions_per_chunk} questions for an upcoming \\<br>                        quiz/examination. The questions should be diverse in nature \\<br>                            across the document. Restrict the questions to the \\<br>                                context information provided.\"<br>        )<br>        self.nodes = nodes<br>        self._metadata_mode = metadata_mode<br>        self._show_progress = show_progress<br>    @classmethod<br>    def from_documents(<br>        cls,<br>        documents: List[Document],<br>        llm: Optional[LLM] = None,<br>        transformations: Optional[List[TransformComponent]] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        num_questions_per_chunk: int = 10,<br>        text_question_template: BasePromptTemplate | None = None,<br>        text_qa_template: BasePromptTemplate | None = None,<br>        question_gen_query: str | None = None,<br>        required_keywords: List[str] | None = None,<br>        exclude_keywords: List[str] | None = None,<br>        show_progress: bool = False,<br>    ) -> DatasetGenerator:<br>        \"\"\"Generate dataset from documents.\"\"\"<br>        llm = llm or Settings.llm<br>        transformations = transformations or Settings.transformations<br>        callback_manager = callback_manager or Settings.callback_manager<br>        nodes = run_transformations(<br>            documents, transformations, show_progress=show_progress<br>        )<br>        # use node postprocessor to filter nodes<br>        required_keywords = required_keywords or []<br>        exclude_keywords = exclude_keywords or []<br>        node_postprocessor = KeywordNodePostprocessor(<br>            callback_manager=callback_manager,<br>            required_keywords=required_keywords,<br>            exclude_keywords=exclude_keywords,<br>        )<br>        node_with_scores = [NodeWithScore(node=node) for node in nodes]<br>        node_with_scores = node_postprocessor.postprocess_nodes(node_with_scores)<br>        nodes = [node_with_score.node for node_with_score in node_with_scores]<br>        return cls(<br>            nodes=nodes,<br>            llm=llm,<br>            callback_manager=callback_manager,<br>            num_questions_per_chunk=num_questions_per_chunk,<br>            text_question_template=text_question_template,<br>            text_qa_template=text_qa_template,<br>            question_gen_query=question_gen_query,<br>            show_progress=show_progress,<br>        )<br>    async def _agenerate_dataset(<br>        self,<br>        nodes: List[BaseNode],<br>        num: int | None = None,<br>        generate_response: bool = False,<br>    ) -> QueryResponseDataset:<br>        \"\"\"Node question generator.\"\"\"<br>        query_tasks: List[Coroutine] = []<br>        queries: Dict[str, str] = {}<br>        responses_dict: Dict[str, str] = {}<br>        if self._show_progress:<br>            from tqdm.asyncio import tqdm_asyncio<br>            async_module = tqdm_asyncio<br>        else:<br>            async_module = asyncio<br>        summary_indices: List[SummaryIndex] = []<br>        for node in nodes:<br>            if num is not None and len(query_tasks) >= num:<br>                break<br>            index = SummaryIndex.from_documents(<br>                [<br>                    Document(<br>                        text=node.get_content(metadata_mode=self._metadata_mode),<br>                        metadata=node.metadata,  # type: ignore<br>                    )<br>                ],<br>                callback_manager=self.callback_manager,<br>            )<br>            query_engine = index.as_query_engine(<br>                llm=self.llm,<br>                text_qa_template=self.text_question_template,<br>                use_async=True,<br>            )<br>            task = query_engine.aquery(<br>                self.question_gen_query,<br>            )<br>            query_tasks.append(task)<br>            summary_indices.append(index)<br>        responses = await async_module.gather(*query_tasks)<br>        for idx, response in enumerate(responses):<br>            result = str(response).strip().split(\"\\n\")<br>            cleaned_questions = [<br>                re.sub(r\"^\\d+[\\).\\s]\", \"\", question).strip() for question in result<br>            ]<br>            cleaned_questions = [<br>                question for question in cleaned_questions if len(question) > 0<br>            ]<br>            cur_queries = {<br>                str(uuid.uuid4()): question for question in cleaned_questions<br>            }<br>            queries.update(cur_queries)<br>            if generate_response:<br>                index = summary_indices[idx]<br>                qr_tasks = []<br>                cur_query_items = list(cur_queries.items())<br>                cur_query_keys = [query_id for query_id, _ in cur_query_items]<br>                for query_id, query in cur_query_items:<br>                    qa_query_engine = index.as_query_engine(<br>                        llm=self.llm,<br>                        text_qa_template=self.text_qa_template,<br>                    )<br>                    qr_task = qa_query_engine.aquery(query)<br>                    qr_tasks.append(qr_task)<br>                qr_responses = await async_module.gather(*qr_tasks)<br>                for query_id, qa_response in zip(cur_query_keys, qr_responses):<br>                    responses_dict[query_id] = str(qa_response)<br>            else:<br>                pass<br>        query_ids = list(queries.keys())<br>        if num is not None:<br>            query_ids = query_ids[:num]<br>            # truncate queries, responses to the subset of query ids<br>            queries = {query_id: queries[query_id] for query_id in query_ids}<br>            if generate_response:<br>                responses_dict = {<br>                    query_id: responses_dict[query_id] for query_id in query_ids<br>                }<br>        return QueryResponseDataset(queries=queries, responses=responses_dict)<br>    async def agenerate_questions_from_nodes(self, num: int | None = None) -> List[str]:<br>        \"\"\"Generates questions for each document.\"\"\"<br>        dataset = await self._agenerate_dataset(<br>            self.nodes, num=num, generate_response=False<br>        )<br>        return dataset.questions<br>    async def agenerate_dataset_from_nodes(<br>        self, num: int | None = None<br>    ) -> QueryResponseDataset:<br>        \"\"\"Generates questions for each document.\"\"\"<br>        return await self._agenerate_dataset(<br>            self.nodes, num=num, generate_response=True<br>        )<br>    def generate_questions_from_nodes(self, num: int | None = None) -> List[str]:<br>        \"\"\"Generates questions for each document.\"\"\"<br>        return asyncio_run(self.agenerate_questions_from_nodes(num=num))<br>    def generate_dataset_from_nodes(<br>        self, num: int | None = None<br>    ) -> QueryResponseDataset:<br>        \"\"\"Generates questions for each document.\"\"\"<br>        return asyncio_run(self.agenerate_dataset_from_nodes(num=num))<br>    def _get_prompts(self) -> PromptDictType:<br>        \"\"\"Get prompts.\"\"\"<br>        return {<br>            \"text_question_template\": self.text_question_template,<br>            \"text_qa_template\": self.text_qa_template,<br>        }<br>    def _get_prompt_modules(self) -> PromptMixinType:<br>        \"\"\"Get prompt modules.\"\"\"<br>        return {}<br>    def _update_prompts(self, prompts: PromptDictType) -> None:<br>        \"\"\"Update prompts.\"\"\"<br>        if \"text_question_template\" in prompts:<br>            self.text_question_template = prompts[\"text_question_template\"]<br>        if \"text_qa_template\" in prompts:<br>            self.text_qa_template = prompts[\"text_qa_template\"]<br></code>`` |\n"
    },
    {
      "id": "1f455e7a-cd59-4910-874b-e5b06ddf7020",
      "size": 2366,
      "headers": {
        "h1": "Pairwise comparison",
        "h2": "PairwiseComparisonEvaluator \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br></code>`<code> | </code>`<code><br>class PairwiseComparisonEvaluator(BaseEvaluator):<br>    \"\"\"Pairwise comparison evaluator.<br>    Evaluates the quality of a response vs. a \"reference\" response given a question by<br>    having an LLM judge which response is better.<br>    Outputs whether the </code>response<code> given is better than the </code>reference<code> response.<br>    Args:<br>        eval_template (Optional[Union[str, BasePromptTemplate]]):<br>            The template to use for evaluation.<br>        enforce_consensus (bool): Whether to enforce consensus (consistency if we<br>            flip the order of the answers). Defaults to True.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        llm: Optional[LLM] = None,<br>        eval_template: Optional[Union[BasePromptTemplate, str]] = None,<br>        parser_function: Callable[<br>            [str], Tuple[Optional[bool], Optional[float], Optional[str]]<br>        ] = _default_parser_function,<br>        enforce_consensus: bool = True,<br>    ) -> None:<br>        self._llm = llm or Settings.llm<br>        self._eval_template: BasePromptTemplate<br>        if isinstance(eval_template, str):<br>            self._eval_template = PromptTemplate(eval_template)<br>        else:<br>            self._eval_template = eval_template or DEFAULT_EVAL_TEMPLATE<br>        self._enforce_consensus = enforce_consensus<br>        self._parser_function = parser_function<br>    def _get_prompts(self) -> PromptDictType:<br>        \"\"\"Get prompts.\"\"\"<br>        return {<br>            \"eval_template\": self._eval_template,<br>        }<br>    def _update_prompts(self, prompts: PromptDictType) -> None:<br>        \"\"\"Update prompts.\"\"\"<br>        if \"eval_template\" in prompts:<br>            self._eval_template = prompts[\"eval_template\"]<br>    async def _get_eval_result(<br>        self,<br>        query: str,<br>        response: str,<br>        second_response: str,<br>        reference: Optional[str],<br>    ) -> EvaluationResult:<br>        \"\"\"Get evaluation result.\"\"\"<br>        eval_response = await self._llm.apredict(<br>            prompt=self._eval_template,<br>            query=query,<br>            answer_1=response,<br>            answer_2=second_response,<br>            reference=reference or \"\",<br>        )<br>        # Extract from response<br>        passing, score, feedback = self._parser_function(eval_response)<br>        if passing is None and score is None and feedback is None:<br>            return EvaluationResult(<br>                query=query,<br>                invalid_result=True,<br>                invalid_reason=\"Output cannot be parsed\",<br>                feedback=eval_response,<br>            )<br>        else:<br>            return EvaluationResult(<br>                query=query,<br>                response=eval_response,<br>                passing=passing,<br>                score=score,<br>                feedback=eval_response,<br>                pairwise_source=EvaluationSource.ORIGINAL,<br>            )<br>    async def _resolve_results(<br>        self,<br>        eval_result: EvaluationResult,<br>        flipped_eval_result: EvaluationResult,<br>    ) -> EvaluationResult:<br>        \"\"\"Resolve eval results from evaluation + flipped evaluation.<br>        Args:<br>            eval_result (EvaluationResult): Result when answer_1 is shown first<br>            flipped_eval_result (EvaluationResult): Result when answer_2 is shown first<br>        Returns:<br>            EvaluationResult: The final evaluation result<br>        \"\"\"<br>        # add pairwise_source to eval_result and flipped_eval_result<br>        eval_result.pairwise_source = EvaluationSource.ORIGINAL<br>        flipped_eval_result.pairwise_source = EvaluationSource.FLIPPED<br>        # count the votes for each of the 2 answers<br>        votes_1 = 0.0<br>        votes_2 = 0.0<br>        if eval_result.score is not None and flipped_eval_result.score is not None:<br>            votes_1 = eval_result.score + (1 - flipped_eval_result.score)<br>            votes_2 = (1 - eval_result.score) + flipped_eval_result.score<br>        if votes_1 + votes_2 != 2:  # each round, the judge can give a total of 1 vote<br>            raise ValueError(\"Impossible score results. Total amount of votes is 2.\")<br>        # get the judges (original and flipped) who voted for answer_1<br>        voters_1 = [eval_result] * (eval_result.score == 1.0) + [<br>            flipped_eval_result<br>        ] * (flipped_eval_result.score == 0.0)<br>        # get the judges (original and flipped) who voted for answer_2<br>        voters_2 = [eval_result] * (eval_result.score == 0.0) + [<br>            flipped_eval_result<br>        ] * (flipped_eval_result.score == 1.0)<br>        if votes_1 > votes_2:<br>            return voters_1[0]  # return any voter for answer_1<br>        elif votes_2 > votes_1:<br>            return voters_2[0]  # return any vote for answer_2<br>        else:<br>            if (<br>                eval_result.score == 0.5<br>            ):  # votes_1 == votes_2 can only happen if both are 1.0 (so actual tie)<br>                # doesn't matter which one we return here<br>                return eval_result<br>            else:  # Inconclusive case!<br>                return EvaluationResult(<br>                    query=eval_result.query,<br>                    response=\"\",<br>                    passing=None,<br>                    score=0.5,<br>                    feedback=\"\",<br>                    pairwise_source=EvaluationSource.NEITHER,<br>                )<br>    async def aevaluate(<br>        self,<br>        query: Optional[str] = None,<br>        response: Optional[str] = None,<br>        contexts: Optional[Sequence[str]] = None,<br>        second_response: Optional[str] = None,<br>        reference: Optional[str] = None,<br>        sleep_time_in_seconds: int = 0,<br>        **kwargs: Any,<br>    ) -> EvaluationResult:<br>        del kwargs  # Unused<br>        del contexts  # Unused<br>        if query is None or response is None or second_response is None:<br>            raise ValueError(<br>                \"query, response, second_response, and reference must be provided\"<br>            )<br>        await asyncio.sleep(sleep_time_in_seconds)<br>        eval_result = await self._get_eval_result(<br>            query, response, second_response, reference<br>        )<br>        if self._enforce_consensus and not eval_result.invalid_result:<br>            # Flip the order of the answers and see if the answer is consistent<br>            # (which means that the score should flip from 0 to 1 and vice-versa)<br>            # if not, then we return a tie<br>            flipped_eval_result = await self._get_eval_result(<br>                query, second_response, response, reference<br>            )<br>            if not flipped_eval_result.invalid_result:<br>                resolved_eval_result = await self._resolve_results(<br>                    eval_result, flipped_eval_result<br>                )<br>            else:<br>                resolved_eval_result = EvaluationResult(<br>                    query=eval_result.query,<br>                    response=eval_result.response,<br>                    feedback=flipped_eval_result.response,<br>                    invalid_result=True,<br>                    invalid_reason=\"Output cannot be parsed.\",<br>                )<br>        else:<br>            resolved_eval_result = eval_result<br>        return resolved_eval_result<br></code>`` |\n"
    },
    {
      "id": "83b687ce-b7f5-482d-a47a-fc10845c7184",
      "size": 3878,
      "headers": {
        "h1": "Llm compiler",
        "h2": "LLMCompilerAgentWorker \\#",
        "h3": ""
      },
      "text": "| ``<code><br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br></code>`<code> | </code>`<code><br>class LLMCompilerAgentWorker(BaseAgentWorker):<br>    \"\"\"LLMCompiler Agent Worker.<br>    LLMCompiler is an agent framework that allows async multi-function calling and query planning.<br>    Here is the implementation.<br>    Source Repo (paper linked): https://github.com/SqueezeAILab/LLMCompiler?tab=readme-ov-file<br>    \"\"\"<br>    def __init__(<br>        self,<br>        tools: Sequence[BaseTool],<br>        llm: LLM,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        planner_example_prompt_str: Optional[str] = None,<br>        stop: Optional[List[str]] = None,<br>        joiner_prompt: Optional[PromptTemplate] = None,<br>        max_replans: int = 3,<br>    ) -> None:<br>        self.callback_manager = callback_manager or llm.callback_manager<br>        self.planner_example_prompt_str = (<br>            planner_example_prompt_str or PLANNER_EXAMPLE_PROMPT<br>        )<br>        self.system_prompt = generate_llm_compiler_prompt(<br>            tools, example_prompt=self.planner_example_prompt_str<br>        )<br>        self.system_prompt_replan = generate_llm_compiler_prompt(<br>            tools, is_replan=True, example_prompt=self.planner_example_prompt_str<br>        )<br>        self.llm = llm<br>        # TODO: make tool_retriever work<br>        self.tools = tools<br>        self.output_parser = LLMCompilerPlanParser(tools=tools)<br>        self.stop = stop<br>        self.max_replans = max_replans<br>        self.verbose = verbose<br>        # joiner program<br>        self.joiner_prompt = joiner_prompt or PromptTemplate(OUTPUT_PROMPT)<br>        self.joiner_program = LLMTextCompletionProgram.from_defaults(<br>            output_parser=LLMCompilerJoinerParser(),<br>            output_cls=JoinerOutput,<br>            prompt=self.joiner_prompt,<br>            llm=self.llm,<br>            verbose=verbose,<br>        )<br>        # if len(tools) > 0 and tool_retriever is not None:<br>        #     raise ValueError(\"Cannot specify both tools and tool_retriever\")<br>        # elif len(tools) > 0:<br>        #     self._get_tools = lambda _: tools<br>        # elif tool_retriever is not None:<br>        #     tool_retriever_c = cast(ObjectRetriever[BaseTool], tool_retriever)<br>        #     self._get_tools = lambda message: tool_retriever_c.retrieve(message)<br>        # else:<br>        #     self._get_tools = lambda _: []<br>    @classmethod<br>    def from_tools(<br>        cls,<br>        tools: Optional[Sequence[BaseTool]] = None,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        llm: Optional[LLM] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        **kwargs: Any,<br>    ) -> \"LLMCompilerAgentWorker\":<br>        \"\"\"Convenience constructor method from set of of BaseTools (Optional).<br>        Returns:<br>            LLMCompilerAgentWorker: the LLMCompilerAgentWorker instance<br>        \"\"\"<br>        llm = llm or OpenAI(model=DEFAULT_MODEL_NAME)<br>        if callback_manager is not None:<br>            llm.callback_manager = callback_manager<br>        return cls(<br>            tools=tools or [],<br>            tool_retriever=tool_retriever,<br>            llm=llm,<br>            callback_manager=callback_manager,<br>            verbose=verbose,<br>        )<br>    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>        \"\"\"Initialize step from task.\"\"\"<br>        sources: List[ToolOutput] = []<br>        # temporary memory for new messages<br>        new_memory = ChatMemoryBuffer.from_defaults()<br>        # put user message in memory<br>        new_memory.put(ChatMessage(content=task.input, role=MessageRole.USER))<br>        # initialize task state<br>        task_state = {<br>            \"sources\": sources,<br>            \"new_memory\": new_memory,<br>        }<br>        task.extra_state.update(task_state)<br>        return TaskStep(<br>            task_id=task.task_id,<br>            step_id=str(uuid.uuid4()),<br>            input=task.input,<br>            step_state={\"is_replan\": False, \"contexts\": [], \"replans\": 0},<br>        )<br>    def get_tools(self, input: str) -> List[AsyncBaseTool]:<br>        \"\"\"Get tools.\"\"\"<br>        # return [adapt_to_async_tool(t) for t in self._get_tools(input)]<br>        return [adapt_to_async_tool(t) for t in self.tools]<br>    async def arun_llm(<br>        self,<br>        input: str,<br>        previous_context: Optional[str] = None,<br>        is_replan: bool = False,<br>    ) -> ChatResponse:<br>        \"\"\"Run LLM.\"\"\"<br>        if is_replan:<br>            system_prompt = self.system_prompt_replan<br>            assert previous_context is not None, \"previous_context cannot be None\"<br>            human_prompt = f\"Question: {input}\\n{previous_context}\\n\"<br>        else:<br>            system_prompt = self.system_prompt<br>            human_prompt = f\"Question: {input}\"<br>        messages = [<br>            ChatMessage(role=MessageRole.SYSTEM, content=system_prompt),<br>            ChatMessage(role=MessageRole.USER, content=human_prompt),<br>        ]<br>        return await self.llm.achat(messages)<br>    async def ajoin(<br>        self,<br>        input: str,<br>        tasks: Dict[int, LLMCompilerTask],<br>        is_final: bool = False,<br>    ) -> JoinerOutput:<br>        \"\"\"Join answer using LLM/agent.\"\"\"<br>        agent_scratchpad = \"\\n\\n\"<br>        agent_scratchpad += \"\".join(<br>            [<br>                task.get_thought_action_observation(<br>                    include_action=True, include_thought=True<br>                )<br>                for task in tasks.values()<br>                if not task.is_join<br>            ]<br>        )<br>        agent_scratchpad = agent_scratchpad.strip()<br>        output = self.joiner_program(<br>            query_str=input,<br>            context_str=agent_scratchpad,<br>        )<br>        output = cast(JoinerOutput, output)<br>        if self.verbose:<br>            print_text(f\"> Thought: {output.thought}\\n\", color=\"pink\")<br>            print_text(f\"> Answer: {output.answer}\\n\", color=\"pink\")<br>        if is_final:<br>            output.is_replan = False<br>        return output<br>    def _get_task_step_response(<br>        self,<br>        task: Task,<br>        llmc_tasks: Dict[int, LLMCompilerTask],<br>        answer: str,<br>        joiner_thought: str,<br>        step: TaskStep,<br>        is_replan: bool,<br>    ) -> TaskStepOutput:<br>        \"\"\"Get task step response.\"\"\"<br>        agent_answer = AgentChatResponse(response=answer, sources=[])<br>        if not is_replan:<br>            # generate final answer<br>            new_steps = []<br>            # put in memory<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(content=answer, role=MessageRole.ASSISTANT)<br>            )<br>        else:<br>            # Collect contexts for the subsequent replanner<br>            context = generate_context_for_replanner(<br>                tasks=llmc_tasks, joiner_thought=joiner_thought<br>            )<br>            new_contexts = step.step_state[\"contexts\"] + [context]<br>            # TODO: generate new steps<br>            new_steps = [<br>                step.get_next_step(<br>                    step_id=str(uuid.uuid4()),<br>                    input=None,<br>                    step_state={<br>                        \"is_replan\": is_replan,<br>                        \"contexts\": new_contexts,<br>                        \"replans\": step.step_state[\"replans\"] + 1,<br>                    },<br>                )<br>            ]<br>        return TaskStepOutput(<br>            output=agent_answer,<br>            task_step=step,<br>            next_steps=new_steps,<br>            is_last=not is_replan,<br>        )<br>    async def _arun_step(<br>        self,<br>        step: TaskStep,<br>        task: Task,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        if self.verbose:<br>            print(<br>                f\"> Running step {step.step_id} for task {task.task_id}.\\n\"<br>                f\"> Step count: {step.step_state['replans']}\"<br>            )<br>        is_final_iter = (<br>            step.step_state[\"is_replan\"]<br>            and step.step_state[\"replans\"] >= self.max_replans<br>        )<br>        if len(step.step_state[\"contexts\"]) == 0:<br>            formatted_contexts = None<br>        else:<br>            formatted_contexts = format_contexts(step.step_state[\"contexts\"])<br>        llm_response = await self.arun_llm(<br>            task.input,<br>            previous_context=formatted_contexts,<br>            is_replan=step.step_state[\"is_replan\"],<br>        )<br>        if self.verbose:<br>            print_text(f\"> Plan: {llm_response.message.content}\\n\", color=\"pink\")<br>        # return task dict (will generate plan, parse into dictionary)<br>        task_dict = self.output_parser.parse(cast(str, llm_response.message.content))<br>        # execute via task executor<br>        task_fetching_unit = TaskFetchingUnit.from_tasks(<br>            task_dict, verbose=self.verbose<br>        )<br>        await task_fetching_unit.schedule()<br>        ## join tasks - get response<br>        tasks = cast(Dict[int, LLMCompilerTask], task_fetching_unit.tasks)<br>        joiner_output = await self.ajoin(<br>            task.input,<br>            tasks,<br>            is_final=is_final_iter,<br>        )<br>        # get task step response (with new steps planned)<br>        return self._get_task_step_response(<br>            task,<br>            llmc_tasks=tasks,<br>            answer=joiner_output.answer,<br>            joiner_thought=joiner_output.thought,<br>            step=step,<br>            is_replan=joiner_output.is_replan,<br>        )<br>    @trace_method(\"run_step\")<br>    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        return asyncio.run(self.arun_step(step=step, task=task, **kwargs))<br>    @trace_method(\"run_step\")<br>    async def arun_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        return await self._arun_step(step, task)<br>    @trace_method(\"run_step\")<br>    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        # # TODO: figure out if we need a different type for TaskStepOutput<br>        # return self._run_step_stream(step, task)<br>        raise NotImplementedError<br>    @trace_method(\"run_step\")<br>    async def astream_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        raise NotImplementedError<br>        # \"\"\"Run step (async stream).\"\"\"<br>        # return await self._arun_step_stream(step, task)<br>    def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>        \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>        # add new messages to memory<br>        task.memory.put_messages(task.extra_state[\"new_memory\"].get_all())<br>        # reset new memory<br>        task.extra_state[\"new_memory\"].reset()<br></code>`` |\n"
    },
    {
      "id": "ac5e6f2e-efb5-4f35-bbcf-7075acd2b30b",
      "size": 2191,
      "headers": {
        "h1": "Openai legacy",
        "h2": "ContextRetrieverOpenAIAgent \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br></code>`<code> | </code>`<code><br>class ContextRetrieverOpenAIAgent(BaseOpenAIAgent):<br>    \"\"\"ContextRetriever OpenAI Agent.<br>    This agent performs retrieval from BaseRetriever before<br>    calling the LLM. Allows it to augment user message with context.<br>    NOTE: this is a beta feature, function interfaces might change.<br>    Args:<br>        tools (List[BaseTool]): A list of tools.<br>        retriever (BaseRetriever): A retriever.<br>        qa_prompt (Optional[PromptTemplate]): A QA prompt.<br>        context_separator (str): A context separator.<br>        llm (Optional[OpenAI]): An OpenAI LLM.<br>        chat_history (Optional[List[ChatMessage]]): A chat history.<br>        prefix_messages: List[ChatMessage]: A list of prefix messages.<br>        verbose (bool): Whether to print debug statements.<br>        max_function_calls (int): Maximum number of function calls.<br>        callback_manager (Optional[CallbackManager]): A callback manager.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        tools: List[BaseTool],<br>        retriever: BaseRetriever,<br>        qa_prompt: PromptTemplate,<br>        context_separator: str,<br>        llm: OpenAI,<br>        memory: BaseMemory,<br>        prefix_messages: List[ChatMessage],<br>        verbose: bool = False,<br>        max_function_calls: int = DEFAULT_MAX_FUNCTION_CALLS,<br>        callback_manager: Optional[CallbackManager] = None,<br>    ) -> None:<br>        super().__init__(<br>            llm=llm,<br>            memory=memory,<br>            prefix_messages=prefix_messages,<br>            verbose=verbose,<br>            max_function_calls=max_function_calls,<br>            callback_manager=callback_manager,<br>        )<br>        self._tools = tools<br>        self._qa_prompt = qa_prompt<br>        self._retriever = retriever<br>        self._context_separator = context_separator<br>    @classmethod<br>    def from_tools_and_retriever(<br>        cls,<br>        tools: List[BaseTool],<br>        retriever: BaseRetriever,<br>        qa_prompt: Optional[PromptTemplate] = None,<br>        context_separator: str = \"\\n\",<br>        llm: Optional[LLM] = None,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        memory: Optional[BaseMemory] = None,<br>        memory_cls: Type[BaseMemory] = ChatMemoryBuffer,<br>        verbose: bool = False,<br>        max_function_calls: int = DEFAULT_MAX_FUNCTION_CALLS,<br>        callback_manager: Optional[CallbackManager] = None,<br>        system_prompt: Optional[str] = None,<br>        prefix_messages: Optional[List[ChatMessage]] = None,<br>    ) -> \"ContextRetrieverOpenAIAgent\":<br>        \"\"\"Create a ContextRetrieverOpenAIAgent from a retriever.<br>        Args:<br>            retriever (BaseRetriever): A retriever.<br>            qa_prompt (Optional[PromptTemplate]): A QA prompt.<br>            context_separator (str): A context separator.<br>            llm (Optional[OpenAI]): An OpenAI LLM.<br>            chat_history (Optional[ChatMessageHistory]): A chat history.<br>            verbose (bool): Whether to print debug statements.<br>            max_function_calls (int): Maximum number of function calls.<br>            callback_manager (Optional[CallbackManager]): A callback manager.<br>        \"\"\"<br>        qa_prompt = qa_prompt or DEFAULT_QA_PROMPT<br>        chat_history = chat_history or []<br>        llm = llm or Settings.llm<br>        if not isinstance(llm, OpenAI):<br>            raise ValueError(\"llm must be a OpenAI instance\")<br>        if callback_manager is not None:<br>            llm.callback_manager = callback_manager<br>        memory = memory or memory_cls.from_defaults(chat_history=chat_history, llm=llm)<br>        if not is_function_calling_model(llm.model):<br>            raise ValueError(<br>                f\"Model name {llm.model} does not support function calling API.\"<br>            )<br>        if system_prompt is not None:<br>            if prefix_messages is not None:<br>                raise ValueError(<br>                    \"Cannot specify both system_prompt and prefix_messages\"<br>                )<br>            prefix_messages = [ChatMessage(content=system_prompt, role=\"system\")]<br>        prefix_messages = prefix_messages or []<br>        return cls(<br>            tools=tools,<br>            retriever=retriever,<br>            qa_prompt=qa_prompt,<br>            context_separator=context_separator,<br>            llm=llm,<br>            memory=memory,<br>            prefix_messages=prefix_messages,<br>            verbose=verbose,<br>            max_function_calls=max_function_calls,<br>            callback_manager=callback_manager,<br>        )<br>    def _get_tools(self, message: str) -> List[BaseTool]:<br>        \"\"\"Get tools.\"\"\"<br>        return self._tools<br>    def _build_formatted_message(self, message: str) -> str:<br>        # augment user message<br>        retrieved_nodes_w_scores: List[NodeWithScore] = self._retriever.retrieve(<br>            message<br>        )<br>        retrieved_nodes = [node.node for node in retrieved_nodes_w_scores]<br>        retrieved_texts = [node.get_content() for node in retrieved_nodes]<br>        # format message<br>        context_str = self._context_separator.join(retrieved_texts)<br>        return self._qa_prompt.format(context_str=context_str, query_str=message)<br>    def chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Union[str, dict] = \"auto\",<br>    ) -> AgentChatResponse:<br>        \"\"\"Chat.\"\"\"<br>        formatted_message = self._build_formatted_message(message)<br>        if self._verbose:<br>            print_text(formatted_message + \"\\n\", color=\"yellow\")<br>        return super().chat(<br>            formatted_message, chat_history=chat_history, tool_choice=tool_choice<br>        )<br>    async def achat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Union[str, dict] = \"auto\",<br>    ) -> AgentChatResponse:<br>        \"\"\"Chat.\"\"\"<br>        formatted_message = self._build_formatted_message(message)<br>        if self._verbose:<br>            print_text(formatted_message + \"\\n\", color=\"yellow\")<br>        return await super().achat(<br>            formatted_message, chat_history=chat_history, tool_choice=tool_choice<br>        )<br>    def get_tools(self, message: str) -> List[BaseTool]:<br>        \"\"\"Get tools.\"\"\"<br>        return self._get_tools(message)<br></code>`` |\n"
    },
    {
      "id": "03a64438-a407-4fd2-8b10-44fe03a0be0f",
      "size": 1027,
      "headers": {
        "h1": "Instructor",
        "h2": "InstructorEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>89<br>90<br>91<br>92<br>93<br>94<br>95<br>96<br>97<br></code>`<code> | </code>`<code><br>class InstructorEmbedding(BaseEmbedding):<br>    query_instruction: Optional[str] = Field(<br>        description=\"Instruction to prepend to query text.\"<br>    )<br>    text_instruction: Optional[str] = Field(<br>        description=\"Instruction to prepend to text.\"<br>    )<br>    cache_folder: Optional[str] = Field(<br>        description=\"Cache folder for huggingface files.\"<br>    )<br>    _model: Any = PrivateAttr()<br>    def __init__(<br>        self,<br>        model_name: str = DEFAULT_INSTRUCT_MODEL,<br>        query_instruction: Optional[str] = None,<br>        text_instruction: Optional[str] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        cache_folder: Optional[str] = None,<br>        device: Optional[str] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>    ):<br>        super().__init__(<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            model_name=model_name,<br>            query_instruction=query_instruction,<br>            text_instruction=text_instruction,<br>            cache_folder=cache_folder,<br>        )<br>        self._model = INSTRUCTOR(model_name, cache_folder=cache_folder, device=device)<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"InstructorEmbedding\"<br>    def _format_query_text(self, query_text: str) -> List[str]:<br>        \"\"\"Format query text.\"\"\"<br>        instruction = self.query_instruction<br>        if instruction is None:<br>            instruction = get_query_instruct_for_model_name(self.model_name)<br>        return [instruction, query_text]<br>    def _format_text(self, text: str) -> List[str]:<br>        \"\"\"Format text.\"\"\"<br>        instruction = self.text_instruction<br>        if instruction is None:<br>            instruction = get_text_instruct_for_model_name(self.model_name)<br>        return [instruction, text]<br>    def _embed(self, instruct_sentence_pairs: List[List[str]]) -> List[List[float]]:<br>        \"\"\"Embed sentences.\"\"\"<br>        return self._model.encode(instruct_sentence_pairs).tolist()<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        query_pair = self._format_query_text(query)<br>        return self._embed([query_pair])[0]<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding async.\"\"\"<br>        return self._get_query_embedding(query)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding async.\"\"\"<br>        return self._get_text_embedding(text)<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        text_pair = self._format_text(text)<br>        return self._embed([text_pair])[0]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        text_pairs = [self._format_text(text) for text in texts]<br>        return self._embed(text_pairs)<br></code>`` |\n"
    },
    {
      "id": "96671927-9bcb-43db-9c3d-22a6f6972f18",
      "size": 5104,
      "headers": {
        "h1": "Bedrock",
        "h2": "BedrockEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br></code>`<code> | </code>`<code><br>class BedrockEmbedding(BaseEmbedding):<br>    model_name: str = Field(description=\"The modelId of the Bedrock model to use.\")<br>    profile_name: Optional[str] = Field(<br>        description=\"The name of aws profile to use. If not given, then the default profile is used.\",<br>    )<br>    aws_access_key_id: Optional[str] = Field(description=\"AWS Access Key ID to use\")<br>    aws_secret_access_key: Optional[str] = Field(<br>        description=\"AWS Secret Access Key to use\"<br>    )<br>    aws_session_token: Optional[str] = Field(description=\"AWS Session Token to use\")<br>    region_name: Optional[str] = Field(<br>        description=\"AWS region name to use. Uses region configured in AWS CLI if not passed\",<br>    )<br>    botocore_session: Optional[Any] = Field(<br>        description=\"Use this Botocore session instead of creating a new default one.\",<br>        exclude=True,<br>    )<br>    botocore_config: Optional[Any] = Field(<br>        description=\"Custom configuration object to use instead of the default generated one.\",<br>        exclude=True,<br>    )<br>    max_retries: int = Field(<br>        default=10, description=\"The maximum number of API retries.\", gt=0<br>    )<br>    timeout: float = Field(<br>        default=60.0,<br>        description=\"The timeout for the Bedrock API request in seconds. It will be used for both connect and read timeouts.\",<br>    )<br>    additional_kwargs: Dict[str, Any] = Field(<br>        default_factory=dict, description=\"Additional kwargs for the bedrock client.\"<br>    )<br>    _client: Any = PrivateAttr()<br>    def __init__(<br>        self,<br>        model_name: str = Models.TITAN_EMBEDDING,<br>        profile_name: Optional[str] = None,<br>        aws_access_key_id: Optional[str] = None,<br>        aws_secret_access_key: Optional[str] = None,<br>        aws_session_token: Optional[str] = None,<br>        region_name: Optional[str] = None,<br>        client: Optional[Any] = None,<br>        botocore_session: Optional[Any] = None,<br>        botocore_config: Optional[Any] = None,<br>        additional_kwargs: Optional[Dict[str, Any]] = None,<br>        max_retries: int = 10,<br>        timeout: float = 60.0,<br>        callback_manager: Optional[CallbackManager] = None,<br>        # base class<br>        system_prompt: Optional[str] = None,<br>        messages_to_prompt: Optional[Callable[[Sequence[ChatMessage]], str]] = None,<br>        completion_to_prompt: Optional[Callable[[str], str]] = None,<br>        pydantic_program_mode: PydanticProgramMode = PydanticProgramMode.DEFAULT,<br>        output_parser: Optional[BaseOutputParser] = None,<br>        **kwargs: Any,<br>    ):<br>        additional_kwargs = additional_kwargs or {}<br>        session_kwargs = {<br>            \"profile_name\": profile_name,<br>            \"region_name\": region_name,<br>            \"aws_access_key_id\": aws_access_key_id,<br>            \"aws_secret_access_key\": aws_secret_access_key,<br>            \"aws_session_token\": aws_session_token,<br>            \"botocore_session\": botocore_session,<br>        }<br>        try:<br>            import boto3<br>            from botocore.config import Config<br>            config = (<br>                Config(<br>                    retries={\"max_attempts\": max_retries, \"mode\": \"standard\"},<br>                    connect_timeout=timeout,<br>                    read_timeout=timeout,<br>                )<br>                if botocore_config is None<br>                else botocore_config<br>            )<br>            session = boto3.Session(**session_kwargs)<br>        except ImportError:<br>            raise ImportError(<br>                \"boto3 package not found, install with\" \"'pip install boto3'\"<br>            )<br>        super().__init__(<br>            model_name=model_name,<br>            max_retries=max_retries,<br>            timeout=timeout,<br>            botocore_config=config,<br>            profile_name=profile_name,<br>            aws_access_key_id=aws_access_key_id,<br>            aws_secret_access_key=aws_secret_access_key,<br>            aws_session_token=aws_session_token,<br>            region_name=region_name,<br>            botocore_session=botocore_session,<br>            additional_kwargs=additional_kwargs,<br>            callback_manager=callback_manager,<br>            system_prompt=system_prompt,<br>            messages_to_prompt=messages_to_prompt,<br>            completion_to_prompt=completion_to_prompt,<br>            pydantic_program_mode=pydantic_program_mode,<br>            output_parser=output_parser,<br>            **kwargs,<br>        )<br>        # Prior to general availability, custom boto3 wheel files were<br>        # distributed that used the bedrock service to invokeModel.<br>        # This check prevents any services still using those wheel files<br>        # from breaking<br>        if client is not None:<br>            self._client = client<br>        elif \"bedrock-runtime\" in session.get_available_services():<br>            self._client = session.client(\"bedrock-runtime\", config=config)<br>        else:<br>            self._client = session.client(\"bedrock\", config=config)<br>    @staticmethod<br>    def list_supported_models() -> Dict[str, List[str]]:<br>        list_models = {}<br>        for provider in PROVIDERS:<br>            list_models[provider.value] = [<br>                m.value for m in Models if provider.value in m.value<br>            ]<br>        return list_models<br>    @classmethod<br>    def class_name(self) -> str:<br>        return \"BedrockEmbedding\"<br>    @deprecated(<br>        version=\"0.9.48\",<br>        reason=(<br>            \"Use the provided kwargs in the constructor, \"<br>            \"set_credentials will be removed in future releases.\"<br>        ),<br>        action=\"once\",<br>    )<br>    def set_credentials(<br>        self,<br>        aws_region: Optional[str] = None,<br>        aws_access_key_id: Optional[str] = None,<br>        aws_secret_access_key: Optional[str] = None,<br>        aws_session_token: Optional[str] = None,<br>        aws_profile: Optional[str] = None,<br>    ) -> None:<br>        aws_region = aws_region or os.getenv(\"AWS_REGION\")<br>        aws_access_key_id = aws_access_key_id or os.getenv(\"AWS_ACCESS_KEY_ID\")<br>        aws_secret_access_key = aws_secret_access_key or os.getenv(<br>            \"AWS_SECRET_ACCESS_KEY\"<br>        )<br>        aws_session_token = aws_session_token or os.getenv(\"AWS_SESSION_TOKEN\")<br>        if aws_region is None:<br>            warnings.warn(<br>                \"AWS_REGION not found. Set environment variable AWS_REGION or set aws_region\"<br>            )<br>        if aws_access_key_id is None:<br>            warnings.warn(<br>                \"AWS_ACCESS_KEY_ID not found. Set environment variable AWS_ACCESS_KEY_ID or set aws_access_key_id\"<br>            )<br>            assert aws_access_key_id is not None<br>        if aws_secret_access_key is None:<br>            warnings.warn(<br>                \"AWS_SECRET_ACCESS_KEY not found. Set environment variable AWS_SECRET_ACCESS_KEY or set aws_secret_access_key\"<br>            )<br>            assert aws_secret_access_key is not None<br>        if aws_session_token is None:<br>            warnings.warn(<br>                \"AWS_SESSION_TOKEN not found. Set environment variable AWS_SESSION_TOKEN or set aws_session_token\"<br>            )<br>            assert aws_session_token is not None<br>        session_kwargs = {<br>            \"profile_name\": aws_profile,<br>            \"region_name\": aws_region,<br>            \"aws_access_key_id\": aws_access_key_id,<br>            \"aws_secret_access_key\": aws_secret_access_key,<br>            \"aws_session_token\": aws_session_token,<br>        }<br>        try:<br>            import boto3<br>            session = boto3.Session(**session_kwargs)<br>        except ImportError:<br>            raise ImportError(<br>                \"boto3 package not found, install with\" \"'pip install boto3'\"<br>            )<br>        if \"bedrock-runtime\" in session.get_available_services():<br>            self._client = session.client(\"bedrock-runtime\")<br>        else:<br>            self._client = session.client(\"bedrock\")<br>    @classmethod<br>    @deprecated(<br>        version=\"0.9.48\",<br>        reason=(<br>            \"Use the provided kwargs in the constructor, \"<br>            \"set_credentials will be removed in future releases.\"<br>        ),<br>        action=\"once\",<br>    )<br>    def from_credentials(<br>        cls,<br>        model_name: str = Models.TITAN_EMBEDDING,<br>        aws_region: Optional[str] = None,<br>        aws_access_key_id: Optional[str] = None,<br>        aws_secret_access_key: Optional[str] = None,<br>        aws_session_token: Optional[str] = None,<br>        aws_profile: Optional[str] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>    ) -> \"BedrockEmbedding\":<br>        \"\"\"<br>        Instantiate using AWS credentials.<br>        Args:<br>            model_name (str) : Name of the model<br>            aws_access_key_id (str): AWS access key ID<br>            aws_secret_access_key (str): AWS secret access key<br>            aws_session_token (str): AWS session token<br>            aws_region (str): AWS region where the service is located<br>            aws_profile (str): AWS profile, when None, default profile is chosen automatically<br>        Example:<br>                .. code-block:: python<br>                    from llama_index.embeddings import BedrockEmbedding<br>                    # Define the model name<br>                    model_name = \"your_model_name\"<br>                    embeddings = BedrockEmbedding.from_credentials(<br>                        model_name,<br>                        aws_access_key_id,<br>                        aws_secret_access_key,<br>                        aws_session_token,<br>                        aws_region,<br>                        aws_profile,<br>                    )<br>        \"\"\"<br>        session_kwargs = {<br>            \"profile_name\": aws_profile,<br>            \"region_name\": aws_region,<br>            \"aws_access_key_id\": aws_access_key_id,<br>            \"aws_secret_access_key\": aws_secret_access_key,<br>            \"aws_session_token\": aws_session_token,<br>        }<br>        try:<br>            import boto3<br>            session = boto3.Session(**session_kwargs)<br>        except ImportError:<br>            raise ImportError(<br>                \"boto3 package not found, install with\" \"'pip install boto3'\"<br>            )<br>        if \"bedrock-runtime\" in session.get_available_services():<br>            client = session.client(\"bedrock-runtime\")<br>        else:<br>            client = session.client(\"bedrock\")<br>        return cls(<br>            client=client,<br>            model=model_name,<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            verbose=verbose,<br>        )<br>    def _get_embedding(<br>        self, payload: Union[str, List[str]], type: Literal[\"text\", \"query\"]<br>    ) -> Union[Embedding, List[Embedding]]:<br>        \"\"\"Get the embedding for the given payload.<br>        Args:<br>            payload (Union[str, List[str]]): The text or list of texts for which the embeddings are to be obtained.<br>            type (Literal[&quot;text&quot;, &quot;query&quot;]): The type of the payload. It can be either \"text\" or \"query\".<br>        Returns:<br>            Union[Embedding, List[Embedding]]: The embedding or list of embeddings for the given payload. If the payload is a list of strings, then the response will be a list of embeddings.<br>        \"\"\"<br>        if self._client is None:<br>            self.set_credentials()<br>        if self._client is None:<br>            raise ValueError(\"Client not set\")<br>        provider = self.model_name.split(\".\")[0]<br>        request_body = self._get_request_body(provider, payload, type)<br>        response = self._client.invoke_model(<br>            body=request_body,<br>            modelId=self.model_name,<br>            accept=\"application/json\",<br>            contentType=\"application/json\",<br>        )<br>        resp = json.loads(response.get(\"body\").read().decode(\"utf-8\"))<br>        identifiers = PROVIDER_SPECIFIC_IDENTIFIERS.get(provider, None)<br>        if identifiers is None:<br>            raise ValueError(\"Provider not supported\")<br>        return identifiers[\"get_embeddings_func\"](resp, isinstance(payload, list))<br>    def _get_query_embedding(self, query: str) -> Embedding:<br>        return self._get_embedding(query, \"query\")<br>    def _get_text_embedding(self, text: str) -> Embedding:<br>        return self._get_embedding(text, \"text\")<br>    def _get_text_embeddings(self, texts: List[str]) -> List[Embedding]:<br>        provider = self.model_name.split(\".\")[0]<br>        if provider == PROVIDERS.COHERE:<br>            return self._get_embedding(texts, \"text\")<br>        return super()._get_text_embeddings(texts)<br>    def _get_request_body(<br>        self,<br>        provider: str,<br>        payload: Union[str, List[str]],<br>        input_type: Literal[\"text\", \"query\"],<br>    ) -> Any:<br>        \"\"\"Build the request body as per the provider.<br>        Currently supported providers are amazon, cohere.<br>        amazon:<br>            Sample Payload of type str<br>            \"Hello World!\"<br>        cohere:<br>            Sample Payload of type dict of following format<br>            {<br>                'texts': [\"This is a test document\", \"This is another document\"],<br>                'input_type': 'search_document'<br>            }<br>        \"\"\"<br>        if provider == PROVIDERS.AMAZON:<br>            if isinstance(payload, list):<br>                raise ValueError(\"Amazon provider does not support list of texts\")<br>            titan_body_request = {\"inputText\": payload}<br>            # Titan Embedding V2.0 has additional body parameters to check.<br>            if \"dimensions\" in self.additional_kwargs:<br>                if self.model_name == Models.TITAN_EMBEDDING_V2_0:<br>                    titan_body_request[\"dimensions\"] = self.additional_kwargs[<br>                        \"dimensions\"<br>                    ]<br>                else:<br>                    raise ValueError(<br>                        \"'dimensions' param not supported outside of 'titan-embed-text-v2:0' model.\"<br>                    )<br>            if \"normalize\" in self.additional_kwargs:<br>                if self.model_name == Models.TITAN_EMBEDDING_V2_0:<br>                    titan_body_request[\"normalize\"] = self.additional_kwargs[<br>                        \"normalize\"<br>                    ]<br>                else:<br>                    raise ValueError(<br>                        \"'normalize' param not supported outside of 'titan-embed-text-v2:0' model.\"<br>                    )<br>            request_body = json.dumps(titan_body_request)<br>        elif provider == PROVIDERS.COHERE:<br>            input_types = {<br>                \"text\": \"search_document\",<br>                \"query\": \"search_query\",<br>            }<br>            payload = [payload] if isinstance(payload, str) else payload<br>            payload = [p[:2048] if len(p) > 2048 else p for p in payload]<br>            request_body = json.dumps(<br>                {<br>                    \"texts\": payload,<br>                    \"input_type\": input_types[input_type],<br>                }<br>            )<br>        else:<br>            raise ValueError(\"Provider not supported\")<br>        return request_body<br>    async def _aget_query_embedding(self, query: str) -> Embedding:<br>        return self._get_embedding(query, \"query\")<br>    async def _aget_text_embedding(self, text: str) -> Embedding:<br>        return self._get_embedding(text, \"text\")<br></code>`` |\n"
    },
    {
      "id": "5d5bcbde-6cae-411b-9685-42debb570b6f",
      "size": 2680,
      "headers": {
        "h1": "Alephalpha",
        "h2": "AlephAlphaEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br></code>`<code> | </code>`<code><br>class AlephAlphaEmbedding(BaseEmbedding):<br>    \"\"\"AlephAlphaEmbedding uses the Aleph Alpha API to generate embeddings for text.\"\"\"<br>    model: str = Field(<br>        default=DEFAULT_ALEPHALPHA_MODEL, description=\"The Aleph Alpha model to use.\"<br>    )<br>    token: str = Field(default=None, description=\"The Aleph Alpha API token.\")<br>    representation: Optional[str] = Field(<br>        default=SemanticRepresentation.Query,<br>        description=\"The representation type to use for generating embeddings.\",<br>    )<br>    compress_to_size: Optional[int] = Field(<br>        default=None,<br>        description=\"The size to compress the embeddings to.\",<br>        gt=0,<br>    )<br>    base_url: Optional[str] = Field(<br>        default=DEFAULT_ALEPHALPHA_HOST, description=\"The hostname of the API base_url.\"<br>    )<br>    timeout: Optional[float] = Field(<br>        default=None, description=\"The timeout to use in seconds.\", gte=0<br>    )<br>    max_retries: int = Field(<br>        default=10, description=\"The maximum number of API retries.\", gte=0<br>    )<br>    normalize: Optional[bool] = Field(<br>        default=False, description=\"Return normalized embeddings.\"<br>    )<br>    hosting: Optional[str] = Field(default=None, description=\"The hosting to use.\")<br>    nice: bool = Field(default=False, description=\"Whether to be nice to the API.\")<br>    verify_ssl: bool = Field(default=True, description=\"Whether to verify SSL.\")<br>    additional_kwargs: Dict[str, Any] = Field(<br>        default_factory=dict, description=\"Additional kwargs for the Aleph Alpha API.\"<br>    )<br>    # Instance variables initialized via Pydantic's mechanism<br>    _client: Any = PrivateAttr()<br>    _aclient: Any = PrivateAttr()<br>    def __init__(<br>        self,<br>        model: str = DEFAULT_ALEPHALPHA_MODEL,<br>        token: Optional[str] = None,<br>        representation: Optional[str] = None,<br>        base_url: Optional[str] = DEFAULT_ALEPHALPHA_HOST,<br>        hosting: Optional[str] = None,<br>        timeout: Optional[float] = None,<br>        max_retries: int = 10,<br>        nice: bool = False,<br>        verify_ssl: bool = True,<br>        additional_kwargs: Optional[Dict[str, Any]] = None,<br>    ):<br>        \"\"\"<br>        A class representation for generating embeddings using the AlephAlpha API.<br>        Args:<br>            token: The token to use for the AlephAlpha API.<br>            model: The model to use for generating embeddings.<br>            base_url: The base URL of the AlephAlpha API.<br>            nice: Whether to use the \"nice\" mode for the AlephAlpha API.<br>            additional_kwargs: Additional kwargs for the AlephAlpha API.<br>        \"\"\"<br>        additional_kwargs = additional_kwargs or {}<br>        super().__init__(<br>            model=model,<br>            representation=representation,<br>            base_url=base_url,<br>            token=token,<br>            nice=nice,<br>            additional_kwargs=additional_kwargs,<br>        )<br>        self.token = get_from_param_or_env(\"aa_token\", token, \"AA_TOKEN\", \"\")<br>        if representation is not None and isinstance(representation, str):<br>            try:<br>                representation_enum = SemanticRepresentation[<br>                    representation.capitalize()<br>                ]<br>            except KeyError:<br>                raise ValueError(<br>                    f\"{representation} is not a valid representation type. Available types are: {list(SemanticRepresentation.__members__.keys())}\"<br>                )<br>            self.representation = representation_enum<br>        else:<br>            self.representation = representation<br>        self._client = None<br>        self._aclient = None<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"AlephAlphaEmbedding\"<br>    def _get_credential_kwargs(self) -> Dict[str, Any]:<br>        return {<br>            \"token\": self.token,<br>            \"host\": self.base_url,<br>            \"hosting\": self.hosting,<br>            \"request_timeout_seconds\": self.timeout,<br>            \"total_retries\": self.max_retries,<br>            \"nice\": self.nice,<br>            \"verify_ssl\": self.verify_ssl,<br>        }<br>    def _get_client(self) -> Client:<br>        if self._client is None:<br>            self._client = Client(**self._get_credential_kwargs())<br>        return self._client<br>    def _get_aclient(self) -> AsyncClient:<br>        if self._aclient is None:<br>            self._aclient = AsyncClient(**self._get_credential_kwargs())<br>        return self._aclient<br>    def _get_embedding(self, text: str, representation: str) -> List[float]:<br>        \"\"\"Embed sentence using AlephAlpha.\"\"\"<br>        client = self._get_client()<br>        request = SemanticEmbeddingRequest(<br>            prompt=Prompt.from_text(text),<br>            representation=representation or self.representation,<br>            compress_to_size=self.compress_to_size,<br>            normalize=self.normalize,<br>        )<br>        result = client.semantic_embed(request=request, model=self.model)<br>        return result.embedding<br>    async def _aget_embedding(self, text: str, representation: str) -> List[float]:<br>        \"\"\"Get embedding async.\"\"\"<br>        aclient = self._get_aclient()<br>        request = SemanticEmbeddingRequest(<br>            prompt=Prompt.from_text(text),<br>            representation=representation or self.representation,<br>            compress_to_size=self.compress_to_size,<br>            normalize=self.normalize,<br>        )<br>        result = await aclient.semantic_embed(request=request, model=self.model)<br>        return result.embedding<br>    def _get_embeddings(<br>        self, texts: List[str], representation: str<br>    ) -> List[List[float]]:<br>        \"\"\"Embed sentences using AlephAlpha.\"\"\"<br>        client = self._get_client()<br>        request = BatchSemanticEmbeddingRequest(<br>            prompts=[Prompt.from_text(text) for text in texts],<br>            representation=representation or self.representation,<br>            compress_to_size=self.compress_to_size,<br>            normalize=self.normalize,<br>        )<br>        result: BatchSemanticEmbeddingResponse = client.batch_semantic_embed(<br>            request=request, model=self.model<br>        )<br>        return result.embeddings<br>    async def _aget_embeddings(<br>        self, texts: List[str], representation: str<br>    ) -> List[List[float]]:<br>        \"\"\"Get embeddings async.\"\"\"<br>        aclient = self._get_aclient()<br>        request = BatchSemanticEmbeddingRequest(<br>            prompts=[Prompt.from_text(text) for text in texts],<br>            representation=representation or self.representation,<br>            compress_to_size=self.compress_to_size,<br>            normalize=self.normalize,<br>        )<br>        result: BatchSemanticEmbeddingResponse = await aclient.batch_semantic_embed(<br>            request=request, model=self.model<br>        )<br>        return result.embeddings<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding. For query embeddings, representation='query'.\"\"\"<br>        return self._get_embedding(query, SemanticRepresentation.Query)<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding async. For query embeddings, representation='query'.\"\"\"<br>        return self._aget_embedding(query, SemanticRepresentation.Query)<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding. For text embeddings, representation='document'.\"\"\"<br>        return self._get_embedding(text, SemanticRepresentation.Document)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding async.\"\"\"<br>        return self._aget_embedding(text, SemanticRepresentation.Document)<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        return self._get_embeddings(texts, SemanticRepresentation.Document)<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings async.\"\"\"<br>        return self._aget_embeddings(texts, SemanticRepresentation.Document)<br></code>`` |\n"
    },
    {
      "id": "7906b15d-262a-45b1-bd77-154073e99c75",
      "size": 2217,
      "headers": {
        "h1": "Anyscale",
        "h2": "AnyscaleEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br></code>`<code> | </code>`<code><br>class AnyscaleEmbedding(BaseEmbedding):<br>    \"\"\"<br>    Anyscale class for embeddings.<br>    Args:<br>        model (str): Model for embedding.<br>            Defaults to \"thenlper/gte-large\"<br>    \"\"\"<br>    additional_kwargs: Dict[str, Any] = Field(<br>        default_factory=dict, description=\"Additional kwargs for the OpenAI API.\"<br>    )<br>    api_key: str = Field(description=\"The Anyscale API key.\")<br>    api_base: str = Field(description=\"The base URL for Anyscale API.\")<br>    api_version: str = Field(description=\"The version for OpenAI API.\")<br>    max_retries: int = Field(<br>        default=10, description=\"Maximum number of retries.\", gte=0<br>    )<br>    timeout: float = Field(default=60.0, description=\"Timeout for each request.\", gte=0)<br>    default_headers: Optional[Dict[str, str]] = Field(<br>        default=None, description=\"The default headers for API requests.\"<br>    )<br>    reuse_client: bool = Field(<br>        default=True,<br>        description=(<br>            \"Reuse the Anyscale client between requests. When doing anything with large \"<br>            \"volumes of async API calls, setting this to false can improve stability.\"<br>        ),<br>    )<br>    _query_engine: Optional[str] = PrivateAttr()<br>    _text_engine: Optional[str] = PrivateAttr()<br>    _client: Optional[OpenAI] = PrivateAttr()<br>    _aclient: Optional[AsyncOpenAI] = PrivateAttr()<br>    _http_client: Optional[httpx.Client] = PrivateAttr()<br>    def __init__(<br>        self,<br>        model: str = DEFAULT_MODEL,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        additional_kwargs: Optional[Dict[str, Any]] = None,<br>        api_key: Optional[str] = None,<br>        api_base: Optional[str] = DEFAULT_API_BASE,<br>        api_version: Optional[str] = None,<br>        max_retries: int = 10,<br>        timeout: float = 60.0,<br>        reuse_client: bool = True,<br>        callback_manager: Optional[CallbackManager] = None,<br>        default_headers: Optional[Dict[str, str]] = None,<br>        http_client: Optional[httpx.Client] = None,<br>        **kwargs: Any,<br>    ) -> None:<br>        additional_kwargs = additional_kwargs or {}<br>        api_key, api_base, api_version = resolve_anyscale_credentials(<br>            api_key=api_key,<br>            api_base=api_base,<br>            api_version=api_version,<br>        )<br>        if \"model_name\" in kwargs:<br>            model_name = kwargs.pop(\"model_name\")<br>        else:<br>            model_name = model<br>        super().__init__(<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            model_name=model_name,<br>            additional_kwargs=additional_kwargs,<br>            api_key=api_key,<br>            api_base=api_base,<br>            api_version=api_version,<br>            max_retries=max_retries,<br>            reuse_client=reuse_client,<br>            timeout=timeout,<br>            default_headers=default_headers,<br>            **kwargs,<br>        )<br>        self._query_engine = model_name<br>        self._text_engine = model_name<br>        self._client = None<br>        self._aclient = None<br>        self._http_client = http_client<br>    def _get_client(self) -> OpenAI:<br>        if not self.reuse_client:<br>            return OpenAI(**self._get_credential_kwargs())<br>        if self._client is None:<br>            self._client = OpenAI(**self._get_credential_kwargs())<br>        return self._client<br>    def _get_aclient(self) -> AsyncOpenAI:<br>        if not self.reuse_client:<br>            return AsyncOpenAI(**self._get_credential_kwargs())<br>        if self._aclient is None:<br>            self._aclient = AsyncOpenAI(**self._get_credential_kwargs())<br>        return self._aclient<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"AnyscaleEmbedding\"<br>    def _get_credential_kwargs(self) -> Dict[str, Any]:<br>        return {<br>            \"api_key\": self.api_key,<br>            \"base_url\": self.api_base,<br>            \"max_retries\": self.max_retries,<br>            \"timeout\": self.timeout,<br>            \"default_headers\": self.default_headers,<br>            \"http_client\": self._http_client,<br>        }<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        client = self._get_client()<br>        return get_embedding(<br>            client,<br>            query,<br>            engine=self._query_engine,<br>            **self.additional_kwargs,<br>        )<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        aclient = self._get_aclient()<br>        return await aget_embedding(<br>            aclient,<br>            query,<br>            engine=self._query_engine,<br>            **self.additional_kwargs,<br>        )<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        client = self._get_client()<br>        return get_embedding(<br>            client,<br>            text,<br>            engine=self._text_engine,<br>            **self.additional_kwargs,<br>        )<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously get text embedding.\"\"\"<br>        aclient = self._get_aclient()<br>        return await aget_embedding(<br>            aclient,<br>            text,<br>            engine=self._text_engine,<br>            **self.additional_kwargs,<br>        )<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"<br>        Get text embeddings.<br>        By default, this is a wrapper around _get_text_embedding.<br>        Can be overridden for batch queries.<br>        \"\"\"<br>        client = self._get_client()<br>        return get_embeddings(<br>            client,<br>            texts,<br>            engine=self._text_engine,<br>            **self.additional_kwargs,<br>        )<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Asynchronously get text embeddings.\"\"\"<br>        aclient = self._get_aclient()<br>        return await aget_embeddings(<br>            aclient,<br>            texts,<br>            engine=self._text_engine,<br>            **self.additional_kwargs,<br>        )<br></code>`` |\n"
    },
    {
      "id": "8a6468a8-de1a-49ac-b804-1ae4cfa8cb53",
      "size": 1221,
      "headers": {
        "h1": "Mistralai",
        "h2": "MistralAIEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br></code>`<code> | </code>`<code><br>class MistralAIEmbedding(BaseEmbedding):<br>    \"\"\"Class for MistralAI embeddings.<br>    Args:<br>        model_name (str): Model for embedding.<br>            Defaults to \"mistral-embed\".<br>        api_key (Optional[str]): API key to access the model. Defaults to None.<br>    \"\"\"<br>    # Instance variables initialized via Pydantic's mechanism<br>    _client: Mistral = PrivateAttr()<br>    def __init__(<br>        self,<br>        model_name: str = \"mistral-embed\",<br>        api_key: Optional[str] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>        **kwargs: Any,<br>    ):<br>        super().__init__(<br>            model_name=model_name,<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            **kwargs,<br>        )<br>        api_key = get_from_param_or_env(\"api_key\", api_key, \"MISTRAL_API_KEY\", \"\")<br>        if not api_key:<br>            raise ValueError(<br>                \"You must provide an API key to use mistralai. \"<br>                \"You can either pass it in as an argument or set it </code>MISTRAL_API_KEY<code>.\"<br>            )<br>        self._client = Mistral(api_key=api_key)<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"MistralAIEmbedding\"<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return (<br>            self._client.embeddings.create(model=self.model_name, inputs=[query])<br>            .data[0]<br>            .embedding<br>        )<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        return (<br>            (<br>                await self._client.embeddings.create_async(<br>                    model=self.model_name, inputs=[query]<br>                )<br>            )<br>            .data[0]<br>            .embedding<br>        )<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return (<br>            self._client.embeddings.create(model=self.model_name, inputs=[text])<br>            .data[0]<br>            .embedding<br>        )<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously get text embedding.\"\"\"<br>        return (<br>            await self._client.embeddings.create(<br>                model=self.model_name,<br>                inputs=[text],<br>            )<br>            .data[0]<br>            .embedding<br>        )<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        embedding_response = self._client.embeddings.create(<br>            model=self.model_name, inputs=texts<br>        ).data<br>        return [embed.embedding for embed in embedding_response]<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Asynchronously get text embeddings.\"\"\"<br>        embedding_response = await self._client.embeddings.create_async(<br>            model=self.model_name, inputs=texts<br>        )<br>        return [embed.embedding for embed in embedding_response.data]<br></code>`` |\n"
    },
    {
      "id": "20c0653f-1936-4b3d-8406-c961ed114812",
      "size": 1802,
      "headers": {
        "h1": "Openinference",
        "h2": "OpenInferenceCallbackHandler \\#",
        "h3": ""
      },
      "text": "| ``<code><br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br></code>`<code> | </code>`<code><br>class OpenInferenceCallbackHandler(BaseCallbackHandler):<br>    \"\"\"Callback handler for storing generation data in OpenInference format.<br>    OpenInference is an open standard for capturing and storing AI model<br>    inferences. It enables production LLMapp servers to seamlessly integrate<br>    with LLM observability solutions such as Arize and Phoenix.<br>    For more information on the specification, see<br>    https://github.com/Arize-ai/open-inference-spec<br>    \"\"\"<br>    def __init__(<br>        self,<br>        callback: Optional[Callable[[List[QueryData], List[NodeData]], None]] = None,<br>    ) -> None:<br>        \"\"\"Initializes the OpenInferenceCallbackHandler.<br>        Args:<br>            callback (Optional[Callable[[List[QueryData], List[NodeData]], None]], optional): A<br>            callback function that will be called when a query trace is<br>            completed, often used for logging or persisting query data.<br>        \"\"\"<br>        super().__init__(event_starts_to_ignore=[], event_ends_to_ignore=[])<br>        self._callback = callback<br>        self._trace_data = TraceData()<br>        self._query_data_buffer: List[QueryData] = []<br>        self._node_data_buffer: List[NodeData] = []<br>    def start_trace(self, trace_id: Optional[str] = None) -> None:<br>        if trace_id == \"query\" or trace_id == \"chat\":<br>            self._trace_data = TraceData()<br>            self._trace_data.query_data.timestamp = datetime.now().isoformat()<br>            self._trace_data.query_data.id = _generate_random_id()<br>    def end_trace(<br>        self,<br>        trace_id: Optional[str] = None,<br>        trace_map: Optional[Dict[str, List[str]]] = None,<br>    ) -> None:<br>        if trace_id == \"query\" or trace_id == \"chat\":<br>            self._query_data_buffer.append(self._trace_data.query_data)<br>            self._node_data_buffer.extend(self._trace_data.node_datas)<br>            self._trace_data = TraceData()<br>            if self._callback is not None:<br>                self._callback(self._query_data_buffer, self._node_data_buffer)<br>    def on_event_start(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        parent_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> str:<br>        if payload is not None:<br>            if event_type is CBEventType.QUERY:<br>                query_text = payload[EventPayload.QUERY_STR]<br>                self._trace_data.query_data.query_text = query_text<br>            elif event_type is CBEventType.LLM:<br>                if prompt := payload.get(EventPayload.PROMPT, None):<br>                    self._trace_data.query_data.llm_prompt = prompt<br>                if messages := payload.get(EventPayload.MESSAGES, None):<br>                    self._trace_data.query_data.llm_messages = [<br>                        (m.role.value, m.content) for m in messages<br>                    ]<br>                    # For chat engines there is no query event and thus the<br>                    # query text will be None, in this case we set the query<br>                    # text to the last message passed to the LLM<br>                    if self._trace_data.query_data.query_text is None:<br>                        self._trace_data.query_data.query_text = messages[-1].content<br>        return event_id<br>    def on_event_end(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> None:<br>        if payload is None:<br>            return<br>        if event_type is CBEventType.RETRIEVE:<br>            for node_with_score in payload[EventPayload.NODES]:<br>                node = node_with_score.node<br>                score = node_with_score.score<br>                self._trace_data.query_data.node_ids.append(node.hash)<br>                self._trace_data.query_data.scores.append(score)<br>                self._trace_data.node_datas.append(<br>                    NodeData(<br>                        id=node.hash,<br>                        node_text=node.text,<br>                    )<br>                )<br>        elif event_type is CBEventType.LLM:<br>            if self._trace_data.query_data.response_text is None:<br>                if response := payload.get(EventPayload.RESPONSE, None):<br>                    if isinstance(response, ChatResponse):<br>                        # If the response is of class ChatResponse the string<br>                        # representation has the format \"<role>: <message>\",<br>                        # but we want just the message<br>                        response_text = response.message.content<br>                    else:<br>                        response_text = str(response)<br>                    self._trace_data.query_data.response_text = response_text<br>                elif completion := payload.get(EventPayload.COMPLETION, None):<br>                    self._trace_data.query_data.response_text = str(completion)<br>        elif event_type is CBEventType.EMBEDDING:<br>            self._trace_data.query_data.query_embedding = payload[<br>                EventPayload.EMBEDDINGS<br>            ][0]<br>    def flush_query_data_buffer(self) -> List[QueryData]:<br>        \"\"\"Clears the query data buffer and returns the data.<br>        Returns:<br>            List[QueryData]: The query data.<br>        \"\"\"<br>        query_data_buffer = self._query_data_buffer<br>        self._query_data_buffer = []<br>        return query_data_buffer<br>    def flush_node_data_buffer(self) -> List[NodeData]:<br>        \"\"\"Clears the node data buffer and returns the data.<br>        Returns:<br>            List[NodeData]: The node data.<br>        \"\"\"<br>        node_data_buffer = self._node_data_buffer<br>        self._node_data_buffer = []<br>        return node_data_buffer<br></code>`` |\n"
    },
    {
      "id": "18e30738-4064-4d95-9b41-29a978365a9f",
      "size": 1051,
      "headers": {
        "h1": "Correctness",
        "h2": "CorrectnessEvaluator \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br></code>`<code> | </code>`<code><br>class CorrectnessEvaluator(BaseEvaluator):<br>    \"\"\"Correctness evaluator.<br>    Evaluates the correctness of a question answering system.<br>    This evaluator depends on </code>reference<code> answer to be provided, in addition to the<br>    query string and response string.<br>    It outputs a score between 1 and 5, where 1 is the worst and 5 is the best,<br>    along with a reasoning for the score.<br>    Passing is defined as a score greater than or equal to the given threshold.<br>    Args:<br>        eval_template (Optional[Union[BasePromptTemplate, str]]):<br>            Template for the evaluation prompt.<br>        score_threshold (float): Numerical threshold for passing the evaluation,<br>            defaults to 4.0.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        llm: Optional[LLM] = None,<br>        eval_template: Optional[Union[BasePromptTemplate, str]] = None,<br>        score_threshold: float = 4.0,<br>        parser_function: Callable[<br>            [str], Tuple[Optional[float], Optional[str]]<br>        ] = default_parser,<br>    ) -> None:<br>        self._llm = llm or Settings.llm<br>        self._eval_template: BasePromptTemplate<br>        if isinstance(eval_template, str):<br>            self._eval_template = PromptTemplate(eval_template)<br>        else:<br>            self._eval_template = eval_template or DEFAULT_EVAL_TEMPLATE<br>        self._score_threshold = score_threshold<br>        self.parser_function = parser_function<br>    def _get_prompts(self) -> PromptDictType:<br>        \"\"\"Get prompts.\"\"\"<br>        return {<br>            \"eval_template\": self._eval_template,<br>        }<br>    def _update_prompts(self, prompts: PromptDictType) -> None:<br>        \"\"\"Update prompts.\"\"\"<br>        if \"eval_template\" in prompts:<br>            self._eval_template = prompts[\"eval_template\"]<br>    async def aevaluate(<br>        self,<br>        query: Optional[str] = None,<br>        response: Optional[str] = None,<br>        contexts: Optional[Sequence[str]] = None,<br>        reference: Optional[str] = None,<br>        sleep_time_in_seconds: int = 0,<br>        **kwargs: Any,<br>    ) -> EvaluationResult:<br>        del kwargs  # Unused<br>        del contexts  # Unused<br>        await asyncio.sleep(sleep_time_in_seconds)<br>        if query is None or response is None:<br>            raise ValueError(\"query, and response must be provided\")<br>        eval_response = await self._llm.apredict(<br>            prompt=self._eval_template,<br>            query=query,<br>            generated_answer=response,<br>            reference_answer=reference or \"(NO REFERENCE ANSWER SUPPLIED)\",<br>        )<br>        # Use the parser function<br>        score, reasoning = self.parser_function(eval_response)<br>        return EvaluationResult(<br>            query=query,<br>            response=response,<br>            passing=score >= self._score_threshold if score is not None else None,<br>            score=score,<br>            feedback=reasoning,<br>        )<br></code>`` |\n"
    },
    {
      "id": "f28d45f5-4dd9-409f-b901-a28976996a9b",
      "size": 2852,
      "headers": {
        "h1": "Openai",
        "h2": "OpenAIEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br></code>`<code> | </code>`<code><br>class OpenAIEmbedding(BaseEmbedding):<br>    \"\"\"OpenAI class for embeddings.<br>    Args:<br>        mode (str): Mode for embedding.<br>            Defaults to OpenAIEmbeddingMode.TEXT_SEARCH_MODE.<br>            Options are:<br>            - OpenAIEmbeddingMode.SIMILARITY_MODE<br>            - OpenAIEmbeddingMode.TEXT_SEARCH_MODE<br>        model (str): Model for embedding.<br>            Defaults to OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002.<br>            Options are:<br>            - OpenAIEmbeddingModelType.DAVINCI<br>            - OpenAIEmbeddingModelType.CURIE<br>            - OpenAIEmbeddingModelType.BABBAGE<br>            - OpenAIEmbeddingModelType.ADA<br>            - OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002<br>    \"\"\"<br>    additional_kwargs: Dict[str, Any] = Field(<br>        default_factory=dict, description=\"Additional kwargs for the OpenAI API.\"<br>    )<br>    api_key: str = Field(description=\"The OpenAI API key.\")<br>    api_base: Optional[str] = Field(<br>        default=DEFAULT_OPENAI_API_BASE, description=\"The base URL for OpenAI API.\"<br>    )<br>    api_version: Optional[str] = Field(<br>        default=DEFAULT_OPENAI_API_VERSION, description=\"The version for OpenAI API.\"<br>    )<br>    max_retries: int = Field(<br>        default=10, description=\"Maximum number of retries.\", gte=0<br>    )<br>    timeout: float = Field(default=60.0, description=\"Timeout for each request.\", gte=0)<br>    default_headers: Optional[Dict[str, str]] = Field(<br>        default=None, description=\"The default headers for API requests.\"<br>    )<br>    reuse_client: bool = Field(<br>        default=True,<br>        description=(<br>            \"Reuse the OpenAI client between requests. When doing anything with large \"<br>            \"volumes of async API calls, setting this to false can improve stability.\"<br>        ),<br>    )<br>    dimensions: Optional[int] = Field(<br>        default=None,<br>        description=(<br>            \"The number of dimensions on the output embedding vectors. \"<br>            \"Works only with v3 embedding models.\"<br>        ),<br>    )<br>    _query_engine: str = PrivateAttr()<br>    _text_engine: str = PrivateAttr()<br>    _client: Optional[OpenAI] = PrivateAttr()<br>    _aclient: Optional[AsyncOpenAI] = PrivateAttr()<br>    _http_client: Optional[httpx.Client] = PrivateAttr()<br>    _async_http_client: Optional[httpx.AsyncClient] = PrivateAttr()<br>    def __init__(<br>        self,<br>        mode: str = OpenAIEmbeddingMode.TEXT_SEARCH_MODE,<br>        model: str = OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002,<br>        embed_batch_size: int = 100,<br>        dimensions: Optional[int] = None,<br>        additional_kwargs: Optional[Dict[str, Any]] = None,<br>        api_key: Optional[str] = None,<br>        api_base: Optional[str] = None,<br>        api_version: Optional[str] = None,<br>        max_retries: int = 10,<br>        timeout: float = 60.0,<br>        reuse_client: bool = True,<br>        callback_manager: Optional[CallbackManager] = None,<br>        default_headers: Optional[Dict[str, str]] = None,<br>        http_client: Optional[httpx.Client] = None,<br>        async_http_client: Optional[httpx.AsyncClient] = None,<br>        num_workers: Optional[int] = None,<br>        **kwargs: Any,<br>    ) -> None:<br>        additional_kwargs = additional_kwargs or {}<br>        if dimensions is not None:<br>            additional_kwargs[\"dimensions\"] = dimensions<br>        api_key, api_base, api_version = self._resolve_credentials(<br>            api_key=api_key,<br>            api_base=api_base,<br>            api_version=api_version,<br>        )<br>        query_engine = get_engine(mode, model, _QUERY_MODE_MODEL_DICT)<br>        text_engine = get_engine(mode, model, _TEXT_MODE_MODEL_DICT)<br>        if \"model_name\" in kwargs:<br>            model_name = kwargs.pop(\"model_name\")<br>            query_engine = text_engine = model_name<br>        else:<br>            model_name = model<br>        super().__init__(<br>            embed_batch_size=embed_batch_size,<br>            dimensions=dimensions,<br>            callback_manager=callback_manager,<br>            model_name=model_name,<br>            additional_kwargs=additional_kwargs,<br>            api_key=api_key,<br>            api_base=api_base,<br>            api_version=api_version,<br>            max_retries=max_retries,<br>            reuse_client=reuse_client,<br>            timeout=timeout,<br>            default_headers=default_headers,<br>            num_workers=num_workers,<br>            **kwargs,<br>        )<br>        self._query_engine = query_engine<br>        self._text_engine = text_engine<br>        self._client = None<br>        self._aclient = None<br>        self._http_client = http_client<br>        self._async_http_client = async_http_client<br>    def _resolve_credentials(<br>        self,<br>        api_key: Optional[str] = None,<br>        api_base: Optional[str] = None,<br>        api_version: Optional[str] = None,<br>    ) -> Tuple[Optional[str], str, str]:<br>        return resolve_openai_credentials(api_key, api_base, api_version)<br>    def _get_client(self) -> OpenAI:<br>        if not self.reuse_client:<br>            return OpenAI(**self._get_credential_kwargs())<br>        if self._client is None:<br>            self._client = OpenAI(**self._get_credential_kwargs())<br>        return self._client<br>    def _get_aclient(self) -> AsyncOpenAI:<br>        if not self.reuse_client:<br>            return AsyncOpenAI(**self._get_credential_kwargs(is_async=True))<br>        if self._aclient is None:<br>            self._aclient = AsyncOpenAI(**self._get_credential_kwargs(is_async=True))<br>        return self._aclient<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"OpenAIEmbedding\"<br>    def _get_credential_kwargs(self, is_async: bool = False) -> Dict[str, Any]:<br>        return {<br>            \"api_key\": self.api_key,<br>            \"base_url\": self.api_base,<br>            \"max_retries\": self.max_retries,<br>            \"timeout\": self.timeout,<br>            \"default_headers\": self.default_headers,<br>            \"http_client\": self._async_http_client if is_async else self._http_client,<br>        }<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        client = self._get_client()<br>        return get_embedding(<br>            client,<br>            query,<br>            engine=self._query_engine,<br>            **self.additional_kwargs,<br>        )<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        aclient = self._get_aclient()<br>        return await aget_embedding(<br>            aclient,<br>            query,<br>            engine=self._query_engine,<br>            **self.additional_kwargs,<br>        )<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        client = self._get_client()<br>        return get_embedding(<br>            client,<br>            text,<br>            engine=self._text_engine,<br>            **self.additional_kwargs,<br>        )<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously get text embedding.\"\"\"<br>        aclient = self._get_aclient()<br>        return await aget_embedding(<br>            aclient,<br>            text,<br>            engine=self._text_engine,<br>            **self.additional_kwargs,<br>        )<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.<br>        By default, this is a wrapper around _get_text_embedding.<br>        Can be overridden for batch queries.<br>        \"\"\"<br>        client = self._get_client()<br>        return get_embeddings(<br>            client,<br>            texts,<br>            engine=self._text_engine,<br>            **self.additional_kwargs,<br>        )<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Asynchronously get text embeddings.\"\"\"<br>        aclient = self._get_aclient()<br>        return await aget_embeddings(<br>            aclient,<br>            texts,<br>            engine=self._text_engine,<br>            **self.additional_kwargs,<br>        )<br></code>`` |\n"
    },
    {
      "id": "cb6d3f02-0cf4-4095-918f-1fa868f5f8ac",
      "size": 1368,
      "headers": {
        "h1": "Llm rails",
        "h2": "LLMRailsEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 11<br> 12<br> 13<br> 14<br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br></code>`<code> | </code>`<code><br>class LLMRailsEmbedding(BaseEmbedding):<br>    \"\"\"LLMRails embedding models.<br>    This class provides an interface to generate embeddings using a model deployed<br>    in an LLMRails cluster. It requires a model_id of the model deployed in the cluster and api key you can obtain<br>    from https://console.llmrails.com/api-keys.<br>    \"\"\"<br>    model_id: str<br>    api_key: str<br>    session: requests.Session<br>    @classmethod<br>    def class_name(self) -> str:<br>        return \"LLMRailsEmbedding\"<br>    def __init__(<br>        self,<br>        api_key: str,<br>        model_id: str = \"embedding-english-v1\",  # or embedding-multi-v1<br>        **kwargs: Any,<br>    ):<br>        retry = Retry(<br>            total=3,<br>            connect=3,<br>            read=2,<br>            allowed_methods=[\"POST\"],<br>            backoff_factor=2,<br>            status_forcelist=[502, 503, 504],<br>        )<br>        session = requests.Session()<br>        session.mount(\"https://api.llmrails.com\", HTTPAdapter(max_retries=retry))<br>        session.headers = {\"X-API-KEY\": api_key}<br>        super().__init__(model_id=model_id, api_key=api_key, session=session, **kwargs)<br>    def _get_embedding(self, text: str) -> List[float]:<br>        \"\"\"<br>        Generate an embedding for a single query text.<br>        Args:<br>            text (str): The query text to generate an embedding for.<br>        Returns:<br>            List[float]: The embedding for the input query text.<br>        \"\"\"<br>        try:<br>            response = self.session.post(<br>                \"https://api.llmrails.com/v1/embeddings\",<br>                json={\"input\": [text], \"model\": self.model_id},<br>            )<br>            response.raise_for_status()<br>            return response.json()[\"data\"][0][\"embedding\"]<br>        except requests.exceptions.HTTPError as e:<br>            logger.error(f\"Error while embedding text {e}.\")<br>            raise ValueError(f\"Unable to embed given text {e}\")<br>    async def _aget_embedding(self, text: str) -> List[float]:<br>        \"\"\"<br>        Generate an embedding for a single query text.<br>        Args:<br>            text (str): The query text to generate an embedding for.<br>        Returns:<br>            List[float]: The embedding for the input query text.<br>        \"\"\"<br>        try:<br>            import httpx<br>        except ImportError:<br>            raise ImportError(<br>                \"The httpx library is required to use the async version of \"<br>                \"this function. Install it with </code>pip install httpx<code>.\"<br>            )<br>        try:<br>            async with httpx.AsyncClient() as client:<br>                response = await client.post(<br>                    \"https://api.llmrails.com/v1/embeddings\",<br>                    headers={\"X-API-KEY\": self.api_key},<br>                    json={\"input\": [text], \"model\": self.model_id},<br>                )<br>                response.raise_for_status()<br>            return response.json()[\"data\"][0][\"embedding\"]<br>        except httpx._exceptions.HTTPError as e:<br>            logger.error(f\"Error while embedding text {e}.\")<br>            raise ValueError(f\"Unable to embed given text {e}\")<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        return self._get_embedding(text)<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        return self._get_embedding(query)<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        return await self._aget_embedding(query)<br>    async def _aget_text_embedding(self, query: str) -> List[float]:<br>        return await self._aget_embedding(query)<br></code>`` |\n"
    },
    {
      "id": "57740b44-5d79-4734-83dd-60d985e11955",
      "size": 1167,
      "headers": {
        "h1": "Ollama",
        "h2": "OllamaEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>89<br>90<br>91<br>92<br>93<br>94<br>95<br>96<br>97<br>98<br></code>`<code> | </code>`<code><br>class OllamaEmbedding(BaseEmbedding):<br>    \"\"\"Class for Ollama embeddings.\"\"\"<br>    base_url: str = Field(description=\"Base url the model is hosted by Ollama\")<br>    model_name: str = Field(description=\"The Ollama model to use.\")<br>    embed_batch_size: int = Field(<br>        default=DEFAULT_EMBED_BATCH_SIZE,<br>        description=\"The batch size for embedding calls.\",<br>        gt=0,<br>        lte=2048,<br>    )<br>    ollama_additional_kwargs: Dict[str, Any] = Field(<br>        default_factory=dict, description=\"Additional kwargs for the Ollama API.\"<br>    )<br>    _client: Client = PrivateAttr()<br>    _async_client: AsyncClient = PrivateAttr()<br>    def __init__(<br>        self,<br>        model_name: str,<br>        base_url: str = \"http://localhost:11434\",<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        ollama_additional_kwargs: Optional[Dict[str, Any]] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        **kwargs: Any,<br>    ) -> None:<br>        super().__init__(<br>            model_name=model_name,<br>            base_url=base_url,<br>            embed_batch_size=embed_batch_size,<br>            ollama_additional_kwargs=ollama_additional_kwargs or {},<br>            callback_manager=callback_manager,<br>            **kwargs,<br>        )<br>        self._client = Client(host=self.base_url)<br>        self._async_client = AsyncClient(host=self.base_url)<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"OllamaEmbedding\"<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self.get_general_text_embedding(query)<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        return await self.aget_general_text_embedding(query)<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self.get_general_text_embedding(text)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously get text embedding.\"\"\"<br>        return await self.aget_general_text_embedding(text)<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        embeddings_list: List[List[float]] = []<br>        for text in texts:<br>            embeddings = self.get_general_text_embedding(text)<br>            embeddings_list.append(embeddings)<br>        return embeddings_list<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Asynchronously get text embeddings.\"\"\"<br>        return await asyncio.gather(<br>            *[self.aget_general_text_embedding(text) for text in texts]<br>        )<br>    def get_general_text_embedding(self, texts: str) -> List[float]:<br>        \"\"\"Get Ollama embedding.\"\"\"<br>        result = self._client.embeddings(<br>            model=self.model_name, prompt=texts, options=self.ollama_additional_kwargs<br>        )<br>        return result[\"embedding\"]<br>    async def aget_general_text_embedding(self, prompt: str) -> List[float]:<br>        \"\"\"Asynchronously get Ollama embedding.\"\"\"<br>        result = await self._async_client.embeddings(<br>            model=self.model_name, prompt=prompt, options=self.ollama_additional_kwargs<br>        )<br>        return result[\"embedding\"]<br></code>`` |\n"
    },
    {
      "id": "c5716d77-18d4-4a81-8721-9ce3d2218bf7",
      "size": 1653,
      "headers": {
        "h1": "Vertex",
        "h2": "VertexTextEmbedding \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br></code>`<code> | </code>`<code><br>class VertexTextEmbedding(BaseEmbedding):<br>    embed_mode: VertexEmbeddingMode = Field(description=\"The embedding mode to use.\")<br>    additional_kwargs: Dict[str, Any] = Field(<br>        default_factory=dict, description=\"Additional kwargs for the Vertex.\"<br>    )<br>    client_email: Optional[str] = Field(<br>        description=\"The client email for the VertexAI credentials.\"<br>    )<br>    token_uri: Optional[str] = Field(<br>        description=\"The token URI for the VertexAI credentials.\"<br>    )<br>    private_key_id: Optional[str] = Field(<br>        description=\"The private key ID for the VertexAI credentials.\"<br>    )<br>    private_key: Optional[str] = Field(<br>        description=\"The private key for the VertexAI credentials.\"<br>    )<br>    _model: TextEmbeddingModel = PrivateAttr()<br>    def __init__(<br>        self,<br>        model_name: str = \"textembedding-gecko@003\",<br>        project: Optional[str] = None,<br>        location: Optional[str] = None,<br>        credentials: Optional[auth_credentials.Credentials] = None,<br>        embed_mode: VertexEmbeddingMode = VertexEmbeddingMode.RETRIEVAL_MODE,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>        additional_kwargs: Optional[Dict[str, Any]] = None,<br>        num_workers: Optional[int] = None,<br>        client_email: Optional[str] = None,<br>        token_uri: Optional[str] = None,<br>        private_key_id: Optional[str] = None,<br>        private_key: Optional[str] = None,<br>    ) -> None:<br>        if credentials is None:<br>            if client_email and token_uri and private_key_id and private_key:<br>                info = {<br>                    \"client_email\": client_email,<br>                    \"token_uri\": token_uri,<br>                    \"private_key_id\": private_key_id,<br>                    \"private_key\": private_key.replace(\"\\\\n\", \"\\n\"),<br>                }<br>                credentials = service_account.Credentials.from_service_account_info(<br>                    info<br>                )<br>            else:<br>                raise ValueError(<br>                    \"Either provide credentials or all of client_email, token_uri, private_key_id, and private_key.\"<br>                )<br>        init_vertexai(project=project, location=location, credentials=credentials)<br>        callback_manager = callback_manager or CallbackManager([])<br>        additional_kwargs = additional_kwargs or {}<br>        super().__init__(<br>            embed_mode=embed_mode,<br>            project=project,<br>            location=location,<br>            credentials=credentials,<br>            additional_kwargs=additional_kwargs,<br>            model_name=model_name,<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            num_workers=num_workers,<br>            client_email=client_email,<br>            token_uri=token_uri,<br>            private_key_id=private_key_id,<br>            private_key=private_key,<br>        )<br>        self._model = TextEmbeddingModel.from_pretrained(model_name)<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"VertexTextEmbedding\"<br>    def _get_text_embeddings(self, texts: List[str]) -> List[Embedding]:<br>        texts = _get_embedding_request(<br>            texts=texts,<br>            embed_mode=self.embed_mode,<br>            is_query=False,<br>            model_name=self.model_name,<br>        )<br>        embeddings = self._model.get_embeddings(texts, **self.additional_kwargs)<br>        return [embedding.values for embedding in embeddings]<br>    def _get_text_embedding(self, text: str) -> Embedding:<br>        return self._get_text_embeddings([text])[0]<br>    async def _aget_text_embedding(self, text: str) -> Embedding:<br>        return (await self._aget_text_embeddings([text]))[0]<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[Embedding]:<br>        texts = _get_embedding_request(<br>            texts=texts,<br>            embed_mode=self.embed_mode,<br>            is_query=False,<br>            model_name=self.model_name,<br>        )<br>        embeddings = await self._model.get_embeddings_async(<br>            texts, **self.additional_kwargs<br>        )<br>        return [embedding.values for embedding in embeddings]<br>    def _get_query_embedding(self, query: str) -> Embedding:<br>        texts = _get_embedding_request(<br>            texts=[query],<br>            embed_mode=self.embed_mode,<br>            is_query=True,<br>            model_name=self.model_name,<br>        )<br>        embeddings = self._model.get_embeddings(texts, **self.additional_kwargs)<br>        return embeddings[0].values<br>    async def _aget_query_embedding(self, query: str) -> Embedding:<br>        texts = _get_embedding_request(<br>            texts=[query],<br>            embed_mode=self.embed_mode,<br>            is_query=True,<br>            model_name=self.model_name,<br>        )<br>        embeddings = await self._model.get_embeddings_async(<br>            texts, **self.additional_kwargs<br>        )<br>        return embeddings[0].values<br></code>`` |\n"
    },
    {
      "id": "a771afa1-5704-4768-aa44-d1699810adca",
      "size": 1717,
      "headers": {
        "h1": "Dashscope",
        "h2": "DashScopeAgent \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br></code>`<code> | </code>`<code><br>class DashScopeAgent(BaseAgent):<br>    \"\"\"<br>    DashScope agent simple wrapper for Alibaba cloud bailian high-level agent api.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        app_id: str,<br>        chat_session: bool = True,<br>        workspace: str = None,<br>        api_key: str = None,<br>        verbose: bool = False,<br>    ) -> None:<br>        \"\"\"Init params.<br>        Args:<br>            app_id (str): id of Alibaba cloud bailian application<br>            chat_session (bool): When need to keep chat session, defaults to True.<br>            workspace(str, </code>optional<code>): Workspace of Alibaba cloud bailian<br>            api_key (str, optional): The api api_key, can be None,<br>                if None, will get from ENV DASHSCOPE_API_KEY.<br>            verbose: Output verbose info or not.<br>        \"\"\"<br>        self.app_id = app_id<br>        self.chat_session = chat_session<br>        self.workspace = workspace<br>        self.api_key = api_key<br>        self._verbose = verbose<br>        self._session_id = None<br>    @trace_method(\"chat\")<br>    def chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None, **kwargs<br>    ) -> AgentChatResponse:<br>        return self._chat(message=message, stream=False, **kwargs)<br>    async def achat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> AgentChatResponse:<br>        raise NotImplementedError(\"achat not implemented\")<br>    @trace_method(\"chat\")<br>    def stream_chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None, **kwargs<br>    ) -> StreamingAgentChatResponse:<br>        return self._chat(message=message, stream=True, **kwargs)<br>    async def astream_chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> StreamingAgentChatResponse:<br>        raise NotImplementedError(\"astream_chat not implemented\")<br>    def reset(self) -> None:<br>        self._session_id = None<br>    @property<br>    def chat_history(self) -> List[ChatMessage]:<br>        raise NotImplementedError(\"chat_history not implemented\")<br>    @property<br>    def get_session_id(self) -> str:<br>        return self._session_id<br>    def _chat(<br>        self,<br>        message: str,<br>        stream: bool = False,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        **kwargs,<br>    ) -> Union[AgentChatResponse, StreamingAgentChatResponse]:<br>        \"\"\"Call app completion service.<br>        Args:<br>            message (str): Message for chatting with LLM.<br>            chat_history (List[ChatMessage], </code>optional<code>): The user provided chat history. Defaults to None.<br>            **kwargs:<br>                session_id(str, </code>optional<code>): Session if for multiple rounds call.<br>                biz_params(dict, </code>optional<code>): The extra parameters for flow or plugin.<br>        Raises:<br>            ValueError: The request failed with http code and message.<br>        Returns:<br>            Union[AgentChatResponse, StreamingAgentChatResponse]<br>        \"\"\"<br>        if stream:<br>            kwargs[\"stream\"] = True<br>        if self.chat_session:<br>            kwargs[\"session_id\"] = self._session_id<br>        response = Application.call(<br>            app_id=self.app_id,<br>            prompt=message,<br>            history=None,<br>            workspace=self.workspace,<br>            api_key=self.api_key,<br>            **kwargs,<br>        )<br>        if stream:<br>            return StreamingAgentChatResponse(<br>                chat_stream=(self.from_dashscope_response(rsp) for rsp in response)<br>            )<br>        else:<br>            if response.status_code != HTTPStatus.OK:<br>                raise ValueError(<br>                    f\"Chat failed with status: {response.status_code}, request id: {response.request_id}, \"<br>                    f\"code: {response.code}, message: {response.message}\"<br>                )<br>            if self._verbose:<br>                print(\"Got chat response: %s\" % response)<br>            self._session_id = response.output.session_id<br>            return AgentChatResponse(response=response.output.text)<br>    def from_dashscope_response(self, response: ApplicationResponse) -> ChatResponse:<br>        if response.status_code != HTTPStatus.OK:<br>            raise ValueError(<br>                f\"Chat failed with status: {response.status_code}, request id: {response.request_id}, \"<br>                f\"code: {response.code}, message: {response.message}\"<br>            )<br>        if self._verbose and response.output.finish_reason == \"stop\":<br>            print(\"Got final chat response: %s\" % response)<br>        self._session_id = response.output.session_id<br>        return ChatResponse(<br>            message=ChatMessage(<br>                role=MessageRole.ASSISTANT, content=response.output.text<br>            )<br>        )<br></code>`` |\n"
    },
    {
      "id": "4d4f0759-6b74-4ca8-9c05-02fd7616dee0",
      "size": 1579,
      "headers": {
        "h1": "Token counter",
        "h2": "TokenCountingHandler \\#",
        "h3": ""
      },
      "text": "| ``<code><br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br></code>`<code> | </code>`<code><br>class TokenCountingHandler(PythonicallyPrintingBaseHandler):<br>    \"\"\"Callback handler for counting tokens in LLM and Embedding events.<br>    Args:<br>        tokenizer:<br>            Tokenizer to use. Defaults to the global tokenizer<br>            (see llama_index.core.utils.globals_helper).<br>        event_starts_to_ignore: List of event types to ignore at the start of a trace.<br>        event_ends_to_ignore: List of event types to ignore at the end of a trace.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        tokenizer: Optional[Callable[[str], List]] = None,<br>        event_starts_to_ignore: Optional[List[CBEventType]] = None,<br>        event_ends_to_ignore: Optional[List[CBEventType]] = None,<br>        verbose: bool = False,<br>        logger: Optional[logging.Logger] = None,<br>    ) -> None:<br>        self.llm_token_counts: List[TokenCountingEvent] = []<br>        self.embedding_token_counts: List[TokenCountingEvent] = []<br>        self.tokenizer = tokenizer or get_tokenizer()<br>        self._token_counter = TokenCounter(tokenizer=self.tokenizer)<br>        self._verbose = verbose<br>        super().__init__(<br>            event_starts_to_ignore=event_starts_to_ignore or [],<br>            event_ends_to_ignore=event_ends_to_ignore or [],<br>            logger=logger,<br>        )<br>    def start_trace(self, trace_id: Optional[str] = None) -> None:<br>        return<br>    def end_trace(<br>        self,<br>        trace_id: Optional[str] = None,<br>        trace_map: Optional[Dict[str, List[str]]] = None,<br>    ) -> None:<br>        return<br>    def on_event_start(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        parent_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> str:<br>        return event_id<br>    def on_event_end(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> None:<br>        \"\"\"Count the LLM or Embedding tokens as needed.\"\"\"<br>        if (<br>            event_type == CBEventType.LLM<br>            and event_type not in self.event_ends_to_ignore<br>            and payload is not None<br>        ):<br>            self.llm_token_counts.append(<br>                get_llm_token_counts(<br>                    token_counter=self._token_counter,<br>                    payload=payload,<br>                    event_id=event_id,<br>                )<br>            )<br>            if self._verbose:<br>                self._print(<br>                    \"LLM Prompt Token Usage: \"<br>                    f\"{self.llm_token_counts[-1].prompt_token_count}\\n\"<br>                    \"LLM Completion Token Usage: \"<br>                    f\"{self.llm_token_counts[-1].completion_token_count}\",<br>                )<br>        elif (<br>            event_type == CBEventType.EMBEDDING<br>            and event_type not in self.event_ends_to_ignore<br>            and payload is not None<br>        ):<br>            total_chunk_tokens = 0<br>            for chunk in payload.get(EventPayload.CHUNKS, []):<br>                self.embedding_token_counts.append(<br>                    TokenCountingEvent(<br>                        event_id=event_id,<br>                        prompt=chunk,<br>                        prompt_token_count=self._token_counter.get_string_tokens(chunk),<br>                        completion=\"\",<br>                        completion_token_count=0,<br>                    )<br>                )<br>                total_chunk_tokens += self.embedding_token_counts[-1].total_token_count<br>            if self._verbose:<br>                self._print(f\"Embedding Token Usage: {total_chunk_tokens}\")<br>    @property<br>    def total_llm_token_count(self) -> int:<br>        \"\"\"Get the current total LLM token count.\"\"\"<br>        return sum([x.total_token_count for x in self.llm_token_counts])<br>    @property<br>    def prompt_llm_token_count(self) -> int:<br>        \"\"\"Get the current total LLM prompt token count.\"\"\"<br>        return sum([x.prompt_token_count for x in self.llm_token_counts])<br>    @property<br>    def completion_llm_token_count(self) -> int:<br>        \"\"\"Get the current total LLM completion token count.\"\"\"<br>        return sum([x.completion_token_count for x in self.llm_token_counts])<br>    @property<br>    def total_embedding_token_count(self) -> int:<br>        \"\"\"Get the current total Embedding token count.\"\"\"<br>        return sum([x.total_token_count for x in self.embedding_token_counts])<br>    def reset_counts(self) -> None:<br>        \"\"\"Reset the token counts.\"\"\"<br>        self.llm_token_counts = []<br>        self.embedding_token_counts = []<br></code>`` |\n"
    },
    {
      "id": "d3f6e259-6b61-4a81-92d4-db75928ffdb3",
      "size": 2788,
      "headers": {
        "h1": "Ibm",
        "h2": "WatsonxEmbeddings \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br></code>`<code> | </code>``<code><br>class WatsonxEmbeddings(BaseEmbedding):<br>    \"\"\"<br>    IBM watsonx.ai embeddings.<br>    Example:<br>        </code>pip install llama-index-embeddings-ibm<code><br>        </code>`<code>python<br>        from llama_index.embeddings.ibm import WatsonxEmbeddings<br>        watsonx_llm = WatsonxEmbeddings(<br>            model_id=\"ibm/slate-125m-english-rtrvr\",<br>            url=\"https://us-south.ml.cloud.ibm.com\",<br>            apikey=\"*****\",<br>            project_id=\"*****\",<br>        )<br>        </code>`<code><br>    \"\"\"<br>    model_id: str = Field(<br>        default=DEFAULT_EMBED_MODEL,<br>        description=\"\"\"Type of model to use.\"\"\",<br>        allow_mutation=False,<br>    )<br>    truncate_input_tokens: Optional[int] = Field(<br>        default=None,<br>        description=\"\"\"Represents the maximum number of input tokens accepted.\"\"\",<br>    )<br>    project_id: Optional[str] = Field(<br>        default=None,<br>        description=\"ID of the Watson Studio project.\",<br>        allow_mutation=False,<br>    )<br>    space_id: Optional[str] = Field(<br>        default=None,<br>        description=\"\"\"ID of the Watson Studio space.\"\"\",<br>        allow_mutation=False,<br>    )<br>    url: Optional[SecretStr] = Field(<br>        default=None,<br>        description=\"\"\"Url to Watson Machine Learning or CPD instance\"\"\",<br>        allow_mutation=False,<br>    )<br>    apikey: Optional[SecretStr] = Field(<br>        default=None,<br>        description=\"\"\"Apikey to Watson Machine Learning or CPD instance\"\"\",<br>        allow_mutation=False,<br>    )<br>    token: Optional[SecretStr] = Field(<br>        default=None, description=\"\"\"Token to CPD instance\"\"\", allow_mutation=False<br>    )<br>    password: Optional[SecretStr] = Field(<br>        default=None, description=\"\"\"Password to CPD instance\"\"\", allow_mutation=False<br>    )<br>    username: Optional[SecretStr] = Field(<br>        default=None, description=\"\"\"Username to CPD instance\"\"\", allow_mutation=False<br>    )<br>    instance_id: Optional[SecretStr] = Field(<br>        default=None,<br>        description=\"\"\"Instance_id of CPD instance\"\"\",<br>        allow_mutation=False,<br>    )<br>    version: Optional[SecretStr] = Field(<br>        default=None, description=\"\"\"Version of CPD instance\"\"\", allow_mutation=False<br>    )<br>    verify: Union[str, bool, None] = Field(<br>        default=None,<br>        description=\"\"\"User can pass as verify one of following:<br>        the path to a CA_BUNDLE file<br>        the path of directory with certificates of trusted CAs<br>        True - default path to truststore will be taken<br>        False - no verification will be made\"\"\",<br>        allow_mutation=False,<br>    )<br>    _embed_model: Embeddings = PrivateAttr()<br>    def __init__(<br>        self,<br>        model_id: str,<br>        truncate_input_tokens: Optional[int] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        project_id: Optional[str] = None,<br>        space_id: Optional[str] = None,<br>        url: Optional[str] = None,<br>        apikey: Optional[str] = None,<br>        token: Optional[str] = None,<br>        password: Optional[str] = None,<br>        username: Optional[str] = None,<br>        instance_id: Optional[str] = None,<br>        version: Optional[str] = None,<br>        verify: Union[str, bool, None] = None,<br>        api_client: Optional[APIClient] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        **kwargs: Any,<br>    ):<br>        callback_manager = callback_manager or CallbackManager([])<br>        if isinstance(api_client, APIClient):<br>            project_id = api_client.default_project_id or project_id<br>            space_id = api_client.default_space_id or space_id<br>            creds = {}<br>        else:<br>            creds = resolve_watsonx_credentials(<br>                url=url,<br>                apikey=apikey,<br>                token=token,<br>                username=username,<br>                password=password,<br>                instance_id=instance_id,<br>            )<br>        url = creds.get(\"url\").get_secret_value() if creds.get(\"url\") else None<br>        apikey = creds.get(\"apikey\").get_secret_value() if creds.get(\"apikey\") else None<br>        token = creds.get(\"token\").get_secret_value() if creds.get(\"token\") else None<br>        password = (<br>            creds.get(\"password\").get_secret_value() if creds.get(\"password\") else None<br>        )<br>        username = (<br>            creds.get(\"username\").get_secret_value() if creds.get(\"username\") else None<br>        )<br>        instance_id = (<br>            creds.get(\"instance_id\").get_secret_value()<br>            if creds.get(\"instance_id\")<br>            else None<br>        )<br>        super().__init__(<br>            model_id=model_id,<br>            truncate_input_tokens=truncate_input_tokens,<br>            project_id=project_id,<br>            space_id=space_id,<br>            url=url,<br>            apikey=apikey,<br>            token=token,<br>            password=password,<br>            username=username,<br>            instance_id=instance_id,<br>            version=version,<br>            verify=verify,<br>            callback_manager=callback_manager,<br>            embed_batch_size=embed_batch_size,<br>            **kwargs,<br>        )<br>        self._embed_model = Embeddings(<br>            model_id=model_id,<br>            params=self.params,<br>            credentials=(<br>                Credentials.from_dict(<br>                    {<br>                        key: value.get_secret_value() if value else None<br>                        for key, value in self._get_credential_kwargs().items()<br>                    },<br>                    _verify=self.verify,<br>                )<br>                if creds<br>                else None<br>            ),<br>            project_id=self.project_id,<br>            space_id=self.space_id,<br>            api_client=api_client,<br>        )<br>    class Config:<br>        validate_assignment = True<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"WatsonxEmbedding\"<br>    def _get_credential_kwargs(self) -> Dict[str, SecretStr | None]:<br>        return {<br>            \"url\": self.url,<br>            \"apikey\": self.apikey,<br>            \"token\": self.token,<br>            \"password\": self.password,<br>            \"username\": self.username,<br>            \"instance_id\": self.instance_id,<br>            \"version\": self.version,<br>        }<br>    @property<br>    def params(self) -> Dict[str, int] | None:<br>        return (<br>            {\"truncate_input_tokens\": self.truncate_input_tokens}<br>            if self.truncate_input_tokens<br>            else None<br>        )<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self._embed_model.embed_query(text=query, params=self.params)<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._get_query_embedding(query=text)<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        return self._embed_model.embed_documents(texts=texts, params=self.params)<br>    ### Async methods<br>    # Asynchronous evaluation is not yet supported for watsonx.ai embeddings<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        return self._get_query_embedding(query)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously get text embedding.\"\"\"<br>        return self._get_text_embedding(text)<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Asynchronously get text embeddings.\"\"\"<br>        return self._get_text_embeddings(texts)<br></code>``` |\n"
    },
    {
      "id": "29359361-8c3f-4110-8e24-2778e4b44a53",
      "size": 1267,
      "headers": {
        "h1": "Faithfullness",
        "h2": "FaithfulnessEvaluator \\#",
        "h3": ""
      },
      "text": "| ``<code><br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br></code>`<code> | </code>`<code><br>class FaithfulnessEvaluator(BaseEvaluator):<br>    \"\"\"<br>    Faithfulness evaluator.<br>    Evaluates whether a response is faithful to the contexts<br>    (i.e. whether the response is supported by the contexts or hallucinated.)<br>    This evaluator only considers the response string and the list of context strings.<br>    Args:<br>        raise_error(bool): Whether to raise an error when the response is invalid.<br>            Defaults to False.<br>        eval_template(Optional[Union[str, BasePromptTemplate]]):<br>            The template to use for evaluation.<br>        refine_template(Optional[Union[str, BasePromptTemplate]]):<br>            The template to use for refining the evaluation.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        llm: Optional[LLM] = None,<br>        raise_error: bool = False,<br>        eval_template: Optional[Union[str, BasePromptTemplate]] = None,<br>        refine_template: Optional[Union[str, BasePromptTemplate]] = None,<br>    ) -> None:<br>        \"\"\"Init params.\"\"\"<br>        self._llm = llm or Settings.llm<br>        self._raise_error = raise_error<br>        self._eval_template: BasePromptTemplate<br>        if isinstance(eval_template, str):<br>            self._eval_template = PromptTemplate(eval_template)<br>        if isinstance(eval_template, BasePromptTemplate):<br>            self._eval_template = eval_template<br>        else:<br>            model_name = self._llm.metadata.model_name<br>            self._eval_template = TEMPLATES_CATALOG.get(<br>                model_name, DEFAULT_EVAL_TEMPLATE<br>            )<br>        self._refine_template: BasePromptTemplate<br>        if isinstance(refine_template, str):<br>            self._refine_template = PromptTemplate(refine_template)<br>        else:<br>            self._refine_template = refine_template or DEFAULT_REFINE_TEMPLATE<br>    def _get_prompts(self) -> PromptDictType:<br>        \"\"\"Get prompts.\"\"\"<br>        return {<br>            \"eval_template\": self._eval_template,<br>            \"refine_template\": self._refine_template,<br>        }<br>    def _update_prompts(self, prompts: PromptDictType) -> None:<br>        \"\"\"Update prompts.\"\"\"<br>        if \"eval_template\" in prompts:<br>            self._eval_template = prompts[\"eval_template\"]<br>        if \"refine_template\" in prompts:<br>            self._refine_template = prompts[\"refine_template\"]<br>    async def aevaluate(<br>        self,<br>        query: str | None = None,<br>        response: str | None = None,<br>        contexts: Sequence[str] | None = None,<br>        sleep_time_in_seconds: int = 0,<br>        **kwargs: Any,<br>    ) -> EvaluationResult:<br>        \"\"\"Evaluate whether the response is faithful to the contexts.\"\"\"<br>        del kwargs  # Unused<br>        await asyncio.sleep(sleep_time_in_seconds)<br>        if contexts is None or response is None:<br>            raise ValueError(\"contexts and response must be provided\")<br>        docs = [Document(text=context) for context in contexts]<br>        index = SummaryIndex.from_documents(docs)<br>        query_engine = index.as_query_engine(<br>            llm=self._llm,<br>            text_qa_template=self._eval_template,<br>            refine_template=self._refine_template,<br>        )<br>        response_obj = await query_engine.aquery(response)<br>        raw_response_txt = str(response_obj)<br>        if \"yes\" in raw_response_txt.lower():<br>            passing = True<br>        else:<br>            passing = False<br>            if self._raise_error:<br>                raise ValueError(\"The response is invalid\")<br>        return EvaluationResult(<br>            query=query,<br>            response=response,<br>            contexts=contexts,<br>            passing=passing,<br>            score=1.0 if passing else 0.0,<br>            feedback=raw_response_txt,<br>        )<br></code>`` |\n"
    }
  ]
}