{
  "input_url": "https://docs.llamaindex.ai/en/stable/",
  "total_pages": 50,
  "data": [
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/adapter/#llama_index.embeddings.adapter.LinearAdapterEmbeddingModel)\n\n# Adapter\n\n## LinearAdapterEmbeddingModel`module-attribute`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/adapter/\\#llama_index.embeddings.adapter.LinearAdapterEmbeddingModel \"Permanent link\")\n\n```\nLinearAdapterEmbeddingModel = AdapterEmbeddingModel\n\n```\n\n## AdapterEmbeddingModel [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/adapter/\\#llama_index.embeddings.adapter.AdapterEmbeddingModel \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nAdapter for any embedding model.\n\nThis is a wrapper around any embedding model that adds an adapter layer on top of it.\nThis is useful for finetuning an embedding model on a downstream task.\nThe embedding model can be any model - it does not need to expose gradients.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `base_embed_model` | `BaseEmbedding` | Base embedding model. | _required_ |\n| `adapter_path` | `str` | Path to adapter. | _required_ |\n| `adapter_cls` | `Optional[Type[Any]]` | Adapter class. Defaults to None, in which case a linear adapter is used. | `None` |\n| `transform_query` | `bool` | Whether to transform query embeddings. Defaults to True. | `True` |\n| `device` | `Optional[str]` | Device to use. Defaults to None. | `None` |\n| `embed_batch_size` | `int` | Batch size for embedding. Defaults to 10. | `DEFAULT_EMBED_BATCH_SIZE` |\n| `callback_manager` | `Optional[CallbackManager]` | Callback manager. Defaults to None. | `None` |\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-adapter/llama_index/embeddings/adapter/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>``` | ```<br>class AdapterEmbeddingModel(BaseEmbedding):<br>    \"\"\"Adapter for any embedding model.<br>    This is a wrapper around any embedding model that adds an adapter layer \\<br>        on top of it.<br>    This is useful for finetuning an embedding model on a downstream task.<br>    The embedding model can be any model - it does not need to expose gradients.<br>    Args:<br>        base_embed_model (BaseEmbedding): Base embedding model.<br>        adapter_path (str): Path to adapter.<br>        adapter_cls (Optional[Type[Any]]): Adapter class. Defaults to None, in which \\<br>            case a linear adapter is used.<br>        transform_query (bool): Whether to transform query embeddings. Defaults to True.<br>        device (Optional[str]): Device to use. Defaults to None.<br>        embed_batch_size (int): Batch size for embedding. Defaults to 10.<br>        callback_manager (Optional[CallbackManager]): Callback manager. \\<br>            Defaults to None.<br>    \"\"\"<br>    _base_embed_model: BaseEmbedding = PrivateAttr()<br>    _adapter: Any = PrivateAttr()<br>    _transform_query: bool = PrivateAttr()<br>    _device: Optional[str] = PrivateAttr()<br>    _target_device: Any = PrivateAttr()<br>    def __init__(<br>        self,<br>        base_embed_model: BaseEmbedding,<br>        adapter_path: str,<br>        adapter_cls: Optional[Type[Any]] = None,<br>        transform_query: bool = True,<br>        device: Optional[str] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>    ) -> None:<br>        \"\"\"Init params.\"\"\"<br>        import torch<br>        from llama_index.embeddings.adapter.utils import BaseAdapter, LinearLayer<br>        super().__init__(<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            model_name=f\"Adapter for {base_embed_model.model_name}\",<br>        )<br>        if device is None:<br>            device = infer_torch_device()<br>            logger.info(f\"Use pytorch device: {device}\")<br>        self._target_device = torch.device(device)<br>        self._base_embed_model = base_embed_model<br>        if adapter_cls is None:<br>            adapter_cls = LinearLayer<br>        else:<br>            adapter_cls = cast(Type[BaseAdapter], adapter_cls)<br>        adapter = adapter_cls.load(adapter_path)<br>        self._adapter = cast(BaseAdapter, adapter)<br>        self._adapter.to(self._target_device)<br>        self._transform_query = transform_query<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"AdapterEmbeddingModel\"<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        import torch<br>        query_embedding = self._base_embed_model._get_query_embedding(query)<br>        if self._transform_query:<br>            query_embedding_t = torch.tensor(query_embedding).to(self._target_device)<br>            query_embedding_t = self._adapter.forward(query_embedding_t)<br>            query_embedding = query_embedding_t.tolist()<br>        return query_embedding<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        import torch<br>        query_embedding = await self._base_embed_model._aget_query_embedding(query)<br>        if self._transform_query:<br>            query_embedding_t = torch.tensor(query_embedding).to(self._target_device)<br>            query_embedding_t = self._adapter.forward(query_embedding_t)<br>            query_embedding = query_embedding_t.tolist()<br>        return query_embedding<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        return self._base_embed_model._get_text_embedding(text)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        return await self._base_embed_model._aget_text_embedding(text)<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Adapter - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/adapter/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/fastembed/#llama_index.embeddings.fastembed.FastEmbedEmbedding)\n\n# Fastembed\n\n## FastEmbedEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/fastembed/\\#llama_index.embeddings.fastembed.FastEmbedEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nQdrant FastEmbedding models.\nFastEmbed is a lightweight, fast, Python library built for embedding generation.\nSee more documentation at:\n\\\\* https://github.com/qdrant/fastembed/\n\\\\* https://qdrant.github.io/fastembed/.\n\nTo use this class, you must install the `fastembed` Python package.\n\n`pip install fastembed`\nExample:\nfrom llama\\_index.embeddings.fastembed import FastEmbedEmbedding\nfastembed = FastEmbedEmbedding()\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-fastembed/llama_index/embeddings/fastembed/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>89<br>90<br>91<br>92<br>93<br>94<br>95<br>96<br>97<br>98<br>99<br>``` | ```<br>class FastEmbedEmbedding(BaseEmbedding):<br>    \"\"\"<br>    Qdrant FastEmbedding models.<br>    FastEmbed is a lightweight, fast, Python library built for embedding generation.<br>    See more documentation at:<br>    * https://github.com/qdrant/fastembed/<br>    * https://qdrant.github.io/fastembed/.<br>    To use this class, you must install the `fastembed` Python package.<br>    `pip install fastembed`<br>    Example:<br>        from llama_index.embeddings.fastembed import FastEmbedEmbedding<br>        fastembed = FastEmbedEmbedding()<br>    \"\"\"<br>    model_name: str = Field(<br>        \"BAAI/bge-small-en-v1.5\",<br>        description=\"Name of the FastEmbedding model to use.\\n\"<br>        \"Defaults to 'BAAI/bge-small-en-v1.5'.\\n\"<br>        \"Find the list of supported models at \"<br>        \"https://qdrant.github.io/fastembed/examples/Supported_Models/\",<br>    )<br>    max_length: int = Field(<br>        512,<br>        description=\"The maximum number of tokens. Defaults to 512.\\n\"<br>        \"Unknown behavior for values > 512.\",<br>    )<br>    cache_dir: Optional[str] = Field(<br>        None,<br>        description=\"The path to the cache directory.\\n\"<br>        \"Defaults to `local_cache` in the parent directory\",<br>    )<br>    threads: Optional[int] = Field(<br>        None,<br>        description=\"The number of threads single onnxruntime session can use.\\n\"<br>        \"Defaults to None\",<br>    )<br>    doc_embed_type: Literal[\"default\", \"passage\"] = Field(<br>        \"default\",<br>        description=\"Type of embedding method to use for documents.\\n\"<br>        \"Available options are 'default' and 'passage'.\",<br>    )<br>    _model: Any = PrivateAttr()<br>    @classmethod<br>    def class_name(self) -> str:<br>        return \"FastEmbedEmbedding\"<br>    def __init__(<br>        self,<br>        model_name: Optional[str] = \"BAAI/bge-small-en-v1.5\",<br>        max_length: Optional[int] = 512,<br>        cache_dir: Optional[str] = None,<br>        threads: Optional[int] = None,<br>        doc_embed_type: Literal[\"default\", \"passage\"] = \"default\",<br>    ):<br>        super().__init__(<br>            model_name=model_name,<br>            max_length=max_length,<br>            threads=threads,<br>            doc_embed_type=doc_embed_type,<br>        )<br>        self._model = TextEmbedding(<br>            model_name=model_name,<br>            max_length=max_length,<br>            cache_dir=cache_dir,<br>            threads=threads,<br>        )<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        embeddings: List[np.ndarray]<br>        if self.doc_embed_type == \"passage\":<br>            embeddings = list(self._model.passage_embed(text))<br>        else:<br>            embeddings = list(self._model.embed(text))<br>        return embeddings[0].tolist()<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        query_embeddings: np.ndarray = next(self._model.query_embed(query))<br>        return query_embeddings.tolist()<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        return self._get_query_embedding(query)<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Fastembed - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/fastembed/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "# Agentops\n\nBack to top",
      "metadata": {
        "title": "Agentops - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/agentops/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/agent/#core-agent-classes)\n\n# Core Agent Classes [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#core-agent-classes \"Permanent link\")\n\n## Base Types [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#base-types \"Permanent link\")\n\nBase agent types.\n\n### BaseAgent [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.types.BaseAgent \"Permanent link\")\n\nBases: `BaseChatEngine`, `BaseQueryEngine`\n\nBase Agent.\n\nSource code in `llama-index-core/llama_index/core/base/agent/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>``` | ```<br>class BaseAgent(BaseChatEngine, BaseQueryEngine):<br>    \"\"\"Base Agent.\"\"\"<br>    def _get_prompts(self) -> PromptDictType:<br>        \"\"\"Get prompts.\"\"\"<br>        # TODO: the ReAct agent does not explicitly specify prompts, would need a<br>        # refactor to expose those prompts<br>        return {}<br>    def _get_prompt_modules(self) -> PromptMixinType:<br>        \"\"\"Get prompt modules.\"\"\"<br>        return {}<br>    def _update_prompts(self, prompts: PromptDictType) -> None:<br>        \"\"\"Update prompts.\"\"\"<br>    # ===== Query Engine Interface =====<br>    @trace_method(\"query\")<br>    def _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:<br>        agent_response = self.chat(<br>            query_bundle.query_str,<br>            chat_history=[],<br>        )<br>        return Response(<br>            response=str(agent_response), source_nodes=agent_response.source_nodes<br>        )<br>    @trace_method(\"query\")<br>    async def _aquery(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:<br>        agent_response = await self.achat(<br>            query_bundle.query_str,<br>            chat_history=[],<br>        )<br>        return Response(<br>            response=str(agent_response), source_nodes=agent_response.source_nodes<br>        )<br>    def stream_chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> StreamingAgentChatResponse:<br>        raise NotImplementedError(\"stream_chat not implemented\")<br>    async def astream_chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> StreamingAgentChatResponse:<br>        raise NotImplementedError(\"astream_chat not implemented\")<br>``` |\n\n### BaseAgentWorker [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.types.BaseAgentWorker \"Permanent link\")\n\nBases: `PromptMixin`, `DispatcherSpanMixin`\n\nBase agent worker.\n\nSource code in `llama-index-core/llama_index/core/base/agent/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>``` | ```<br>class BaseAgentWorker(PromptMixin, DispatcherSpanMixin):<br>    \"\"\"Base agent worker.\"\"\"<br>    def _get_prompts(self) -> PromptDictType:<br>        \"\"\"Get prompts.\"\"\"<br>        # TODO: the ReAct agent does not explicitly specify prompts, would need a<br>        # refactor to expose those prompts<br>        return {}<br>    def _get_prompt_modules(self) -> PromptMixinType:<br>        \"\"\"Get prompt modules.\"\"\"<br>        return {}<br>    def _update_prompts(self, prompts: PromptDictType) -> None:<br>        \"\"\"Update prompts.\"\"\"<br>    @abstractmethod<br>    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>        \"\"\"Initialize step from task.\"\"\"<br>    @abstractmethod<br>    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>    @abstractmethod<br>    async def arun_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        raise NotImplementedError<br>    @abstractmethod<br>    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        # TODO: figure out if we need a different type for TaskStepOutput<br>        raise NotImplementedError<br>    @abstractmethod<br>    async def astream_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async stream).\"\"\"<br>        raise NotImplementedError<br>    @abstractmethod<br>    def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>        \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>    def set_callback_manager(self, callback_manager: CallbackManager) -> None:<br>        \"\"\"Set callback manager.\"\"\"<br>        # TODO: make this abstractmethod (right now will break some agent impls)<br>    def as_agent(self, **kwargs: Any) -> \"AgentRunner\":<br>        \"\"\"Return as an agent runner.\"\"\"<br>        from llama_index.core.agent.runner.base import AgentRunner<br>        return AgentRunner(self, **kwargs)<br>``` |\n\n#### initialize\\_step`abstractmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.types.BaseAgentWorker.initialize_step \"Permanent link\")\n\n```\ninitialize_step(task: Task, **kwargs: Any) -> TaskStep\n\n```\n\nInitialize step from task.\n\nSource code in `llama-index-core/llama_index/core/base/agent/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>208<br>209<br>210<br>``` | ```<br>@abstractmethod<br>def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>    \"\"\"Initialize step from task.\"\"\"<br>``` |\n\n#### run\\_step`abstractmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.types.BaseAgentWorker.run_step \"Permanent link\")\n\n```\nrun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step.\n\nSource code in `llama-index-core/llama_index/core/base/agent/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>212<br>213<br>214<br>``` | ```<br>@abstractmethod<br>def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step.\"\"\"<br>``` |\n\n#### arun\\_step`abstractmethod``async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.types.BaseAgentWorker.arun_step \"Permanent link\")\n\n```\narun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async).\n\nSource code in `llama-index-core/llama_index/core/base/agent/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>216<br>217<br>218<br>219<br>220<br>221<br>``` | ```<br>@abstractmethod<br>async def arun_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async).\"\"\"<br>    raise NotImplementedError<br>``` |\n\n#### stream\\_step`abstractmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.types.BaseAgentWorker.stream_step \"Permanent link\")\n\n```\nstream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (stream).\n\nSource code in `llama-index-core/llama_index/core/base/agent/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>223<br>224<br>225<br>226<br>227<br>``` | ```<br>@abstractmethod<br>def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step (stream).\"\"\"<br>    # TODO: figure out if we need a different type for TaskStepOutput<br>    raise NotImplementedError<br>``` |\n\n#### astream\\_step`abstractmethod``async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.types.BaseAgentWorker.astream_step \"Permanent link\")\n\n```\nastream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async stream).\n\nSource code in `llama-index-core/llama_index/core/base/agent/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>229<br>230<br>231<br>232<br>233<br>234<br>``` | ```<br>@abstractmethod<br>async def astream_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async stream).\"\"\"<br>    raise NotImplementedError<br>``` |\n\n#### finalize\\_task`abstractmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.types.BaseAgentWorker.finalize_task \"Permanent link\")\n\n```\nfinalize_task(task: Task, **kwargs: Any) -> None\n\n```\n\nFinalize task, after all the steps are completed.\n\nSource code in `llama-index-core/llama_index/core/base/agent/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>236<br>237<br>238<br>``` | ```<br>@abstractmethod<br>def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>    \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>``` |\n\n#### set\\_callback\\_manager [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.types.BaseAgentWorker.set_callback_manager \"Permanent link\")\n\n```\nset_callback_manager(callback_manager: CallbackManager) -> None\n\n```\n\nSet callback manager.\n\nSource code in `llama-index-core/llama_index/core/base/agent/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>240<br>241<br>``` | ```<br>def set_callback_manager(self, callback_manager: CallbackManager) -> None:<br>    \"\"\"Set callback manager.\"\"\"<br>``` |\n\n#### as\\_agent [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.types.BaseAgentWorker.as_agent \"Permanent link\")\n\n```\nas_agent(**kwargs: Any) -> AgentRunner\n\n```\n\nReturn as an agent runner.\n\nSource code in `llama-index-core/llama_index/core/base/agent/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>244<br>245<br>246<br>247<br>248<br>``` | ```<br>def as_agent(self, **kwargs: Any) -> \"AgentRunner\":<br>    \"\"\"Return as an agent runner.\"\"\"<br>    from llama_index.core.agent.runner.base import AgentRunner<br>    return AgentRunner(self, **kwargs)<br>``` |\n\n### Task [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.types.Task \"Permanent link\")\n\nBases: `BaseModel`\n\nAgent Task.\n\nRepresents a \"run\" of an agent given a user input.\n\nSource code in `llama-index-core/llama_index/core/base/agent/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>``` | ```<br>class Task(BaseModel):<br>    \"\"\"Agent Task.<br>    Represents a \"run\" of an agent given a user input.<br>    \"\"\"<br>    model_config = ConfigDict(arbitrary_types_allowed=True)<br>    task_id: str = Field(<br>        default_factory=lambda: str(uuid.uuid4()), description=\"Task ID\"<br>    )<br>    input: str = Field(..., description=\"User input\")<br>    # NOTE: this is state that may be modified throughout the course of execution of the task<br>    memory: SerializeAsAny[BaseMemory] = Field(<br>        ...,<br>        description=(<br>            \"Conversational Memory. Maintains state before execution of this task.\"<br>        ),<br>    )<br>    callback_manager: CallbackManager = Field(<br>        default_factory=lambda: CallbackManager([]),<br>        exclude=True,<br>        description=\"Callback manager for the task.\",<br>    )<br>    extra_state: Dict[str, Any] = Field(<br>        default_factory=dict,<br>        description=(<br>            \"Additional user-specified state for a given task. \"<br>            \"Can be modified throughout the execution of a task.\"<br>        ),<br>    )<br>``` |\n\n### TaskStep [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.types.TaskStep \"Permanent link\")\n\nBases: `BaseModel`\n\nAgent task step.\n\nRepresents a single input step within the execution run (\"Task\") of an agent\ngiven a user input.\n\nThe output is returned as a `TaskStepOutput`.\n\nSource code in `llama-index-core/llama_index/core/base/agent/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>``` | ```<br>class TaskStep(BaseModel):<br>    \"\"\"Agent task step.<br>    Represents a single input step within the execution run (\"Task\") of an agent<br>    given a user input.<br>    The output is returned as a `TaskStepOutput`.<br>    \"\"\"<br>    task_id: str = Field(..., description=\"Task ID\")<br>    step_id: str = Field(..., description=\"Step ID\")<br>    input: Optional[str] = Field(default=None, description=\"User input\")<br>    # memory: BaseMemory = Field(<br>    #     ..., description=\"Conversational Memory\"<br>    # )<br>    step_state: Dict[str, Any] = Field(<br>        default_factory=dict, description=\"Additional state for a given step.\"<br>    )<br>    # NOTE: the state below may change throughout the course of execution<br>    # this tracks the relationships to other steps<br>    next_steps: Dict[str, \"TaskStep\"] = Field(<br>        default_factory=dict, description=\"Next steps to be executed.\"<br>    )<br>    prev_steps: Dict[str, \"TaskStep\"] = Field(<br>        default_factory=dict,<br>        description=\"Previous steps that were dependencies for this step.\",<br>    )<br>    is_ready: bool = Field(<br>        default=True, description=\"Is this step ready to be executed?\"<br>    )<br>    def get_next_step(<br>        self,<br>        step_id: str,<br>        input: Optional[str] = None,<br>        step_state: Optional[Dict[str, Any]] = None,<br>    ) -> \"TaskStep\":<br>        \"\"\"Convenience function to get next step.<br>        Preserve task_id, memory, step_state.<br>        \"\"\"<br>        return TaskStep(<br>            task_id=self.task_id,<br>            step_id=step_id,<br>            input=input,<br>            # memory=self.memory,<br>            step_state=step_state or self.step_state,<br>        )<br>    def link_step(<br>        self,<br>        next_step: \"TaskStep\",<br>    ) -> None:<br>        \"\"\"Link to next step.<br>        Add link from this step to next, and from next step to current.<br>        \"\"\"<br>        self.next_steps[next_step.step_id] = next_step<br>        next_step.prev_steps[self.step_id] = self<br>``` |\n\n#### get\\_next\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.types.TaskStep.get_next_step \"Permanent link\")\n\n```\nget_next_step(step_id: str, input: Optional[str] = None, step_state: Optional[Dict[str, Any]] = None) -> TaskStep\n\n```\n\nConvenience function to get next step.\n\nPreserve task\\_id, memory, step\\_state.\n\nSource code in `llama-index-core/llama_index/core/base/agent/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>``` | ```<br>def get_next_step(<br>    self,<br>    step_id: str,<br>    input: Optional[str] = None,<br>    step_state: Optional[Dict[str, Any]] = None,<br>) -> \"TaskStep\":<br>    \"\"\"Convenience function to get next step.<br>    Preserve task_id, memory, step_state.<br>    \"\"\"<br>    return TaskStep(<br>        task_id=self.task_id,<br>        step_id=step_id,<br>        input=input,<br>        # memory=self.memory,<br>        step_state=step_state or self.step_state,<br>    )<br>``` |\n\n#### link\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.types.TaskStep.link_step \"Permanent link\")\n\n```\nlink_step(next_step: TaskStep) -> None\n\n```\n\nLink to next step.\n\nAdd link from this step to next, and from next step to current.\n\nSource code in `llama-index-core/llama_index/core/base/agent/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>``` | ```<br>def link_step(<br>    self,<br>    next_step: \"TaskStep\",<br>) -> None:<br>    \"\"\"Link to next step.<br>    Add link from this step to next, and from next step to current.<br>    \"\"\"<br>    self.next_steps[next_step.step_id] = next_step<br>    next_step.prev_steps[self.step_id] = self<br>``` |\n\n### TaskStepOutput [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.types.TaskStepOutput \"Permanent link\")\n\nBases: `BaseModel`\n\nAgent task step output.\n\nSource code in `llama-index-core/llama_index/core/base/agent/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>``` | ```<br>class TaskStepOutput(BaseModel):<br>    \"\"\"Agent task step output.\"\"\"<br>    output: Any = Field(..., description=\"Task step output\")<br>    task_step: TaskStep = Field(..., description=\"Task step input\")<br>    next_steps: List[TaskStep] = Field(..., description=\"Next steps to be executed.\")<br>    is_last: bool = Field(default=False, description=\"Is this the last step?\")<br>    def __str__(self) -> str:<br>        \"\"\"String representation.\"\"\"<br>        return str(self.output)<br>``` |\n\n## Runners [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#runners \"Permanent link\")\n\n### AgentRunner [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.AgentRunner \"Permanent link\")\n\nBases: `BaseAgentRunner`\n\nAgent runner.\n\nTop-level agent orchestrator that can create tasks, run each step in a task,\nor run a task e2e. Stores state and keeps track of tasks.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `agent_worker` | `BaseAgentWorker` | step executor | _required_ |\n| `chat_history` | `Optional[List[ChatMessage]]` | chat history. Defaults to None. | `None` |\n| `state` | `Optional[AgentState]` | agent state. Defaults to None. | `None` |\n| `memory` | `Optional[BaseMemory]` | memory. Defaults to None. | `None` |\n| `llm` | `Optional[LLM]` | LLM. Defaults to None. | `None` |\n| `callback_manager` | `Optional[CallbackManager]` | callback manager. Defaults to None. | `None` |\n| `init_task_state_kwargs` | `Optional[dict]` | init task state kwargs. Defaults to None. | `None` |\n\nSource code in `llama-index-core/llama_index/core/agent/runner/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>472<br>473<br>474<br>475<br>476<br>477<br>478<br>479<br>480<br>481<br>482<br>483<br>484<br>485<br>486<br>487<br>488<br>489<br>490<br>491<br>492<br>493<br>494<br>495<br>496<br>497<br>498<br>499<br>500<br>501<br>502<br>503<br>504<br>505<br>506<br>507<br>508<br>509<br>510<br>511<br>512<br>513<br>514<br>515<br>516<br>517<br>518<br>519<br>520<br>521<br>522<br>523<br>524<br>525<br>526<br>527<br>528<br>529<br>530<br>531<br>532<br>533<br>534<br>535<br>536<br>537<br>538<br>539<br>540<br>541<br>542<br>543<br>544<br>545<br>546<br>547<br>548<br>549<br>550<br>551<br>552<br>553<br>554<br>555<br>556<br>557<br>558<br>559<br>560<br>561<br>562<br>563<br>564<br>565<br>566<br>567<br>568<br>569<br>570<br>571<br>572<br>573<br>574<br>575<br>576<br>577<br>578<br>579<br>580<br>581<br>582<br>583<br>584<br>585<br>586<br>587<br>588<br>589<br>590<br>591<br>592<br>593<br>594<br>595<br>596<br>597<br>598<br>599<br>600<br>601<br>602<br>603<br>604<br>605<br>606<br>607<br>608<br>609<br>610<br>611<br>612<br>613<br>614<br>615<br>616<br>617<br>618<br>619<br>620<br>621<br>622<br>623<br>624<br>625<br>626<br>627<br>628<br>629<br>630<br>631<br>632<br>633<br>634<br>635<br>636<br>637<br>638<br>639<br>640<br>641<br>642<br>643<br>644<br>645<br>646<br>647<br>648<br>649<br>650<br>651<br>652<br>653<br>654<br>655<br>656<br>657<br>658<br>659<br>660<br>661<br>662<br>663<br>664<br>665<br>666<br>667<br>668<br>669<br>670<br>671<br>672<br>673<br>674<br>675<br>676<br>677<br>678<br>679<br>680<br>681<br>682<br>683<br>684<br>685<br>686<br>687<br>688<br>689<br>690<br>691<br>692<br>693<br>694<br>695<br>696<br>697<br>698<br>699<br>700<br>701<br>702<br>703<br>704<br>705<br>706<br>707<br>708<br>709<br>710<br>711<br>712<br>713<br>714<br>715<br>716<br>717<br>718<br>719<br>720<br>721<br>722<br>723<br>724<br>725<br>726<br>727<br>728<br>729<br>730<br>731<br>732<br>733<br>734<br>735<br>736<br>``` | ```<br>class AgentRunner(BaseAgentRunner):<br>    \"\"\"Agent runner.<br>    Top-level agent orchestrator that can create tasks, run each step in a task,<br>    or run a task e2e. Stores state and keeps track of tasks.<br>    Args:<br>        agent_worker (BaseAgentWorker): step executor<br>        chat_history (Optional[List[ChatMessage]], optional): chat history. Defaults to None.<br>        state (Optional[AgentState], optional): agent state. Defaults to None.<br>        memory (Optional[BaseMemory], optional): memory. Defaults to None.<br>        llm (Optional[LLM], optional): LLM. Defaults to None.<br>        callback_manager (Optional[CallbackManager], optional): callback manager. Defaults to None.<br>        init_task_state_kwargs (Optional[dict], optional): init task state kwargs. Defaults to None.<br>    \"\"\"<br>    # # TODO: implement this in Pydantic<br>    def __init__(<br>        self,<br>        agent_worker: BaseAgentWorker,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        state: Optional[AgentState] = None,<br>        memory: Optional[BaseMemory] = None,<br>        llm: Optional[LLM] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        init_task_state_kwargs: Optional[dict] = None,<br>        delete_task_on_finish: bool = False,<br>        default_tool_choice: str = \"auto\",<br>        verbose: bool = False,<br>    ) -> None:<br>        \"\"\"Initialize.\"\"\"<br>        self.agent_worker = agent_worker<br>        self.state = state or AgentState()<br>        self.memory = memory or ChatMemoryBuffer.from_defaults(chat_history, llm=llm)<br>        # get and set callback manager<br>        if callback_manager is not None:<br>            self.agent_worker.set_callback_manager(callback_manager)<br>            self.callback_manager = callback_manager<br>        else:<br>            # TODO: This is *temporary*<br>            # Stopgap before having a callback on the BaseAgentWorker interface.<br>            # Doing that requires a bit more refactoring to make sure existing code<br>            # doesn't break.<br>            if hasattr(self.agent_worker, \"callback_manager\"):<br>                self.callback_manager = (<br>                    self.agent_worker.callback_manager or CallbackManager()<br>                )<br>            else:<br>                self.callback_manager = CallbackManager()<br>        self.init_task_state_kwargs = init_task_state_kwargs or {}<br>        self.delete_task_on_finish = delete_task_on_finish<br>        self.default_tool_choice = default_tool_choice<br>        self.verbose = verbose<br>    @staticmethod<br>    def from_llm(<br>        tools: Optional[List[BaseTool]] = None,<br>        llm: Optional[LLM] = None,<br>        **kwargs: Any,<br>    ) -> \"AgentRunner\":<br>        from llama_index.core.agent import ReActAgent<br>        if os.getenv(\"IS_TESTING\"):<br>            return ReActAgent.from_tools(<br>                tools=tools,<br>                llm=llm,<br>                **kwargs,<br>            )<br>        try:<br>            from llama_index.llms.openai import OpenAI  # pants: no-infer-dep<br>            from llama_index.llms.openai.utils import (<br>                is_function_calling_model,<br>            )  # pants: no-infer-dep<br>        except ImportError:<br>            raise ImportError(<br>                \"`llama-index-llms-openai` package not found. Please \"<br>                \"install by running `pip install llama-index-llms-openai`.\"<br>            )<br>        if isinstance(llm, OpenAI) and is_function_calling_model(llm.model):<br>            from llama_index.agent.openai import OpenAIAgent  # pants: no-infer-dep<br>            return OpenAIAgent.from_tools(<br>                tools=tools,<br>                llm=llm,<br>                **kwargs,<br>            )<br>        else:<br>            return ReActAgent.from_tools(<br>                tools=tools,<br>                llm=llm,<br>                **kwargs,<br>            )<br>    @property<br>    def chat_history(self) -> List[ChatMessage]:<br>        return self.memory.get_all()<br>    def reset(self) -> None:<br>        self.memory.reset()<br>        self.state.reset()<br>    def create_task(self, input: str, **kwargs: Any) -> Task:<br>        \"\"\"Create task.\"\"\"<br>        if not self.init_task_state_kwargs:<br>            extra_state = kwargs.pop(\"extra_state\", {})<br>        else:<br>            if \"extra_state\" in kwargs:<br>                raise ValueError(<br>                    \"Cannot specify both `extra_state` and `init_task_state_kwargs`\"<br>                )<br>            else:<br>                extra_state = self.init_task_state_kwargs<br>        callback_manager = kwargs.pop(\"callback_manager\", self.callback_manager)<br>        task = Task(<br>            input=input,<br>            memory=self.memory,<br>            extra_state=extra_state,<br>            callback_manager=callback_manager,<br>            **kwargs,<br>        )<br>        # # put input into memory<br>        # self.memory.put(ChatMessage(content=input, role=MessageRole.USER))<br>        # get initial step from task, and put it in the step queue<br>        initial_step = self.agent_worker.initialize_step(task)<br>        task_state = TaskState(<br>            task=task,<br>            step_queue=deque([initial_step]),<br>        )<br>        # add it to state<br>        self.state.task_dict[task.task_id] = task_state<br>        return task<br>    def delete_task(<br>        self,<br>        task_id: str,<br>    ) -> None:<br>        \"\"\"Delete task.<br>        NOTE: this will not delete any previous executions from memory.<br>        \"\"\"<br>        self.state.task_dict.pop(task_id)<br>    def list_tasks(self, **kwargs: Any) -> List[Task]:<br>        \"\"\"List tasks.\"\"\"<br>        return [task_state.task for task_state in self.state.task_dict.values()]<br>    def get_task(self, task_id: str, **kwargs: Any) -> Task:<br>        \"\"\"Get task.\"\"\"<br>        return self.state.get_task(task_id)<br>    def get_upcoming_steps(self, task_id: str, **kwargs: Any) -> List[TaskStep]:<br>        \"\"\"Get upcoming steps.\"\"\"<br>        return list(self.state.get_step_queue(task_id))<br>    def get_completed_steps(self, task_id: str, **kwargs: Any) -> List[TaskStepOutput]:<br>        \"\"\"Get completed steps.\"\"\"<br>        return self.state.get_completed_steps(task_id)<br>    def get_task_output(self, task_id: str, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Get task output.\"\"\"<br>        completed_steps = self.get_completed_steps(task_id)<br>        if len(completed_steps) == 0:<br>            raise ValueError(f\"No completed steps for task_id: {task_id}\")<br>        return completed_steps[-1]<br>    def get_completed_tasks(self, **kwargs: Any) -> List[Task]:<br>        \"\"\"Get completed tasks.\"\"\"<br>        task_states = list(self.state.task_dict.values())<br>        completed_tasks = []<br>        for task_state in task_states:<br>            completed_steps = self.get_completed_steps(task_state.task.task_id)<br>            if len(completed_steps) > 0 and completed_steps[-1].is_last:<br>                completed_tasks.append(task_state.task)<br>        return completed_tasks<br>    @dispatcher.span<br>    def _run_step(<br>        self,<br>        task_id: str,<br>        step: Optional[TaskStep] = None,<br>        input: Optional[str] = None,<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Execute step.\"\"\"<br>        task = self.state.get_task(task_id)<br>        step_queue = self.state.get_step_queue(task_id)<br>        step = step or step_queue.popleft()<br>        if input is not None:<br>            step.input = input<br>        dispatcher.event(<br>            AgentRunStepStartEvent(task_id=task_id, step=step, input=input)<br>        )<br>        if self.verbose:<br>            print(f\"> Running step {step.step_id}. Step input: {step.input}\")<br>        # TODO: figure out if you can dynamically swap in different step executors<br>        # not clear when you would do that by theoretically possible<br>        if mode == ChatResponseMode.WAIT:<br>            cur_step_output = self.agent_worker.run_step(step, task, **kwargs)<br>        elif mode == ChatResponseMode.STREAM:<br>            cur_step_output = self.agent_worker.stream_step(step, task, **kwargs)<br>        else:<br>            raise ValueError(f\"Invalid mode: {mode}\")<br>        # append cur_step_output next steps to queue<br>        next_steps = cur_step_output.next_steps<br>        step_queue.extend(next_steps)<br>        # add cur_step_output to completed steps<br>        completed_steps = self.state.get_completed_steps(task_id)<br>        completed_steps.append(cur_step_output)<br>        dispatcher.event(AgentRunStepEndEvent(step_output=cur_step_output))<br>        return cur_step_output<br>    @dispatcher.span<br>    async def _arun_step(<br>        self,<br>        task_id: str,<br>        step: Optional[TaskStep] = None,<br>        input: Optional[str] = None,<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Execute step.\"\"\"<br>        dispatcher.event(<br>            AgentRunStepStartEvent(task_id=task_id, step=step, input=input)<br>        )<br>        task = self.state.get_task(task_id)<br>        step_queue = self.state.get_step_queue(task_id)<br>        step = step or step_queue.popleft()<br>        if input is not None:<br>            step.input = input<br>        if self.verbose:<br>            print(f\"> Running step {step.step_id}. Step input: {step.input}\")<br>        # TODO: figure out if you can dynamically swap in different step executors<br>        # not clear when you would do that by theoretically possible<br>        if mode == ChatResponseMode.WAIT:<br>            cur_step_output = await self.agent_worker.arun_step(step, task, **kwargs)<br>        elif mode == ChatResponseMode.STREAM:<br>            cur_step_output = await self.agent_worker.astream_step(step, task, **kwargs)<br>        else:<br>            raise ValueError(f\"Invalid mode: {mode}\")<br>        # append cur_step_output next steps to queue<br>        next_steps = cur_step_output.next_steps<br>        step_queue.extend(next_steps)<br>        # add cur_step_output to completed steps<br>        completed_steps = self.state.get_completed_steps(task_id)<br>        completed_steps.append(cur_step_output)<br>        dispatcher.event(AgentRunStepEndEvent(step_output=cur_step_output))<br>        return cur_step_output<br>    @dispatcher.span<br>    def run_step(<br>        self,<br>        task_id: str,<br>        input: Optional[str] = None,<br>        step: Optional[TaskStep] = None,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        step = validate_step_from_args(task_id, input, step, **kwargs)<br>        return self._run_step(<br>            task_id, step, input=input, mode=ChatResponseMode.WAIT, **kwargs<br>        )<br>    @dispatcher.span<br>    async def arun_step(<br>        self,<br>        task_id: str,<br>        input: Optional[str] = None,<br>        step: Optional[TaskStep] = None,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        step = validate_step_from_args(task_id, input, step, **kwargs)<br>        return await self._arun_step(<br>            task_id, step, input=input, mode=ChatResponseMode.WAIT, **kwargs<br>        )<br>    @dispatcher.span<br>    def stream_step(<br>        self,<br>        task_id: str,<br>        input: Optional[str] = None,<br>        step: Optional[TaskStep] = None,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        step = validate_step_from_args(task_id, input, step, **kwargs)<br>        return self._run_step(<br>            task_id, step, input=input, mode=ChatResponseMode.STREAM, **kwargs<br>        )<br>    @dispatcher.span<br>    async def astream_step(<br>        self,<br>        task_id: str,<br>        input: Optional[str] = None,<br>        step: Optional[TaskStep] = None,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async stream).\"\"\"<br>        step = validate_step_from_args(task_id, input, step, **kwargs)<br>        return await self._arun_step(<br>            task_id, step, input=input, mode=ChatResponseMode.STREAM, **kwargs<br>        )<br>    @dispatcher.span<br>    def finalize_response(<br>        self,<br>        task_id: str,<br>        step_output: Optional[TaskStepOutput] = None,<br>    ) -> AGENT_CHAT_RESPONSE_TYPE:<br>        \"\"\"Finalize response.\"\"\"<br>        if step_output is None:<br>            step_output = self.state.get_completed_steps(task_id)[-1]<br>        if not step_output.is_last:<br>            raise ValueError(<br>                \"finalize_response can only be called on the last step output\"<br>            )<br>        if not isinstance(<br>            step_output.output,<br>            (AgentChatResponse, StreamingAgentChatResponse),<br>        ):<br>            raise ValueError(<br>                \"When `is_last` is True, cur_step_output.output must be \"<br>                f\"AGENT_CHAT_RESPONSE_TYPE: {step_output.output}\"<br>            )<br>        # finalize task<br>        self.agent_worker.finalize_task(self.state.get_task(task_id))<br>        if self.delete_task_on_finish:<br>            self.delete_task(task_id)<br>        # Attach all sources generated across all steps<br>        step_output.output.sources = self.get_task(task_id).extra_state.get(<br>            \"sources\", []<br>        )<br>        step_output.output.set_source_nodes()<br>        return cast(AGENT_CHAT_RESPONSE_TYPE, step_output.output)<br>    @dispatcher.span<br>    def _chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Union[str, dict] = \"auto\",<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>    ) -> AGENT_CHAT_RESPONSE_TYPE:<br>        \"\"\"Chat with step executor.\"\"\"<br>        if chat_history is not None:<br>            self.memory.set(chat_history)<br>        task = self.create_task(message)<br>        result_output = None<br>        dispatcher.event(AgentChatWithStepStartEvent(user_msg=message))<br>        while True:<br>            # pass step queue in as argument, assume step executor is stateless<br>            cur_step_output = self._run_step(<br>                task.task_id, mode=mode, tool_choice=tool_choice<br>            )<br>            if cur_step_output.is_last:<br>                result_output = cur_step_output<br>                break<br>            # ensure tool_choice does not cause endless loops<br>            tool_choice = \"auto\"<br>        result = self.finalize_response(<br>            task.task_id,<br>            result_output,<br>        )<br>        dispatcher.event(AgentChatWithStepEndEvent(response=result))<br>        return result<br>    @dispatcher.span<br>    async def _achat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Union[str, dict] = \"auto\",<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>    ) -> AGENT_CHAT_RESPONSE_TYPE:<br>        \"\"\"Chat with step executor.\"\"\"<br>        if chat_history is not None:<br>            self.memory.set(chat_history)<br>        task = self.create_task(message)<br>        result_output = None<br>        dispatcher.event(AgentChatWithStepStartEvent(user_msg=message))<br>        while True:<br>            # pass step queue in as argument, assume step executor is stateless<br>            cur_step_output = await self._arun_step(<br>                task.task_id, mode=mode, tool_choice=tool_choice<br>            )<br>            if cur_step_output.is_last:<br>                result_output = cur_step_output<br>                break<br>            # ensure tool_choice does not cause endless loops<br>            tool_choice = \"auto\"<br>        result = self.finalize_response(<br>            task.task_id,<br>            result_output,<br>        )<br>        dispatcher.event(AgentChatWithStepEndEvent(response=result))<br>        return result<br>    @dispatcher.span<br>    @trace_method(\"chat\")<br>    def chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Optional[Union[str, dict]] = None,<br>    ) -> AgentChatResponse:<br>        # override tool choice is provided as input.<br>        if tool_choice is None:<br>            tool_choice = self.default_tool_choice<br>        with self.callback_manager.event(<br>            CBEventType.AGENT_STEP,<br>            payload={EventPayload.MESSAGES: [message]},<br>        ) as e:<br>            chat_response = self._chat(<br>                message=message,<br>                chat_history=chat_history,<br>                tool_choice=tool_choice,<br>                mode=ChatResponseMode.WAIT,<br>            )<br>            assert isinstance(chat_response, AgentChatResponse)<br>            e.on_end(payload={EventPayload.RESPONSE: chat_response})<br>        return chat_response<br>    @dispatcher.span<br>    @trace_method(\"chat\")<br>    async def achat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Optional[Union[str, dict]] = None,<br>    ) -> AgentChatResponse:<br>        # override tool choice is provided as input.<br>        if tool_choice is None:<br>            tool_choice = self.default_tool_choice<br>        with self.callback_manager.event(<br>            CBEventType.AGENT_STEP,<br>            payload={EventPayload.MESSAGES: [message]},<br>        ) as e:<br>            chat_response = await self._achat(<br>                message=message,<br>                chat_history=chat_history,<br>                tool_choice=tool_choice,<br>                mode=ChatResponseMode.WAIT,<br>            )<br>            assert isinstance(chat_response, AgentChatResponse)<br>            e.on_end(payload={EventPayload.RESPONSE: chat_response})<br>        return chat_response<br>    @dispatcher.span<br>    @trace_method(\"chat\")<br>    def stream_chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Optional[Union[str, dict]] = None,<br>    ) -> StreamingAgentChatResponse:<br>        # override tool choice is provided as input.<br>        if tool_choice is None:<br>            tool_choice = self.default_tool_choice<br>        with self.callback_manager.event(<br>            CBEventType.AGENT_STEP,<br>            payload={EventPayload.MESSAGES: [message]},<br>        ) as e:<br>            chat_response = self._chat(<br>                message, chat_history, tool_choice, mode=ChatResponseMode.STREAM<br>            )<br>            assert isinstance(chat_response, StreamingAgentChatResponse) or (<br>                isinstance(chat_response, AgentChatResponse)<br>                and chat_response.is_dummy_stream<br>            )<br>            e.on_end(payload={EventPayload.RESPONSE: chat_response})<br>        return chat_response  # type: ignore<br>    @dispatcher.span<br>    @trace_method(\"chat\")<br>    async def astream_chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Optional[Union[str, dict]] = None,<br>    ) -> StreamingAgentChatResponse:<br>        # override tool choice is provided as input.<br>        if tool_choice is None:<br>            tool_choice = self.default_tool_choice<br>        with self.callback_manager.event(<br>            CBEventType.AGENT_STEP,<br>            payload={EventPayload.MESSAGES: [message]},<br>        ) as e:<br>            chat_response = await self._achat(<br>                message, chat_history, tool_choice, mode=ChatResponseMode.STREAM<br>            )<br>            assert isinstance(chat_response, StreamingAgentChatResponse) or (<br>                isinstance(chat_response, AgentChatResponse)<br>                and chat_response.is_dummy_stream<br>            )<br>            e.on_end(payload={EventPayload.RESPONSE: chat_response})<br>        return chat_response  # type: ignore<br>    def undo_step(self, task_id: str) -> None:<br>        \"\"\"Undo previous step.\"\"\"<br>        raise NotImplementedError(\"undo_step not implemented\")<br>``` |\n\n#### create\\_task [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.AgentRunner.create_task \"Permanent link\")\n\n```\ncreate_task(input: str, **kwargs: Any) -> Task\n\n```\n\nCreate task.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>``` | ```<br>def create_task(self, input: str, **kwargs: Any) -> Task:<br>    \"\"\"Create task.\"\"\"<br>    if not self.init_task_state_kwargs:<br>        extra_state = kwargs.pop(\"extra_state\", {})<br>    else:<br>        if \"extra_state\" in kwargs:<br>            raise ValueError(<br>                \"Cannot specify both `extra_state` and `init_task_state_kwargs`\"<br>            )<br>        else:<br>            extra_state = self.init_task_state_kwargs<br>    callback_manager = kwargs.pop(\"callback_manager\", self.callback_manager)<br>    task = Task(<br>        input=input,<br>        memory=self.memory,<br>        extra_state=extra_state,<br>        callback_manager=callback_manager,<br>        **kwargs,<br>    )<br>    # # put input into memory<br>    # self.memory.put(ChatMessage(content=input, role=MessageRole.USER))<br>    # get initial step from task, and put it in the step queue<br>    initial_step = self.agent_worker.initialize_step(task)<br>    task_state = TaskState(<br>        task=task,<br>        step_queue=deque([initial_step]),<br>    )<br>    # add it to state<br>    self.state.task_dict[task.task_id] = task_state<br>    return task<br>``` |\n\n#### delete\\_task [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.AgentRunner.delete_task \"Permanent link\")\n\n```\ndelete_task(task_id: str) -> None\n\n```\n\nDelete task.\n\nNOTE: this will not delete any previous executions from memory.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>``` | ```<br>def delete_task(<br>    self,<br>    task_id: str,<br>) -> None:<br>    \"\"\"Delete task.<br>    NOTE: this will not delete any previous executions from memory.<br>    \"\"\"<br>    self.state.task_dict.pop(task_id)<br>``` |\n\n#### list\\_tasks [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.AgentRunner.list_tasks \"Permanent link\")\n\n```\nlist_tasks(**kwargs: Any) -> List[Task]\n\n```\n\nList tasks.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>351<br>352<br>353<br>``` | ```<br>def list_tasks(self, **kwargs: Any) -> List[Task]:<br>    \"\"\"List tasks.\"\"\"<br>    return [task_state.task for task_state in self.state.task_dict.values()]<br>``` |\n\n#### get\\_task [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.AgentRunner.get_task \"Permanent link\")\n\n```\nget_task(task_id: str, **kwargs: Any) -> Task\n\n```\n\nGet task.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>355<br>356<br>357<br>``` | ```<br>def get_task(self, task_id: str, **kwargs: Any) -> Task:<br>    \"\"\"Get task.\"\"\"<br>    return self.state.get_task(task_id)<br>``` |\n\n#### get\\_upcoming\\_steps [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.AgentRunner.get_upcoming_steps \"Permanent link\")\n\n```\nget_upcoming_steps(task_id: str, **kwargs: Any) -> List[TaskStep]\n\n```\n\nGet upcoming steps.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>359<br>360<br>361<br>``` | ```<br>def get_upcoming_steps(self, task_id: str, **kwargs: Any) -> List[TaskStep]:<br>    \"\"\"Get upcoming steps.\"\"\"<br>    return list(self.state.get_step_queue(task_id))<br>``` |\n\n#### get\\_completed\\_steps [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.AgentRunner.get_completed_steps \"Permanent link\")\n\n```\nget_completed_steps(task_id: str, **kwargs: Any) -> List[TaskStepOutput]\n\n```\n\nGet completed steps.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>363<br>364<br>365<br>``` | ```<br>def get_completed_steps(self, task_id: str, **kwargs: Any) -> List[TaskStepOutput]:<br>    \"\"\"Get completed steps.\"\"\"<br>    return self.state.get_completed_steps(task_id)<br>``` |\n\n#### get\\_task\\_output [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.AgentRunner.get_task_output \"Permanent link\")\n\n```\nget_task_output(task_id: str, **kwargs: Any) -> TaskStepOutput\n\n```\n\nGet task output.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>367<br>368<br>369<br>370<br>371<br>372<br>``` | ```<br>def get_task_output(self, task_id: str, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Get task output.\"\"\"<br>    completed_steps = self.get_completed_steps(task_id)<br>    if len(completed_steps) == 0:<br>        raise ValueError(f\"No completed steps for task_id: {task_id}\")<br>    return completed_steps[-1]<br>``` |\n\n#### get\\_completed\\_tasks [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.AgentRunner.get_completed_tasks \"Permanent link\")\n\n```\nget_completed_tasks(**kwargs: Any) -> List[Task]\n\n```\n\nGet completed tasks.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>``` | ```<br>def get_completed_tasks(self, **kwargs: Any) -> List[Task]:<br>    \"\"\"Get completed tasks.\"\"\"<br>    task_states = list(self.state.task_dict.values())<br>    completed_tasks = []<br>    for task_state in task_states:<br>        completed_steps = self.get_completed_steps(task_state.task.task_id)<br>        if len(completed_steps) > 0 and completed_steps[-1].is_last:<br>            completed_tasks.append(task_state.task)<br>    return completed_tasks<br>``` |\n\n#### run\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.AgentRunner.run_step \"Permanent link\")\n\n```\nrun_step(task_id: str, input: Optional[str] = None, step: Optional[TaskStep] = None, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>469<br>470<br>471<br>472<br>473<br>474<br>475<br>476<br>477<br>478<br>479<br>480<br>481<br>``` | ```<br>@dispatcher.span<br>def run_step(<br>    self,<br>    task_id: str,<br>    input: Optional[str] = None,<br>    step: Optional[TaskStep] = None,<br>    **kwargs: Any,<br>) -> TaskStepOutput:<br>    \"\"\"Run step.\"\"\"<br>    step = validate_step_from_args(task_id, input, step, **kwargs)<br>    return self._run_step(<br>        task_id, step, input=input, mode=ChatResponseMode.WAIT, **kwargs<br>    )<br>``` |\n\n#### arun\\_step`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.AgentRunner.arun_step \"Permanent link\")\n\n```\narun_step(task_id: str, input: Optional[str] = None, step: Optional[TaskStep] = None, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async).\n\nSource code in `llama-index-core/llama_index/core/agent/runner/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>483<br>484<br>485<br>486<br>487<br>488<br>489<br>490<br>491<br>492<br>493<br>494<br>495<br>``` | ```<br>@dispatcher.span<br>async def arun_step(<br>    self,<br>    task_id: str,<br>    input: Optional[str] = None,<br>    step: Optional[TaskStep] = None,<br>    **kwargs: Any,<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async).\"\"\"<br>    step = validate_step_from_args(task_id, input, step, **kwargs)<br>    return await self._arun_step(<br>        task_id, step, input=input, mode=ChatResponseMode.WAIT, **kwargs<br>    )<br>``` |\n\n#### stream\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.AgentRunner.stream_step \"Permanent link\")\n\n```\nstream_step(task_id: str, input: Optional[str] = None, step: Optional[TaskStep] = None, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (stream).\n\nSource code in `llama-index-core/llama_index/core/agent/runner/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>497<br>498<br>499<br>500<br>501<br>502<br>503<br>504<br>505<br>506<br>507<br>508<br>509<br>``` | ```<br>@dispatcher.span<br>def stream_step(<br>    self,<br>    task_id: str,<br>    input: Optional[str] = None,<br>    step: Optional[TaskStep] = None,<br>    **kwargs: Any,<br>) -> TaskStepOutput:<br>    \"\"\"Run step (stream).\"\"\"<br>    step = validate_step_from_args(task_id, input, step, **kwargs)<br>    return self._run_step(<br>        task_id, step, input=input, mode=ChatResponseMode.STREAM, **kwargs<br>    )<br>``` |\n\n#### astream\\_step`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.AgentRunner.astream_step \"Permanent link\")\n\n```\nastream_step(task_id: str, input: Optional[str] = None, step: Optional[TaskStep] = None, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async stream).\n\nSource code in `llama-index-core/llama_index/core/agent/runner/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>511<br>512<br>513<br>514<br>515<br>516<br>517<br>518<br>519<br>520<br>521<br>522<br>523<br>``` | ```<br>@dispatcher.span<br>async def astream_step(<br>    self,<br>    task_id: str,<br>    input: Optional[str] = None,<br>    step: Optional[TaskStep] = None,<br>    **kwargs: Any,<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async stream).\"\"\"<br>    step = validate_step_from_args(task_id, input, step, **kwargs)<br>    return await self._arun_step(<br>        task_id, step, input=input, mode=ChatResponseMode.STREAM, **kwargs<br>    )<br>``` |\n\n#### finalize\\_response [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.AgentRunner.finalize_response \"Permanent link\")\n\n```\nfinalize_response(task_id: str, step_output: Optional[TaskStepOutput] = None) -> AGENT_CHAT_RESPONSE_TYPE\n\n```\n\nFinalize response.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>525<br>526<br>527<br>528<br>529<br>530<br>531<br>532<br>533<br>534<br>535<br>536<br>537<br>538<br>539<br>540<br>541<br>542<br>543<br>544<br>545<br>546<br>547<br>548<br>549<br>550<br>551<br>552<br>553<br>554<br>555<br>556<br>557<br>558<br>559<br>560<br>``` | ```<br>@dispatcher.span<br>def finalize_response(<br>    self,<br>    task_id: str,<br>    step_output: Optional[TaskStepOutput] = None,<br>) -> AGENT_CHAT_RESPONSE_TYPE:<br>    \"\"\"Finalize response.\"\"\"<br>    if step_output is None:<br>        step_output = self.state.get_completed_steps(task_id)[-1]<br>    if not step_output.is_last:<br>        raise ValueError(<br>            \"finalize_response can only be called on the last step output\"<br>        )<br>    if not isinstance(<br>        step_output.output,<br>        (AgentChatResponse, StreamingAgentChatResponse),<br>    ):<br>        raise ValueError(<br>            \"When `is_last` is True, cur_step_output.output must be \"<br>            f\"AGENT_CHAT_RESPONSE_TYPE: {step_output.output}\"<br>        )<br>    # finalize task<br>    self.agent_worker.finalize_task(self.state.get_task(task_id))<br>    if self.delete_task_on_finish:<br>        self.delete_task(task_id)<br>    # Attach all sources generated across all steps<br>    step_output.output.sources = self.get_task(task_id).extra_state.get(<br>        \"sources\", []<br>    )<br>    step_output.output.set_source_nodes()<br>    return cast(AGENT_CHAT_RESPONSE_TYPE, step_output.output)<br>``` |\n\n#### undo\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.AgentRunner.undo_step \"Permanent link\")\n\n```\nundo_step(task_id: str) -> None\n\n```\n\nUndo previous step.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>734<br>735<br>736<br>``` | ```<br>def undo_step(self, task_id: str) -> None:<br>    \"\"\"Undo previous step.\"\"\"<br>    raise NotImplementedError(\"undo_step not implemented\")<br>``` |\n\n### ParallelAgentRunner [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.ParallelAgentRunner \"Permanent link\")\n\nBases: `BaseAgentRunner`\n\nParallel agent runner.\n\nExecutes steps in queue in parallel. Requires async support.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/parallel.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>472<br>473<br>474<br>475<br>476<br>477<br>478<br>479<br>480<br>481<br>482<br>483<br>484<br>485<br>486<br>487<br>488<br>489<br>490<br>491<br>492<br>493<br>494<br>495<br>496<br>``` | ```<br>class ParallelAgentRunner(BaseAgentRunner):<br>    \"\"\"Parallel agent runner.<br>    Executes steps in queue in parallel. Requires async support.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        agent_worker: BaseAgentWorker,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        state: Optional[DAGAgentState] = None,<br>        memory: Optional[BaseMemory] = None,<br>        llm: Optional[LLM] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        init_task_state_kwargs: Optional[dict] = None,<br>        delete_task_on_finish: bool = False,<br>    ) -> None:<br>        \"\"\"Initialize.\"\"\"<br>        self.memory = memory or ChatMemoryBuffer.from_defaults(chat_history, llm=llm)<br>        self.state = state or DAGAgentState()<br>        self.callback_manager = callback_manager or CallbackManager([])<br>        self.init_task_state_kwargs = init_task_state_kwargs or {}<br>        self.agent_worker = agent_worker<br>        self.delete_task_on_finish = delete_task_on_finish<br>    @property<br>    def chat_history(self) -> List[ChatMessage]:<br>        return self.memory.get_all()<br>    def reset(self) -> None:<br>        self.memory.reset()<br>    def create_task(self, input: str, **kwargs: Any) -> Task:<br>        \"\"\"Create task.\"\"\"<br>        task = Task(<br>            input=input,<br>            memory=self.memory,<br>            extra_state=self.init_task_state_kwargs,<br>            **kwargs,<br>        )<br>        # # put input into memory<br>        # self.memory.put(ChatMessage(content=input, role=MessageRole.USER))<br>        # add it to state<br>        # get initial step from task, and put it in the step queue<br>        initial_step = self.agent_worker.initialize_step(task)<br>        task_state = DAGTaskState(<br>            task=task,<br>            root_step=initial_step,<br>            step_queue=deque([initial_step]),<br>        )<br>        self.state.task_dict[task.task_id] = task_state<br>        return task<br>    def delete_task(<br>        self,<br>        task_id: str,<br>    ) -> None:<br>        \"\"\"Delete task.<br>        NOTE: this will not delete any previous executions from memory.<br>        \"\"\"<br>        self.state.task_dict.pop(task_id)<br>    def get_completed_tasks(self, **kwargs: Any) -> List[Task]:<br>        \"\"\"Get completed tasks.\"\"\"<br>        task_states = list(self.state.task_dict.values())<br>        return [<br>            task_state.task<br>            for task_state in task_states<br>            if len(task_state.completed_steps) > 0<br>            and task_state.completed_steps[-1].is_last<br>        ]<br>    def get_task_output(self, task_id: str, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Get task output.\"\"\"<br>        task_state = self.state.task_dict[task_id]<br>        if len(task_state.completed_steps) == 0:<br>            raise ValueError(f\"No completed steps for task_id: {task_id}\")<br>        return task_state.completed_steps[-1]<br>    def list_tasks(self, **kwargs: Any) -> List[Task]:<br>        \"\"\"List tasks.\"\"\"<br>        task_states = list(self.state.task_dict.values())<br>        return [task_state.task for task_state in task_states]<br>    def get_task(self, task_id: str, **kwargs: Any) -> Task:<br>        \"\"\"Get task.\"\"\"<br>        return self.state.get_task(task_id)<br>    def get_upcoming_steps(self, task_id: str, **kwargs: Any) -> List[TaskStep]:<br>        \"\"\"Get upcoming steps.\"\"\"<br>        return list(self.state.get_step_queue(task_id))<br>    def get_completed_steps(self, task_id: str, **kwargs: Any) -> List[TaskStepOutput]:<br>        \"\"\"Get completed steps.\"\"\"<br>        return self.state.get_completed_steps(task_id)<br>    def run_steps_in_queue(<br>        self,<br>        task_id: str,<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>        **kwargs: Any,<br>    ) -> List[TaskStepOutput]:<br>        \"\"\"Execute steps in queue.<br>        Run all steps in queue, clearing it out.<br>        Assume that all steps can be run in parallel.<br>        \"\"\"<br>        return asyncio_run(self.arun_steps_in_queue(task_id, mode=mode, **kwargs))<br>    async def arun_steps_in_queue(<br>        self,<br>        task_id: str,<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>        **kwargs: Any,<br>    ) -> List[TaskStepOutput]:<br>        \"\"\"Execute all steps in queue.<br>        All steps in queue are assumed to be ready.<br>        \"\"\"<br>        # first pop all steps from step_queue<br>        steps: List[TaskStep] = []<br>        while len(self.state.get_step_queue(task_id)) > 0:<br>            steps.append(self.state.get_step_queue(task_id).popleft())<br>        # take every item in the queue, and run it<br>        tasks = []<br>        for step in steps:<br>            tasks.append(self._arun_step(task_id, step=step, mode=mode, **kwargs))<br>        return await asyncio.gather(*tasks)<br>    def _run_step(<br>        self,<br>        task_id: str,<br>        step: Optional[TaskStep] = None,<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Execute step.\"\"\"<br>        task = self.state.get_task(task_id)<br>        task_queue = self.state.get_step_queue(task_id)<br>        step = step or task_queue.popleft()<br>        if not step.is_ready:<br>            raise ValueError(f\"Step {step.step_id} is not ready\")<br>        if mode == ChatResponseMode.WAIT:<br>            cur_step_output: TaskStepOutput = self.agent_worker.run_step(<br>                step, task, **kwargs<br>            )<br>        elif mode == ChatResponseMode.STREAM:<br>            cur_step_output = self.agent_worker.stream_step(step, task, **kwargs)<br>        else:<br>            raise ValueError(f\"Invalid mode: {mode}\")<br>        for next_step in cur_step_output.next_steps:<br>            if next_step.is_ready:<br>                task_queue.append(next_step)<br>        # add cur_step_output to completed steps<br>        completed_steps = self.state.get_completed_steps(task_id)<br>        completed_steps.append(cur_step_output)<br>        return cur_step_output<br>    async def _arun_step(<br>        self,<br>        task_id: str,<br>        step: Optional[TaskStep] = None,<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Execute step.\"\"\"<br>        task = self.state.get_task(task_id)<br>        task_queue = self.state.get_step_queue(task_id)<br>        step = step or task_queue.popleft()<br>        if not step.is_ready:<br>            raise ValueError(f\"Step {step.step_id} is not ready\")<br>        if mode == ChatResponseMode.WAIT:<br>            cur_step_output = await self.agent_worker.arun_step(step, task, **kwargs)<br>        elif mode == ChatResponseMode.STREAM:<br>            cur_step_output = await self.agent_worker.astream_step(step, task, **kwargs)<br>        else:<br>            raise ValueError(f\"Invalid mode: {mode}\")<br>        for next_step in cur_step_output.next_steps:<br>            if next_step.is_ready:<br>                task_queue.append(next_step)<br>        # add cur_step_output to completed steps<br>        completed_steps = self.state.get_completed_steps(task_id)<br>        completed_steps.append(cur_step_output)<br>        return cur_step_output<br>    def run_step(<br>        self,<br>        task_id: str,<br>        input: Optional[str] = None,<br>        step: Optional[TaskStep] = None,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        return self._run_step(task_id, step, mode=ChatResponseMode.WAIT, **kwargs)<br>    async def arun_step(<br>        self,<br>        task_id: str,<br>        input: Optional[str] = None,<br>        step: Optional[TaskStep] = None,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        return await self._arun_step(<br>            task_id, step, mode=ChatResponseMode.WAIT, **kwargs<br>        )<br>    def stream_step(<br>        self,<br>        task_id: str,<br>        input: Optional[str] = None,<br>        step: Optional[TaskStep] = None,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        return self._run_step(task_id, step, mode=ChatResponseMode.STREAM, **kwargs)<br>    async def astream_step(<br>        self,<br>        task_id: str,<br>        input: Optional[str] = None,<br>        step: Optional[TaskStep] = None,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async stream).\"\"\"<br>        return await self._arun_step(<br>            task_id, step, mode=ChatResponseMode.STREAM, **kwargs<br>        )<br>    def finalize_response(<br>        self,<br>        task_id: str,<br>        step_output: Optional[TaskStepOutput] = None,<br>    ) -> AGENT_CHAT_RESPONSE_TYPE:<br>        \"\"\"Finalize response.\"\"\"<br>        if step_output is None:<br>            step_output = self.state.get_completed_steps(task_id)[-1]<br>        if not step_output.is_last:<br>            raise ValueError(<br>                \"finalize_response can only be called on the last step output\"<br>            )<br>        if not isinstance(<br>            step_output.output,<br>            (AgentChatResponse, StreamingAgentChatResponse),<br>        ):<br>            raise ValueError(<br>                \"When `is_last` is True, cur_step_output.output must be \"<br>                f\"AGENT_CHAT_RESPONSE_TYPE: {step_output.output}\"<br>            )<br>        # finalize task<br>        self.agent_worker.finalize_task(self.state.get_task(task_id))<br>        if self.delete_task_on_finish:<br>            self.delete_task(task_id)<br>        return cast(AGENT_CHAT_RESPONSE_TYPE, step_output.output)<br>    def _chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Union[str, dict] = \"auto\",<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>    ) -> AGENT_CHAT_RESPONSE_TYPE:<br>        \"\"\"Chat with step executor.\"\"\"<br>        if chat_history is not None:<br>            self.memory.set(chat_history)<br>        task = self.create_task(message)<br>        result_output = None<br>        while True:<br>            # pass step queue in as argument, assume step executor is stateless<br>            cur_step_outputs = self.run_steps_in_queue(task.task_id, mode=mode)<br>            # check if a step output is_last<br>            is_last = any(<br>                cur_step_output.is_last for cur_step_output in cur_step_outputs<br>            )<br>            if is_last:<br>                if len(cur_step_outputs) > 1:<br>                    raise ValueError(<br>                        \"More than one step output returned in final step.\"<br>                    )<br>                cur_step_output = cur_step_outputs[0]<br>                result_output = cur_step_output<br>                break<br>        return self.finalize_response(<br>            task.task_id,<br>            result_output,<br>        )<br>    async def _achat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Union[str, dict] = \"auto\",<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>    ) -> AGENT_CHAT_RESPONSE_TYPE:<br>        \"\"\"Chat with step executor.\"\"\"<br>        if chat_history is not None:<br>            self.memory.set(chat_history)<br>        task = self.create_task(message)<br>        result_output = None<br>        while True:<br>            # pass step queue in as argument, assume step executor is stateless<br>            cur_step_outputs = await self.arun_steps_in_queue(task.task_id, mode=mode)<br>            # check if a step output is_last<br>            is_last = any(<br>                cur_step_output.is_last for cur_step_output in cur_step_outputs<br>            )<br>            if is_last:<br>                if len(cur_step_outputs) > 1:<br>                    raise ValueError(<br>                        \"More than one step output returned in final step.\"<br>                    )<br>                cur_step_output = cur_step_outputs[0]<br>                result_output = cur_step_output<br>                break<br>        return self.finalize_response(<br>            task.task_id,<br>            result_output,<br>        )<br>    @trace_method(\"chat\")<br>    def chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Union[str, dict] = \"auto\",<br>    ) -> AgentChatResponse:<br>        with self.callback_manager.event(<br>            CBEventType.AGENT_STEP,<br>            payload={EventPayload.MESSAGES: [message]},<br>        ) as e:<br>            chat_response = self._chat(<br>                message, chat_history, tool_choice, mode=ChatResponseMode.WAIT<br>            )<br>            assert isinstance(chat_response, AgentChatResponse)<br>            e.on_end(payload={EventPayload.RESPONSE: chat_response})<br>        return chat_response<br>    @trace_method(\"chat\")<br>    async def achat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Union[str, dict] = \"auto\",<br>    ) -> AgentChatResponse:<br>        with self.callback_manager.event(<br>            CBEventType.AGENT_STEP,<br>            payload={EventPayload.MESSAGES: [message]},<br>        ) as e:<br>            chat_response = await self._achat(<br>                message, chat_history, tool_choice, mode=ChatResponseMode.WAIT<br>            )<br>            assert isinstance(chat_response, AgentChatResponse)<br>            e.on_end(payload={EventPayload.RESPONSE: chat_response})<br>        return chat_response<br>    @trace_method(\"chat\")<br>    def stream_chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Union[str, dict] = \"auto\",<br>    ) -> StreamingAgentChatResponse:<br>        with self.callback_manager.event(<br>            CBEventType.AGENT_STEP,<br>            payload={EventPayload.MESSAGES: [message]},<br>        ) as e:<br>            chat_response = self._chat(<br>                message, chat_history, tool_choice, mode=ChatResponseMode.STREAM<br>            )<br>            e.on_end(payload={EventPayload.RESPONSE: chat_response})<br>        return chat_response  # type: ignore<br>    @trace_method(\"chat\")<br>    async def astream_chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Union[str, dict] = \"auto\",<br>    ) -> StreamingAgentChatResponse:<br>        with self.callback_manager.event(<br>            CBEventType.AGENT_STEP,<br>            payload={EventPayload.MESSAGES: [message]},<br>        ) as e:<br>            chat_response = await self._achat(<br>                message, chat_history, tool_choice, mode=ChatResponseMode.STREAM<br>            )<br>            e.on_end(payload={EventPayload.RESPONSE: chat_response})<br>        return chat_response  # type: ignore<br>    def undo_step(self, task_id: str) -> None:<br>        \"\"\"Undo previous step.\"\"\"<br>        raise NotImplementedError(\"undo_step not implemented\")<br>``` |\n\n#### create\\_task [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.ParallelAgentRunner.create_task \"Permanent link\")\n\n```\ncreate_task(input: str, **kwargs: Any) -> Task\n\n```\n\nCreate task.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/parallel.py`\n\n|     |     |\n| --- | --- |\n| ```<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>``` | ```<br>def create_task(self, input: str, **kwargs: Any) -> Task:<br>    \"\"\"Create task.\"\"\"<br>    task = Task(<br>        input=input,<br>        memory=self.memory,<br>        extra_state=self.init_task_state_kwargs,<br>        **kwargs,<br>    )<br>    # # put input into memory<br>    # self.memory.put(ChatMessage(content=input, role=MessageRole.USER))<br>    # add it to state<br>    # get initial step from task, and put it in the step queue<br>    initial_step = self.agent_worker.initialize_step(task)<br>    task_state = DAGTaskState(<br>        task=task,<br>        root_step=initial_step,<br>        step_queue=deque([initial_step]),<br>    )<br>    self.state.task_dict[task.task_id] = task_state<br>    return task<br>``` |\n\n#### delete\\_task [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.ParallelAgentRunner.delete_task \"Permanent link\")\n\n```\ndelete_task(task_id: str) -> None\n\n```\n\nDelete task.\n\nNOTE: this will not delete any previous executions from memory.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/parallel.py`\n\n|     |     |\n| --- | --- |\n| ```<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>``` | ```<br>def delete_task(<br>    self,<br>    task_id: str,<br>) -> None:<br>    \"\"\"Delete task.<br>    NOTE: this will not delete any previous executions from memory.<br>    \"\"\"<br>    self.state.task_dict.pop(task_id)<br>``` |\n\n#### get\\_completed\\_tasks [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.ParallelAgentRunner.get_completed_tasks \"Permanent link\")\n\n```\nget_completed_tasks(**kwargs: Any) -> List[Task]\n\n```\n\nGet completed tasks.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/parallel.py`\n\n|     |     |\n| --- | --- |\n| ```<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>``` | ```<br>def get_completed_tasks(self, **kwargs: Any) -> List[Task]:<br>    \"\"\"Get completed tasks.\"\"\"<br>    task_states = list(self.state.task_dict.values())<br>    return [<br>        task_state.task<br>        for task_state in task_states<br>        if len(task_state.completed_steps) > 0<br>        and task_state.completed_steps[-1].is_last<br>    ]<br>``` |\n\n#### get\\_task\\_output [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.ParallelAgentRunner.get_task_output \"Permanent link\")\n\n```\nget_task_output(task_id: str, **kwargs: Any) -> TaskStepOutput\n\n```\n\nGet task output.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/parallel.py`\n\n|     |     |\n| --- | --- |\n| ```<br>150<br>151<br>152<br>153<br>154<br>155<br>``` | ```<br>def get_task_output(self, task_id: str, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Get task output.\"\"\"<br>    task_state = self.state.task_dict[task_id]<br>    if len(task_state.completed_steps) == 0:<br>        raise ValueError(f\"No completed steps for task_id: {task_id}\")<br>    return task_state.completed_steps[-1]<br>``` |\n\n#### list\\_tasks [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.ParallelAgentRunner.list_tasks \"Permanent link\")\n\n```\nlist_tasks(**kwargs: Any) -> List[Task]\n\n```\n\nList tasks.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/parallel.py`\n\n|     |     |\n| --- | --- |\n| ```<br>157<br>158<br>159<br>160<br>``` | ```<br>def list_tasks(self, **kwargs: Any) -> List[Task]:<br>    \"\"\"List tasks.\"\"\"<br>    task_states = list(self.state.task_dict.values())<br>    return [task_state.task for task_state in task_states]<br>``` |\n\n#### get\\_task [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.ParallelAgentRunner.get_task \"Permanent link\")\n\n```\nget_task(task_id: str, **kwargs: Any) -> Task\n\n```\n\nGet task.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/parallel.py`\n\n|     |     |\n| --- | --- |\n| ```<br>162<br>163<br>164<br>``` | ```<br>def get_task(self, task_id: str, **kwargs: Any) -> Task:<br>    \"\"\"Get task.\"\"\"<br>    return self.state.get_task(task_id)<br>``` |\n\n#### get\\_upcoming\\_steps [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.ParallelAgentRunner.get_upcoming_steps \"Permanent link\")\n\n```\nget_upcoming_steps(task_id: str, **kwargs: Any) -> List[TaskStep]\n\n```\n\nGet upcoming steps.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/parallel.py`\n\n|     |     |\n| --- | --- |\n| ```<br>166<br>167<br>168<br>``` | ```<br>def get_upcoming_steps(self, task_id: str, **kwargs: Any) -> List[TaskStep]:<br>    \"\"\"Get upcoming steps.\"\"\"<br>    return list(self.state.get_step_queue(task_id))<br>``` |\n\n#### get\\_completed\\_steps [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.ParallelAgentRunner.get_completed_steps \"Permanent link\")\n\n```\nget_completed_steps(task_id: str, **kwargs: Any) -> List[TaskStepOutput]\n\n```\n\nGet completed steps.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/parallel.py`\n\n|     |     |\n| --- | --- |\n| ```<br>170<br>171<br>172<br>``` | ```<br>def get_completed_steps(self, task_id: str, **kwargs: Any) -> List[TaskStepOutput]:<br>    \"\"\"Get completed steps.\"\"\"<br>    return self.state.get_completed_steps(task_id)<br>``` |\n\n#### run\\_steps\\_in\\_queue [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.ParallelAgentRunner.run_steps_in_queue \"Permanent link\")\n\n```\nrun_steps_in_queue(task_id: str, mode: ChatResponseMode = ChatResponseMode.WAIT, **kwargs: Any) -> List[TaskStepOutput]\n\n```\n\nExecute steps in queue.\n\nRun all steps in queue, clearing it out.\n\nAssume that all steps can be run in parallel.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/parallel.py`\n\n|     |     |\n| --- | --- |\n| ```<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>``` | ```<br>def run_steps_in_queue(<br>    self,<br>    task_id: str,<br>    mode: ChatResponseMode = ChatResponseMode.WAIT,<br>    **kwargs: Any,<br>) -> List[TaskStepOutput]:<br>    \"\"\"Execute steps in queue.<br>    Run all steps in queue, clearing it out.<br>    Assume that all steps can be run in parallel.<br>    \"\"\"<br>    return asyncio_run(self.arun_steps_in_queue(task_id, mode=mode, **kwargs))<br>``` |\n\n#### arun\\_steps\\_in\\_queue`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.ParallelAgentRunner.arun_steps_in_queue \"Permanent link\")\n\n```\narun_steps_in_queue(task_id: str, mode: ChatResponseMode = ChatResponseMode.WAIT, **kwargs: Any) -> List[TaskStepOutput]\n\n```\n\nExecute all steps in queue.\n\nAll steps in queue are assumed to be ready.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/parallel.py`\n\n|     |     |\n| --- | --- |\n| ```<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>``` | ```<br>async def arun_steps_in_queue(<br>    self,<br>    task_id: str,<br>    mode: ChatResponseMode = ChatResponseMode.WAIT,<br>    **kwargs: Any,<br>) -> List[TaskStepOutput]:<br>    \"\"\"Execute all steps in queue.<br>    All steps in queue are assumed to be ready.<br>    \"\"\"<br>    # first pop all steps from step_queue<br>    steps: List[TaskStep] = []<br>    while len(self.state.get_step_queue(task_id)) > 0:<br>        steps.append(self.state.get_step_queue(task_id).popleft())<br>    # take every item in the queue, and run it<br>    tasks = []<br>    for step in steps:<br>        tasks.append(self._arun_step(task_id, step=step, mode=mode, **kwargs))<br>    return await asyncio.gather(*tasks)<br>``` |\n\n#### run\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.ParallelAgentRunner.run_step \"Permanent link\")\n\n```\nrun_step(task_id: str, input: Optional[str] = None, step: Optional[TaskStep] = None, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/parallel.py`\n\n|     |     |\n| --- | --- |\n| ```<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>``` | ```<br>def run_step(<br>    self,<br>    task_id: str,<br>    input: Optional[str] = None,<br>    step: Optional[TaskStep] = None,<br>    **kwargs: Any,<br>) -> TaskStepOutput:<br>    \"\"\"Run step.\"\"\"<br>    return self._run_step(task_id, step, mode=ChatResponseMode.WAIT, **kwargs)<br>``` |\n\n#### arun\\_step`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.ParallelAgentRunner.arun_step \"Permanent link\")\n\n```\narun_step(task_id: str, input: Optional[str] = None, step: Optional[TaskStep] = None, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async).\n\nSource code in `llama-index-core/llama_index/core/agent/runner/parallel.py`\n\n|     |     |\n| --- | --- |\n| ```<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>``` | ```<br>async def arun_step(<br>    self,<br>    task_id: str,<br>    input: Optional[str] = None,<br>    step: Optional[TaskStep] = None,<br>    **kwargs: Any,<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async).\"\"\"<br>    return await self._arun_step(<br>        task_id, step, mode=ChatResponseMode.WAIT, **kwargs<br>    )<br>``` |\n\n#### stream\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.ParallelAgentRunner.stream_step \"Permanent link\")\n\n```\nstream_step(task_id: str, input: Optional[str] = None, step: Optional[TaskStep] = None, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (stream).\n\nSource code in `llama-index-core/llama_index/core/agent/runner/parallel.py`\n\n|     |     |\n| --- | --- |\n| ```<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>``` | ```<br>def stream_step(<br>    self,<br>    task_id: str,<br>    input: Optional[str] = None,<br>    step: Optional[TaskStep] = None,<br>    **kwargs: Any,<br>) -> TaskStepOutput:<br>    \"\"\"Run step (stream).\"\"\"<br>    return self._run_step(task_id, step, mode=ChatResponseMode.STREAM, **kwargs)<br>``` |\n\n#### astream\\_step`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.ParallelAgentRunner.astream_step \"Permanent link\")\n\n```\nastream_step(task_id: str, input: Optional[str] = None, step: Optional[TaskStep] = None, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async stream).\n\nSource code in `llama-index-core/llama_index/core/agent/runner/parallel.py`\n\n|     |     |\n| --- | --- |\n| ```<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>``` | ```<br>async def astream_step(<br>    self,<br>    task_id: str,<br>    input: Optional[str] = None,<br>    step: Optional[TaskStep] = None,<br>    **kwargs: Any,<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async stream).\"\"\"<br>    return await self._arun_step(<br>        task_id, step, mode=ChatResponseMode.STREAM, **kwargs<br>    )<br>``` |\n\n#### finalize\\_response [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.ParallelAgentRunner.finalize_response \"Permanent link\")\n\n```\nfinalize_response(task_id: str, step_output: Optional[TaskStepOutput] = None) -> AGENT_CHAT_RESPONSE_TYPE\n\n```\n\nFinalize response.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/parallel.py`\n\n|     |     |\n| --- | --- |\n| ```<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>``` | ```<br>def finalize_response(<br>    self,<br>    task_id: str,<br>    step_output: Optional[TaskStepOutput] = None,<br>) -> AGENT_CHAT_RESPONSE_TYPE:<br>    \"\"\"Finalize response.\"\"\"<br>    if step_output is None:<br>        step_output = self.state.get_completed_steps(task_id)[-1]<br>    if not step_output.is_last:<br>        raise ValueError(<br>            \"finalize_response can only be called on the last step output\"<br>        )<br>    if not isinstance(<br>        step_output.output,<br>        (AgentChatResponse, StreamingAgentChatResponse),<br>    ):<br>        raise ValueError(<br>            \"When `is_last` is True, cur_step_output.output must be \"<br>            f\"AGENT_CHAT_RESPONSE_TYPE: {step_output.output}\"<br>        )<br>    # finalize task<br>    self.agent_worker.finalize_task(self.state.get_task(task_id))<br>    if self.delete_task_on_finish:<br>        self.delete_task(task_id)<br>    return cast(AGENT_CHAT_RESPONSE_TYPE, step_output.output)<br>``` |\n\n#### undo\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.ParallelAgentRunner.undo_step \"Permanent link\")\n\n```\nundo_step(task_id: str) -> None\n\n```\n\nUndo previous step.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/parallel.py`\n\n|     |     |\n| --- | --- |\n| ```<br>494<br>495<br>496<br>``` | ```<br>def undo_step(self, task_id: str) -> None:<br>    \"\"\"Undo previous step.\"\"\"<br>    raise NotImplementedError(\"undo_step not implemented\")<br>``` |\n\n## Workers [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#workers \"Permanent link\")\n\n### CustomSimpleAgentWorker [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.CustomSimpleAgentWorker \"Permanent link\")\n\nBases: `BaseModel`, `BaseAgentWorker`\n\nCustom simple agent worker.\n\nThis is \"simple\" in the sense that some of the scaffolding is setup already.\nAssumptions:\n\\- assumes that the agent has tools, llm, callback manager, and tool retriever\n\\- has a `from_tools` convenience function\n\\- assumes that the agent is sequential, and doesn't take in any additional\nintermediate inputs.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `tools` | `Sequence[BaseTool]` | Tools to use for reasoning | _required_ |\n| `llm` | `LLM` | LLM to use | _required_ |\n| `callback_manager` | `CallbackManager` | Callback manager | `None` |\n| `tool_retriever` | `Optional[ObjectRetriever[BaseTool]]` | Tool retriever | `None` |\n| `verbose` | `bool` | Whether to print out reasoning steps | `False` |\n\nSource code in `llama-index-core/llama_index/core/agent/custom/simple.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>``` | ```<br>class CustomSimpleAgentWorker(BaseModel, BaseAgentWorker):<br>    \"\"\"Custom simple agent worker.<br>    This is \"simple\" in the sense that some of the scaffolding is setup already.<br>    Assumptions:<br>    - assumes that the agent has tools, llm, callback manager, and tool retriever<br>    - has a `from_tools` convenience function<br>    - assumes that the agent is sequential, and doesn't take in any additional<br>    intermediate inputs.<br>    Args:<br>        tools (Sequence[BaseTool]): Tools to use for reasoning<br>        llm (LLM): LLM to use<br>        callback_manager (CallbackManager): Callback manager<br>        tool_retriever (Optional[ObjectRetriever[BaseTool]]): Tool retriever<br>        verbose (bool): Whether to print out reasoning steps<br>    \"\"\"<br>    model_config = ConfigDict(arbitrary_types_allowed=True)<br>    tools: Sequence[BaseTool] = Field(..., description=\"Tools to use for reasoning\")<br>    llm: LLM = Field(..., description=\"LLM to use\")<br>    callback_manager: CallbackManager = Field(<br>        default_factory=lambda: CallbackManager([]), exclude=True<br>    )<br>    tool_retriever: Optional[ObjectRetriever[BaseTool]] = Field(<br>        default=None, description=\"Tool retriever\"<br>    )<br>    verbose: bool = Field(False, description=\"Whether to print out reasoning steps\")<br>    _get_tools: Callable[[str], Sequence[BaseTool]] = PrivateAttr()<br>    def __init__(<br>        self,<br>        tools: Sequence[BaseTool],<br>        llm: LLM,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        **kwargs: Any,<br>    ) -> None:<br>        callback_manager = callback_manager or CallbackManager([])<br>        super().__init__(<br>            tools=tools,<br>            llm=llm,<br>            callback_manager=callback_manager or CallbackManager([]),<br>            tool_retriever=tool_retriever,<br>            verbose=verbose,<br>            **kwargs,<br>        )<br>        if len(tools) > 0 and tool_retriever is not None:<br>            raise ValueError(\"Cannot specify both tools and tool_retriever\")<br>        elif len(tools) > 0:<br>            self._get_tools = lambda _: tools<br>        elif tool_retriever is not None:<br>            tool_retriever_c = cast(ObjectRetriever[BaseTool], tool_retriever)<br>            self._get_tools = lambda message: tool_retriever_c.retrieve(message)<br>        else:<br>            self._get_tools = lambda _: []<br>    @classmethod<br>    def from_tools(<br>        cls,<br>        tools: Optional[Sequence[BaseTool]] = None,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        llm: Optional[LLM] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        **kwargs: Any,<br>    ) -> \"CustomSimpleAgentWorker\":<br>        \"\"\"Convenience constructor method from set of of BaseTools (Optional).\"\"\"<br>        llm = llm or Settings.llm<br>        if callback_manager is not None:<br>            llm.callback_manager = callback_manager<br>        return cls(<br>            tools=tools or [],<br>            tool_retriever=tool_retriever,<br>            llm=llm,<br>            callback_manager=callback_manager or CallbackManager([]),<br>            verbose=verbose,<br>            **kwargs,<br>        )<br>    @abstractmethod<br>    def _initialize_state(self, task: Task, **kwargs: Any) -> Dict[str, Any]:<br>        \"\"\"Initialize state.\"\"\"<br>    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>        \"\"\"Initialize step from task.\"\"\"<br>        sources: List[ToolOutput] = []<br>        # temporary memory for new messages<br>        new_memory = ChatMemoryBuffer.from_defaults()<br>        # initialize initial state<br>        initial_state = {<br>            \"sources\": sources,<br>            \"memory\": new_memory,<br>        }<br>        step_state = self._initialize_state(task, **kwargs)<br>        # if intersecting keys, error<br>        if set(step_state.keys()).intersection(set(initial_state.keys())):<br>            raise ValueError(<br>                f\"Step state keys {step_state.keys()} and initial state keys {initial_state.keys()} intersect.\"<br>                f\"*NOTE*: initial state keys {initial_state.keys()} are reserved.\"<br>            )<br>        step_state.update(initial_state)<br>        return TaskStep(<br>            task_id=task.task_id,<br>            step_id=str(uuid.uuid4()),<br>            input=task.input,<br>            step_state=step_state,<br>        )<br>    def get_tools(self, input: str) -> List[AsyncBaseTool]:<br>        \"\"\"Get tools.\"\"\"<br>        return [adapt_to_async_tool(t) for t in self._get_tools(input)]<br>    def _get_task_step_response(<br>        self, agent_response: AGENT_CHAT_RESPONSE_TYPE, step: TaskStep, is_done: bool<br>    ) -> TaskStepOutput:<br>        \"\"\"Get task step response.\"\"\"<br>        if is_done:<br>            new_steps = []<br>        else:<br>            new_steps = [<br>                step.get_next_step(<br>                    step_id=str(uuid.uuid4()),<br>                    # NOTE: input is unused<br>                    input=None,<br>                )<br>            ]<br>        return TaskStepOutput(<br>            output=agent_response,<br>            task_step=step,<br>            is_last=is_done,<br>            next_steps=new_steps,<br>        )<br>    @abstractmethod<br>    def _run_step(<br>        self, state: Dict[str, Any], task: Task, input: Optional[str] = None<br>    ) -> Tuple[AgentChatResponse, bool]:<br>        \"\"\"Run step.<br>        Returns:<br>            Tuple of (agent_response, is_done)<br>        \"\"\"<br>    async def _arun_step(<br>        self, state: Dict[str, Any], task: Task, input: Optional[str] = None<br>    ) -> Tuple[AgentChatResponse, bool]:<br>        \"\"\"Run step (async).<br>        Can override this method if you want to run the step asynchronously.<br>        Returns:<br>            Tuple of (agent_response, is_done)<br>        \"\"\"<br>        raise NotImplementedError(<br>            \"This agent does not support async.\" \"Please implement _arun_step.\"<br>        )<br>    @trace_method(\"run_step\")<br>    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        agent_response, is_done = self._run_step(<br>            step.step_state, task, input=step.input<br>        )<br>        response = self._get_task_step_response(agent_response, step, is_done)<br>        # sync step state with task state<br>        task.extra_state.update(step.step_state)<br>        return response<br>    @trace_method(\"run_step\")<br>    async def arun_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        agent_response, is_done = await self._arun_step(<br>            step.step_state, task, input=step.input<br>        )<br>        response = self._get_task_step_response(agent_response, step, is_done)<br>        task.extra_state.update(step.step_state)<br>        return response<br>    @trace_method(\"run_step\")<br>    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        raise NotImplementedError(\"This agent does not support streaming.\")<br>    @trace_method(\"run_step\")<br>    async def astream_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async stream).\"\"\"<br>        raise NotImplementedError(\"This agent does not support streaming.\")<br>    @abstractmethod<br>    def _finalize_task(self, state: Dict[str, Any], **kwargs: Any) -> None:<br>        \"\"\"Finalize task, after all the steps are completed.<br>        State is all the step states.<br>        \"\"\"<br>    def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>        \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>        # add new messages to memory<br>        task.memory.set(task.memory.get() + task.extra_state[\"memory\"].get_all())<br>        # reset new memory<br>        task.extra_state[\"memory\"].reset()<br>        self._finalize_task(task.extra_state, **kwargs)<br>    def set_callback_manager(self, callback_manager: CallbackManager) -> None:<br>        \"\"\"Set callback manager.\"\"\"<br>        # TODO: make this abstractmethod (right now will break some agent impls)<br>        self.callback_manager = callback_manager<br>``` |\n\n#### from\\_tools`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.CustomSimpleAgentWorker.from_tools \"Permanent link\")\n\n```\nfrom_tools(tools: Optional[Sequence[BaseTool]] = None, tool_retriever: Optional[ObjectRetriever[BaseTool]] = None, llm: Optional[LLM] = None, callback_manager: Optional[CallbackManager] = None, verbose: bool = False, **kwargs: Any) -> CustomSimpleAgentWorker\n\n```\n\nConvenience constructor method from set of of BaseTools (Optional).\n\nSource code in `llama-index-core/llama_index/core/agent/custom/simple.py`\n\n|     |     |\n| --- | --- |\n| ```<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>``` | ```<br>@classmethod<br>def from_tools(<br>    cls,<br>    tools: Optional[Sequence[BaseTool]] = None,<br>    tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>    llm: Optional[LLM] = None,<br>    callback_manager: Optional[CallbackManager] = None,<br>    verbose: bool = False,<br>    **kwargs: Any,<br>) -> \"CustomSimpleAgentWorker\":<br>    \"\"\"Convenience constructor method from set of of BaseTools (Optional).\"\"\"<br>    llm = llm or Settings.llm<br>    if callback_manager is not None:<br>        llm.callback_manager = callback_manager<br>    return cls(<br>        tools=tools or [],<br>        tool_retriever=tool_retriever,<br>        llm=llm,<br>        callback_manager=callback_manager or CallbackManager([]),<br>        verbose=verbose,<br>        **kwargs,<br>    )<br>``` |\n\n#### initialize\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.CustomSimpleAgentWorker.initialize_step \"Permanent link\")\n\n```\ninitialize_step(task: Task, **kwargs: Any) -> TaskStep\n\n```\n\nInitialize step from task.\n\nSource code in `llama-index-core/llama_index/core/agent/custom/simple.py`\n\n|     |     |\n| --- | --- |\n| ```<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>``` | ```<br>def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>    \"\"\"Initialize step from task.\"\"\"<br>    sources: List[ToolOutput] = []<br>    # temporary memory for new messages<br>    new_memory = ChatMemoryBuffer.from_defaults()<br>    # initialize initial state<br>    initial_state = {<br>        \"sources\": sources,<br>        \"memory\": new_memory,<br>    }<br>    step_state = self._initialize_state(task, **kwargs)<br>    # if intersecting keys, error<br>    if set(step_state.keys()).intersection(set(initial_state.keys())):<br>        raise ValueError(<br>            f\"Step state keys {step_state.keys()} and initial state keys {initial_state.keys()} intersect.\"<br>            f\"*NOTE*: initial state keys {initial_state.keys()} are reserved.\"<br>        )<br>    step_state.update(initial_state)<br>    return TaskStep(<br>        task_id=task.task_id,<br>        step_id=str(uuid.uuid4()),<br>        input=task.input,<br>        step_state=step_state,<br>    )<br>``` |\n\n#### get\\_tools [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.CustomSimpleAgentWorker.get_tools \"Permanent link\")\n\n```\nget_tools(input: str) -> List[AsyncBaseTool]\n\n```\n\nGet tools.\n\nSource code in `llama-index-core/llama_index/core/agent/custom/simple.py`\n\n|     |     |\n| --- | --- |\n| ```<br>155<br>156<br>157<br>``` | ```<br>def get_tools(self, input: str) -> List[AsyncBaseTool]:<br>    \"\"\"Get tools.\"\"\"<br>    return [adapt_to_async_tool(t) for t in self._get_tools(input)]<br>``` |\n\n#### run\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.CustomSimpleAgentWorker.run_step \"Permanent link\")\n\n```\nrun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step.\n\nSource code in `llama-index-core/llama_index/core/agent/custom/simple.py`\n\n|     |     |\n| --- | --- |\n| ```<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>``` | ```<br>@trace_method(\"run_step\")<br>def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step.\"\"\"<br>    agent_response, is_done = self._run_step(<br>        step.step_state, task, input=step.input<br>    )<br>    response = self._get_task_step_response(agent_response, step, is_done)<br>    # sync step state with task state<br>    task.extra_state.update(step.step_state)<br>    return response<br>``` |\n\n#### arun\\_step`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.CustomSimpleAgentWorker.arun_step \"Permanent link\")\n\n```\narun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async).\n\nSource code in `llama-index-core/llama_index/core/agent/custom/simple.py`\n\n|     |     |\n| --- | --- |\n| ```<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>``` | ```<br>@trace_method(\"run_step\")<br>async def arun_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async).\"\"\"<br>    agent_response, is_done = await self._arun_step(<br>        step.step_state, task, input=step.input<br>    )<br>    response = self._get_task_step_response(agent_response, step, is_done)<br>    task.extra_state.update(step.step_state)<br>    return response<br>``` |\n\n#### stream\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.CustomSimpleAgentWorker.stream_step \"Permanent link\")\n\n```\nstream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (stream).\n\nSource code in `llama-index-core/llama_index/core/agent/custom/simple.py`\n\n|     |     |\n| --- | --- |\n| ```<br>230<br>231<br>232<br>233<br>``` | ```<br>@trace_method(\"run_step\")<br>def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step (stream).\"\"\"<br>    raise NotImplementedError(\"This agent does not support streaming.\")<br>``` |\n\n#### astream\\_step`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.CustomSimpleAgentWorker.astream_step \"Permanent link\")\n\n```\nastream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async stream).\n\nSource code in `llama-index-core/llama_index/core/agent/custom/simple.py`\n\n|     |     |\n| --- | --- |\n| ```<br>235<br>236<br>237<br>238<br>239<br>240<br>``` | ```<br>@trace_method(\"run_step\")<br>async def astream_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async stream).\"\"\"<br>    raise NotImplementedError(\"This agent does not support streaming.\")<br>``` |\n\n#### finalize\\_task [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.CustomSimpleAgentWorker.finalize_task \"Permanent link\")\n\n```\nfinalize_task(task: Task, **kwargs: Any) -> None\n\n```\n\nFinalize task, after all the steps are completed.\n\nSource code in `llama-index-core/llama_index/core/agent/custom/simple.py`\n\n|     |     |\n| --- | --- |\n| ```<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>``` | ```<br>def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>    \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>    # add new messages to memory<br>    task.memory.set(task.memory.get() + task.extra_state[\"memory\"].get_all())<br>    # reset new memory<br>    task.extra_state[\"memory\"].reset()<br>    self._finalize_task(task.extra_state, **kwargs)<br>``` |\n\n#### set\\_callback\\_manager [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.CustomSimpleAgentWorker.set_callback_manager \"Permanent link\")\n\n```\nset_callback_manager(callback_manager: CallbackManager) -> None\n\n```\n\nSet callback manager.\n\nSource code in `llama-index-core/llama_index/core/agent/custom/simple.py`\n\n|     |     |\n| --- | --- |\n| ```<br>258<br>259<br>260<br>261<br>``` | ```<br>def set_callback_manager(self, callback_manager: CallbackManager) -> None:<br>    \"\"\"Set callback manager.\"\"\"<br>    # TODO: make this abstractmethod (right now will break some agent impls)<br>    self.callback_manager = callback_manager<br>``` |\n\n### MultimodalReActAgentWorker [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.MultimodalReActAgentWorker \"Permanent link\")\n\nBases: `BaseAgentWorker`\n\nMultimodal ReAct Agent worker.\n\n**NOTE**: This is a BETA feature.\n\nSource code in `llama-index-core/llama_index/core/agent/react_multimodal/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>472<br>473<br>474<br>475<br>476<br>477<br>478<br>479<br>480<br>481<br>482<br>483<br>484<br>485<br>486<br>487<br>488<br>489<br>490<br>491<br>492<br>493<br>494<br>495<br>496<br>497<br>498<br>499<br>500<br>501<br>502<br>503<br>504<br>505<br>506<br>507<br>508<br>509<br>510<br>511<br>512<br>513<br>514<br>515<br>516<br>517<br>518<br>519<br>520<br>521<br>522<br>523<br>524<br>``` | ```<br>class MultimodalReActAgentWorker(BaseAgentWorker):<br>    \"\"\"Multimodal ReAct Agent worker.<br>    **NOTE**: This is a BETA feature.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        tools: Sequence[BaseTool],<br>        multi_modal_llm: MultiModalLLM,<br>        max_iterations: int = 10,<br>        react_chat_formatter: Optional[ReActChatFormatter] = None,<br>        output_parser: Optional[ReActOutputParser] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>    ) -> None:<br>        self._multi_modal_llm = multi_modal_llm<br>        self.callback_manager = callback_manager or CallbackManager([])<br>        self._max_iterations = max_iterations<br>        self._react_chat_formatter = react_chat_formatter or ReActChatFormatter(<br>            system_header=REACT_MM_CHAT_SYSTEM_HEADER<br>        )<br>        self._output_parser = output_parser or ReActOutputParser()<br>        self._verbose = verbose<br>        try:<br>            from llama_index.multi_modal_llms.openai.utils import (<br>                generate_openai_multi_modal_chat_message,<br>            )  # pants: no-infer-dep<br>            self._add_user_step_to_reasoning = partial(<br>                add_user_step_to_reasoning,<br>                generate_chat_message_fn=generate_openai_multi_modal_chat_message,  # type: ignore<br>            )<br>        except ImportError:<br>            raise ImportError(<br>                \"`llama-index-multi-modal-llms-openai` package cannot be found. \"<br>                \"Please install it by using `pip install `llama-index-multi-modal-llms-openai`\"<br>            )<br>        if len(tools) > 0 and tool_retriever is not None:<br>            raise ValueError(\"Cannot specify both tools and tool_retriever\")<br>        elif len(tools) > 0:<br>            self._get_tools = lambda _: tools<br>        elif tool_retriever is not None:<br>            tool_retriever_c = cast(ObjectRetriever[BaseTool], tool_retriever)<br>            self._get_tools = lambda message: tool_retriever_c.retrieve(message)<br>        else:<br>            self._get_tools = lambda _: []<br>    @classmethod<br>    def from_tools(<br>        cls,<br>        tools: Optional[Sequence[BaseTool]] = None,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        multi_modal_llm: Optional[MultiModalLLM] = None,<br>        max_iterations: int = 10,<br>        react_chat_formatter: Optional[ReActChatFormatter] = None,<br>        output_parser: Optional[ReActOutputParser] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        **kwargs: Any,<br>    ) -> \"MultimodalReActAgentWorker\":<br>        \"\"\"Convenience constructor method from set of of BaseTools (Optional).<br>        NOTE: kwargs should have been exhausted by this point. In other words<br>        the various upstream components such as BaseSynthesizer (response synthesizer)<br>        or BaseRetriever should have picked up off their respective kwargs in their<br>        constructions.<br>        Returns:<br>            ReActAgent<br>        \"\"\"<br>        if multi_modal_llm is None:<br>            try:<br>                from llama_index.multi_modal_llms.openai import (<br>                    OpenAIMultiModal,<br>                )  # pants: no-infer-dep<br>                multi_modal_llm = multi_modal_llm or OpenAIMultiModal(<br>                    model=\"gpt-4-vision-preview\", max_new_tokens=1000<br>                )<br>            except ImportError:<br>                raise ImportError(<br>                    \"`llama-index-multi-modal-llms-openai` package cannot be found. \"<br>                    \"Please install it by using `pip install `llama-index-multi-modal-llms-openai`\"<br>                )<br>        return cls(<br>            tools=tools or [],<br>            tool_retriever=tool_retriever,<br>            multi_modal_llm=multi_modal_llm,<br>            max_iterations=max_iterations,<br>            react_chat_formatter=react_chat_formatter,<br>            output_parser=output_parser,<br>            callback_manager=callback_manager,<br>            verbose=verbose,<br>        )<br>    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>        \"\"\"Initialize step from task.\"\"\"<br>        sources: List[ToolOutput] = []<br>        current_reasoning: List[BaseReasoningStep] = []<br>        # temporary memory for new messages<br>        new_memory = ChatMemoryBuffer.from_defaults()<br>        # validation<br>        if \"image_docs\" not in task.extra_state:<br>            raise ValueError(\"Image docs not found in task extra state.\")<br>        # initialize task state<br>        task_state = {<br>            \"sources\": sources,<br>            \"current_reasoning\": current_reasoning,<br>            \"new_memory\": new_memory,<br>        }<br>        task.extra_state.update(task_state)<br>        return TaskStep(<br>            task_id=task.task_id,<br>            step_id=str(uuid.uuid4()),<br>            input=task.input,<br>            step_state={\"is_first\": True, \"image_docs\": task.extra_state[\"image_docs\"]},<br>        )<br>    def get_tools(self, input: str) -> List[AsyncBaseTool]:<br>        \"\"\"Get tools.\"\"\"<br>        return [adapt_to_async_tool(t) for t in self._get_tools(input)]<br>    def _extract_reasoning_step(<br>        self, output: ChatResponse, is_streaming: bool = False<br>    ) -> Tuple[str, List[BaseReasoningStep], bool]:<br>        \"\"\"<br>        Extracts the reasoning step from the given output.<br>        This method parses the message content from the output,<br>        extracts the reasoning step, and determines whether the processing is<br>        complete. It also performs validation checks on the output and<br>        handles possible errors.<br>        \"\"\"<br>        if output.message.content is None:<br>            raise ValueError(\"Got empty message.\")<br>        message_content = output.message.content<br>        current_reasoning = []<br>        try:<br>            reasoning_step = self._output_parser.parse(message_content, is_streaming)<br>        except BaseException as exc:<br>            raise ValueError(f\"Could not parse output: {message_content}\") from exc<br>        if self._verbose:<br>            print_text(f\"{reasoning_step.get_content()}\\n\", color=\"pink\")<br>        current_reasoning.append(reasoning_step)<br>        if reasoning_step.is_done:<br>            return message_content, current_reasoning, True<br>        reasoning_step = cast(ActionReasoningStep, reasoning_step)<br>        if not isinstance(reasoning_step, ActionReasoningStep):<br>            raise ValueError(f\"Expected ActionReasoningStep, got {reasoning_step}\")<br>        return message_content, current_reasoning, False<br>    def _process_actions(<br>        self,<br>        task: Task,<br>        tools: Sequence[AsyncBaseTool],<br>        output: ChatResponse,<br>        is_streaming: bool = False,<br>    ) -> Tuple[List[BaseReasoningStep], bool]:<br>        tools_dict: Dict[str, AsyncBaseTool] = {<br>            tool.metadata.get_name(): tool for tool in tools<br>        }<br>        _, current_reasoning, is_done = self._extract_reasoning_step(<br>            output, is_streaming<br>        )<br>        if is_done:<br>            return current_reasoning, True<br>        # call tool with input<br>        reasoning_step = cast(ActionReasoningStep, current_reasoning[-1])<br>        tool = tools_dict[reasoning_step.action]<br>        with self.callback_manager.event(<br>            CBEventType.FUNCTION_CALL,<br>            payload={<br>                EventPayload.FUNCTION_CALL: reasoning_step.action_input,<br>                EventPayload.TOOL: tool.metadata,<br>            },<br>        ) as event:<br>            tool_output = tool.call(**reasoning_step.action_input)<br>            event.on_end(payload={EventPayload.FUNCTION_OUTPUT: str(tool_output)})<br>        task.extra_state[\"sources\"].append(tool_output)<br>        observation_step = ObservationReasoningStep(<br>            observation=str(tool_output), return_direct=tool.metadata.return_direct<br>        )<br>        current_reasoning.append(observation_step)<br>        if self._verbose:<br>            print_text(f\"{observation_step.get_content()}\\n\", color=\"blue\")<br>        return current_reasoning, tool.metadata.return_direct<br>    async def _aprocess_actions(<br>        self,<br>        task: Task,<br>        tools: Sequence[AsyncBaseTool],<br>        output: ChatResponse,<br>        is_streaming: bool = False,<br>    ) -> Tuple[List[BaseReasoningStep], bool]:<br>        tools_dict = {tool.metadata.name: tool for tool in tools}<br>        _, current_reasoning, is_done = self._extract_reasoning_step(<br>            output, is_streaming<br>        )<br>        if is_done:<br>            return current_reasoning, True<br>        # call tool with input<br>        reasoning_step = cast(ActionReasoningStep, current_reasoning[-1])<br>        tool = tools_dict[reasoning_step.action]<br>        with self.callback_manager.event(<br>            CBEventType.FUNCTION_CALL,<br>            payload={<br>                EventPayload.FUNCTION_CALL: reasoning_step.action_input,<br>                EventPayload.TOOL: tool.metadata,<br>            },<br>        ) as event:<br>            tool_output = await tool.acall(**reasoning_step.action_input)<br>            event.on_end(payload={EventPayload.FUNCTION_OUTPUT: str(tool_output)})<br>        task.extra_state[\"sources\"].append(tool_output)<br>        observation_step = ObservationReasoningStep(<br>            observation=str(tool_output), return_direct=tool.metadata.return_direct<br>        )<br>        current_reasoning.append(observation_step)<br>        if self._verbose:<br>            print_text(f\"{observation_step.get_content()}\\n\", color=\"blue\")<br>        return current_reasoning, tool.metadata.return_direct<br>    def _get_response(<br>        self,<br>        current_reasoning: List[BaseReasoningStep],<br>        sources: List[ToolOutput],<br>    ) -> AgentChatResponse:<br>        \"\"\"Get response from reasoning steps.\"\"\"<br>        if len(current_reasoning) == 0:<br>            raise ValueError(\"No reasoning steps were taken.\")<br>        elif len(current_reasoning) == self._max_iterations:<br>            raise ValueError(\"Reached max iterations.\")<br>        if isinstance(current_reasoning[-1], ResponseReasoningStep):<br>            response_step = cast(ResponseReasoningStep, current_reasoning[-1])<br>            response_str = response_step.response<br>        elif (<br>            isinstance(current_reasoning[-1], ObservationReasoningStep)<br>            and current_reasoning[-1].return_direct<br>        ):<br>            response_str = current_reasoning[-1].observation<br>        else:<br>            response_str = current_reasoning[-1].get_content()<br>        # TODO: add sources from reasoning steps<br>        return AgentChatResponse(response=response_str, sources=sources)<br>    def _get_task_step_response(<br>        self, agent_response: AGENT_CHAT_RESPONSE_TYPE, step: TaskStep, is_done: bool<br>    ) -> TaskStepOutput:<br>        \"\"\"Get task step response.\"\"\"<br>        if is_done:<br>            new_steps = []<br>        else:<br>            new_steps = [<br>                step.get_next_step(<br>                    step_id=str(uuid.uuid4()),<br>                    # NOTE: input is unused<br>                    input=None,<br>                )<br>            ]<br>        return TaskStepOutput(<br>            output=agent_response,<br>            task_step=step,<br>            is_last=is_done,<br>            next_steps=new_steps,<br>        )<br>    def _run_step(<br>        self,<br>        step: TaskStep,<br>        task: Task,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        # This is either not None on the first step or if the user specifies<br>        # an intermediate step in the middle<br>        if step.input is not None:<br>            self._add_user_step_to_reasoning(<br>                step=step,<br>                memory=task.extra_state[\"new_memory\"],<br>                current_reasoning=task.extra_state[\"current_reasoning\"],<br>                verbose=self._verbose,<br>            )<br>        # TODO: see if we want to do step-based inputs<br>        tools = self.get_tools(task.input)<br>        input_chat = self._react_chat_formatter.format(<br>            tools,<br>            chat_history=task.memory.get_all()<br>            + task.extra_state[\"new_memory\"].get_all(),<br>            current_reasoning=task.extra_state[\"current_reasoning\"],<br>        )<br>        # send prompt<br>        chat_response = self._multi_modal_llm.chat(input_chat)<br>        # given react prompt outputs, call tools or return response<br>        reasoning_steps, is_done = self._process_actions(<br>            task, tools, output=chat_response<br>        )<br>        task.extra_state[\"current_reasoning\"].extend(reasoning_steps)<br>        agent_response = self._get_response(<br>            task.extra_state[\"current_reasoning\"], task.extra_state[\"sources\"]<br>        )<br>        if is_done:<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(content=agent_response.response, role=MessageRole.ASSISTANT)<br>            )<br>        return self._get_task_step_response(agent_response, step, is_done)<br>    async def _arun_step(<br>        self,<br>        step: TaskStep,<br>        task: Task,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        if step.input is not None:<br>            self._add_user_step_to_reasoning(<br>                step=step,<br>                memory=task.extra_state[\"new_memory\"],<br>                current_reasoning=task.extra_state[\"current_reasoning\"],<br>                verbose=self._verbose,<br>            )<br>        # TODO: see if we want to do step-based inputs<br>        tools = self.get_tools(task.input)<br>        input_chat = self._react_chat_formatter.format(<br>            tools,<br>            chat_history=task.memory.get_all()<br>            + task.extra_state[\"new_memory\"].get_all(),<br>            current_reasoning=task.extra_state[\"current_reasoning\"],<br>        )<br>        # send prompt<br>        chat_response = await self._multi_modal_llm.achat(input_chat)<br>        # given react prompt outputs, call tools or return response<br>        reasoning_steps, is_done = await self._aprocess_actions(<br>            task, tools, output=chat_response<br>        )<br>        task.extra_state[\"current_reasoning\"].extend(reasoning_steps)<br>        agent_response = self._get_response(<br>            task.extra_state[\"current_reasoning\"], task.extra_state[\"sources\"]<br>        )<br>        if is_done:<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(content=agent_response.response, role=MessageRole.ASSISTANT)<br>            )<br>        return self._get_task_step_response(agent_response, step, is_done)<br>    def _run_step_stream(<br>        self,<br>        step: TaskStep,<br>        task: Task,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        raise NotImplementedError(\"Stream step not implemented yet.\")<br>    async def _arun_step_stream(<br>        self,<br>        step: TaskStep,<br>        task: Task,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        raise NotImplementedError(\"Stream step not implemented yet.\")<br>    @trace_method(\"run_step\")<br>    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        return self._run_step(step, task)<br>    @trace_method(\"run_step\")<br>    async def arun_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        return await self._arun_step(step, task)<br>    @trace_method(\"run_step\")<br>    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        # TODO: figure out if we need a different type for TaskStepOutput<br>        return self._run_step_stream(step, task)<br>    @trace_method(\"run_step\")<br>    async def astream_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async stream).\"\"\"<br>        return await self._arun_step_stream(step, task)<br>    def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>        \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>        # add new messages to memory<br>        task.memory.set(<br>            task.memory.get_all() + task.extra_state[\"new_memory\"].get_all()<br>        )<br>        # reset new memory<br>        task.extra_state[\"new_memory\"].reset()<br>    def set_callback_manager(self, callback_manager: CallbackManager) -> None:<br>        \"\"\"Set callback manager.\"\"\"<br>        # TODO: make this abstractmethod (right now will break some agent impls)<br>        self.callback_manager = callback_manager<br>``` |\n\n#### from\\_tools`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.MultimodalReActAgentWorker.from_tools \"Permanent link\")\n\n```\nfrom_tools(tools: Optional[Sequence[BaseTool]] = None, tool_retriever: Optional[ObjectRetriever[BaseTool]] = None, multi_modal_llm: Optional[MultiModalLLM] = None, max_iterations: int = 10, react_chat_formatter: Optional[ReActChatFormatter] = None, output_parser: Optional[ReActOutputParser] = None, callback_manager: Optional[CallbackManager] = None, verbose: bool = False, **kwargs: Any) -> MultimodalReActAgentWorker\n\n```\n\nConvenience constructor method from set of of BaseTools (Optional).\n\nNOTE: kwargs should have been exhausted by this point. In other words\nthe various upstream components such as BaseSynthesizer (response synthesizer)\nor BaseRetriever should have picked up off their respective kwargs in their\nconstructions.\n\n**Returns:**\n\n| Type | Description |\n| --- | --- |\n| `MultimodalReActAgentWorker` | ReActAgent |\n\nSource code in `llama-index-core/llama_index/core/agent/react_multimodal/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>``` | ```<br>@classmethod<br>def from_tools(<br>    cls,<br>    tools: Optional[Sequence[BaseTool]] = None,<br>    tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>    multi_modal_llm: Optional[MultiModalLLM] = None,<br>    max_iterations: int = 10,<br>    react_chat_formatter: Optional[ReActChatFormatter] = None,<br>    output_parser: Optional[ReActOutputParser] = None,<br>    callback_manager: Optional[CallbackManager] = None,<br>    verbose: bool = False,<br>    **kwargs: Any,<br>) -> \"MultimodalReActAgentWorker\":<br>    \"\"\"Convenience constructor method from set of of BaseTools (Optional).<br>    NOTE: kwargs should have been exhausted by this point. In other words<br>    the various upstream components such as BaseSynthesizer (response synthesizer)<br>    or BaseRetriever should have picked up off their respective kwargs in their<br>    constructions.<br>    Returns:<br>        ReActAgent<br>    \"\"\"<br>    if multi_modal_llm is None:<br>        try:<br>            from llama_index.multi_modal_llms.openai import (<br>                OpenAIMultiModal,<br>            )  # pants: no-infer-dep<br>            multi_modal_llm = multi_modal_llm or OpenAIMultiModal(<br>                model=\"gpt-4-vision-preview\", max_new_tokens=1000<br>            )<br>        except ImportError:<br>            raise ImportError(<br>                \"`llama-index-multi-modal-llms-openai` package cannot be found. \"<br>                \"Please install it by using `pip install `llama-index-multi-modal-llms-openai`\"<br>            )<br>    return cls(<br>        tools=tools or [],<br>        tool_retriever=tool_retriever,<br>        multi_modal_llm=multi_modal_llm,<br>        max_iterations=max_iterations,<br>        react_chat_formatter=react_chat_formatter,<br>        output_parser=output_parser,<br>        callback_manager=callback_manager,<br>        verbose=verbose,<br>    )<br>``` |\n\n#### initialize\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.MultimodalReActAgentWorker.initialize_step \"Permanent link\")\n\n```\ninitialize_step(task: Task, **kwargs: Any) -> TaskStep\n\n```\n\nInitialize step from task.\n\nSource code in `llama-index-core/llama_index/core/agent/react_multimodal/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>``` | ```<br>def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>    \"\"\"Initialize step from task.\"\"\"<br>    sources: List[ToolOutput] = []<br>    current_reasoning: List[BaseReasoningStep] = []<br>    # temporary memory for new messages<br>    new_memory = ChatMemoryBuffer.from_defaults()<br>    # validation<br>    if \"image_docs\" not in task.extra_state:<br>        raise ValueError(\"Image docs not found in task extra state.\")<br>    # initialize task state<br>    task_state = {<br>        \"sources\": sources,<br>        \"current_reasoning\": current_reasoning,<br>        \"new_memory\": new_memory,<br>    }<br>    task.extra_state.update(task_state)<br>    return TaskStep(<br>        task_id=task.task_id,<br>        step_id=str(uuid.uuid4()),<br>        input=task.input,<br>        step_state={\"is_first\": True, \"image_docs\": task.extra_state[\"image_docs\"]},<br>    )<br>``` |\n\n#### get\\_tools [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.MultimodalReActAgentWorker.get_tools \"Permanent link\")\n\n```\nget_tools(input: str) -> List[AsyncBaseTool]\n\n```\n\nGet tools.\n\nSource code in `llama-index-core/llama_index/core/agent/react_multimodal/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>229<br>230<br>231<br>``` | ```<br>def get_tools(self, input: str) -> List[AsyncBaseTool]:<br>    \"\"\"Get tools.\"\"\"<br>    return [adapt_to_async_tool(t) for t in self._get_tools(input)]<br>``` |\n\n#### run\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.MultimodalReActAgentWorker.run_step \"Permanent link\")\n\n```\nrun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step.\n\nSource code in `llama-index-core/llama_index/core/agent/react_multimodal/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>487<br>488<br>489<br>490<br>``` | ```<br>@trace_method(\"run_step\")<br>def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step.\"\"\"<br>    return self._run_step(step, task)<br>``` |\n\n#### arun\\_step`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.MultimodalReActAgentWorker.arun_step \"Permanent link\")\n\n```\narun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async).\n\nSource code in `llama-index-core/llama_index/core/agent/react_multimodal/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>492<br>493<br>494<br>495<br>496<br>497<br>``` | ```<br>@trace_method(\"run_step\")<br>async def arun_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async).\"\"\"<br>    return await self._arun_step(step, task)<br>``` |\n\n#### stream\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.MultimodalReActAgentWorker.stream_step \"Permanent link\")\n\n```\nstream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (stream).\n\nSource code in `llama-index-core/llama_index/core/agent/react_multimodal/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>499<br>500<br>501<br>502<br>503<br>``` | ```<br>@trace_method(\"run_step\")<br>def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step (stream).\"\"\"<br>    # TODO: figure out if we need a different type for TaskStepOutput<br>    return self._run_step_stream(step, task)<br>``` |\n\n#### astream\\_step`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.MultimodalReActAgentWorker.astream_step \"Permanent link\")\n\n```\nastream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async stream).\n\nSource code in `llama-index-core/llama_index/core/agent/react_multimodal/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>505<br>506<br>507<br>508<br>509<br>510<br>``` | ```<br>@trace_method(\"run_step\")<br>async def astream_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async stream).\"\"\"<br>    return await self._arun_step_stream(step, task)<br>``` |\n\n#### finalize\\_task [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.MultimodalReActAgentWorker.finalize_task \"Permanent link\")\n\n```\nfinalize_task(task: Task, **kwargs: Any) -> None\n\n```\n\nFinalize task, after all the steps are completed.\n\nSource code in `llama-index-core/llama_index/core/agent/react_multimodal/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>512<br>513<br>514<br>515<br>516<br>517<br>518<br>519<br>``` | ```<br>def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>    \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>    # add new messages to memory<br>    task.memory.set(<br>        task.memory.get_all() + task.extra_state[\"new_memory\"].get_all()<br>    )<br>    # reset new memory<br>    task.extra_state[\"new_memory\"].reset()<br>``` |\n\n#### set\\_callback\\_manager [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.MultimodalReActAgentWorker.set_callback_manager \"Permanent link\")\n\n```\nset_callback_manager(callback_manager: CallbackManager) -> None\n\n```\n\nSet callback manager.\n\nSource code in `llama-index-core/llama_index/core/agent/react_multimodal/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>521<br>522<br>523<br>524<br>``` | ```<br>def set_callback_manager(self, callback_manager: CallbackManager) -> None:<br>    \"\"\"Set callback manager.\"\"\"<br>    # TODO: make this abstractmethod (right now will break some agent impls)<br>    self.callback_manager = callback_manager<br>``` |\n\n### QueryPipelineAgentWorker [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.QueryPipelineAgentWorker \"Permanent link\")\n\nBases: `BaseModel`, `BaseAgentWorker`\n\nQuery Pipeline agent worker.\n\nNOTE: This is now deprecated. Use `FnAgentWorker` instead to build a stateful agent.\n\nBarebones agent worker that takes in a query pipeline.\n\n**Default Workflow**: The default workflow assumes that you compose\na query pipeline with `StatefulFnComponent` objects. This allows you to store, update\nand retrieve state throughout the executions of the query pipeline by the agent.\n\nThe task and step state of the agent are stored in this `state` variable via a special key.\nOf course you can choose to store other variables in this state as well.\n\n**Deprecated Workflow**: The deprecated workflow assumes that the first component in the\nquery pipeline is an `AgentInputComponent` and last is `AgentFnComponent`.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `pipeline` | `QueryPipeline` | Query pipeline | _required_ |\n\nSource code in `llama-index-core/llama_index/core/agent/custom/pipeline_worker.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>``` | ```<br>@deprecated(\"Use `FnAgentWorker` instead to build a stateful agent.\")<br>class QueryPipelineAgentWorker(BaseModel, BaseAgentWorker):<br>    \"\"\"Query Pipeline agent worker.<br>    NOTE: This is now deprecated. Use `FnAgentWorker` instead to build a stateful agent.<br>    Barebones agent worker that takes in a query pipeline.<br>    **Default Workflow**: The default workflow assumes that you compose<br>    a query pipeline with `StatefulFnComponent` objects. This allows you to store, update<br>    and retrieve state throughout the executions of the query pipeline by the agent.<br>    The task and step state of the agent are stored in this `state` variable via a special key.<br>    Of course you can choose to store other variables in this state as well.<br>    **Deprecated Workflow**: The deprecated workflow assumes that the first component in the<br>    query pipeline is an `AgentInputComponent` and last is `AgentFnComponent`.<br>    Args:<br>        pipeline (QueryPipeline): Query pipeline<br>    \"\"\"<br>    model_config = ConfigDict(arbitrary_types_allowed=True)<br>    pipeline: QueryPipeline = Field(..., description=\"Query pipeline\")<br>    callback_manager: CallbackManager = Field(..., exclude=True)<br>    task_key: str = Field(\"task\", description=\"Key to store task in state\")<br>    step_state_key: str = Field(\"step_state\", description=\"Key to store step in state\")<br>    def __init__(<br>        self,<br>        pipeline: QueryPipeline,<br>        callback_manager: Optional[CallbackManager] = None,<br>        **kwargs: Any,<br>    ) -> None:<br>        \"\"\"Initialize.\"\"\"<br>        if callback_manager is not None:<br>            # set query pipeline callback<br>            pipeline.set_callback_manager(callback_manager)<br>        else:<br>            callback_manager = pipeline.callback_manager<br>        super().__init__(<br>            pipeline=pipeline,<br>            callback_manager=callback_manager,<br>            **kwargs,<br>        )<br>        # validate query pipeline<br>        # self.agent_input_component<br>        self.agent_components<br>    @property<br>    def agent_input_component(self) -> AgentInputComponent:<br>        \"\"\"Get agent input component.<br>        NOTE: This is deprecated and will be removed in the future.<br>        \"\"\"<br>        root_key = self.pipeline.get_root_keys()[0]<br>        if not isinstance(self.pipeline.module_dict[root_key], AgentInputComponent):<br>            raise ValueError(<br>                \"Query pipeline first component must be AgentInputComponent, got \"<br>                f\"{self.pipeline.module_dict[root_key]}\"<br>            )<br>        return cast(AgentInputComponent, self.pipeline.module_dict[root_key])<br>    @property<br>    def agent_components(self) -> Sequence[BaseAgentComponent]:<br>        \"\"\"Get agent output component.\"\"\"<br>        return _get_agent_components(self.pipeline)<br>    def preprocess(self, task: Task, step: TaskStep) -> None:<br>        \"\"\"Preprocessing flow.<br>        This runs preprocessing to propagate the task and step as variables<br>        to relevant components in the query pipeline.<br>        Contains deprecated flow of updating agent components.<br>        But also contains main flow of updating StatefulFnComponent components.<br>        \"\"\"<br>        # NOTE: this is deprecated<br>        # partial agent output component with task and step<br>        for agent_fn_component in self.agent_components:<br>            agent_fn_component.partial(task=task, state=step.step_state)<br>        # update stateful components<br>        self.pipeline.update_state(<br>            {self.task_key: task, self.step_state_key: step.step_state}<br>        )<br>    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>        \"\"\"Initialize step from task.\"\"\"<br>        sources: List[ToolOutput] = []<br>        # temporary memory for new messages<br>        new_memory = ChatMemoryBuffer.from_defaults()<br>        # initialize initial state<br>        initial_state = {<br>            \"sources\": sources,<br>            \"memory\": new_memory,<br>        }<br>        return TaskStep(<br>            task_id=task.task_id,<br>            step_id=str(uuid.uuid4()),<br>            input=task.input,<br>            step_state=initial_state,<br>        )<br>    def _get_task_step_response(<br>        self, agent_response: AGENT_CHAT_RESPONSE_TYPE, step: TaskStep, is_done: bool<br>    ) -> TaskStepOutput:<br>        \"\"\"Get task step response.\"\"\"<br>        if is_done:<br>            new_steps = []<br>        else:<br>            new_steps = [<br>                step.get_next_step(<br>                    step_id=str(uuid.uuid4()),<br>                    # NOTE: input is unused<br>                    input=None,<br>                )<br>            ]<br>        return TaskStepOutput(<br>            output=agent_response,<br>            task_step=step,<br>            is_last=is_done,<br>            next_steps=new_steps,<br>        )<br>    @trace_method(\"run_step\")<br>    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        self.preprocess(task, step)<br>        # HACK: do a try/except for now. Fine since old agent components are deprecated<br>        try:<br>            self.agent_input_component<br>            uses_deprecated = True<br>        except ValueError:<br>            uses_deprecated = False<br>        if uses_deprecated:<br>            agent_response, is_done = self.pipeline.run(<br>                state=step.step_state, task=task<br>            )<br>        else:<br>            agent_response, is_done = self.pipeline.run()<br>        response = self._get_task_step_response(agent_response, step, is_done)<br>        # sync step state with task state<br>        task.extra_state.update(step.step_state)<br>        return response<br>    @trace_method(\"run_step\")<br>    async def arun_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        self.preprocess(task, step)<br>        # HACK: do a try/except for now. Fine since old agent components are deprecated<br>        try:<br>            self.agent_input_component<br>            uses_deprecated = True<br>        except ValueError:<br>            uses_deprecated = False<br>        if uses_deprecated:<br>            agent_response, is_done = await self.pipeline.arun(<br>                state=step.step_state, task=task<br>            )<br>        else:<br>            agent_response, is_done = await self.pipeline.arun()<br>        response = self._get_task_step_response(agent_response, step, is_done)<br>        task.extra_state.update(step.step_state)<br>        return response<br>    @trace_method(\"run_step\")<br>    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        raise NotImplementedError(\"This agent does not support streaming.\")<br>    @trace_method(\"run_step\")<br>    async def astream_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async stream).\"\"\"<br>        raise NotImplementedError(\"This agent does not support streaming.\")<br>    def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>        \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>        # add new messages to memory<br>        task.memory.set(task.memory.get() + task.extra_state[\"memory\"].get_all())<br>        # reset new memory<br>        task.extra_state[\"memory\"].reset()<br>    def set_callback_manager(self, callback_manager: CallbackManager) -> None:<br>        \"\"\"Set callback manager.\"\"\"<br>        # TODO: make this abstractmethod (right now will break some agent impls)<br>        self.callback_manager = callback_manager<br>        self.pipeline.set_callback_manager(callback_manager)<br>``` |\n\n#### agent\\_input\\_component`property`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.QueryPipelineAgentWorker.agent_input_component \"Permanent link\")\n\n```\nagent_input_component: AgentInputComponent\n\n```\n\nGet agent input component.\n\nNOTE: This is deprecated and will be removed in the future.\n\n#### agent\\_components`property`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.QueryPipelineAgentWorker.agent_components \"Permanent link\")\n\n```\nagent_components: Sequence[BaseAgentComponent]\n\n```\n\nGet agent output component.\n\n#### preprocess [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.QueryPipelineAgentWorker.preprocess \"Permanent link\")\n\n```\npreprocess(task: Task, step: TaskStep) -> None\n\n```\n\nPreprocessing flow.\n\nThis runs preprocessing to propagate the task and step as variables\nto relevant components in the query pipeline.\n\nContains deprecated flow of updating agent components.\nBut also contains main flow of updating StatefulFnComponent components.\n\nSource code in `llama-index-core/llama_index/core/agent/custom/pipeline_worker.py`\n\n|     |     |\n| --- | --- |\n| ```<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>``` | ```<br>def preprocess(self, task: Task, step: TaskStep) -> None:<br>    \"\"\"Preprocessing flow.<br>    This runs preprocessing to propagate the task and step as variables<br>    to relevant components in the query pipeline.<br>    Contains deprecated flow of updating agent components.<br>    But also contains main flow of updating StatefulFnComponent components.<br>    \"\"\"<br>    # NOTE: this is deprecated<br>    # partial agent output component with task and step<br>    for agent_fn_component in self.agent_components:<br>        agent_fn_component.partial(task=task, state=step.step_state)<br>    # update stateful components<br>    self.pipeline.update_state(<br>        {self.task_key: task, self.step_state_key: step.step_state}<br>    )<br>``` |\n\n#### initialize\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.QueryPipelineAgentWorker.initialize_step \"Permanent link\")\n\n```\ninitialize_step(task: Task, **kwargs: Any) -> TaskStep\n\n```\n\nInitialize step from task.\n\nSource code in `llama-index-core/llama_index/core/agent/custom/pipeline_worker.py`\n\n|     |     |\n| --- | --- |\n| ```<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>``` | ```<br>def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>    \"\"\"Initialize step from task.\"\"\"<br>    sources: List[ToolOutput] = []<br>    # temporary memory for new messages<br>    new_memory = ChatMemoryBuffer.from_defaults()<br>    # initialize initial state<br>    initial_state = {<br>        \"sources\": sources,<br>        \"memory\": new_memory,<br>    }<br>    return TaskStep(<br>        task_id=task.task_id,<br>        step_id=str(uuid.uuid4()),<br>        input=task.input,<br>        step_state=initial_state,<br>    )<br>``` |\n\n#### run\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.QueryPipelineAgentWorker.run_step \"Permanent link\")\n\n```\nrun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step.\n\nSource code in `llama-index-core/llama_index/core/agent/custom/pipeline_worker.py`\n\n|     |     |\n| --- | --- |\n| ```<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>``` | ```<br>@trace_method(\"run_step\")<br>def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step.\"\"\"<br>    self.preprocess(task, step)<br>    # HACK: do a try/except for now. Fine since old agent components are deprecated<br>    try:<br>        self.agent_input_component<br>        uses_deprecated = True<br>    except ValueError:<br>        uses_deprecated = False<br>    if uses_deprecated:<br>        agent_response, is_done = self.pipeline.run(<br>            state=step.step_state, task=task<br>        )<br>    else:<br>        agent_response, is_done = self.pipeline.run()<br>    response = self._get_task_step_response(agent_response, step, is_done)<br>    # sync step state with task state<br>    task.extra_state.update(step.step_state)<br>    return response<br>``` |\n\n#### arun\\_step`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.QueryPipelineAgentWorker.arun_step \"Permanent link\")\n\n```\narun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async).\n\nSource code in `llama-index-core/llama_index/core/agent/custom/pipeline_worker.py`\n\n|     |     |\n| --- | --- |\n| ```<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>``` | ```<br>@trace_method(\"run_step\")<br>async def arun_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async).\"\"\"<br>    self.preprocess(task, step)<br>    # HACK: do a try/except for now. Fine since old agent components are deprecated<br>    try:<br>        self.agent_input_component<br>        uses_deprecated = True<br>    except ValueError:<br>        uses_deprecated = False<br>    if uses_deprecated:<br>        agent_response, is_done = await self.pipeline.arun(<br>            state=step.step_state, task=task<br>        )<br>    else:<br>        agent_response, is_done = await self.pipeline.arun()<br>    response = self._get_task_step_response(agent_response, step, is_done)<br>    task.extra_state.update(step.step_state)<br>    return response<br>``` |\n\n#### stream\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.QueryPipelineAgentWorker.stream_step \"Permanent link\")\n\n```\nstream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (stream).\n\nSource code in `llama-index-core/llama_index/core/agent/custom/pipeline_worker.py`\n\n|     |     |\n| --- | --- |\n| ```<br>233<br>234<br>235<br>236<br>``` | ```<br>@trace_method(\"run_step\")<br>def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step (stream).\"\"\"<br>    raise NotImplementedError(\"This agent does not support streaming.\")<br>``` |\n\n#### astream\\_step`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.QueryPipelineAgentWorker.astream_step \"Permanent link\")\n\n```\nastream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async stream).\n\nSource code in `llama-index-core/llama_index/core/agent/custom/pipeline_worker.py`\n\n|     |     |\n| --- | --- |\n| ```<br>238<br>239<br>240<br>241<br>242<br>243<br>``` | ```<br>@trace_method(\"run_step\")<br>async def astream_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async stream).\"\"\"<br>    raise NotImplementedError(\"This agent does not support streaming.\")<br>``` |\n\n#### finalize\\_task [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.QueryPipelineAgentWorker.finalize_task \"Permanent link\")\n\n```\nfinalize_task(task: Task, **kwargs: Any) -> None\n\n```\n\nFinalize task, after all the steps are completed.\n\nSource code in `llama-index-core/llama_index/core/agent/custom/pipeline_worker.py`\n\n|     |     |\n| --- | --- |\n| ```<br>245<br>246<br>247<br>248<br>249<br>250<br>``` | ```<br>def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>    \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>    # add new messages to memory<br>    task.memory.set(task.memory.get() + task.extra_state[\"memory\"].get_all())<br>    # reset new memory<br>    task.extra_state[\"memory\"].reset()<br>``` |\n\n#### set\\_callback\\_manager [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/\\#llama_index.core.agent.QueryPipelineAgentWorker.set_callback_manager \"Permanent link\")\n\n```\nset_callback_manager(callback_manager: CallbackManager) -> None\n\n```\n\nSet callback manager.\n\nSource code in `llama-index-core/llama_index/core/agent/custom/pipeline_worker.py`\n\n|     |     |\n| --- | --- |\n| ```<br>252<br>253<br>254<br>255<br>256<br>``` | ```<br>def set_callback_manager(self, callback_manager: CallbackManager) -> None:<br>    \"\"\"Set callback manager.\"\"\"<br>    # TODO: make this abstractmethod (right now will break some agent impls)<br>    self.callback_manager = callback_manager<br>    self.pipeline.set_callback_manager(callback_manager)<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Core Agent Classes - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/agent/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/cloudflare_workersai/#llama_index.embeddings.cloudflare_workersai.CloudflareEmbedding)\n\n# Cloudflare workersai\n\n## CloudflareEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/cloudflare_workersai/\\#llama_index.embeddings.cloudflare_workersai.CloudflareEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nCloudflare Workers AI class for generating text embeddings.\n\nThis class allows for the generation of text embeddings using Cloudflare Workers AI with the BAAI general embedding models.\n\nArgs:\naccount\\_id (str): The Cloudflare Account ID.\nauth\\_token (str, Optional): The Cloudflare Auth Token. Alternatively, set up environment variable `CLOUDFLARE_AUTH_TOKEN`.\nmodel (str): The model ID for the embedding service. Cloudflare provides different models for embeddings, check https://developers.cloudflare.com/workers-ai/models/#text-embeddings. Defaults to \"@cf/baai/bge-base-en-v1.5\".\nembed\\_batch\\_size (int): The batch size for embedding generation. Cloudflare's current limit is 100 at max. Defaults to llama\\_index's default.\n\nNote:\nEnsure you have a valid Cloudflare account and have access to the necessary AI services and models. The account ID and authorization token are sensitive details; secure them appropriately.\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-cloudflare-workersai/llama_index/embeddings/cloudflare_workersai/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>``` | ```<br>class CloudflareEmbedding(BaseEmbedding):<br>    \"\"\"<br>    Cloudflare Workers AI class for generating text embeddings.<br>    This class allows for the generation of text embeddings using Cloudflare Workers AI with the BAAI general embedding models.<br>    Args:<br>    account_id (str): The Cloudflare Account ID.<br>    auth_token (str, Optional): The Cloudflare Auth Token. Alternatively, set up environment variable `CLOUDFLARE_AUTH_TOKEN`.<br>    model (str): The model ID for the embedding service. Cloudflare provides different models for embeddings, check https://developers.cloudflare.com/workers-ai/models/#text-embeddings. Defaults to \"@cf/baai/bge-base-en-v1.5\".<br>    embed_batch_size (int): The batch size for embedding generation. Cloudflare's current limit is 100 at max. Defaults to llama_index's default.<br>    Note:<br>    Ensure you have a valid Cloudflare account and have access to the necessary AI services and models. The account ID and authorization token are sensitive details; secure them appropriately.<br>    \"\"\"<br>    account_id: str = Field(default=None, description=\"The Cloudflare Account ID.\")<br>    auth_token: str = Field(default=None, description=\"The Cloudflare Auth Token.\")<br>    model: str = Field(<br>        default=\"@cf/baai/bge-base-en-v1.5\",<br>        description=\"The model to use when calling Cloudflare AI API\",<br>    )<br>    _session: Any = PrivateAttr()<br>    def __init__(<br>        self,<br>        account_id: str,<br>        auth_token: Optional[str] = None,<br>        model: str = \"@cf/baai/bge-base-en-v1.5\",<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>        **kwargs: Any,<br>    ) -> None:<br>        super().__init__(<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            model=model,<br>            **kwargs,<br>        )<br>        self.account_id = account_id<br>        self.auth_token = get_from_param_or_env(<br>            \"auth_token\", auth_token, \"CLOUDFLARE_AUTH_TOKEN\", \"\"<br>        )<br>        self.model = model<br>        self._session = requests.Session()<br>        self._session.headers.update({\"Authorization\": f\"Bearer {self.auth_token}\"})<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"CloudflareEmbedding\"<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self._get_text_embedding(query)<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        return await self._aget_text_embedding(query)<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._get_text_embeddings([text])[0]<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously get text embedding.\"\"\"<br>        result = await self._aget_text_embeddings([text])<br>        return result[0]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        response = self._session.post(<br>            API_URL_TEMPLATE.format(self.account_id, self.model), json={\"text\": texts}<br>        ).json()<br>        if \"result\" not in response:<br>            print(response)<br>            raise RuntimeError(\"Failed to fetch embeddings\")<br>        return response[\"result\"][\"data\"]<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Asynchronously get text embeddings.\"\"\"<br>        import aiohttp<br>        async with aiohttp.ClientSession(trust_env=True) as session:<br>            headers = {<br>                \"Authorization\": f\"Bearer {self.auth_token}\",<br>                \"Accept-Encoding\": \"identity\",<br>            }<br>            async with session.post(<br>                API_URL_TEMPLATE.format(self.account_id, self.model),<br>                json={\"text\": texts},<br>                headers=headers,<br>            ) as response:<br>                resp = await response.json()<br>                if \"result\" not in resp:<br>                    raise RuntimeError(\"Failed to fetch embeddings asynchronously\")<br>                return resp[\"result\"][\"data\"]<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Cloudflare workersai - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/cloudflare_workersai/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/simple/#llama_index.core.chat_engine.SimpleChatEngine)\n\n# Simple\n\n## SimpleChatEngine [\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/simple/\\#llama_index.core.chat_engine.SimpleChatEngine \"Permanent link\")\n\nBases: `BaseChatEngine`\n\nSimple Chat Engine.\n\nHave a conversation with the LLM.\nThis does not make use of a knowledge base.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/simple.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>``` | ```<br>class SimpleChatEngine(BaseChatEngine):<br>    \"\"\"<br>    Simple Chat Engine.<br>    Have a conversation with the LLM.<br>    This does not make use of a knowledge base.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        llm: LLM,<br>        memory: BaseMemory,<br>        prefix_messages: List[ChatMessage],<br>        callback_manager: Optional[CallbackManager] = None,<br>    ) -> None:<br>        self._llm = llm<br>        self._memory = memory<br>        self._prefix_messages = prefix_messages<br>        self.callback_manager = callback_manager or CallbackManager([])<br>    @classmethod<br>    def from_defaults(<br>        cls,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        memory: Optional[BaseMemory] = None,<br>        memory_cls: Type[BaseMemory] = ChatMemoryBuffer,<br>        system_prompt: Optional[str] = None,<br>        prefix_messages: Optional[List[ChatMessage]] = None,<br>        llm: Optional[LLM] = None,<br>        **kwargs: Any,<br>    ) -> \"SimpleChatEngine\":<br>        \"\"\"Initialize a SimpleChatEngine from default parameters.\"\"\"<br>        llm = llm or Settings.llm<br>        chat_history = chat_history or []<br>        memory = memory or memory_cls.from_defaults(chat_history=chat_history, llm=llm)<br>        if system_prompt is not None:<br>            if prefix_messages is not None:<br>                raise ValueError(<br>                    \"Cannot specify both system_prompt and prefix_messages\"<br>                )<br>            prefix_messages = [<br>                ChatMessage(content=system_prompt, role=llm.metadata.system_role)<br>            ]<br>        prefix_messages = prefix_messages or []<br>        return cls(<br>            llm=llm,<br>            memory=memory,<br>            prefix_messages=prefix_messages,<br>            callback_manager=Settings.callback_manager,<br>        )<br>    @trace_method(\"chat\")<br>    def chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> AgentChatResponse:<br>        if chat_history is not None:<br>            self._memory.set(chat_history)<br>        self._memory.put(ChatMessage(content=message, role=\"user\"))<br>        if hasattr(self._memory, \"tokenizer_fn\"):<br>            initial_token_count = len(<br>                self._memory.tokenizer_fn(<br>                    \" \".join([(m.content or \"\") for m in self._prefix_messages])<br>                )<br>            )<br>        else:<br>            initial_token_count = 0<br>        all_messages = self._prefix_messages + self._memory.get(<br>            initial_token_count=initial_token_count<br>        )<br>        chat_response = self._llm.chat(all_messages)<br>        ai_message = chat_response.message<br>        self._memory.put(ai_message)<br>        return AgentChatResponse(response=str(chat_response.message.content))<br>    @trace_method(\"chat\")<br>    def stream_chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> StreamingAgentChatResponse:<br>        if chat_history is not None:<br>            self._memory.set(chat_history)<br>        self._memory.put(ChatMessage(content=message, role=\"user\"))<br>        if hasattr(self._memory, \"tokenizer_fn\"):<br>            initial_token_count = len(<br>                self._memory.tokenizer_fn(<br>                    \" \".join([(m.content or \"\") for m in self._prefix_messages])<br>                )<br>            )<br>        else:<br>            initial_token_count = 0<br>        all_messages = self._prefix_messages + self._memory.get(<br>            initial_token_count=initial_token_count<br>        )<br>        chat_response = StreamingAgentChatResponse(<br>            chat_stream=self._llm.stream_chat(all_messages)<br>        )<br>        thread = Thread(<br>            target=chat_response.write_response_to_history, args=(self._memory,)<br>        )<br>        thread.start()<br>        return chat_response<br>    @trace_method(\"chat\")<br>    async def achat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> AgentChatResponse:<br>        if chat_history is not None:<br>            self._memory.set(chat_history)<br>        self._memory.put(ChatMessage(content=message, role=\"user\"))<br>        if hasattr(self._memory, \"tokenizer_fn\"):<br>            initial_token_count = len(<br>                self._memory.tokenizer_fn(<br>                    \" \".join([(m.content or \"\") for m in self._prefix_messages])<br>                )<br>            )<br>        else:<br>            initial_token_count = 0<br>        all_messages = self._prefix_messages + self._memory.get(<br>            initial_token_count=initial_token_count<br>        )<br>        chat_response = await self._llm.achat(all_messages)<br>        ai_message = chat_response.message<br>        self._memory.put(ai_message)<br>        return AgentChatResponse(response=str(chat_response.message.content))<br>    @trace_method(\"chat\")<br>    async def astream_chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> StreamingAgentChatResponse:<br>        if chat_history is not None:<br>            self._memory.set(chat_history)<br>        self._memory.put(ChatMessage(content=message, role=\"user\"))<br>        if hasattr(self._memory, \"tokenizer_fn\"):<br>            initial_token_count = len(<br>                self._memory.tokenizer_fn(<br>                    \" \".join([(m.content or \"\") for m in self._prefix_messages])<br>                )<br>            )<br>        else:<br>            initial_token_count = 0<br>        all_messages = self._prefix_messages + self._memory.get(<br>            initial_token_count=initial_token_count<br>        )<br>        chat_response = StreamingAgentChatResponse(<br>            achat_stream=await self._llm.astream_chat(all_messages)<br>        )<br>        asyncio.create_task(chat_response.awrite_response_to_history(self._memory))<br>        return chat_response<br>    def reset(self) -> None:<br>        self._memory.reset()<br>    @property<br>    def chat_history(self) -> List[ChatMessage]:<br>        \"\"\"Get chat history.\"\"\"<br>        return self._memory.get_all()<br>``` |\n\n### chat\\_history`property`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/simple/\\#llama_index.core.chat_engine.SimpleChatEngine.chat_history \"Permanent link\")\n\n```\nchat_history: List[ChatMessage]\n\n```\n\nGet chat history.\n\n### from\\_defaults`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/simple/\\#llama_index.core.chat_engine.SimpleChatEngine.from_defaults \"Permanent link\")\n\n```\nfrom_defaults(chat_history: Optional[List[ChatMessage]] = None, memory: Optional[BaseMemory] = None, memory_cls: Type[BaseMemory] = ChatMemoryBuffer, system_prompt: Optional[str] = None, prefix_messages: Optional[List[ChatMessage]] = None, llm: Optional[LLM] = None, **kwargs: Any) -> SimpleChatEngine\n\n```\n\nInitialize a SimpleChatEngine from default parameters.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/simple.py`\n\n|     |     |\n| --- | --- |\n| ```<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>``` | ```<br>@classmethod<br>def from_defaults(<br>    cls,<br>    chat_history: Optional[List[ChatMessage]] = None,<br>    memory: Optional[BaseMemory] = None,<br>    memory_cls: Type[BaseMemory] = ChatMemoryBuffer,<br>    system_prompt: Optional[str] = None,<br>    prefix_messages: Optional[List[ChatMessage]] = None,<br>    llm: Optional[LLM] = None,<br>    **kwargs: Any,<br>) -> \"SimpleChatEngine\":<br>    \"\"\"Initialize a SimpleChatEngine from default parameters.\"\"\"<br>    llm = llm or Settings.llm<br>    chat_history = chat_history or []<br>    memory = memory or memory_cls.from_defaults(chat_history=chat_history, llm=llm)<br>    if system_prompt is not None:<br>        if prefix_messages is not None:<br>            raise ValueError(<br>                \"Cannot specify both system_prompt and prefix_messages\"<br>            )<br>        prefix_messages = [<br>            ChatMessage(content=system_prompt, role=llm.metadata.system_role)<br>        ]<br>    prefix_messages = prefix_messages or []<br>    return cls(<br>        llm=llm,<br>        memory=memory,<br>        prefix_messages=prefix_messages,<br>        callback_manager=Settings.callback_manager,<br>    )<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Simple - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/simple/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/promptlayer/#llama_index.callbacks.promptlayer.PromptLayerHandler)\n\n# Promptlayer\n\n## PromptLayerHandler [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/promptlayer/\\#llama_index.callbacks.promptlayer.PromptLayerHandler \"Permanent link\")\n\nBases: `BaseCallbackHandler`\n\nCallback handler for sending to promptlayer.com.\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-promptlayer/llama_index/callbacks/promptlayer/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 13<br> 14<br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>``` | ```<br>class PromptLayerHandler(BaseCallbackHandler):<br>    \"\"\"Callback handler for sending to promptlayer.com.\"\"\"<br>    pl_tags: Optional[List[str]]<br>    return_pl_id: bool = False<br>    def __init__(self, pl_tags: List[str] = [], return_pl_id: bool = False) -> None:<br>        try:<br>            from promptlayer.utils import get_api_key, promptlayer_api_request<br>            self._promptlayer_api_request = promptlayer_api_request<br>            self._promptlayer_api_key = get_api_key()<br>        except ImportError:<br>            raise ImportError(<br>                \"Please install PromptLAyer with `pip install promptlayer`\"<br>            )<br>        self.pl_tags = pl_tags<br>        self.return_pl_id = return_pl_id<br>        super().__init__(event_starts_to_ignore=[], event_ends_to_ignore=[])<br>    def start_trace(self, trace_id: Optional[str] = None) -> None:<br>        return<br>    def end_trace(<br>        self,<br>        trace_id: Optional[str] = None,<br>        trace_map: Optional[Dict[str, List[str]]] = None,<br>    ) -> None:<br>        return<br>    event_map: Dict[str, Dict[str, Any]] = {}<br>    def add_event(self, event_id: str, **kwargs: Any) -> None:<br>        self.event_map[event_id] = {<br>            \"kwargs\": kwargs,<br>            \"request_start_time\": datetime.datetime.now().timestamp(),<br>        }<br>    def get_event(<br>        self,<br>        event_id: str,<br>    ) -> Dict[str, Any]:<br>        return self.event_map[event_id] or {}<br>    def on_event_start(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        parent_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> str:<br>        if event_type == CBEventType.LLM and payload is not None:<br>            self.add_event(<br>                event_id=event_id, **payload.get(EventPayload.SERIALIZED, {})<br>            )<br>        return event_id<br>    def on_event_end(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> None:<br>        if event_type != CBEventType.LLM or payload is None:<br>            return<br>        request_end_time = datetime.datetime.now().timestamp()<br>        prompt = str(payload.get(EventPayload.PROMPT))<br>        completion = payload.get(EventPayload.COMPLETION)<br>        response = payload.get(EventPayload.RESPONSE)<br>        function_name = PROMPT_LAYER_CHAT_FUNCTION_NAME<br>        event_data = self.get_event(event_id=event_id)<br>        resp: Union[str, Dict]<br>        extra_args = {}<br>        resp = None<br>        if response:<br>            messages = cast(List[ChatMessage], payload.get(EventPayload.MESSAGES, []))<br>            resp = response.message.dict()<br>            assert isinstance(resp, dict)<br>            usage_dict: Dict[str, int] = {}<br>            try:<br>                usage = response.raw.get(\"usage\", None)  # type: ignore<br>                if isinstance(usage, dict):<br>                    usage_dict = {<br>                        \"prompt_tokens\": usage.get(\"prompt_tokens\", 0),<br>                        \"completion_tokens\": usage.get(\"completion_tokens\", 0),<br>                        \"total_tokens\": usage.get(\"total_tokens\", 0),<br>                    }<br>                elif isinstance(usage, BaseModel):<br>                    usage_dict = usage.dict()<br>            except Exception:<br>                pass<br>            extra_args = {<br>                \"messages\": [message.dict() for message in messages],<br>                \"usage\": usage_dict,<br>            }<br>            ## promptlayer needs tool_calls toplevel.<br>            if \"tool_calls\" in response.message.additional_kwargs:<br>                resp[\"tool_calls\"] = [<br>                    tool_call.dict()<br>                    for tool_call in resp[\"additional_kwargs\"][\"tool_calls\"]<br>                ]<br>                del resp[\"additional_kwargs\"][\"tool_calls\"]<br>        if completion:<br>            function_name = PROMPT_LAYER_COMPLETION_FUNCTION_NAME<br>            resp = str(completion)<br>        if resp:<br>            _pl_request_id = self._promptlayer_api_request(<br>                function_name,<br>                \"openai\",<br>                [prompt],<br>                {<br>                    **extra_args,<br>                    **event_data[\"kwargs\"],<br>                },<br>                self.pl_tags,<br>                [resp],<br>                event_data[\"request_start_time\"],<br>                request_end_time,<br>                self._promptlayer_api_key,<br>                return_pl_id=self.return_pl_id,<br>            )<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Promptlayer - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/promptlayer/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/CHANGELOG/#changelog)\n\n# ChangeLog [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#changelog \"Permanent link\")\n\n## \\[2024-09-06\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-09-06 \"Permanent link\")\n\n### `llama-index-core` \\[0.11.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-0117 \"Permanent link\")\n\n- Make SentenceSplitter's secondary\\_chunking\\_regex optional (#15882)\n- force openai structured output (#15706)\n- fix assert error, add type ignore for streaming agents (#15887)\n- Fix image document deserialization issue (#15857)\n\n### `llama-index-graph-stores-kuzu` \\[0.3.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-graph-stores-kuzu-032 \"Permanent link\")\n\n- Bug fix for KuzuPropertyGraphStore: Allow upserting relations even when chunks are absent (#15889)\n\n### `llama-index-llms-bedrock-converse` \\[0.3.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-bedrock-converse-030 \"Permanent link\")\n\n- Removed unused llama-index-llms-anthropic dependency from Bedrock Converse (#15869)\n\n### `llama-index-vector-stores-postgres` \\[0.2.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-postgres-022 \"Permanent link\")\n\n- Fix PGVectorStore with latest pydantic, update pydantic imports (#15886)\n\n### `llama-index-vector-stores-tablestore` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-tablestore-010 \"Permanent link\")\n\n- Add TablestoreVectorStore (#15657)\n\n## \\[2024-09-05\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-09-05 \"Permanent link\")\n\n### `llama-index-core` \\[0.11.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-0116 \"Permanent link\")\n\n- add llama-deploy docs to docs builds (#15794)\n- Add oreilly course cookbooks (#15845)\n\n### `llama-index-readers-box` \\[0.2.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-box-021 \"Permanent link\")\n\n- Various bug fixes (#15836)\n\n### `llama-index-readers-file` \\[0.2.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-file-021 \"Permanent link\")\n\n- Update ImageReader file loading logic (#15848)\n\n### `llama-index-tools-box` \\[0.2.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-tools-box-021 \"Permanent link\")\n\n- Various bug fixes (#15836)\n\n### `llama-index-vector-stores-opensearch` \\[0.2.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-opensearch-021 \"Permanent link\")\n\n- Refresh Opensearch index after delete operation (#15854)\n\n## \\[2024-09-04\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-09-04 \"Permanent link\")\n\n### `llama-index-core` \\[0.11.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-0115 \"Permanent link\")\n\n- remove unneeded assert in property graph retriever (#15832)\n- make simple property graphs serialize again (#15833)\n- fix json schema for fastapi return types on core components (#15816)\n\n### `llama-index-llms-nvidia` \\[0.2.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-nvidia-022 \"Permanent link\")\n\n- NVIDIA llm: Add Completion for starcoder models (#15802)\n\n### `llama-index-llms-ollama` \\[0.3.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-ollama-031 \"Permanent link\")\n\n- add ollama response usage (#15773)\n\n### `llama-index-readers-dashscope` \\[0.2.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-dashscope-021 \"Permanent link\")\n\n- fix pydantic v2 validation errors (#15800)\n\n### `llama-index-readers-discord` \\[0.2.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-discord-021 \"Permanent link\")\n\n- fix: convert Document id from int to string in DiscordReader (#15806)\n\n### `llama-index-vector-stores-mariadb` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-mariadb-010 \"Permanent link\")\n\n- Add MariaDB vector store integration package (#15564)\n\n## \\[2024-09-02\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-09-02 \"Permanent link\")\n\n### `llama-index-core` \\[0.11.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-0114 \"Permanent link\")\n\n- Add mypy to core (#14883)\n- Fix incorrect instrumentation fields/types (#15752)\n- FunctionCallingAgent sources bug + light wrapper to create agent (#15783)\n- Add text to sql advanced workflow nb (#15775)\n- fix: remove context after streaming workflow to enable streaming again (#15776)\n- Fix chat memory persisting and loading methods to use correct JSON format (#15545)\n- Fix `_example_type` class var being read as private attr with Pydantic V2 (#15758)\n\n### `llama-index-embeddings-litellm` \\[0.2.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-litellm-021 \"Permanent link\")\n\n- add dimensions param to LiteLLMEmbedding, fix a bug that prevents reading vars from env (#15770)\n\n### `llama-index-embeddings-upstage` \\[0.2.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-upstage-021 \"Permanent link\")\n\n- Bugfix upstage embedding when initializing the UpstageEmbedding class (#15767)\n\n### `llama-index-embeddings-sagemaker-endpoint` \\[0.2.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-sagemaker-endpoint-022 \"Permanent link\")\n\n- Fix Sagemaker Field required issue (#15778)\n\n### `llama-index-graph-stores-falkordb` \\[0.2.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-graph-stores-falkordb-021 \"Permanent link\")\n\n- fix relations upsert with special chars (#15769)\n\n### `llama-index-graph-stores-neo4j` \\[0.3.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-graph-stores-neo4j-031 \"Permanent link\")\n\n- Add native vector index support for neo4j lpg and fix vector filters (#15759)\n\n### `llama-index-llms-azure-inference` \\[0.2.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-azure-inference-022 \"Permanent link\")\n\n- fix: GitHub Models metadata retrieval (#15747)\n\n### `llama-index-llms-bedrock` \\[0.2.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-bedrock-021 \"Permanent link\")\n\n- Update `base.py` to fix `self` issues (#15729)\n\n### `llama-index-llms-ollama` \\[0.3.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-ollama-031_1 \"Permanent link\")\n\n- add ollama response usage (#15773)\n\n### `llama-index-llms-sagemaker-endpoint` \\[0.2.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-sagemaker-endpoint-022 \"Permanent link\")\n\n- Fix Sagemaker Field required issue (#15778)\n\n### `llama-index-multi-modal-llms-anthropic` \\[0.2.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-multi-modal-llms-anthropic-021 \"Permanent link\")\n\n- Support image type detection without knowing the file name (#15763)\n\n### `llama-index-vector-stores-milvus` \\[0.2.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-milvus-022 \"Permanent link\")\n\n- feat: implement get\\_nodes for MilvusVectorStore (#15696)\n\n### `llama-index-vector-stores-tencentvectordb` \\[0.2.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-tencentvectordb-021 \"Permanent link\")\n\n- fix: tencentvectordb inconsistent attribute name (#15733)\n\n## \\[2024-08-29\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-08-29 \"Permanent link\")\n\n### `llama-index-core` \\[0.11.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-0113 \"Permanent link\")\n\n- refact: merge Context and Session to simplify the workflows api (#15709)\n- chore: stop using deprecated `ctx.data` in workflows docs (#15716)\n- fix: stop streaming workflow events when a step raises (#15714)\n- Fix llm\\_chat\\_callback for multimodal llms (#15700)\n- chore: Increase unit tests coverage for the workflow package (#15691)\n- fix SimpleVectorStore.from\\_persist\\_dir() behaviour (#15534)\n\n### `llama-index-embeddings-azure-openai` \\[0.2.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-azure-openai-025 \"Permanent link\")\n\n- fix json serialization for azure embeddings (#15724)\n\n### `llama-index-graph-stores-kuzu` \\[0.3.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-graph-stores-kuzu-030 \"Permanent link\")\n\n- Add KuzuPropertyGraphStore (#15678)\n\n### `llama-index-indices-managed-vectara` \\[0.2.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-indices-managed-vectara-021 \"Permanent link\")\n\n- added new User Defined Function reranker (#15546)\n\n### `llama-index-llms-mistralai` \\[0.2.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-mistralai-022 \"Permanent link\")\n\n- Fix `random_seed` type in mistral llm (#15701)\n\n### `llama-index-llms-nvidia` \\[0.2.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-nvidia-021 \"Permanent link\")\n\n- Add function/tool calling support to nvidia llm (#15359)\n\n### `llama-index-multi-modal-llms-ollama` \\[0.3.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-multi-modal-llms-ollama-030 \"Permanent link\")\n\n- bump ollama client deps for multimodal llm (#15702)\n\n### `llama-index-readers-web` \\[0.2.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-web-021 \"Permanent link\")\n\n- Fix: Firecrawl scraping url response (#15720)\n\n### `llama-index-selectors-notdiamond` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-selectors-notdiamond-010 \"Permanent link\")\n\n- Adding Not Diamond to llama\\_index (#15703)\n\n### `llama-index-vector-stores-milvus` \\[0.2.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-milvus-023 \"Permanent link\")\n\n- MMR in Milvus vector stores (#15634)\n- feat: implement get\\_nodes for MilvusVectorStore (#15696)\n\n## \\[2024-08-27\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-08-27 \"Permanent link\")\n\n### `llama-index-core` \\[0.11.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-0112 \"Permanent link\")\n\n- fix tool schemas generation for pydantic v2 to handle nested models (#15679)\n- feat: support default values for nested workflows (#15660)\n- feat: allow FunctionTool with just an async fn (#15638)\n- feat: Allow streaming events from steps (#15488)\n- fix auto-retriever pydantic indent error (#15648)\n- Implement Router Query Engine example using workflows (#15635)\n- Add multi step query engine example using workflows (#15438)\n- start traces for llm-level operations (#15542)\n- Pass callback\\_manager to init in CodeSplitter from\\_defaults (#15585)\n\n### `llama-index-embeddings-xinference` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-xinference-010 \"Permanent link\")\n\n- Add Xinference Embedding Class (#15579)\n\n### `llama-index-llms-ai21` \\[0.3.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-ai21-033 \"Permanent link\")\n\n- Integrations: AI21 function calling Support (#15622)\n\n### `llama-index-llms-anthropic` \\[0.3.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-anthropic-030 \"Permanent link\")\n\n- Added support for anthropic models through GCP Vertex AI (#15661)\n\n### `llama-index-llms-cerebras` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-cerebras-010 \"Permanent link\")\n\n- Implement Cerebras Integration (#15665)\n\n### `llama-index-postprocessor-nvidia-rerank` \\[0.3.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-postprocessor-nvidia-rerank-031 \"Permanent link\")\n\n- fix downloaded nim endpoint path (#15645)\n- fix llama-index-postprocessor-nvidia-rerank tests (#15643)\n\n### `llama-index-postprocessor-xinference-rerank` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-postprocessor-xinference-rerank-010 \"Permanent link\")\n\n- add xinference rerank class (#15639)\n\n### `llama-index-vector-stores-alibabacloud-opensearch` \\[0.2.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-alibabacloud-opensearch-021 \"Permanent link\")\n\n- fix set output fields in AlibabaCloudOpenSearchConfig (#15562)\n\n### `llama-index-vector-stores-azureaisearch` \\[0.2.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-azureaisearch-021 \"Permanent link\")\n\n- Upgrade azure-search-documents to 2024-07-01 GA API and Add Support for Scalar and Binary Quantization in Index Creation (#15650)\n\n### `llama-index-vector-stores-neo4j` \\[0.2.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-neo4j-021 \"Permanent link\")\n\n- Neo4j Vector Store: Make Embedding Dimension Check Optional (#15628)\n\n### `llama-inde-vector-stores-milvus` \\[0.2.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-inde-vector-stores-milvus-021 \"Permanent link\")\n\n- Change the default consistency level of Milvus (#15577)\n\n### `llama-index-vector-stores-elasticsearch` \\[0.3.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-elasticsearch-032 \"Permanent link\")\n\n- Fix the ElasticsearchStore key error (#15631)\n\n## \\[2024-08-23\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-08-23 \"Permanent link\")\n\n### `llama-index-core` \\[0.11.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-0111 \"Permanent link\")\n\n- Replacing client-side docs search with algolia (#15574)\n- Add docs on extending workflows (#15573)\n- rename method for nested workflows to add\\_workflows (#15596)\n- chore: fix @step usage in the core codebase (#15588)\n- Modify the validate function in ReflectionWorkflow example notebook to use pydantic model\\_validate\\_json method (#15567)\n- feature: allow concurrent runs of the same workflow instance (#15568)\n- docs: remove redundant pass\\_context=True from docs and examples (#15571)\n\n### `llama-index-embeddings-openai` \\[0.2.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-openai-023 \"Permanent link\")\n\n- fix openai embeddings with pydantic v2 (#15576)\n\n### `llama-index-embeddings-voyageai` \\[0.2.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-voyageai-021 \"Permanent link\")\n\n- bump voyage ai embedding client dep (#15595)\n\n### `llama-index-llms-vertex` \\[0.3.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-vertex-033 \"Permanent link\")\n\n- Vertex LLM: Correctly add function calling part to prompt (#15569)\n- Vertex LLM: Remove manual setting of message content to Function Calling (#15586)\n\n## \\[2024-08-22\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-08-22 \"Permanent link\")\n\n### `llama-index-core` \\[0.11.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-0110 \"Permanent link\")\n\n- removed deprecated `ServiceContext` \\-\\- using this now will print an error with a link to the migration guide\n- removed deprecated `LLMPredictor` \\-\\- using this now will print an error, any existing LLM is a drop-in replacement\n- made `pandas` an optional dependency\n\n### `Everything Else` [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#everything-else \"Permanent link\")\n\n- bumped the minor version of every package to account for the new version of `llama-index-core`\n\n## \\[2024-08-21\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-08-21 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.68\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01068 \"Permanent link\")\n\n- remove nested progress bars in base element node parser (#15550)\n- Adding exhaustive docs for workflows (#15556)\n- Adding multi-strategy workflow with reflection notebook example (#15445)\n- remove openai dep from core (#15527)\n- Improve token counter to handle more response types (#15501)\n- feat: Allow using step decorator without parentheses (#15540)\n- feat: workflow services (aka nested workflows) (#15325)\n- Remove requirement to specify \"allowed\\_query\\_fields\" parameter when using \"cypher\\_validator\" in TextToCypher retriever (#15506)\n\n### `llama-index-embeddings-mistralai` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-mistralai-016 \"Permanent link\")\n\n- fix mistral embeddings usage (#15508)\n\n### `llama-index-embeddings-ollama` \\[0.2.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-ollama-020 \"Permanent link\")\n\n- use ollama client for embeddings (#15478)\n\n### `llama-index-embeddings-openvino` \\[0.2.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-openvino-021 \"Permanent link\")\n\n- support static input shape for openvino embedding and reranker (#15521)\n\n### `llama-index-graph-stores-neptune` \\[0.1.8\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-graph-stores-neptune-018 \"Permanent link\")\n\n- Added code to expose structured schema for Neptune (#15507)\n\n### `llama-index-llms-ai21` \\[0.3.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-ai21-032 \"Permanent link\")\n\n- Integration: AI21 Tools support (#15518)\n\n### `llama-index-llms-bedrock` \\[0.1.13\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-bedrock-0113 \"Permanent link\")\n\n- Support token counting for llama-index integration with bedrock (#15491)\n\n### `llama-index-llms-cohere` \\[0.2.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-cohere-022 \"Permanent link\")\n\n- feat: add tool calling support for achat cohere (#15539)\n\n### `llama-index-llms-gigachat` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-gigachat-010 \"Permanent link\")\n\n- Adding gigachat LLM support (#15313)\n\n### `llama-index-llms-openai` \\[0.1.31\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-openai-0131 \"Permanent link\")\n\n- Fix incorrect type in OpenAI token usage report (#15524)\n- allow streaming token counts for openai (#15548)\n\n### `llama-index-postprocessor-nvidia-rerank` \\[0.2.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-postprocessor-nvidia-rerank-021 \"Permanent link\")\n\n- add truncate support (#15490)\n- Update to 0.2.0, remove old code (#15533)\n- update default model to nvidia/nv-rerankqa-mistral-4b-v3 (#15543)\n\n### `llama-index-readers-bitbucket` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-bitbucket-014 \"Permanent link\")\n\n- Fixing the issues in loading file paths from bitbucket (#15311)\n\n### `llama-index-readers-google` \\[0.3.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-google-031 \"Permanent link\")\n\n- enhance google drive reader for improved functionality and usability (#15512)\n\n### `llama-index-readers-remote` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-remote-016 \"Permanent link\")\n\n- check and sanitize remote reader urls (#15494)\n\n### `llama-index-vector-stores-qdrant` \\[0.2.17\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-qdrant-0217 \"Permanent link\")\n\n- fix: setting IDF modifier in QdrantVectorStore for sparse vectors (#15538)\n\n## \\[2024-08-18\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-08-18 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.67\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01067 \"Permanent link\")\n\n- avoid nltk 3.9 since its broken (#15473)\n- docs: openllmetry now uses instrumentation (#15443)\n- Fix LangChainDeprecationWarning (#15397)\n- Add get/set API to the Context and make it coroutine-safe (#15152)\n- docs: Cleanlab's cookbook (#15352)\n- pass kwargs in `async_add()` for vector stores (#15333)\n- escape json in structured llm (#15404)\n- docs: Add JSONAlyze Query Engine using workflows cookbook (#15408)\n\n### `llama-index-embeddings-gigachat` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-gigachat-010 \"Permanent link\")\n\n- Add GigaChat embedding (#15278)\n\n### `llama-index-finetuning` \\[0.1.12\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-finetuning-0112 \"Permanent link\")\n\n- feat: Integrating Azure OpenAI Finetuning (#15297)\n\n### `llama-index-graph-stores-neptune` \\[0.1.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-graph-stores-neptune-017 \"Permanent link\")\n\n- Exposed NeptuneQueryException and added additional debug information (#15448)\n- Fixed issue #15414 and added ability to do partial matchfor Neptune Analytics (#15415)\n- Use backticks to escape label (#15324)\n\n### `llama-index-llms-cohere` \\[0.2.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-cohere-021 \"Permanent link\")\n\n- feat: add tool calling for cohere (#15144)\n\n### `llama-index-packs-corrective-rag` \\[0.1.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-packs-corrective-rag-012 \"Permanent link\")\n\n- Ports over LongRAGPack, Corrective RAG Pack, and Self-Discover Pack to Workflows (#15160)\n\n### `llama-index-packs-longrag` \\[0.1.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-packs-longrag-011 \"Permanent link\")\n\n- Ports over LongRAGPack, Corrective RAG Pack, and Self-Discover Pack to Workflows (#15160)\n\n### `llama-index-packs-self-discover` \\[0.1.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-packs-self-discover-012 \"Permanent link\")\n\n- Ports over LongRAGPack, Corrective RAG Pack, and Self-Discover Pack to Workflows (#15160)\n\n### `llama-index-readers-preprocess` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-preprocess-014 \"Permanent link\")\n\n- Enhance PreprocessReader (#15302)\n\n## \\[2024-08-15\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-08-15 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.66\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01066 \"Permanent link\")\n\n- Temporarily revert nltk dependency due to latest version being removed from pypi\n- Add citation query engine with workflows example (#15372)\n- bug: Semantic double merging splitter creates chunks larger thank chunk size (#15188)\n- feat: make `send_event()` in workflows assign the target step (#15259)\n- make all workflow events accessible like mappings (#15310)\n\n### `llama-index-indices-managed-bge-m3` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-indices-managed-bge-m3-010 \"Permanent link\")\n\n- Add BGEM3Index (#15197)\n\n### `llama-index-llms-huggingface` \\[0.2.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-huggingface-027 \"Permanent link\")\n\n- update HF's completion\\_to\\_prompt (#15354)\n\n### `llama-index-llms-sambanova` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-sambanova-010 \"Permanent link\")\n\n- Wrapper for SambaNova (Sambaverse and SambaStudio) with Llama-index (#15220)\n\n### `llama-index-packs-code-hierarchy` \\[0.1.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-packs-code-hierarchy-017 \"Permanent link\")\n\n- Update code\\_hierarchy.py adding php support (#15145)\n\n### `llama-index-postprocessor-dashscope-rerank` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-postprocessor-dashscope-rerank-014 \"Permanent link\")\n\n- fix bug when calling llama-index-postprocessor-dashscope-rerank (#15358)\n\n### `llama-index-readers-box` \\[0.1.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-box-012 \"Permanent link\")\n\n- Box refactor: Box File to Llama-Index Document adaptor (#15314)\n\n### `llama-index-readers-gcs` \\[0.1.8\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-gcs-018 \"Permanent link\")\n\n- GCSReader: Implementing ResourcesReaderMixin and FileSystemReaderMixin (#15365)\n\n### `llama-index-tools-box` \\[0.1.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-tools-box-011 \"Permanent link\")\n\n- Box refactor: Box File to Llama-Index Document adaptor (#15314)\n- Box tools for AI Agents (#15236)\n\n### `llama-index-vector-stores-postgres` \\[0.1.14\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-postgres-0114 \"Permanent link\")\n\n- Check if hnsw index exists (#15287)\n\n## \\[2024-08-12\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-08-12 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.65\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01065 \"Permanent link\")\n\n- chore: bump nltk version (#15277)\n\n### `llama-index-tools-box` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-tools-box-010 \"Permanent link\")\n\n- Box tools for AI Agents (#15236)\n\n### `llama-index-multi-modal-llms-gemini` \\[0.1.8\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-multi-modal-llms-gemini-018 \"Permanent link\")\n\n- feat: add default\\_headers to Gemini multi-model (#15296)\n\n### `llama-index-vector-stores-clickhouse` \\[0.2.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-clickhouse-020 \"Permanent link\")\n\n- chore: stop using ServiceContext from the clickhouse integration (#15300)\n\n### `llama-index-experimental` \\[0.2.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-experimental-020 \"Permanent link\")\n\n- chore: remove ServiceContext usage from experimental package (#15301)\n\n### `llama-index-extractors-marvin` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-extractors-marvin-014 \"Permanent link\")\n\n- fix: MarvinMetadataExtractor functionality and apply async support (#15247)\n\n### `llama-index-utils-workflow` \\[0.1.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-utils-workflow-011 \"Permanent link\")\n\n- chore: bump black version (#15288)\n- chore: bump nltk version (#15277)\n\n### `llama-index-readers-microsoft-onedrive` \\[0.1.9\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-microsoft-onedrive-019 \"Permanent link\")\n\n- chore: bump nltk version (#15277)\n\n### `llama-index-embeddings-upstage` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-upstage-013 \"Permanent link\")\n\n- chore: bump nltk version (#15277)\n\n### `llama-index-embeddings-nvidia` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-nvidia-015 \"Permanent link\")\n\n- chore: bump nltk version (#15277)\n\n### `llama-index-embeddings-litellm` \\[0.1.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-litellm-011 \"Permanent link\")\n\n- chore: bump nltk version (#15277)\n\n### `llama-index-legacy` \\[0.9.48post1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-legacy-0948post1 \"Permanent link\")\n\n- chore: bump nltk version (#15277)\n\n### `llama-index-packs-streamlit-chatbot` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-packs-streamlit-chatbot-015 \"Permanent link\")\n\n- chore: bump nltk version (#15277)\n\n### `llama-index-embeddings-huggingface` \\[0.2.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-huggingface-023 \"Permanent link\")\n\n- Feature: added multiprocessing for creating hf embedddings (#15260)\n\n## \\[2024-08-09\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-08-09 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.64\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01064 \"Permanent link\")\n\n- fix: children nodes not carrying metadata from source nodes (#15254)\n- Workflows: fix the validation error in the decorator (#15252)\n- fix: strip '''sql (Markdown SQL code snippet) in SQL Retriever (#15235)\n\n### `llama-index-indices-managed-colbert` \\[0.2.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-indices-managed-colbert-020 \"Permanent link\")\n\n- Remove usage of ServiceContext in Colbert integration (#15249)\n\n### `llama-index-vector-stores-milvus` \\[0.1.23\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-milvus-0123 \"Permanent link\")\n\n- feat: Support Milvus collection properties (#15241)\n\n### `llama-index-llms-cleanlab` \\[0.1.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-cleanlab-012 \"Permanent link\")\n\n- Update models supported by Cleanlab TLM (#15240)\n\n### `llama-index-llms-huggingface` \\[0.2.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-huggingface-026 \"Permanent link\")\n\n- add generation prompt to HF chat template (#15239)\n\n### `llama-index-llms-openvino` \\[0.2.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-openvino-021 \"Permanent link\")\n\n- add generation prompt to HF chat template (#15239)\n\n### `llama-index-graph-stores-neo4j` \\[0.2.14\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-graph-stores-neo4j-0214 \"Permanent link\")\n\n- Neo4jPropertyGraphStore.get() check for id prop (#15228)\n\n### `llama-index-readers-file` \\[0.1.33\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-file-0133 \"Permanent link\")\n\n- Fix fs.open path type (#15226)\n\n## \\[2024-08-08\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-08-08 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.63\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01063 \"Permanent link\")\n\n- add num\\_workers in workflow decorator to resolve step concurrancy issue (#15210)\n- Sub Question Query Engine as workflow notebook example (#15209)\n- Add Llamatrace to workflow notebooks (#15186)\n- Use node hash instead of node text to match nodes in fusion retriever (#15172)\n\n### `llama-index-embeddings-mistralai` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-mistralai-015 \"Permanent link\")\n\n- handle mistral v1.0 client (#15229)\n\n### `llama-index-extractors-relik` \\[0.1.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-extractors-relik-011 \"Permanent link\")\n\n- Fix relik extractor skip error (#15225)\n\n### `llama-index-finetuning` \\[0.1.11\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-finetuning-0111 \"Permanent link\")\n\n- handle mistral v1.0 client (#15229)\n\n### `llama-index-graph-stores-neo4j` \\[0.2.14\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-graph-stores-neo4j-0214_1 \"Permanent link\")\n\n- Add neo4j generic node label (#15191)\n\n### `llama-index-llms-anthropic` \\[0.1.17\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-anthropic-0117 \"Permanent link\")\n\n- Allow for images in Anthropic messages (#15227)\n\n### `llama-index-llms-mistralai` \\[0.1.20\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-mistralai-0120 \"Permanent link\")\n\n- handle mistral v1.0 client (#15229)\n\n### `llama-index-packs-mixture-of-agents` \\[0.1.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-packs-mixture-of-agents-012 \"Permanent link\")\n\n- Update Mixture Of Agents llamapack with workflows (#15232)\n\n### `llama-index-tools-slack` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-tools-slack-014 \"Permanent link\")\n\n- Fixed slack client ref in ToolSpec (#15202)\n\n## \\[2024-08-06\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-08-06 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.62\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01062 \"Permanent link\")\n\n- feat: Allow None metadata filter by using IS\\_EMPTY operator (#15167)\n- fix: use parent source node to node relationships if possible during node parsing (#15182)\n- Use node hash instead of node text to match nodes in fusion retriever (#15172)\n\n### `llama-index-graph-stores-neo4j` \\[0.2.13\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-graph-stores-neo4j-0213 \"Permanent link\")\n\n- Neo4j property graph client side batching (#15179)\n\n### `llama-index-graph-stores-neptune` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-graph-stores-neptune-014 \"Permanent link\")\n\n- PropertyGraphStore support for Amazon Neptune (#15126)\n\n### `llama-index-llms-gemini` \\[0.2.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-gemini-020 \"Permanent link\")\n\n- feat: add default\\_headers to Gemini model (#15141)\n\n### `llama-index-llms-openai` \\[0.1.28\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-openai-0128 \"Permanent link\")\n\n- OpenAI: Support new strict functionality in tool param (#15177)\n\n### `llama-index-vector-stores-opensearch` \\[0.1.14\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-opensearch-0114 \"Permanent link\")\n\n- Add support for full MetadataFilters in Opensearch (#15176)\n\n### `llama-index-vector-stores-qdrant` \\[0.2.15\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-qdrant-0215 \"Permanent link\")\n\n- feat: Allow None metadata filter by using IS\\_EMPTY operator (#15167)\n\n### `llama-index-vector-stores-wordlift` \\[0.3.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-wordlift-030 \"Permanent link\")\n\n- Add support for fields projection and update sample Notebook (#15140)\n\n## \\[2024-08-05\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-08-05 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.61\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01061 \"Permanent link\")\n\n- Tweaks to workflow docs (document `.send_event()`, expand examples) (#15154)\n- Create context manager to instrument event and span tags (#15116)\n- keyval index store index store updated to accept custom collection suffix (#15134)\n- make workflow context able to collect multiples of the same event (#15153)\n- Fix `__str__` method for AsyncStreamingResponse (#15131)\n\n### `llama-index-callbacks-literalai` \\[1.0.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-callbacks-literalai-100 \"Permanent link\")\n\n- feat(integration): add a global handler for Literal AI (#15064)\n\n### `llama-index-extractors-relik` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-extractors-relik-010 \"Permanent link\")\n\n- Add relik kg constructor (#15123)\n\n### `llama-index-graph-stores-neo4j` \\[0.1.12\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-graph-stores-neo4j-0112 \"Permanent link\")\n\n- fix neo4j property graph relation properties when querying (#15068)\n\n### `llama-index-llms-fireworks` \\[0.1.9\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-fireworks-019 \"Permanent link\")\n\n- feat: add default\\_headers to Fireworks llm (#15150)\n\n### `llama-index-llms-gemini` \\[0.1.12\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-gemini-0112 \"Permanent link\")\n\n- Fix: Gemini 1.0 Pro Vision has been official deprecated, switch default model to gemini-1.5-flash (#15000)\n\n### `llama-index-llms-paieas` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-paieas-010 \"Permanent link\")\n\n- Add LLM for AlibabaCloud PaiEas (#14983)\n\n### `llama-index-llms-predibase` \\[0.1.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-predibase-017 \"Permanent link\")\n\n- Fix Predibase Integration for HuggingFace-hosted fine-tuned adapters (#15130)\n\n## \\[2024-02-02\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-02-02 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.60\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01060 \"Permanent link\")\n\n- update `StartEvent` usage to allow for dot notation attribute access (#15124)\n- Add GraphRAGV2 notebook (#15119)\n- Fixed minor bug in DynamicLLMPathExtractor as well as default output parsers not working (#15085)\n- update typing for workflow timeouts (#15102)\n- fix(sql\\_wrapper): dont mention foreign keys when there is none (#14998)\n\n### `llama-index-graph-stores-neo4j` \\[0.2.11\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-graph-stores-neo4j-0211 \"Permanent link\")\n\n- fix neo4j retrieving relation properties (#15111) (#15108)\n\n### `llama-index-llms-vllm` \\[0.1.9\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-vllm-019 \"Permanent link\")\n\n- Update base.py to use @atexit for cleanup (#15047)\n\n### `llama-index-vector-stores-pinecone` \\[0.1.9\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-pinecone-019 \"Permanent link\")\n\n- bump pinecone client version deps (#15121)\n\n### `llama-index-vector-stores-redis` \\[0.2.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-redis-021 \"Permanent link\")\n\n- Handle nested MetadataFilters for Redis vector store (#15093)\n\n### `llama-index-vector-stores-wordlift` \\[0.2.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-wordlift-020 \"Permanent link\")\n\n- Update WordLift Vector Store to use new client package (#15045)\n\n## \\[2024-07-31\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-07-31 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.59\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01059 \"Permanent link\")\n\n- Introduce `Workflow` s for event-driven orchestration (#15067)\n- Added feature to context chat engine allowing previous chunks to be inserted into the current context window (#14889)\n- MLflow Integration added to docs (#14977)\n- docs(literalai): add Literal AI integration to documentation (#15023)\n- expand span coverage for query pipeline (#14997)\n- make re-raising error skip constructor during `asyncio_run()` (#14970)\n\n### `llama-index-embeddings-ollama` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-ollama-013 \"Permanent link\")\n\n- Add proper async embedding support\n\n### `llama-index-embeddings-textembed` \\[0.0.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-textembed-001 \"Permanent link\")\n\n- add support for textembed embedding (#14968)\n\n### `llama-index-graph-stores-falkordb` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-graph-stores-falkordb-015 \"Permanent link\")\n\n- initial implementation FalkorDBPropertyGraphStore (#14936)\n\n### `llama-index-llms-azure-inference` \\[0.1.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-azure-inference-011 \"Permanent link\")\n\n- Fix: Azure AI inference integration support for tools (#15044)\n\n### `llama-index-llms-fireworks` \\[0.1.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-fireworks-017 \"Permanent link\")\n\n- Updates to Default model for support for function calling (#15046)\n\n### `llama-index-llms-ollama` \\[0.2.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-ollama-022 \"Permanent link\")\n\n- toggle for ollama function calling (#14972)\n- Add function calling for Ollama (#14948)\n\n### `llama-index-llms-openllm` \\[0.2.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-openllm-020 \"Permanent link\")\n\n- update to OpenLLM 0.6 (#14935)\n\n### `llama-index-packs-longrag` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-packs-longrag-010 \"Permanent link\")\n\n- Adds a LlamaPack that implements LongRAG (#14916)\n\n### `llama-index-postprocessor-tei-rerank` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-postprocessor-tei-rerank-010 \"Permanent link\")\n\n- Support for Re-Ranker via Text Embedding Interface (#15063)\n\n### `llama-index-readers-confluence` \\[0.1.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-confluence-017 \"Permanent link\")\n\n- confluence reader sort auth parameters priority (#14905)\n\n### `llama-index-readers-file` \\[0.1.31\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-file-0131 \"Permanent link\")\n\n- UnstructuredReader use filename as ID (#14946)\n\n### `llama-index-readers-gitlab` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-gitlab-010 \"Permanent link\")\n\n- Add GitLab reader integration (#15030)\n\n### `llama-index-readers-google` \\[0.2.11\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-google-0211 \"Permanent link\")\n\n- Fix issue with average ratings being a float vs an int (#15070)\n\n### `llama-index-retrievers-bm25` \\[0.2.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-retrievers-bm25-022 \"Permanent link\")\n\n- use proper stemmer in bm25 tokenize (#14965)\n\n### `llama-index-vector-stores-azureaisearch` \\[0.1.13\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-azureaisearch-0113 \"Permanent link\")\n\n- Fix issue with deleting non-existent index (#14949)\n\n### `llama-index-vector-stores-elasticsearch` \\[0.2.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-elasticsearch-025 \"Permanent link\")\n\n- disable embeddings for sparse strategy (#15032)\n\n### `llama-index-vector-stores-kdbai` \\[0.2.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-kdbai-020 \"Permanent link\")\n\n- Update default sparse encoder for Hybrid search (#15019)\n\n### `llama-index-vector-stores-milvus` \\[0.1.22\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-milvus-0122 \"Permanent link\")\n\n- Enhance MilvusVectorStore with flexible index management for overwriting (#15058)\n\n### `llama-index-vector-stores-postgres` \\[0.1.13\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-postgres-0113 \"Permanent link\")\n\n- Adds option to construct PGVectorStore with a HNSW index (#15024)\n\n## \\[2024-07-24\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-07-24 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.58\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01058 \"Permanent link\")\n\n- Fix: Token counter expecting response.raw as dict, got ChatCompletionChunk (#14937)\n- Return proper tool outputs per agent step instead of all (#14885)\n- Minor bug fixes to async structured streaming (#14925)\n\n### `llama-index-llms-fireworks` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-fireworks-016 \"Permanent link\")\n\n- fireworks ai llama3.1 support (#14914)\n\n### `llama-index-multi-modal-llms-anthropic` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-multi-modal-llms-anthropic-016 \"Permanent link\")\n\n- Add claude 3.5 sonnet to multi modal llms (#14932)\n\n### `llama-index-retrievers-bm25` \\[0.2.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-retrievers-bm25-021 \"Permanent link\")\n\n- \ud83d\udc1e fix(integrations): BM25Retriever persist missing arg similarity\\_top\\_k (#14933)\n\n### `llama-index-retrievers-vertexai-search` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-retrievers-vertexai-search-010 \"Permanent link\")\n\n- Llamaindex retriever for Vertex AI Search (#14913)\n\n### `llama-index-vector-stores-deeplake` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-deeplake-015 \"Permanent link\")\n\n- Improved `deeplake.get_nodes()` performance (#14920)\n\n### `llama-index-vector-stores-elasticsearch` \\[0.2.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-elasticsearch-023 \"Permanent link\")\n\n- Bugfix: Don't pass empty list of embeddings to elasticsearch store when using sparse strategy (#14918)\n\n### `llama-index-vector-stores-lindorm` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-lindorm-010 \"Permanent link\")\n\n- Add vector store integration of lindorm (#14623)\n\n### `llama-index-vector-stores-qdrant` \\[0.2.14\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-qdrant-0214 \"Permanent link\")\n\n- feat: allow to limit how many elements retrieve (qdrant) (#14904)\n\n## \\[2024-07-22\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-07-22 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.57\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01057 \"Permanent link\")\n\n- Add an optional parameter similarity\\_score to VectorContextRetrieve\u2026 (#14831)\n- add property extraction (using property names and optional descriptions) for KGs (#14707)\n- able to attach output classes to LLMs (#14747)\n- Add streaming for tool calling / structured extraction (#14759)\n- fix from removing private variables when copying/pickling (#14860)\n- Fix empty array being send to vector store in ingestion pipeline (#14859)\n- optimize ingestion pipeline deduping (#14858)\n- Add an optional parameter similarity\\_score to VectorContextRetriever (#14831)\n\n### `llama-index-llms-azure-openai` \\[0.1.10\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-azure-openai-0110 \"Permanent link\")\n\n- Bugfix: AzureOpenAI may fail with custom azure\\_ad\\_token\\_provider (#14869)\n\n### `llama-index-llms-bedrock-converse` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-bedrock-converse-015 \"Permanent link\")\n\n- feat: \u2728 Implement async functionality in BedrockConverse (#14326)\n\n### `llama-index-llms-langchain` \\[0.3.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-langchain-030 \"Permanent link\")\n\n- make some dependencies optional\n- bump langchain version in integration (#14879)\n\n### `llama-index-llms-ollama` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-ollama-016 \"Permanent link\")\n\n- Bugfix: ollama streaming response (#14830)\n\n### `llama-index-multi-modal-llms-anthropic` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-multi-modal-llms-anthropic-015 \"Permanent link\")\n\n- align deps (#14850)\n\n### `llama-index-readers-notion` \\[0.1.10\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-notion-0110 \"Permanent link\")\n\n- update notion reader to handle duplicate pages, database+page ids (#14861)\n\n### `llama-index-vector-stores-milvus` \\[0.1.21\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-milvus-0121 \"Permanent link\")\n\n- Implements delete\\_nodes() and clear() for Weviate, Opensearch, Milvus, Postgres, and Pinecone Vector Stores (#14800)\n\n### `llama-index-vector-stores-mongodb` \\[0.1.8\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-mongodb-018 \"Permanent link\")\n\n- MongoDB Atlas Vector Search: Enhanced Metadata Filtering (#14856)\n\n### `llama-index-vector-stores-opensearch` \\[0.1.13\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-opensearch-0113 \"Permanent link\")\n\n- Implements delete\\_nodes() and clear() for Weviate, Opensearch, Milvus, Postgres, and Pinecone Vector Stores (#14800)\n\n### `llama-index-vector-stores-pinecone` \\[0.1.8\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-pinecone-018 \"Permanent link\")\n\n- Implements delete\\_nodes() and clear() for Weviate, Opensearch, Milvus, Postgres, and Pinecone Vector Stores (#14800)\n\n### `llama-index-vector-stores-postgres` \\[0.1.12\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-postgres-0112 \"Permanent link\")\n\n- Implements delete\\_nodes() and clear() for Weviate, Opensearch, Milvus, Postgres, and Pinecone Vector Stores (#14800)\n\n### `llama-index-vector-stores-weaviate` \\[1.0.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-weaviate-102 \"Permanent link\")\n\n- Implements delete\\_nodes() and clear() for Weviate, Opensearch, Milvus, Postgres, and Pinecone Vector Stores (#14800)\n\n## \\[2024-07-19\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-07-19 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.56\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01056 \"Permanent link\")\n\n- Fixing the issue where the \\_apply\\_node\\_postprocessors function needs QueryBundle (#14839)\n- Add Context-Only Response Synthesizer (#14439)\n- Fix AgentRunner AgentRunStepStartEvent dispatch (#14828)\n- Improve output format system prompt in ReAct agent (#14814)\n- Remove double curly replacing from output parser utils (#14735)\n- Update simple\\_summarize.py (#14714)\n\n### `llama-index-tools-azure-code-interpreter` \\[0.2.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-tools-azure-code-interpreter-020 \"Permanent link\")\n\n- chore: read AZURE\\_POOL\\_MANAGEMENT\\_ENDPOINT from env vars (#14732)\n\n### `llama-index-llms-azure-inference` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-azure-inference-010 \"Permanent link\")\n\n- Azure AI Inference integration (#14672)\n\n### `llama-index-embeddings-azure-inference` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-azure-inference-010 \"Permanent link\")\n\n- Azure AI Inference integration (#14672)\n\n### `llama-index-llms-bedrock-converse` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-bedrock-converse-015_1 \"Permanent link\")\n\n- feat: \u2728 Implement async functionality in BedrockConverse (#14326)\n\n### `llama-index-embeddings-yandexgpt` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-yandexgpt-015 \"Permanent link\")\n\n- Add new integration for YandexGPT Embedding Model (#14313)\n\n### `llama-index-tools-google` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-tools-google-016 \"Permanent link\")\n\n- Update docstring for gmailtoolspec's search\\_messages tool (#14840)\n\n### `llama-index-postprocessor-nvidia-rerank` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-postprocessor-nvidia-rerank-015 \"Permanent link\")\n\n- add support for nvidia/nv-rerankqa-mistral-4b-v3 (#14844)\n\n### `llama-index-embeddings-openai` \\[0.1.11\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-openai-0111 \"Permanent link\")\n\n- Fix OpenAI Embedding async client bug (#14835)\n\n### `llama-index-embeddings-azure-openai` \\[0.1.11\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-azure-openai-0111 \"Permanent link\")\n\n- Fix Azure OpenAI LLM and Embedding async client bug (#14833)\n\n### `llama-index-llms-azure-openai` \\[0.1.9\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-azure-openai-019 \"Permanent link\")\n\n- Fix Azure OpenAI LLM and Embedding async client bug (#14833)\n\n### `llama-index-multi-modal-llms-openai` \\[0.1.8\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-multi-modal-llms-openai-018 \"Permanent link\")\n\n- Add support for gpt-4o-mini (#14820)\n\n### `llama-index-llms-openai` \\[0.1.26\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-openai-0126 \"Permanent link\")\n\n- Add support for gpt-4o-mini (#14820)\n\n### `llama-index-llms-mistralai` \\[0.1.18\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-mistralai-0118 \"Permanent link\")\n\n- Add support for mistralai nemo model (#14819)\n\n### `llama-index-graph-stores-neo4j` \\[0.2.8\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-graph-stores-neo4j-028 \"Permanent link\")\n\n- Fix bug when sanitize is used in neo4j property graph (#14812)\n- Add filter to get\\_triples in neo4j (#14811)\n\n### `llama-index-vector-stores-azureaisearch` \\[0.1.12\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-azureaisearch-0112 \"Permanent link\")\n\n- feat: add nested filters for azureaisearch (#14795)\n\n### `llama-index-vector-stores-qdrant` \\[0.2.13\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-qdrant-0213 \"Permanent link\")\n\n- feat: Add NOT IN filter for Qdrant vector store (#14791)\n\n### `llama-index-vector-stores-azureaisearch` \\[0.1.11\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-azureaisearch-0111 \"Permanent link\")\n\n- feat: add azureaisearch supported conditions (#14787)\n- feat: azureaisearch support collection string (#14712)\n\n### `llama-index-tools-weather` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-tools-weather-014 \"Permanent link\")\n\n- Fix OpenWeatherMapToolSpec.forecast\\_tommorrow\\_at\\_location (#14745)\n\n### `llama-index-readers-microsoft-sharepoint` \\[0.2.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-microsoft-sharepoint-026 \"Permanent link\")\n\n- follow odata.nextLink (#14708)\n\n### `llama-index-vector-stores-qdrant` \\[0.2.12\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-qdrant-0212 \"Permanent link\")\n\n- Adds Quantization option to QdrantVectorStore (#14740)\n\n### `llama-index-vector-stores-azureaisearch` \\[0.1.10\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-azureaisearch-0110 \"Permanent link\")\n\n- feat: improve azureai search deleting (#14693)\n\n### `llama-index-agent-openai` \\[0.2.9\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-agent-openai-029 \"Permanent link\")\n\n- fix: tools are required for attachments in openai api (#14609)\n\n### `llama-index-readers-box` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-box-010 \"Permanent link\")\n\n- new integration\n\n### `llama-index-embeddings-fastembed` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-fastembed-016 \"Permanent link\")\n\n- fix fastembed python version (#14710)\n\n## \\[2024-07-11\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-07-11 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.55\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01055 \"Permanent link\")\n\n- Various docs updates\n\n### `llama-index-llms-cleanlab` \\[0.1.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-cleanlab-011 \"Permanent link\")\n\n- Add user configurations for Cleanlab LLM integration (#14676)\n\n### `llama-index-readers-file` \\[0.1.30\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-file-0130 \"Permanent link\")\n\n- race between concurrent pptx readers over a single temp filename (#14686)\n\n### `llama-index-tools-exa` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-tools-exa-014 \"Permanent link\")\n\n- changes to Exa search tool getting started and example notebook (#14690)\n\n## \\[2024-07-10\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-07-10 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.54\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01054 \"Permanent link\")\n\n- fix: update operator logic for simple vector store filter (#14674)\n- Add AgentOps integration (#13935)\n\n### `llama-index-embeddings-fastembed` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-fastembed-015 \"Permanent link\")\n\n- chore: update required python version in Qdrant fastembed package (#14677)\n\n### `llama-index-embeddings-huggingface-optimum-intel` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-huggingface-optimum-intel-016 \"Permanent link\")\n\n- Bump version llama-index-embeddings-huggingface-optimum-intel (#14670)\n\n### `llama-index-vector-stores-elasticsearch` \\[0.2.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-elasticsearch-022 \"Permanent link\")\n\n- Added support for custom index settings (#14655)\n\n### `llama-index-callbacks-agentops` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-callbacks-agentops-010 \"Permanent link\")\n\n- Initial release\n\n### `llama-index-indices-managed-vertexai` \\[0.0.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-indices-managed-vertexai-002 \"Permanent link\")\n\n- Fix #14637 Llamaindex managed Vertex AI index needs to be updated. (#14641)\n\n### `llama-index-readers-file` \\[0.1.29\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-file-0129 \"Permanent link\")\n\n- fix unstructured import in simple file reader (#14642)\n\n## \\[2024-07-08\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-07-08 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.53\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01053 \"Permanent link\")\n\n- fix handling react usage in `llm.predict_and_call` for llama-agents (#14556)\n- add the missing arg verbose when `ReActAgent` calling `super().__init__` (#14565)\n- fix `llama-index-core\\llama_index\\core\\node_parser\\text\\utils.py` error when use IngestionPipeline parallel (#14560)\n- deprecate `KnowledgeGraphIndex`, tweak docs (#14575)\n- Fix `ChatSummaryMemoryBuffer` fails to summary chat history with tool callings (#14563)\n- Added `DynamicLLMPathExtractor` for Entity Detection With a Schema inferred by LLMs on the fly (#14566)\n- add cloud document converter (#14608)\n- fix KnowledgeGraphIndex arg 'kg\\_triple\\_extract\\_template' typo error (#14619)\n- Fix: Update `UnstructuredElementNodeParser` due to change in unstructured (#14606)\n- Update ReAct Step to solve issue with incomplete generation (#14587)\n\n### `llama-index-callbacks-promptlayer` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-callbacks-promptlayer-013 \"Permanent link\")\n\n- Conditions logging to promptlayer on successful request (#14632)\n\n### `llama-index-embeddings-databricks` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-databricks-010 \"Permanent link\")\n\n- Add integration embeddings databricks (#14590)\n\n### `llama-index-llms-ai21` \\[0.3.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-ai21-031 \"Permanent link\")\n\n- Fix MessageRole import from the wrong package in AI21 Package (#14596)\n\n### `llama-index-llms-bedrock` \\[0.1.12\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-bedrock-0112 \"Permanent link\")\n\n- handle empty response in Bedrock AnthropicProvider (#14479)\n- add claude 3.5 sonnet support to Bedrock InvokeAPI (#14594)\n\n### `llama-index-llms-bedrock-converse` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-bedrock-converse-014 \"Permanent link\")\n\n- Fix Bedrock Converse's tool use blocks, when there are multiple consecutive function calls (#14386)\n\n### `llama-index-llms-optimum-intel` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-optimum-intel-010 \"Permanent link\")\n\n- add optimum intel with ipex backend to llama-index-integration (#14553)\n\n### `llama-index-llms-qianfan` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-qianfan-010 \"Permanent link\")\n\n- add baidu-qianfan llm (#14414)\n\n### `llama-index-llms-text-generation-inference` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-text-generation-inference-014 \"Permanent link\")\n\n- fix: crash LLMMetadata in model name lookup (#14569)\n- Remove hf embeddings dep from text-embeddings-inference (#14592)\n\n### `llama-index-llms-yi` \\[0.1.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-yi-011 \"Permanent link\")\n\n- update yi llm context\\_window (#14578)\n\n### `llama-index-readers-file` \\[0.1.28\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-file-0128 \"Permanent link\")\n\n- add fs arg to PandasExcelReader.load\\_data (#14554)\n- UnstructuredReader enhancements (#14390)\n\n### `llama-index-readers-web` \\[0.1.22\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-web-0122 \"Permanent link\")\n\n- nit: firecrawl fixes for creating documents (#14579)\n\n### `llama-index-retrievers-bm25` \\[0.2.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-retrievers-bm25-020 \"Permanent link\")\n\n- Update BM25Retriever to use newer (and faster) bm25s library #(14581)\n\n### `llama-index-vector-stores-qdrant` \\[0.2.11\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-qdrant-0211 \"Permanent link\")\n\n- refactor: Don't swallow exceptions from Qdrant collection\\_exists (#14564)\n- add support for qdrant bm42, setting sparse + dense configs (#14577)\n\n## \\[2024-07-03\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-07-03 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.52\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01052 \"Permanent link\")\n\n- fix file reader path bug on windows (#14537)\n- follow up with kwargs propagation in colbert index due to change in parent class (#14522)\n- deprecate query pipeline agent in favor of FnAgentWorker (#14525O)\n\n### `llama-index-callbacks-arize-phoenix` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-callbacks-arize-phoenix-016 \"Permanent link\")\n\n- support latest version of arize #14526\n\n### `llama-index-embeddings-litellm` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-litellm-010 \"Permanent link\")\n\n- Add support for LiteLLM Proxy Server for embeddings (#14523)\n\n### `llama-index-finetuning` \\[0.1.10\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-finetuning-0110 \"Permanent link\")\n\n- Adding device choice from sentence\\_transformers (#14546)\n\n### `llama-index-graph-stores-neo4` \\[0.2.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-graph-stores-neo4-027 \"Permanent link\")\n\n- Fixed ordering of returned nodes on vector queries (#14461)\n\n### `llama-index-llms-bedrock` \\[0.1.10\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-bedrock-0110 \"Permanent link\")\n\n- handle empty response in Bedrock AnthropicProvider (#14479)\n\n### `llama-index-llms-bedrock-converse` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-bedrock-converse-014_1 \"Permanent link\")\n\n- Fix Bedrock Converse's join\\_two\\_dicts function when a new string kwarg is added (#14548)\n\n### `llama-index-llms-upstage` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-upstage-014 \"Permanent link\")\n\n- Add upstage tokenizer and token counting method (#14502)\n\n### `llama-index-readers-azstorage-blob` \\[0.1.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-azstorage-blob-017 \"Permanent link\")\n\n- Fix bug with getting object name for blobs (#14547)\n\n### `llama-index-readers-file` \\[0.1.26\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-file-0126 \"Permanent link\")\n\n- Pandas excel reader load data fix for appending documents (#14501)\n\n### `llama-index-readers-iceberg` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-iceberg-010 \"Permanent link\")\n\n- Add Iceberg Reader integration to LLamaIndex (#14477)\n\n### `llama-index-readers-notion` \\[0.1.8\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-notion-018 \"Permanent link\")\n\n- Added retries (#14488)\n- add `list_databases` method (#14488)\n\n### `llama-index-readers-slack` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-slack-015 \"Permanent link\")\n\n- Enhance SlackReader to fetch Channel IDs from Channel Names/Patterns (#14429)\n\n### `llama-index-readers-web` \\[0.1.21\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-web-0121 \"Permanent link\")\n\n- Add API url to firecrawl reader (#14452)\n\n### `llama-index-retrievers-bm25` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-retrievers-bm25-015 \"Permanent link\")\n\n- fix score in nodes returned by the BM25 retriever (#14495)\n\n### `llama-index-vector-stores-azureaisearch` \\[0.1.9\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-azureaisearch-019 \"Permanent link\")\n\n- add async methods to azure ai search (#14496)\n\n### `llama-index-vector-stores-kdbai` \\[0.1.8\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-kdbai-018 \"Permanent link\")\n\n- Kdbai rest compatible (#14511)\n\n### `llama-index-vector-stores-mongodb` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-mongodb-016 \"Permanent link\")\n\n- Adds Hybrid and Full-Text Search to MongoDBAtlasVectorSearch (#14490)\n\n## \\[2024-06-28\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-06-28 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.51\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01051 \"Permanent link\")\n\n- fixed issue with function calling llms and empty tool calls (#14453)\n- Fix ChatMessage not considered as stringable in query pipeline (#14378)\n- Update schema llm path extractor to also take a list of valid triples (#14357)\n- Pass the kwargs on when `build_index_from_nodes` (#14341)\n\n### `llama-index-agent-dashscope` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-agent-dashscope-010 \"Permanent link\")\n\n- Add Alibaba Cloud dashscope agent (#14318)\n\n### `llama-index-graph-stores-neo4j` \\[0.2.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-graph-stores-neo4j-026 \"Permanent link\")\n\n- Add MetadataFilters to neo4j\\_property\\_graph (#14362)\n\n### `llama-index-llms-nvidia` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-nvidia-014 \"Permanent link\")\n\n- add known context lengths for hosted models (#14436)\n\n### `llama-index-llms-perplexity` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-perplexity-014 \"Permanent link\")\n\n- update available models (#14409)\n\n### `llama-index-llms-predibase` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-predibase-016 \"Permanent link\")\n\n- Better error handling for invalid API token (#14440)\n\n### `llama-index-llms-yi` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-yi-010 \"Permanent link\")\n\n- Integrate Yi model (#14353)\n\n### `llama-index-readers-google` \\[0.2.9\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-google-029 \"Permanent link\")\n\n- Creates Data Loader for Google Chat (#14397)\n\n### `llama-index-readers-s3` \\[0.1.10\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-s3-0110 \"Permanent link\")\n\n- Invalidate s3fs cache in S3Reader (#14441)\n\n### `llama-index-readers-structured-data` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-structured-data-010 \"Permanent link\")\n\n- Add StructuredDataReader support for xlsx, csv, json and jsonl (#14369)\n\n### `llama-index-tools-jina` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-tools-jina-010 \"Permanent link\")\n\n- Integrating a new tool called jina search (#14317)\n\n### `llama-index-vector-stores-astradb` \\[0.1.8\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-astradb-018 \"Permanent link\")\n\n- Update Astra DB vector store to use modern astrapy library (#14407)\n\n### `llama-index-vector-stores-chromadb` \\[0.1.10\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-chromadb-0110 \"Permanent link\")\n\n- Fix the index accessing of ids of chroma get (#14434)\n\n### `llama-index-vector-stores-deeplake` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-deeplake-014 \"Permanent link\")\n\n- Implemented delete\\_nodes() and clear() in deeplake vector store (#14457)\n- Implemented get\\_nodes() in deeplake vector store (#14388)\n\n### `llama-index-vector-stores-elasticsearch` \\[0.2.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-elasticsearch-021 \"Permanent link\")\n\n- Add support for dynamic metadata fields in Elasticsearch index creation (#14431)\n\n### `llama-index-vector-stores-kdbai` \\[0.1.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-kdbai-017 \"Permanent link\")\n\n- Kdbai version compatible (#14402)\n\n## \\[2024-06-24\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-06-24 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.50\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01050 \"Permanent link\")\n\n- added dead simple `FnAgentWorker` for custom agents (#14329)\n- Pass the kwargs on when build\\_index\\_from\\_nodes (#14341)\n- make async utils a bit more robust to nested async (#14356)\n\n### `llama-index-llms-upstage` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-upstage-013 \"Permanent link\")\n\n- every llm is a chat model (#14334)\n\n### `llama-index-packs-rag-evaluator` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-packs-rag-evaluator-015 \"Permanent link\")\n\n- added possibility to run local embedding model in RAG evaluation packages (#14352)\n\n## \\[2024-06-23\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-06-23 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.49\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01049 \"Permanent link\")\n\n- Improvements to `llama-cloud` and client dependencies (#14254)\n\n### `llama-index-indices-managed-llama-cloud` \\[0.2.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-indices-managed-llama-cloud-021 \"Permanent link\")\n\n- Improve the interface and client interactions in `LlamaCloudIndex` (#14254)\n\n### `llama-index-llms-bedrock-converse` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-bedrock-converse-013 \"Permanent link\")\n\n- add claude sonnet 3.5 to bedrock converse (#14306)\n\n### `llama-index-llms-upstage` \\[0.1.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-upstage-012 \"Permanent link\")\n\n- set default context size (#14293)\n- add api\\_key alias on upstage llm and embeddings (#14233)\n\n### `llama-index-storage-kvstore-azure` \\[0.1.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-storage-kvstore-azure-012 \"Permanent link\")\n\n- Optimized inserts (#14321)\n\n### `llama-index-utils-azure` \\[0.1.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-utils-azure-011 \"Permanent link\")\n\n- azure\\_table\\_storage params bug (#14182)\n\n### `llama-index-vector-stores-neo4jvector` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-neo4jvector-016 \"Permanent link\")\n\n- Add neo4j client method (#14314)\n\n## \\[2024-06-21\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-06-21 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.48\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01048 \"Permanent link\")\n\n- Improve efficiency of average precision (#14260)\n- add crewai + llamaindex cookbook (#14266)\n- Add mimetype field to TextNode (#14279)\n- Improve IBM watsonx.ai docs (#14271)\n- Updated frontpage of docs, added agents guide, and more (#14089)\n\n### `llama-index-llms-anthropic` \\[0.1.14\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-anthropic-0114 \"Permanent link\")\n\n- Add support for claude 3.5 (#14277)\n\n### `llama-index-llms-bedrock-converse` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-bedrock-converse-014_2 \"Permanent link\")\n\n- Implement Bedrock Converse API for function calling (#14055)\n\n## \\[2024-06-19\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-06-19 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.47\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01047 \"Permanent link\")\n\n- added average precision as a retrieval metric (#14189)\n- added `.show_jupyter_graph()` method visualizing default simple graph\\_store in jupyter notebooks (#14104)\n- corrected the behaviour of nltk file lookup (#14040)\n- Added helper args to generate\\_qa\\_pairs (#14054)\n- Add new chunking semantic chunking method: double-pass merging (#13629)\n- enable stepwise execution of query pipelines (#14117)\n- Replace tenacity upper limit by only rejecting 8.4.0 (#14218)\n- propagate error\\_on\\_no\\_tool\\_call kwarg in `llm.predict_and_call()` (#14253)\n- in query pipeline, avoid casting nodes as strings and use `get_content()` instead (#14242)\n- Fix NLSQLTableQueryEngine response metadata (#14169)\n- do not overwrite relations in default simple property graph (#14244)\n\n### `llama-index-embeddings-ipex-llm` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-ipex-llm-015 \"Permanent link\")\n\n- Enable selecting Intel GPU for ipex embedding integrations (#14214)\n\n### `llama-index-embeddings-mixedbreadai` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-mixedbreadai-010 \"Permanent link\")\n\n- add mixedbread ai integration (#14161)\n\n### `llama-index-graph-stores-neo4j` \\[0.2.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-graph-stores-neo4j-025 \"Permanent link\")\n\n- Add default node property to neo4j upsert relations (#14095)\n\n### `llama-index-indices-managed-postgresml` \\[0.3.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-indices-managed-postgresml-030 \"Permanent link\")\n\n- Added re-ranking into the PostgresML Managed Index (#14134)\n\n### `llama-index-llms-ai21` \\[0.3.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-ai21-030 \"Permanent link\")\n\n- use async AI21 client for async methods (#14193)\n\n### `llama-index-llms-bedrock-converse` \\[0.1.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-bedrock-converse-012 \"Permanent link\")\n\n- Added (fake) async calls to avoid errors (#14241)\n\n### `llama-index-llms-deepinfra` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-deepinfra-013 \"Permanent link\")\n\n- Add function calling to deep infra llm (#14127)\n\n### `llama-index-llms-ipex-llm` \\[0.1.8\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-ipex-llm-018 \"Permanent link\")\n\n- Enable selecting Intel GPU for ipex embedding integrations (#14214)\n\n### `llama-index-llms-oci-genai` \\[0.1.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-oci-genai-011 \"Permanent link\")\n\n- add command r support oci genai (#14080)\n\n### `llama-index-llms-premai` \\[0.1.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-premai-017 \"Permanent link\")\n\n- Prem AI Templates Llama Index support (#14105)\n\n### `llama-index-llms-you` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-you-010 \"Permanent link\")\n\n- Integrate You.com conversational APIs (#14207)\n\n### `llama-index-readers-mongodb` \\[0.1.8\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-mongodb-018 \"Permanent link\")\n\n- Add metadata field \"collection\\_name\" to SimpleMongoReader (#14245)\n\n### `llama-index-readers-pdf-marker` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-pdf-marker-010 \"Permanent link\")\n\n- add marker-pdf reader (#14099)\n\n### `llama-index-readers-upstage` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-upstage-010 \"Permanent link\")\n\n- Added upstage as a reader (#13415)\n\n### `llama-index-postprocessor-mixedbreadai-rerank` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-postprocessor-mixedbreadai-rerank-010 \"Permanent link\")\n\n- add mixedbread ai integration (#14161)\n\n### `llama-index-vector-stores-lancedb` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-lancedb-016 \"Permanent link\")\n\n- LanceDB: code cleanup, minor updates (#14077)\n\n### `llama-index-vector-stores-opensearch` \\[0.1.12\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-opensearch-0112 \"Permanent link\")\n\n- add option to customize default OpenSearch Client and Engine (#14249)\n\n## \\[2024-06-17\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-06-17 \"Permanent link\")\n\n### `llama-index-core`\\[0.10.46\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core01046 \"Permanent link\")\n\n- Fix Pin tenacity and numpy in core (#14203)\n- Add precision and recall metrics (#14170)\n- Enable Function calling and agent runner for Vertex AI (#14088)\n- Fix for batch\\_gather (#14162)\n\n### `llama-index-utils-huggingface` \\[0.1.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-utils-huggingface-011 \"Permanent link\")\n\n- Remove sentence-transformers dependency from HuggingFace utils package (#14204)\n\n### `llama-index-finetuning` \\[0.1.8\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-finetuning-018 \"Permanent link\")\n\n- Add MistralAI Finetuning API support (#14101)\n\n### `llama-index-llms-mistralai` \\[0.1.16\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-mistralai-0116 \"Permanent link\")\n\n- Update MistralAI (#14199)\n\n### `llama-index-llms-bedrock-converse` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-bedrock-converse-010 \"Permanent link\")\n\n- fix: \ud83d\udc1b Fix Bedrock Converse' pyproject.toml for the PyPI release (#14197)\n\n### `llama-index-utils-azure` \\[0.1.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-utils-azure-011_1 \"Permanent link\")\n\n- Use typical include llama\\_index/ (#14196)\n- Feature/azure\\_table\\_storage (#14182)\n\n### `llama-index-embeddings-nvidia` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-nvidia-014 \"Permanent link\")\n\n- add support for nvidia/nv-embed-v1 (https://huggingface.co/nvidia/NV-Embed-v1) (#14194)\n\n### `llama-index-retrievers-you` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-retrievers-you-013 \"Permanent link\")\n\n- add news retriever (#13934)\n\n### `llama-index-storage-kvstore-azure` \\[0.1.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-storage-kvstore-azure-011 \"Permanent link\")\n\n- Fixes a bug where there is a missing await. (#14177)\n\n### `llama-index-embeddings-nomic` \\[0.4.0post1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-nomic-040post1 \"Permanent link\")\n\n- Restore Nomic Embed einops dependency (#14176)\n\n### `llama-index-retrievers-bm25` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-retrievers-bm25-014 \"Permanent link\")\n\n- Changing BM25Retriever \\_retrieve to use numpy methods (#14015)\n\n### `llama-index-llms-gemini` \\[0.1.11\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-gemini-0111 \"Permanent link\")\n\n- Add missing @llm\\_chat\\_callback() to Gemini.stream\\_chat (#14166)\n\n### `llama-index-llms-vertex` \\[0.2.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-vertex-020 \"Permanent link\")\n\n- Enable Function calling and agent runner for Vertex AI (#14088)\n\n### `llama-index-vector-stores-opensearch` \\[0.1.11\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-opensearch-0111 \"Permanent link\")\n\n- feat: support VectorStoreQueryMode.TEXT\\_SEARCH on OpenSearch VectorStore (#14153)\n\n## \\[2024-06-14\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-06-14 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.45\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01045 \"Permanent link\")\n\n- Fix parsing sql query.py (#14109)\n- Implement NDCG metric (#14100)\n- Fixed System Prompts for Structured Generation (#14026)\n- Split HuggingFace embeddings in HuggingFace API and TextGenerationInference packages (#14013)\n- Add PandasExcelReader class for parsing excel files (#13991)\n- feat: add spans to ingestion pipeline (#14062)\n\n### `llama-index-vector-stores-qdrant` \\[0.2.10\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-qdrant-0210 \"Permanent link\")\n\n- Fix Qdrant nodes (#14149)\n\n### `llama-index-readers-mongodb` \\[0.1.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-mongodb-017 \"Permanent link\")\n\n- Fixes TypeError: sequence item : expected str instance, int found\n\n### `llama-index-indices-managed-vertexai` \\[0.0.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-indices-managed-vertexai-001 \"Permanent link\")\n\n- feat: Add Managed Index for LlamaIndex on Vertex AI for RAG (#13626)\n\n### `llama-index-llms-oci-genai` \\[0.1.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-oci-genai-011_1 \"Permanent link\")\n\n- Feature/add command r support oci genai (#14080)\n\n### `llama-index-vector-stores-milvus` \\[0.1.20\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-milvus-0120 \"Permanent link\")\n\n- MilvusVectorStore: always include text\\_key in output\\_fields (#14076)\n\n### `llama-index-packs-mixture-of-agents` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-packs-mixture-of-agents-010 \"Permanent link\")\n\n- Add Mixture Of Agents paper implementation (#14112)\n\n### `llama-index-llms-text-generation-inference` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-text-generation-inference-010 \"Permanent link\")\n\n- Split HuggingFace embeddings in HuggingFace API and TextGenerationInference packages (#14013)\n\n### `llama-index-llms-huggingface-api` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-huggingface-api-010 \"Permanent link\")\n\n- Split HuggingFace embeddings in HuggingFace API and TextGenerationInference packages (#14013)\n\n### `llama-index-embeddings-huggingface-api` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-huggingface-api-010 \"Permanent link\")\n\n- Split HuggingFace embeddings in HuggingFace API and TextGenerationInference packages (#14013)\n\n### `llama-index-utils-huggingface` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-utils-huggingface-010 \"Permanent link\")\n\n- Split HuggingFace embeddings in HuggingFace API and TextGenerationInference packages (#14013)\n\n### `llama-index-llms-watsonx` \\[0.1.8\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-watsonx-018 \"Permanent link\")\n\n- Feat: IBM watsonx.ai llm and embeddings integration (#13600)\n\n### `llama-index-llms-ibm` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-ibm-010 \"Permanent link\")\n\n- Feat: IBM watsonx.ai llm and embeddings integration (#13600)\n\n### `llama-index-embeddings-ibm` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-ibm-010 \"Permanent link\")\n\n- Feat: IBM watsonx.ai llm and embeddings integration (#13600)\n\n### `llama-index-vector-stores-milvus` \\[0.1.19\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-milvus-0119 \"Permanent link\")\n\n- Fix to milvus filter enum parsing (#14111)\n\n### `llama-index-llms-anthropic` \\[0.1.13\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-anthropic-0113 \"Permanent link\")\n\n- fix anthropic llm calls (#14108)\n\n### `llama-index-storage-index-store-postgres` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-storage-index-store-postgres-014 \"Permanent link\")\n\n- Wrong mongo name was used instead of Postgres (#14107)\n\n### `llama-index-embeddings-bedrock` \\[0.2.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-bedrock-021 \"Permanent link\")\n\n- Remove unnecessary excluded from fields in Bedrock embedding (#14085)\n\n### `llama-index-finetuning` \\[0.1.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-finetuning-017 \"Permanent link\")\n\n- Feature/added trust remote code (#14102)\n\n### `llama-index-readers-file` \\[0.1.25\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-file-0125 \"Permanent link\")\n\n- nit: fix for pandas excel reader (#14086)\n\n### `llama-index-llms-anthropic` \\[0.1.12\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-anthropic-0112 \"Permanent link\")\n\n- Update anthropic dependency to 0.26.2 minimum version (#14091)\n\n### `llama-index-llms-llama-cpp` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-llama-cpp-014 \"Permanent link\")\n\n- Add support for Llama 3 Instruct prompt format (#14072)\n\n### `llama-index-llms-bedrock-converse` \\[0.1.8\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-bedrock-converse-018 \"Permanent link\")\n\n- Implement Bedrock Converse API for function calling (#14055)\n\n### `llama-index-vector-stores-postgres` \\[0.1.11\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-postgres-0111 \"Permanent link\")\n\n- fix/postgres-metadata-in-filter-single-elem (#14035)\n\n### `llama-index-readers-file` \\[0.1.24\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-file-0124 \"Permanent link\")\n\n- Add PandasExcelReader class for parsing excel files (#13991)\n\n### `llama-index-embeddings-ipex-llm` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-ipex-llm-014 \"Permanent link\")\n\n- Update dependency of llama-index-embeddings-ipex-llm\n\n### `llama-index-embeddings-gemini` \\[0.1.8\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-gemini-018 \"Permanent link\")\n\n- Add api key as field in Gemini Embedding (#14061)\n\n### `llama-index-vector-stores-milvus` \\[0.1.18\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-milvus-0118 \"Permanent link\")\n\n- Expand milvus vector store filter options (#13961)\n\n## \\[2024-06-10\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-06-10 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.44\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01044 \"Permanent link\")\n\n- Add WEBP and GIF to supported image types for SimpleDirectoryReader (#14038)\n- refactor: add spans to abstractmethods via mixin (#14003)\n- Adding streaming support for SQLAutoVectorQueryEngine (#13947)\n- add option to specify embed\\_model to NLSQLTableQueryEngine (#14006)\n- add spans for multimodal LLMs (#13966)\n- change to compact in auto prev next (#13940)\n- feat: add exception events for streaming errors (#13917)\n- feat: add spans for tools (#13916)\n\n### `llama-index-embeddings-azure-openai` \\[0.1.10\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-azure-openai-0110 \"Permanent link\")\n\n- Fix error when using azure\\_ad without setting the API key (#13970)\n\n### `llama-index-embeddings-jinaai` \\[0.2.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-jinaai-020 \"Permanent link\")\n\n- add Jina Embeddings MultiModal (#13861)\n\n### `llama-index-embeddings-nomic` \\[0.3.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-nomic-030 \"Permanent link\")\n\n- Add Nomic multi modal embeddings (#13920)\n\n### `llama-index-graph-stores-neo4j` \\[0.2.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-graph-stores-neo4j-023 \"Permanent link\")\n\n- ensure cypher returns list before iterating (#13938)\n\n### `llama-index-llms-ai21` \\[0.2.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-ai21-020 \"Permanent link\")\n\n- Add AI21 Labs Jamba-Instruct Support (#14030)\n\n### `llama-index-llms-deepinfra` \\[0.1.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-deepinfra-012 \"Permanent link\")\n\n- fix(deepinfrallm): default max\\_tokens (#13998)\n\n### `llama-index-llms-vllm` \\[0.1.8\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-vllm-018 \"Permanent link\")\n\n- correct `__del__()` Vllm (#14053)\n\n### `llama-index-packs-zenguard` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-packs-zenguard-010 \"Permanent link\")\n\n- Add ZenGuard llamapack (#13959)\n\n### `llama-index-readers-google` \\[0.2.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-google-027 \"Permanent link\")\n\n- fix how class attributes are set in google drive reader (#14022)\n- Add Google Maps Text Search Reader (#13884)\n\n### `llama-index-readers-jira` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-jira-014 \"Permanent link\")\n\n- Jira personal access token with hosted instances (#13890)\n\n### `llama-index-readers-mongodb` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-mongodb-016 \"Permanent link\")\n\n- set document ids when loading (#14000)\n\n### `llama-index-retrievers-duckdb-retriever` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-retrievers-duckdb-retriever-010 \"Permanent link\")\n\n- Add DuckDBRetriever (#13929)\n\n### `llama-index-vector-stores-chroma` \\[0.1.9\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-chroma-019 \"Permanent link\")\n\n- Add inclusion filter to chromadb (#14010)\n\n### `llama-index-vector-stores-lancedb` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-lancedb-015 \"Permanent link\")\n\n- Fix LanceDBVectorStore `add()` logic (#13993)\n\n### `llama-index-vector-stores-milvus` \\[0.1.17\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-milvus-0117 \"Permanent link\")\n\n- Support all filter operators for Milvus vector store (#13745)\n\n### `llama-index-vector-stores-postgres` \\[0.1.10\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-postgres-0110 \"Permanent link\")\n\n- Broaden SQLAlchemy support in llama-index-vector-stores-postgres to 1.4+ (#13936)\n\n### `llama-index-vector-stores-qdrant` \\[0.2.9\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-qdrant-029 \"Permanent link\")\n\n- Qdrant: Create payload index for `doc_id` (#14001)\n\n## \\[2024-06-02\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-06-02 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.43\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01043 \"Permanent link\")\n\n- use default UUIDs when possible for property graph index vector stores (#13886)\n- avoid empty or duplicate inserts in property graph index (#13891)\n- Fix cur depth for `get_rel_map` in simple property graph store (#13888)\n- (bandaid) disable instrumentation from logging generators (#13901)\n- Add backwards compatibility to Dispatcher.get\\_dispatch\\_event() method (#13895)\n- Fix: Incorrect naming of acreate\\_plan in StructuredPlannerAgent (#13879)\n\n### `llama-index-graph-stores-neo4j` \\[0.2.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-graph-stores-neo4j-022 \"Permanent link\")\n\n- Handle cases where type is missing (neo4j property graph) (#13875)\n- Rename `Neo4jPGStore` to `Neo4jPropertyGraphStore` (with backward compat) (#13891)\n\n### `llama-index-llms-openai` \\[0.1.22\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-openai-0122 \"Permanent link\")\n\n- Improve the retry mechanism of OpenAI (#13878)\n\n### `llama-index-readers-web` \\[0.1.18\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-web-0118 \"Permanent link\")\n\n- AsyncWebPageReader: made it actually async; it was exhibiting blocking behavior (#13897)\n\n### `llama-index-vector-stores-opensearch` \\[0.1.10\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-opensearch-0110 \"Permanent link\")\n\n- Fix/OpenSearch filter logic (#13804)\n\n## \\[2024-05-31\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-05-31 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.42\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01042 \"Permanent link\")\n\n- Allow proper setting of the vector store in property graph index (#13816)\n- fix imports in langchain bridge (#13871)\n\n### `llama-index-graph-stores-nebula` \\[0.2.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-graph-stores-nebula-020 \"Permanent link\")\n\n- NebulaGraph support for PropertyGraphStore (#13816)\n\n### `llama-index-llms-langchain` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-langchain-015 \"Permanent link\")\n\n- fix fireworks imports in langchain llm (#13871)\n\n### `llama-index-llms-openllm` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-openllm-015 \"Permanent link\")\n\n- feat(openllm): 0.5 sdk integrations update (#13848)\n\n### `llama-index-llms-premai` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-premai-015 \"Permanent link\")\n\n- Update SDK compatibility (#13836)\n\n### `llama-index-readers-google` \\[0.2.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-google-026 \"Permanent link\")\n\n- Fixed a bug with tokens causing an infinite loop in GoogleDriveReader (#13863)\n\n## \\[2024-05-30\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-05-30 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.41\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01041 \"Permanent link\")\n\n- pass embeddings from index to property graph retriever (#13843)\n- protect instrumentation event/span handlers from each other (#13823)\n- add missing events for completion streaming (#13824)\n- missing callback\\_manager.on\\_event\\_end when there is exception (#13825)\n\n### `llama-index-llms-gemini` \\[0.1.10\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-gemini-0110 \"Permanent link\")\n\n- use `model` kwarg for model name for gemini (#13791)\n\n### `llama-index-llms-mistralai` \\[0.1.15\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-mistralai-0115 \"Permanent link\")\n\n- Add mistral code model (#13807)\n- update mistral codestral with fill in middle endpoint (#13810)\n\n### `llama-index-llms-openllm` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-openllm-015_1 \"Permanent link\")\n\n- 0.5 integrations update (#13848)\n\n### `llama-index-llms-vertex` \\[0.1.8\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-vertex-018 \"Permanent link\")\n\n- Safety setting for Pydantic Error for Vertex Integration (#13817)\n\n### `llama-index-readers-smart-pdf-loader` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-smart-pdf-loader-015 \"Permanent link\")\n\n- handle path objects in smart pdf reader (#13847)\n\n## \\[2024-05-28\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-05-28 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.40\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01040 \"Permanent link\")\n\n- Added `PropertyGraphIndex` and other supporting abstractions. See the [full guide](https://docs.llamaindex.ai/en/latest/module_guides/indexing/lpg_index_guide/) for more details (#13747)\n- Updated `AutoPrevNextNodePostprocessor` to allow passing in response mode and LLM (#13771)\n- fix type handling with return direct (#13776)\n- Correct the method name to `_aget_retrieved_ids_and_texts` in retrievval evaluator (#13765)\n- fix: QueryTransformComponent incorrect call `self._query_transform` (#13756)\n- implement more filters for `SimpleVectorStoreIndex` (#13365)\n\n### `llama-index-embeddings-bedrock` \\[0.2.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-bedrock-020 \"Permanent link\")\n\n- Added support for Bedrock Titan Embeddings v2 (#13580)\n\n### `llama-index-embeddings-oci-genai` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-oci-genai-010 \"Permanent link\")\n\n- add Oracle Cloud Infrastructure (OCI) Generative AI (#13631)\n\n### `llama-index-embeddings-huggingface` \\[0.2.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-huggingface-021 \"Permanent link\")\n\n- Expose \"safe\\_serialization\" parameter from AutoModel (#11939)\n\n### `llama-index-graph-stores-neo4j` \\[0.2.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-graph-stores-neo4j-020 \"Permanent link\")\n\n- Added `Neo4jPGStore` for property graph support (#13747)\n\n### `llama-index-indices-managed-dashscope` \\[0.1.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-indices-managed-dashscope-011 \"Permanent link\")\n\n- Added dashscope managed index (#13378)\n\n### `llama-index-llms-oci-genai` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-oci-genai-010 \"Permanent link\")\n\n- add Oracle Cloud Infrastructure (OCI) Generative AI (#13631)\n\n### `llama-index-readers-feishu-wiki` \\[0.1.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-feishu-wiki-011 \"Permanent link\")\n\n- fix undefined variable (#13768)\n\n### `llama-index-packs-secgpt` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-packs-secgpt-010 \"Permanent link\")\n\n- SecGPT - LlamaIndex Integration #13127\n\n### `llama-index-vector-stores-hologres` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-hologres-010 \"Permanent link\")\n\n- Add Hologres vector db (#13619)\n\n### `llama-index-vector-stores-milvus` \\[0.1.16\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-milvus-0116 \"Permanent link\")\n\n- Remove FlagEmbedding as Milvus's dependency (#13767)\nUnify the collection construction regardless of the value of enable\\_sparse (#13773)\n\n### `llama-index-vector-stores-opensearch` \\[0.1.9\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-opensearch-019 \"Permanent link\")\n\n- refactor to put helper methods inside class definition (#13749)\n\n## \\[2024-05-24\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-05-24 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.39\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01039 \"Permanent link\")\n\n- Add VectorMemory and SimpleComposableMemory (#13352)\n- Improve MarkdownReader to ignore headers in code blocks (#13694)\n- proper async element node parsers (#13698)\n- return only the message content in function calling worker (#13677)\n- nit: fix multimodal query engine to use metadata (#13712)\n- Add notebook with workaround for lengthy tool descriptions and QueryPlanTool (#13701)\n\n### `llama-index-embeddings-ipex-llm` \\[0.1.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-ipex-llm-012 \"Permanent link\")\n\n- Improve device selection (#13644)\n\n### `llama-index-indices-managed-postgresml` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-indices-managed-postgresml-013 \"Permanent link\")\n\n- Add the PostgresML Managed Index (#13623)\n\n### `llama-index-indices-managed-vectara` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-indices-managed-vectara-014 \"Permanent link\")\n\n- Added chat engine, streaming, factual consistency score, and more (#13639)\n\n### `llama-index-llms-deepinfra` \\[0.0.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-deepinfra-001 \"Permanent link\")\n\n- Add Integration for DeepInfra LLM Models (#13652)\n\n### `llama-index-llm-ipex-llm` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llm-ipex-llm-013 \"Permanent link\")\n\n- add GPU support for llama-index-llm-ipex-llm (#13691)\n\n### `llama-index-llms-lmstudio` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-lmstudio-010 \"Permanent link\")\n\n- lmstudio integration (#13557)\n\n### `llama-index-llms-ollama` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-ollama-015 \"Permanent link\")\n\n- Use aiter\\_lines function to iterate over lines in ollama integration (#13699)\n\n### `llama-index-llms-vertex` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-vertex-016 \"Permanent link\")\n\n- Added safety\\_settings parameter for gemini (#13568)\n\n### `llama-index-postprocessor-voyageai-rerank` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-postprocessor-voyageai-rerank-013 \"Permanent link\")\n\n- VoyageAI reranking bug fix (#13622)\n\n### `llama-index-retrievers-mongodb-atlas-bm25-retriever` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-retrievers-mongodb-atlas-bm25-retriever-014 \"Permanent link\")\n\n- Add missing return (#13720)\n\n### `llama-index-readers-web` \\[0.1.17\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-web-0117 \"Permanent link\")\n\n- Add Scrapfly Web Loader (#13654)\n\n### `llama-index-vector-stores-postgres` \\[0.1.9\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-postgres-019 \"Permanent link\")\n\n- fix bug with delete and special chars (#13651)\n\n### `llama-index-vector-stores-supabase` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-supabase-015 \"Permanent link\")\n\n- Try-catch in case the .\\_client attribute is not present (#13681)\n\n## \\[2024-05-21\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-05-21 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.38\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01038 \"Permanent link\")\n\n- Enabling streaming in BaseSQLTableQueryEngine (#13599)\n- Fix nonetype errors in relational node parsers (#13615)\n- feat(instrumentation): new spans for ALL llms (#13565)\n- Properly Limit the number of generated questions (#13596)\n- Pass 'exclude\\_llm\\_metadata\\_keys' and 'exclude\\_embed\\_metadata\\_keys' in element Node Parsers (#13567)\n- Add batch mode to QueryPipeline (#13203)\n- Improve SentenceEmbeddingOptimizer to respect Settings.embed\\_model (#13514)\n- ReAct output parser robustness changes (#13459)\n- fix for pydantic tool calling with a single argument (#13522)\n- Avoid unexpected error when stream chat doesn't yield (#13422)\n\n### `llama-index-embeddings-nomic` \\[0.2.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-nomic-020 \"Permanent link\")\n\n- Implement local Nomic Embed with the inference\\_mode parameter (#13607)\n\n### `llama-index-embeddings-nvidia` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-nvidia-013 \"Permanent link\")\n\n- Deprecate `mode()` in favor of `__init__(base_url=...)` (#13572)\n- add snowflake/arctic-embed-l support (#13555)\n\n### `llama-index-embeddings-openai` \\[0.1.10\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-openai-0110 \"Permanent link\")\n\n- update how retries get triggered for openai (#13608)\n\n### `llama-index-embeddings-upstage` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-upstage-010 \"Permanent link\")\n\n- Integrations: upstage LLM and Embeddings (#13193)\n\n### `llama-index-llms-gemini` \\[0.1.8\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-gemini-018 \"Permanent link\")\n\n- feat: add gemini new models to multimodal LLM and regular (#13539)\n\n### `llama-index-llms-groq` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-groq-014 \"Permanent link\")\n\n- fix: enable tool use (#13566)\n\n### `llama-index-llms-lmstudio` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-lmstudio-010_1 \"Permanent link\")\n\n- Add support for lmstudio integration (#13557)\n\n### `llama-index-llms-nvidia` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-nvidia-013 \"Permanent link\")\n\n- Deprecate `mode()` in favor of `__init__(base_url=...)` (#13572)\n\n### `llama-index-llms-openai` \\[0.1.20\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-openai-0120 \"Permanent link\")\n\n- update how retries get triggered for openai (#13608)\n\n### `llama-index-llms-unify` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-unify-010 \"Permanent link\")\n\n- Add Unify LLM Support (#12921)\n\n### `llama-index-llms-upstage` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-upstage-010 \"Permanent link\")\n\n- Integrations: upstage LLM and Embeddings (#13193)\n\n### `llama-index-llms-vertex` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-vertex-016_1 \"Permanent link\")\n\n- Adding Support for MedLM Models (#11911)\n\n### `llama_index.postprocessor.dashscope_rerank` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama_indexpostprocessordashscope_rerank-010 \"Permanent link\")\n\n- Add dashscope rerank for postprocessor (#13353)\n\n### `llama-index-postprocessor-nvidia-rerank` \\[0.1.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-postprocessor-nvidia-rerank-012 \"Permanent link\")\n\n- Deprecate `mode()` in favor of `__init__(base_url=...)` (#13572)\n\n### `llama-index-readers-mongodb` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-mongodb-015 \"Permanent link\")\n\n- SimpleMongoReader should allow optional fields in metadata (#13575)\n\n### `llama-index-readers-papers` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-papers-015 \"Permanent link\")\n\n- fix: (ArxivReader) set exclude\\_hidden to False when reading data from hidden directory (#13578)\n\n### `llama-index-readers-sec-filings` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-sec-filings-015 \"Permanent link\")\n\n- fix: sec\\_filings header when making requests to sec.gov #13548\n\n### `llama-index-readers-web` \\[0.1.16\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-web-0116 \"Permanent link\")\n\n- Added firecrawl search mode (#13560)\n- Updated Browserbase web reader (#13535)\n\n### `llama-index-tools-cassandra` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-tools-cassandra-010 \"Permanent link\")\n\n- added Cassandra database tool spec for agents (#13423)\n\n### `llama-index-vector-stores-azureaisearch` \\[0.1.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-azureaisearch-017 \"Permanent link\")\n\n- Allow querying AzureAISearch without non-null metadata field (#13531)\n\n### `llama-index-vector-stores-elasticsearch` \\[0.2.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-elasticsearch-020 \"Permanent link\")\n\n- Integrate VectorStore from Elasticsearch client (#13291)\n\n### `llama-index-vector-stores-milvus` \\[0.1.14\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-milvus-0114 \"Permanent link\")\n\n- Fix the filter expression construction of Milvus vector store (#13591)\n\n### `llama-index-vector-stores-supabase` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-supabase-014 \"Permanent link\")\n\n- Disconnect when deleted (#13611)\n\n### `llama-index-vector-stores-wordlift` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-wordlift-010 \"Permanent link\")\n\n- Added the WordLift Vector Store (#13028)\n\n## \\[2024-05-14\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-05-14 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.37\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01037 \"Permanent link\")\n\n- Add image\\_documents at call time for `MultiModalLLMCompletionProgram` (#13467)\n- fix RuntimeError by switching to asyncio from threading (#13486)\n- Add support for prompt kwarg (#13405)\n- VectorStore -> BasePydanticVectorStore (#13439)\n- fix: user\\_message does not exist bug (#13432)\n- import missing response type (#13382)\n- add `CallbackManager` to `MultiModalLLM` (#13400)\n\n### `llama-index-llms-bedrock` \\[0.1.8\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-bedrock-018 \"Permanent link\")\n\n- Remove \"Truncate\" parameter from Bedrock Cohere invoke model request (#13442)\n\n### `llama-index-readers-web` \\[0.1.14\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-web-0114 \"Permanent link\")\n\n- Trafilatura kwargs and progress bar for trafilatura web reader (#13454)\n\n### `llama-index-vector-stores-postgres` \\[0.1.8\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-postgres-018 \"Permanent link\")\n\n- Fix #9522 - SQLAlchemy warning when using hybrid search (#13476)\n\n### `llama-index-vector-stores-lantern` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-lantern-014 \"Permanent link\")\n\n- Fix #9522 - SQLAlchemy warning when using hybrid search (#13476)\n\n### `llama-index-callbacks-uptrain` \\[0.2.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-callbacks-uptrain-020 \"Permanent link\")\n\n- update UpTrain Callback Handler to support new Upgratin eval schema (#13479)\n\n### `llama-index-vector-stores-zep` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-zep-013 \"Permanent link\")\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-vearch` \\[0.1.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-vearch-011 \"Permanent link\")\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-upstash` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-upstash-014 \"Permanent link\")\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-typesense` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-typesense-013 \"Permanent link\")\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-timescalerevector` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-timescalerevector-013 \"Permanent link\")\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-tencentvectordb` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-tencentvectordb-014 \"Permanent link\")\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-tair` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-tair-013 \"Permanent link\")\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-singlestoredb` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-singlestoredb-013 \"Permanent link\")\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-rocksetdb` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-rocksetdb-013 \"Permanent link\")\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-neptune` \\[0.1.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-neptune-011 \"Permanent link\")\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-neo4jvector` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-neo4jvector-015 \"Permanent link\")\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-myscale` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-myscale-013 \"Permanent link\")\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-metal` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-metal-013 \"Permanent link\")\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-jaguar` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-jaguar-013 \"Permanent link\")\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-epsilla` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-epsilla-013 \"Permanent link\")\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-dynamodb` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-dynamodb-013 \"Permanent link\")\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-dashvector` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-dashvector-013 \"Permanent link\")\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-chatgpt-plugin` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-chatgpt-plugin-013 \"Permanent link\")\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-baiduvectordb` \\[0.1.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-baiduvectordb-011 \"Permanent link\")\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-bagel` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-bagel-013 \"Permanent link\")\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-awsdocdb` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-awsdocdb-015 \"Permanent link\")\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-awadb` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-awadb-013 \"Permanent link\")\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-vector-stores-alibabacloud-opensearch` \\[0.1.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-alibabacloud-opensearch-011 \"Permanent link\")\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-readers-wordlift` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-wordlift-014 \"Permanent link\")\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-readers-guru` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-guru-014 \"Permanent link\")\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-readers-pebblo` \\[0.1.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-pebblo-011 \"Permanent link\")\n\n- VectorStore -> BasePydanticVectorStore (#13439)\n\n### `llama-index-postprocessor-voyageai-rerank` \\[0.1.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-postprocessor-voyageai-rerank-012 \"Permanent link\")\n\n- bump rerank versions (#13465)\n\n### `llama-index-postprocessor-sbert-rerank` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-postprocessor-sbert-rerank-014 \"Permanent link\")\n\n- bump rerank versions (#13465)\n\n### `llama-index-postprocessor-rankllm-rerank` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-postprocessor-rankllm-rerank-013 \"Permanent link\")\n\n- bump rerank versions (#13465)\n\n### `llama-index-postprocessor-rankgpt-rerank` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-postprocessor-rankgpt-rerank-014 \"Permanent link\")\n\n- bump rerank versions (#13465)\n\n### `llama-index-postprocessor-openvino-rerank` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-postprocessor-openvino-rerank-013 \"Permanent link\")\n\n- bump rerank versions (#13465)\n\n### `llama-index-postprocessor-nvidia-rerank` \\[0.1.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-postprocessor-nvidia-rerank-011 \"Permanent link\")\n\n- bump rerank versions (#13465)\n\n### `llama-index-postprocessor-jinaai-rerank` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-postprocessor-jinaai-rerank-013 \"Permanent link\")\n\n- bump rerank versions (#13465)\n\n### `llama-index-postprocessor-flag-embedding-rerank` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-postprocessor-flag-embedding-rerank-013 \"Permanent link\")\n\n- bump rerank versions (#13465)\n\n### `llama-index-postprocessor-colbert-rerank` \\[0.1.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-postprocessor-colbert-rerank-012 \"Permanent link\")\n\n- bump rerank versions (#13465)\n\n### `llama-index-postprocessor-cohere-rerank` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-postprocessor-cohere-rerank-016 \"Permanent link\")\n\n- bump rerank versions (#13465)\n\n### `llama-index-multi-modal-llms-openai` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-multi-modal-llms-openai-016 \"Permanent link\")\n\n- gpt-4o support (#13463)\n\n### `llama-index-llms-openai` \\[0.1.19\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-openai-0119 \"Permanent link\")\n\n- gpt-4o support (#13463)\n\n### `llama-index-packs-rag-fusion-query-pipeline` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-packs-rag-fusion-query-pipeline-014 \"Permanent link\")\n\n- fix the RAG fusion pipeline (#13413)\n\n### `llama-index-agent-openai` \\[0.2.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-agent-openai-025 \"Permanent link\")\n\n- fix: update OpenAIAssistantAgent to use attachments (#13341)\n\n### `llama-index-embeddings-deepinfra` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-deepinfra-010 \"Permanent link\")\n\n- new embeddings integration (#13323)\n\n### `llama-index-llms-mlx` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-mlx-010 \"Permanent link\")\n\n- new llm integration (#13231)\n\n### `llama-index-vector-stores-milvus` \\[0.1.12\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-milvus-0112 \"Permanent link\")\n\n- fix: Corrected connection parameters in connections.connect() (#13448)\n\n### `llama-index-vector-stores-azureaisearch` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-azureaisearch-016 \"Permanent link\")\n\n- fix AzureAiSearchVectorStore metadata f-string (#13435)\n\n### `llama-index-vector-stores-mongodb` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-mongodb-015 \"Permanent link\")\n\n- adds Unit and Integration tests for MongoDBAtlasVectorSearch (#12854)\n\n### `llama-index-llms-huggingface` \\[0.2.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-huggingface-020 \"Permanent link\")\n\n- update llama-index-llms-huggingface dependency (#13420)\n\n### `llama-index-vector-store-relyt` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-store-relyt-010 \"Permanent link\")\n\n- new vector store integration\n\n### `llama-index-storage-kvstore-redis` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-storage-kvstore-redis-015 \"Permanent link\")\n\n- Implement async methods in RedisKVStore (#12943)\n\n### `llama-index-packs-cohere-citation-chat` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-packs-cohere-citation-chat-015 \"Permanent link\")\n\n- pin llama-index-llms-cohere dependency (#13417)\n\n### `llama-index-llms-cohere` \\[0.2.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-cohere-020 \"Permanent link\")\n\n- pin cohere dependency (#13417)\n\n### `llama-index-tools-azure-code-interpreter` \\[0.1.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-tools-azure-code-interpreter-011 \"Permanent link\")\n\n- fix indexing issue and runtime error message (#13414)\n\n### `llama-index-postprocessor-cohere-rerank` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-postprocessor-cohere-rerank-015 \"Permanent link\")\n\n- fix Cohere Rerank bug (#13410)\n\n### `llama-index-indices-managed-llama-cloud` \\[0.1.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-indices-managed-llama-cloud-017 \"Permanent link\")\n\n- fix retriever integration (#13409)\n\n### `llama-index-tools-azure-code-interpreter` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-tools-azure-code-interpreter-010 \"Permanent link\")\n\n- new tool\n\n### `llama-index-readers-google` \\[0.2.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-google-025 \"Permanent link\")\n\n- fix missing authorized\\_user\\_info check on GoogleDriveReader (#13394)\n\n### `llama-index-storage-kvstore-firestore` \\[0.2.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-storage-kvstore-firestore-021 \"Permanent link\")\n\n- await Firestore's AsyncDocumentReference (#13386)\n\n### `llama-index-llms-nvidia` \\[0.1.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-nvidia-012 \"Permanent link\")\n\n- add dynamic model listing support (#13398)\n\n## \\[2024-05-09\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-05-09 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.36\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01036 \"Permanent link\")\n\n- add start\\_char\\_idx and end\\_char\\_idx with MarkdownElementParser (#13377)\n- use handlers from global default (#13368)\n\n### `llama-index-readers-pebblo` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-pebblo-010 \"Permanent link\")\n\n- Initial release (#13128)\n\n### `llama-index-llms-cohere` \\[0.1.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-cohere-017 \"Permanent link\")\n\n- Call Cohere RAG inference with documents argument (#13196)\n\n### `llama-index-vector-scores-kdbai` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-scores-kdbai-016 \"Permanent link\")\n\n- update add method decode utf-8 (#13194)\n\n### `llama-index-vector-stores-alibabacloud-opensearch` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-alibabacloud-opensearch-010 \"Permanent link\")\n\n- Initial release (#13286)\n\n### `llama-index-tools-multion` \\[0.2.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-tools-multion-020 \"Permanent link\")\n\n- update tool to use updated api/sdk (#13373)\n\n### `llama-index-vector-sores-weaviate` \\[1.0.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-sores-weaviate-100 \"Permanent link\")\n\n- Update to weaviate client v4 (#13229)\n\n### `llama-index-readers-file` \\[0.1.22\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-file-0122 \"Permanent link\")\n\n- fix bug where PDFReader ignores extra\\_info (#13369)\n\n### `llama-index-llms-azure-openai` \\[0.1.8\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-azure-openai-018 \"Permanent link\")\n\n- Add sync httpx client support (#13370)\n\n### `llama-index-llms-openai` \\[0.1.18\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-openai-0118 \"Permanent link\")\n\n- Add sync httpx client support (#13370)\n- Add missing openai model token context (#13337)\n\n### `llama-index-readers-github` \\[0.1.9\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-github-019 \"Permanent link\")\n\n- Add fail\\_on\\_http\\_error (#13366)\n\n### `llama-index-vector-stores-pinecone` \\[0.1.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-pinecone-017 \"Permanent link\")\n\n- Add attribution tag for pinecone (#13329)\n\n### `llama-index-llms-nvidia` \\[0.1.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-nvidia-011 \"Permanent link\")\n\n- set default max\\_tokens to 1024 (#13371)\n\n### `llama-index-readers-papers` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-papers-015_1 \"Permanent link\")\n\n- Fix hiddent temp directory issue for arxiv reader (#13351)\n\n### `llama-index-embeddings-nvidia` \\[0.1.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-nvidia-011 \"Permanent link\")\n\n- fix truncate passing aget\\_query\\_embedding and get\\_text\\_embedding (#13367)\n\n### `llama-index-llms-anyscare` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-anyscare-014 \"Permanent link\")\n\n- Add llama-3 models (#13336)\n\n## \\[2024-05-07\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-05-07 \"Permanent link\")\n\n### `llama-index-agent-introspective` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-agent-introspective-010 \"Permanent link\")\n\n- Add CRITIC and reflection agent integrations (#13108)\n\n### `llama-index-core` \\[0.10.35\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01035 \"Permanent link\")\n\n- fix `from_defaults()` erasing summary memory buffer history (#13325)\n- use existing async event loop instead of `asyncio.run()` in core (#13309)\n- fix async streaming from query engine in condense question chat engine (#13306)\n- Handle ValueError in extract\\_table\\_summaries in element node parsers (#13318)\n- Handle llm properly for QASummaryQueryEngineBuilder and RouterQueryEngine (#13281)\n- expand instrumentation payloads (#13302)\n- Fix Bug in sql join statement missing schema (#13277)\n\n### `llama-index-embeddings-jinaai` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-jinaai-015 \"Permanent link\")\n\n- add encoding\\_type parameters in JinaEmbedding class (#13172)\n- fix encoding type access in JinaEmbeddings (#13315)\n\n### `llama-index-embeddings-nvidia` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-nvidia-010 \"Permanent link\")\n\n- add nvidia nim embeddings support (#13177)\n\n### `llama-index-llms-mistralai` \\[0.1.12\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-mistralai-0112 \"Permanent link\")\n\n- Fix async issue when streaming with Mistral AI (#13292)\n\n### `llama-index-llms-nvidia` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-nvidia-010 \"Permanent link\")\n\n- add nvidia nim llm support (#13176)\n\n### `llama-index-postprocessor-nvidia-rerank` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-postprocessor-nvidia-rerank-010 \"Permanent link\")\n\n- add nvidia nim rerank support (#13178)\n\n### `llama-index-readers-file` \\[0.1.21\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-file-0121 \"Permanent link\")\n\n- Update MarkdownReader to parse text before first header (#13327)\n\n### `llama-index-readers-web` \\[0.1.13\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-web-0113 \"Permanent link\")\n\n- feat: Spider Web Loader (#13200)\n\n### `llama-index-vector-stores-vespa` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-vespa-010 \"Permanent link\")\n\n- Add VectorStore integration for Vespa (#13213)\n\n### `llama-index-vector-stores-vertexaivectorsearch` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-vertexaivectorsearch-010 \"Permanent link\")\n\n- Add support for Vertex AI Vector Search as Vector Store (#13186)\n\n## \\[2024-05-02\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-05-02 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.34\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01034 \"Permanent link\")\n\n- remove error ignoring during chat engine streaming (#13160)\n- add structured planning agent (#13149)\n- update base class for planner agent (#13228)\n- Fix: Error when parse file using SimpleFileNodeParser and file's extension doesn't in FILE\\_NODE\\_PARSERS (#13156)\n- add matching `source_node.node_id` verification to node parsers (#13109)\n- Retrieval Metrics: Updating HitRate and MRR for Evaluation@K documents retrieved. Also adding RR as a separate metric (#12997)\n- Add chat summary memory buffer (#13155)\n\n### `llama-index-indices-managed-zilliz` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-indices-managed-zilliz-013 \"Permanent link\")\n\n- ZillizCloudPipelineIndex accepts flexible params to create pipelines (#10134, #10112)\n\n### `llama-index-llms-huggingface` \\[0.1.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-huggingface-017 \"Permanent link\")\n\n- Add tool usage support with text-generation-inference integration from Hugging Face (#12471)\n\n### `llama-index-llms-maritalk` \\[0.2.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-maritalk-020 \"Permanent link\")\n\n- Add streaming for maritalk (#13207)\n\n### `llama-index-llms-mistral-rs` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-mistral-rs-010 \"Permanent link\")\n\n- Integrate mistral.rs LLM (#13105)\n\n### `llama-index-llms-mymagic` \\[0.1.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-mymagic-017 \"Permanent link\")\n\n- mymagicai api update (#13148)\n\n### `llama-index-llms-nvidia-triton` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-nvidia-triton-015 \"Permanent link\")\n\n- Streaming Support for Nvidia's Triton Integration (#13135)\n\n### `llama-index-llms-ollama` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-ollama-013 \"Permanent link\")\n\n- added async support to ollama llms (#13150)\n\n### `llama-index-readers-microsoft-sharepoint` \\[0.2.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-microsoft-sharepoint-022 \"Permanent link\")\n\n- Exclude access control metadata keys from LLMs and embeddings - SharePoint Reader (#13184)\n\n### `llama-index-readers-web` \\[0.1.11\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-web-0111 \"Permanent link\")\n\n- feat: Browserbase Web Reader (#12877)\n\n### `llama-index-readers-youtube-metadata` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-youtube-metadata-010 \"Permanent link\")\n\n- Added YouTube Metadata Reader (#12975)\n\n### `llama-index-storage-kvstore-redis` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-storage-kvstore-redis-014 \"Permanent link\")\n\n- fix redis kvstore key that was in bytes (#13201)\n\n### `llama-index-vector-stores-azureaisearch` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-azureaisearch-015 \"Permanent link\")\n\n- Respect filter condition for Azure AI Search (#13215)\n\n### `llama-index-vector-stores-chroma` \\[0.1.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-chroma-017 \"Permanent link\")\n\n- small bump for new chroma client version (#13158)\n\n### `llama-index-vector-stores-firestore` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-firestore-010 \"Permanent link\")\n\n- Adding Firestore Vector Store (#12048)\n\n### `llama-index-vector-stores-kdbai` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-kdbai-015 \"Permanent link\")\n\n- small fix to returned IDs after `add()` (#12515)\n\n### `llama-index-vector-stores-milvus` \\[0.1.11\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-milvus-0111 \"Permanent link\")\n\n- Add hybrid retrieval mode to MilvusVectorStore (#13122)\n\n### `llama-index-vector-stores-postgres` \\[0.1.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-postgres-017 \"Permanent link\")\n\n- parameterize queries in pgvector store (#13199)\n\n## \\[2024-04-27\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-04-27 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.33\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01033 \"Permanent link\")\n\n- add agent\\_worker.as\\_agent() (#13061)\n\n### `llama-index-embeddings-bedrock` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-bedrock-015 \"Permanent link\")\n\n- Use Bedrock cohere character limit (#13126)\n\n### `llama-index-tools-google` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-tools-google-015 \"Permanent link\")\n\n- Change default value for attendees to empty list (#13134)\n\n### `llama-index-graph-stores-falkordb` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-graph-stores-falkordb-014 \"Permanent link\")\n\n- Skip index creation error when index already exists (#13085)\n\n### `llama-index-tools-google` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-tools-google-014 \"Permanent link\")\n\n- Fix datetime for google calendar create\\_event api (#13132)\n\n### `llama-index-llms-anthropic` \\[0.1.11\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-anthropic-0111 \"Permanent link\")\n\n- Merge multiple prompts into one (#13131)\n\n### `llama-index-indices-managed-llama-cloud` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-indices-managed-llama-cloud-016 \"Permanent link\")\n\n- Use MetadataFilters in LlamaCloud Retriever (#13117)\n\n### `llama-index-graph-stores-kuzu` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-graph-stores-kuzu-013 \"Permanent link\")\n\n- Fix kuzu integration .execute() calls (#13100)\n\n### `llama-index-vector-stores-lantern` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-lantern-013 \"Permanent link\")\n\n- Maintenance update to keep up to date with lantern builds (#13116)\n\n## \\[2024-04-25\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-04-25 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.32\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01032 \"Permanent link\")\n\n- Corrected wrong output type for `OutputKeys.from_keys()` (#13086)\n- add run\\_jobs to aws base embedding (#13096)\n- allow user to customize the keyword extractor prompt template (#13083)\n- (CondenseQuestionChatEngine) Do not condense the question if there's no conversation history (#13069)\n- QueryPlanTool: Execute tool calls in subsequent (dependent) nodes in the query plan (#13047)\n- Fix for fusion retriever sometime return Nonetype query(s) before similarity search (#13112)\n\n### `llama-index-embeddings-ipex-llm` \\[0.1.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-ipex-llm-011 \"Permanent link\")\n\n- Support llama-index-embeddings-ipex-llm for Intel GPUs (#13097)\n\n### `llama-index-packs-raft-dataset` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-packs-raft-dataset-014 \"Permanent link\")\n\n- Fix bug in raft dataset generator - multiple system prompts (#12751)\n\n### `llama-index-readers-microsoft-sharepoint` \\[0.2.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-microsoft-sharepoint-021 \"Permanent link\")\n\n- Add access control related metadata to SharePoint reader (#13067)\n\n### `llama-index-vector-stores-pinecone` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-pinecone-016 \"Permanent link\")\n\n- Nested metadata filter support (#13113)\n\n### `llama-index-vector-stores-qdrant` \\[0.2.8\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-qdrant-028 \"Permanent link\")\n\n- Nested metadata filter support (#13113)\n\n## \\[2024-04-23\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-04-23 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.31\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01031 \"Permanent link\")\n\n- fix async streaming response from query engine (#12953)\n- enforce uuid in element node parsers (#12951)\n- add function calling LLM program (#12980)\n- make the PydanticSingleSelector work with async api (#12964)\n- fix query pipeline's arun\\_with\\_intermediates (#13002)\n\n### `llama-index-agent-coa` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-agent-coa-010 \"Permanent link\")\n\n- Add COA Agent integration (#13043)\n\n### `llama-index-agent-lats` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-agent-lats-010 \"Permanent link\")\n\n- Official LATs agent integration (#13031)\n\n### `llama-index-agent-llm-compiler` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-agent-llm-compiler-010 \"Permanent link\")\n\n- Add LLMCompiler Agent Integration (#13044)\n\n### `llama-index-llms-anthropic` \\[0.1.10\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-anthropic-0110 \"Permanent link\")\n\n- Add the ability to pass custom headers to Anthropic LLM requests (#12819)\n\n### `llama-index-llms-bedrock` \\[0.1.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-bedrock-017 \"Permanent link\")\n\n- Adding claude 3 opus to BedRock integration (#13033)\n\n### `llama-index-llms-fireworks` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-fireworks-015 \"Permanent link\")\n\n- Add new Llama 3 and Mixtral 8x22b model into Llama Index for Fireworks (#12970)\n\n### `llama-index-llms-openai` \\[0.1.16\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-openai-0116 \"Permanent link\")\n\n- Fix AsyncOpenAI \"RuntimeError: Event loop is closed bug\" when instances of AsyncOpenAI are rapidly created & destroyed (#12946)\n- Don't retry on all OpenAI APIStatusError exceptions - just InternalServerError (#12947)\n\n### `llama-index-llms-watsonx` \\[0.1.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-watsonx-017 \"Permanent link\")\n\n- Updated IBM watsonx foundation models (#12973)\n\n### `llama-index-packs-code-hierarchy` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-packs-code-hierarchy-016 \"Permanent link\")\n\n- Return the parent node if the query node is not present (#12983)\n- fixed bug when function is defined twice (#12941)\n\n### `llama-index-program-openai` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-program-openai-016 \"Permanent link\")\n\n- dding support for streaming partial instances of Pydantic output class in OpenAIPydanticProgram (#13021)\n\n### `llama-index-readers-openapi` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-openapi-010 \"Permanent link\")\n\n- add reader for openapi files (#12998)\n\n### `llama-index-readers-slack` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-slack-014 \"Permanent link\")\n\n- Avoid infinite loop when not handled exception is raised (#12963)\n\n### `llama-index-readers-web` \\[0.1.10\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-web-0110 \"Permanent link\")\n\n- Improve whole site reader to remove duplicate links (#12977)\n\n### `llama-index-retrievers-bedrock` \\[0.1.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-retrievers-bedrock-011 \"Permanent link\")\n\n- Fix Bedrock KB retriever to use query bundle (#12910)\n\n### `llama-index-vector-stores-awsdocdb` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-awsdocdb-010 \"Permanent link\")\n\n- Integrating AWS DocumentDB as a vector storage method (#12217)\n\n### `llama-index-vector-stores-databricks` \\[0.1.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-databricks-012 \"Permanent link\")\n\n- Fix databricks vector search metadata (#12999)\n\n### `llama-index-vector-stores-neo4j` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-neo4j-014 \"Permanent link\")\n\n- Neo4j metadata filtering support (#12923)\n\n### `llama-index-vector-stores-pinecone` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-pinecone-015 \"Permanent link\")\n\n- Fix error querying PineconeVectorStore using sparse query mode (#12967)\n\n### `llama-index-vector-stores-qdrant` \\[0.2.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-qdrant-025 \"Permanent link\")\n\n- Many fixes for async and checking if collection exists (#12916)\n\n### `llama-index-vector-stores-weaviate` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-weaviate-015 \"Permanent link\")\n\n- Adds the index deletion functionality to the WeviateVectoreStore (#12993)\n\n## \\[2024-04-17\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-04-17 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.30\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01030 \"Permanent link\")\n\n- Add intermediate outputs to QueryPipeline (#12683)\n- Fix show progress causing results to be out of order (#12897)\n- add OR filter condition support to simple vector store (#12823)\n- improved custom agent init (#12824)\n- fix pipeline load without docstore (#12808)\n- Use async `_aprocess_actions` in `_arun_step_stream` (#12846)\n- provide the exception to the StreamChatErrorEvent (#12879)\n- fix bug in load and search tool spec (#12902)\n\n### `llama-index-embeddings-azure-opena` \\[0.1.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-azure-opena-017 \"Permanent link\")\n\n- Expose azure\\_ad\\_token\\_provider argument to support token expiration (#12818)\n\n### `llama-index-embeddings-cohere` \\[0.1.8\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-cohere-018 \"Permanent link\")\n\n- Add httpx\\_async\\_client option (#12896)\n\n### `llama-index-embeddings-ipex-llm` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-ipex-llm-010 \"Permanent link\")\n\n- add ipex-llm embedding integration (#12740)\n\n### `llama-index-embeddings-octoai` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-octoai-010 \"Permanent link\")\n\n- add octoai embeddings (#12857)\n\n### `llama-index-llms-azure-openai` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-azure-openai-016 \"Permanent link\")\n\n- Expose azure\\_ad\\_token\\_provider argument to support token expiration (#12818)\n\n### `llama-index-llms-ipex-llm` \\[0.1.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-ipex-llm-012 \"Permanent link\")\n\n- add support for loading \"low-bit format\" model to IpexLLM integration (#12785)\n\n### `llama-index-llms-mistralai` \\[0.1.11\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-mistralai-0111 \"Permanent link\")\n\n- support `open-mixtral-8x22b` (#12894)\n\n### `llama-index-packs-agents-lats` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-packs-agents-lats-010 \"Permanent link\")\n\n- added LATS agent pack (#12735)\n\n### `llama-index-readers-smart-pdf-loader` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-smart-pdf-loader-014 \"Permanent link\")\n\n- Use passed in metadata for documents (#12844)\n\n### `llama-index-readers-web` \\[0.1.9\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-web-019 \"Permanent link\")\n\n- added Firecrawl Web Loader (#12825)\n\n### `llama-index-vector-stores-milvus` \\[0.1.10\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-milvus-0110 \"Permanent link\")\n\n- use batch insertions into Milvus vector store (#12837)\n\n### `llama-index-vector-stores-vearch` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-vearch-010 \"Permanent link\")\n\n- add vearch to vector stores (#10972)\n\n## \\[2024-04-13\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-04-13 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.29\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01029 \"Permanent link\")\n\n- **BREAKING** Moved `PandasQueryEngine` and `PandasInstruction` parser to `llama-index-experimental` (#12419)\n- new install: `pip install -U llama-index-experimental`\n- new import: `from llama_index.experimental.query_engine import PandasQueryEngine`\n- Fixed some core dependencies to make python3.12 work nicely (#12762)\n- update async utils `run_jobs()` to include tqdm description (#12812)\n- Refactor kvdocstore delete methods (#12681)\n\n### `llama-index-llms-bedrock` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-bedrock-016 \"Permanent link\")\n\n- Support for Mistral Large from Bedrock (#12804)\n\n### `llama-index-llms-openvino` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-openvino-010 \"Permanent link\")\n\n- Added OpenVino LLMs (#12639)\n\n### `llama-index-llms-predibase` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-predibase-014 \"Permanent link\")\n\n- Update LlamaIndex-Predibase Integration to latest API (#12736)\n- Enable choice of either Predibase-hosted or HuggingFace-hosted fine-tuned adapters in LlamaIndex-Predibase integration (#12789)\n\n### `llama-index-output-parsers-guardrails` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-output-parsers-guardrails-013 \"Permanent link\")\n\n- Modernize GuardrailsOutputParser (#12676)\n\n### `llama-index-packs-agents-coa` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-packs-agents-coa-010 \"Permanent link\")\n\n- Chain-of-Abstraction Agent Pack (#12757)\n\n### `llama-index-packs-code-hierarchy` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-packs-code-hierarchy-013 \"Permanent link\")\n\n- Fixed issue with chunking multi-byte characters (#12715)\n\n### `llama-index-packs-raft-dataset` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-packs-raft-dataset-014_1 \"Permanent link\")\n\n- Fix bug in raft dataset generator - multiple system prompts (#12751)\n\n### `llama-index-postprocessor-openvino-rerank` \\[0.1.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-postprocessor-openvino-rerank-012 \"Permanent link\")\n\n- Add openvino rerank support (#12688)\n\n### `llama-index-readers-file` \\[0.1.18\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-file-0118 \"Permanent link\")\n\n- convert to Path in docx reader if input path str (#12807)\n- make pip check work for optional pdf packages (#12758)\n\n### `llama-index-readers-s3` \\[0.1.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-s3-017 \"Permanent link\")\n\n- wrong doc id when using default s3 endpoint in S3Reader (#12803)\n\n### `llama-index-retrievers-bedrock` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-retrievers-bedrock-010 \"Permanent link\")\n\n- Add Amazon Bedrock knowledge base integration as retriever (#12737)\n\n### `llama-index-retrievers-mongodb-atlas-bm25-retriever` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-retrievers-mongodb-atlas-bm25-retriever-013 \"Permanent link\")\n\n- Add mongodb atlas bm25 retriever (#12519)\n\n### `llama-index-storage-chat-store-redis` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-storage-chat-store-redis-013 \"Permanent link\")\n\n- fix message serialization in redis chat store (#12802)\n\n### `llama-index-vector-stores-astra-db` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-astra-db-016 \"Permanent link\")\n\n- Relax dependency version to accept astrapy `1.*` (#12792)\n\n### `llama-index-vector-stores-couchbase` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-couchbase-010 \"Permanent link\")\n\n- Add support for Couchbase as a Vector Store (#12680)\n\n### `llama-index-vector-stores-elasticsearch` \\[0.1.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-elasticsearch-017 \"Permanent link\")\n\n- Fix elasticsearch hybrid rrf window\\_size (#12695)\n\n### `llama-index-vector-stores-milvus` \\[0.1.8\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-milvus-018 \"Permanent link\")\n\n- Added support to retrieve metadata fields from milvus (#12626)\n\n### `llama-index-vector-stores-redis` \\[0.2.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-redis-020 \"Permanent link\")\n\n- Modernize redis vector store, use redisvl (#12386)\n\n### `llama-index-vector-stores-qdrant` \\[0.2.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-qdrant-020 \"Permanent link\")\n\n- refactor: Switch default Qdrant sparse encoder (#12512)\n\n## \\[2024-04-09\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-04-09 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.28\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01028 \"Permanent link\")\n\n- Support indented code block fences in markdown node parser (#12393)\n- Pass in output parser to guideline evaluator (#12646)\n- Added example of query pipeline + memory (#12654)\n- Add missing node postprocessor in CondensePlusContextChatEngine async mode (#12663)\n- Added `return_direct` option to tools /tool metadata (#12587)\n- Add retry for batch eval runner (#12647)\n- Thread-safe instrumentation (#12638)\n- Coroutine-safe instrumentation Spans #12589\n- Add in-memory loading for non-default filesystems in PDFReader (#12659)\n- Remove redundant tokenizer call in sentence splitter (#12655)\n- Add SynthesizeComponent import to shortcut imports (#12655)\n- Improved truncation in SimpleSummarize (#12655)\n- adding err handling in eval\\_utils default\\_parser for correctness (#12624)\n- Add async\\_postprocess\\_nodes at RankGPT Postprocessor Nodes (#12620)\n- Fix MarkdownNodeParser ref\\_doc\\_id (#12615)\n\n### `llama-index-embeddings-openvino` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-openvino-015 \"Permanent link\")\n\n- Added initial support for openvino embeddings (#12643)\n\n### `llama-index-llms-anthropic` \\[0.1.9\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-anthropic-019 \"Permanent link\")\n\n- add anthropic tool calling (#12591)\n\n### `llama-index-llms-ipex-llm` \\[0.1.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-ipex-llm-011 \"Permanent link\")\n\n- add ipex-llm integration (#12322)\n- add more data types support to ipex-llm llm integration (#12635)\n\n### `llama-index-llms-openllm` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-openllm-014 \"Permanent link\")\n\n- Proper PrivateAttr usage in OpenLLM (#12655)\n\n### `llama-index-multi-modal-llms-anthropic` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-multi-modal-llms-anthropic-014 \"Permanent link\")\n\n- Bumped anthropic dep version (#12655)\n\n### `llama-index-multi-modal-llms-gemini` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-multi-modal-llms-gemini-015 \"Permanent link\")\n\n- bump generativeai dep (#12645)\n\n### `llama-index-packs-dense-x-retrieval` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-packs-dense-x-retrieval-014 \"Permanent link\")\n\n- Add streaming support for DenseXRetrievalPack (#12607)\n\n### `llama-index-readers-mongodb` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-mongodb-014 \"Permanent link\")\n\n- Improve efficiency of MongoDB reader (#12664)\n\n### `llama-index-readers-wikipedia` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-wikipedia-014 \"Permanent link\")\n\n- Added multilingual support for the Wikipedia reader (#12616)\n\n### `llama-index-storage-index-store-elasticsearch` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-storage-index-store-elasticsearch-013 \"Permanent link\")\n\n- remove invalid chars from default collection name (#12672)\n\n### `llama-index-vector-stores-milvus` \\[0.1.8\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-milvus-018_1 \"Permanent link\")\n\n- Added support to retrieve metadata fields from milvus (#12626)\n- Bug fix - Similarity metric is always IP for MilvusVectorStore (#12611)\n\n## \\[2024-04-04\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-04-04 \"Permanent link\")\n\n### `llama-index-agent-openai` \\[0.2.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-agent-openai-022 \"Permanent link\")\n\n- Update imports for message thread typing (#12437)\n\n### `llama-index-core` \\[0.10.27\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01027 \"Permanent link\")\n\n- Fix for pydantic query engine outputs being blank (#12469)\n- Add span\\_id attribute to Events (instrumentation) (#12417)\n- Fix RedisDocstore node retrieval from docs property (#12324)\n- Add node-postprocessors to retriever\\_tool (#12415)\n- FLAREInstructQueryEngine : delegating retriever api if the query engine supports it (#12503)\n- Make chat message to dict safer (#12526)\n- fix check in batch eval runner for multi-kwargs (#12563)\n- Fixes agent\\_react\\_multimodal\\_step.py bug with partial args (#12566)\n\n### `llama-index-embeddings-clip` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-clip-015 \"Permanent link\")\n\n- Added support to load clip model from local file path (#12577)\n\n### `llama-index-embeddings-cloudflar-workersai` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-cloudflar-workersai-010 \"Permanent link\")\n\n- text embedding integration: Cloudflare Workers AI (#12446)\n\n### `llama-index-embeddings-voyageai` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-voyageai-014 \"Permanent link\")\n\n- Fix pydantic issue in class definition (#12469)\n\n### `llama-index-finetuning` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-finetuning-015 \"Permanent link\")\n\n- Small typo fix in QA generation prompt (#12470)\n\n### `llama-index-graph-stores-falkordb` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-graph-stores-falkordb-013 \"Permanent link\")\n\n- Replace redis driver with FalkorDB driver (#12434)\n\n### `llama-index-llms-anthropic` \\[0.1.8\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-anthropic-018 \"Permanent link\")\n\n- Add ability to pass custom HTTP headers to Anthropic client (#12558)\n\n### `llama-index-llms-cohere` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-cohere-016 \"Permanent link\")\n\n- Add support for Cohere Command R+ model (#12581)\n\n### `llama-index-llms-databricks` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-databricks-010 \"Permanent link\")\n\n- Integrations with DataBricks LLM API (#12432)\n\n### `llama-index-llms-watsonx` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-watsonx-016 \"Permanent link\")\n\n- Updated Watsonx foundation models (#12493)\n- Updated base model name on watsonx integration #12491\n\n### `lama-index-postprocessor-rankllm-rerank` \\[0.1.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#lama-index-postprocessor-rankllm-rerank-012 \"Permanent link\")\n\n- Add RankGPT support inside RankLLM (#12475)\n\n### `llama-index-readers-microsoft-sharepoint` \\[0.1.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-microsoft-sharepoint-017 \"Permanent link\")\n\n- Use recursive strategy by default for SharePoint (#12557)\n\n### `llama-index-readers-web` \\[0.1.8\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-web-018 \"Permanent link\")\n\n- Readability web page reader fix playwright async api bug (#12520)\n\n### `llama-index-vector-stores-kdbai` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-kdbai-015_1 \"Permanent link\")\n\n- small `to_list` fix (#12515)\n\n### `llama-index-vector-stores-neptune` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-neptune-010 \"Permanent link\")\n\n- Add support for Neptune Analytics as a Vector Store (#12423)\n\n### `llama-index-vector-stores-postgres` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-postgres-015 \"Permanent link\")\n\n- fix(postgres): numeric metadata filters (#12583)\n\n## \\[2024-03-31\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-03-31 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.26\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01026 \"Permanent link\")\n\n- pass proper query bundle in QueryFusionRetriever (#12387)\n- Update llama\\_parse\\_json\\_element.py to fix error on lists (#12402)\n- Add node postprocessors to retriever tool (#12415)\n- Fix bug where user specified llm is not respected in fallback logic in element node parsers(#12403)\n- log proper LLM response key for async callback manager events (#12421)\n- Deduplicate the two built-in react system prompts; Also make it read from a Markdown file (#12307)\n- fix bug in BatchEvalRunner for multi-evaluator eval\\_kwargs\\_lists (#12418)\n- add the callback manager event for vector store index insert\\_nodes (#12443)\n- fixes an issue with serializing chat messages into chat stores when they contain pydantic API objects (#12394)\n- fixes an issue with slow memory.get() operation (caused by multiple calls to get\\_all()) (#12394)\n- fixes an issue where an agent+tool message pair is cut from the memory (#12394)\n- Added `FnNodeMapping` for object index (#12391)\n- Make object mapping optional / hidden for object index (#12391)\n- Make object index easier to create from existing vector db (#12391)\n- When LLM failed to follow the react response template, tell it so #12300\n\n### `llama-index-embeddings-cohere` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-cohere-015 \"Permanent link\")\n\n- Bump cohere version to 5.1.1 (#12279)\n\n### `llama-index-embeddings-itrex` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-itrex-010 \"Permanent link\")\n\n- add Intel Extension for Transformers embedding model (#12410)\n\n### `llama-index-graph-stores-neo4j` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-graph-stores-neo4j-014 \"Permanent link\")\n\n- make neo4j query insensitive (#12337)\n\n### `llama-index-llms-cohere` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-cohere-015 \"Permanent link\")\n\n- Bump cohere version to 5.1.1 (#12279)\n\n### `llama-index-llms-ipex-llm` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-ipex-llm-010 \"Permanent link\")\n\n- add ipex-llm integration (#12322)\n\n### `llama-index-llms-litellm` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-litellm-014 \"Permanent link\")\n\n- Fix litellm ChatMessage role validation error (#12449)\n\n### `llama-index-llms-openai` \\[0.1.14\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-openai-0114 \"Permanent link\")\n\n- Use `FunctionCallingLLM` base class in OpenAI (#12227)\n\n### `llama-index-packs-self-rag` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-packs-self-rag-014 \"Permanent link\")\n\n- Fix llama-index-core dep (#12374)\n\n### `llama-index-postprocessor-cohere-rerank` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-postprocessor-cohere-rerank-014 \"Permanent link\")\n\n- Bump cohere version to 5.1.1 (#12279)\n\n### `llama-index-postprocessor-rankllm-rerank` \\[0.1.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-postprocessor-rankllm-rerank-011 \"Permanent link\")\n\n- Added RankLLM rerank (#12296)\n- RankLLM fixes (#12399)\n\n### `llama-index-readers-papers` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-papers-014 \"Permanent link\")\n\n- Fixed bug with path names (#12366)\n\n### `llama-index-vector-stores-analyticdb` \\[0.1.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-analyticdb-011 \"Permanent link\")\n\n- Add AnalyticDB VectorStore (#12230)\n\n### `llama-index-vector-stores-kdbai` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-kdbai-014 \"Permanent link\")\n\n- Fixed typo in imports/readme (#12370)\n\n### `llama-index-vector-stores-qdrant` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-qdrant-015 \"Permanent link\")\n\n- add `in` filter operator for qdrant (#12376)\n\n## \\[2024-03-27\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-03-27 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.25\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01025 \"Permanent link\")\n\n- Add score to NodeWithScore in KnowledgeGraphQueryEngine (#12326)\n- Batch eval runner fixes (#12302)\n\n### `llama-index-embeddings-cohere` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-cohere-015_1 \"Permanent link\")\n\n- Added support for binary / quantized embeddings (#12321)\n\n### `llama-index-llms-mistralai` \\[0.1.10\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-mistralai-0110 \"Permanent link\")\n\n- add support for custom endpoints to MistralAI (#12328)\n\n### `llama-index-storage-kvstore-redis` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-storage-kvstore-redis-013 \"Permanent link\")\n\n- Fix RedisDocstore node retrieval from docs property (#12324)\n\n## \\[2024-03-26\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-03-26 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.24\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01024 \"Permanent link\")\n\n- pretty prints in `LlamaDebugHandler` (#12216)\n- stricter interpreter constraints on pandas query engine (#12278)\n- PandasQueryEngine can now execute 'pd.\\*' functions (#12240)\n- delete proper metadata in docstore delete function (#12276)\n- improved openai agent parsing function hook (#12062)\n- add raise\\_on\\_error flag for SimpleDirectoryReader (#12263)\n- remove un-caught openai import in core (#12262)\n- Fix download\\_llama\\_dataset and download\\_llama\\_pack (#12273)\n- Implement EvalQueryEngineTool (#11679)\n- Expand instrumenation Span coverage for AgentRunner (#12249)\n- Adding concept of function calling agent/llm (mistral supported for now) (#12222, )\n\n### `llama-index-embeddings-huggingface` \\[0.2.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-huggingface-020 \"Permanent link\")\n\n- Use `sentence-transformers` as a backend (#12277)\n\n### `llama-index-postprocessor-voyageai-rerank` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-postprocessor-voyageai-rerank-010 \"Permanent link\")\n\n- Added voyageai as a reranker (#12111)\n\n### `llama-index-readers-gcs` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-gcs-010 \"Permanent link\")\n\n- Added google cloud storage reader (#12259)\n\n### `llama-index-readers-google` \\[0.2.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-google-021 \"Permanent link\")\n\n- Support for different drives (#12146)\n- Remove unnecessary PyDrive dependency from Google Drive Reader (#12257)\n\n### `llama-index-readers-readme` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-readme-010 \"Permanent link\")\n\n- added readme.com reader (#12246)\n\n### `llama-index-packs-raft` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-packs-raft-013 \"Permanent link\")\n\n- added pack for RAFT (#12275)\n\n## \\[2024-03-23\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-03-23 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.23\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01023 \"Permanent link\")\n\n- Added `(a)predict_and_call()` function to base LLM class + openai + mistralai (#12188)\n- fixed bug with `wait()` in async agent streaming (#12187)\n\n### `llama-index-embeddings-alephalpha` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-alephalpha-010 \"Permanent link\")\n\n- Added alephalpha embeddings (#12149)\n\n### `llama-index-llms-alephalpha` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-alephalpha-010 \"Permanent link\")\n\n- Added alephalpha LLM (#12149)\n\n### `llama-index-llms-openai` \\[0.1.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-openai-017 \"Permanent link\")\n\n- fixed bug with `wait()` in async agent streaming (#12187)\n\n### `llama-index-readers-docugami` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-docugami-014 \"Permanent link\")\n\n- fixed import errors in docugami reader (#12154)\n\n### `llama-index-readers-file` \\[0.1.12\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-file-0112 \"Permanent link\")\n\n- fix PDFReader for remote fs (#12186)\n\n## \\[2024-03-21\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-03-21 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.22\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01022 \"Permanent link\")\n\n- Updated docs backend from sphinx to mkdocs, added ALL api reference, some light re-org, better search (#11301)\n- Added async loading to `BaseReader` class (although its fake async for now) (#12156)\n- Fix path implementation for non-local FS in `SimpleDirectoryReader` (#12141)\n- add args/kwargs to spans, payloads for retrieval events, in instrumentation (#12147)\n- \\[react agent\\] Upon exception, say so, so that Agent can correct itself (#12137)\n\n### `llama-index-embeddings-together` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-together-013 \"Permanent link\")\n\n- Added rate limit handling (#12127)\n\n### `llama-index-graph-stores-neptune` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-graph-stores-neptune-013 \"Permanent link\")\n\n- Add Amazon Neptune Support as Graph Store (#12097)\n\n### `llama-index-llms-vllm` \\[0.1.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-vllm-017 \"Permanent link\")\n\n- fix VllmServer to work without CUDA-required vllm core (#12003)\n\n### `llama-index-readers-s3` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-s3-014 \"Permanent link\")\n\n- Use S3FS in S3Reader (#12061)\n\n### `llama-index-storage-docstore-postgres` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-storage-docstore-postgres-013 \"Permanent link\")\n\n- Added proper kvstore dep (#12157)\n\n### `llama-index-storage-index-store-postgres` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-storage-index-store-postgres-013 \"Permanent link\")\n\n- Added proper kvstore dep (#12157)\n\n### `llama-index-vector-stores-elasticsearch` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-elasticsearch-016 \"Permanent link\")\n\n- fix unclosed session in es add function #12135\n\n### `llama-index-vector-stores-kdbai` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-kdbai-013 \"Permanent link\")\n\n- Add support for `KDBAIVectorStore` (#11967)\n\n## \\[2024-03-20\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-03-20 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.21\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01021 \"Permanent link\")\n\n- Lazy init for async elements StreamingAgentChatResponse (#12116)\n- Fix streaming generators get bug by SynthesisEndEvent (#12092)\n- CLIP embedding more models (#12063)\n\n### `llama-index-packs-raptor` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-packs-raptor-013 \"Permanent link\")\n\n- Add `num_workers` to summary module (#)\n\n### `llama-index-readers-telegram` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-telegram-015 \"Permanent link\")\n\n- Fix datetime fields (#12112)\n- Add ability to select time period of posts/messages (#12078)\n\n### `llama-index-embeddings-openai` \\[0.1.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-openai-017 \"Permanent link\")\n\n- Add api version/base api as optional for open ai embedding (#12091)\n\n### `llama-index-networks` \\[0.2.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-networks-021 \"Permanent link\")\n\n- Add node postprocessing to network retriever (#12027)\n- Add privacy-safe networks demo (#12027)\n\n### `llama-index-callbacks-langfuse` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-callbacks-langfuse-013 \"Permanent link\")\n\n- Chore: bumps min version of langfuse dep (#12077)\n\n### `llama-index-embeddings-google` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-google-014 \"Permanent link\")\n\n- Chore: bumps google-generativeai dep (#12085)\n\n### `llama-index-embeddings-gemini` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-gemini-015 \"Permanent link\")\n\n- Chore: bumps google-generativeai dep (#12085)\n\n### `llama-index-llms-gemini` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-gemini-016 \"Permanent link\")\n\n- Chore: bumps google-generativeai dep (#12085)\n\n### `llama-index-llms-palm` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-palm-014 \"Permanent link\")\n\n- Chore: bumps google-generativeai dep (#12085)\n\n### `llama-index-multi-modal-llms-google` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-multi-modal-llms-google-014 \"Permanent link\")\n\n- Chore: bumps google-generativeai dep (#12085)\n\n### `llama-index-vector-stores-google` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-google-015 \"Permanent link\")\n\n- Chore: bumps google-generativeai dep (#12085)\n\n### `llama-index-storage-kvstore-elasticsearch` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-storage-kvstore-elasticsearch-010 \"Permanent link\")\n\n- New integration (#12068)\n\n### `llama-index-readers-google` \\[0.1.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-google-017 \"Permanent link\")\n\n- Fix - Google Drive Issue of not loading same name files (#12022)\n\n### `llama-index-vector-stores-upstash` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-upstash-013 \"Permanent link\")\n\n- Adding Metadata Filtering support for UpstashVectorStore (#12054)\n\n### `llama-index-packs-raptor` \\[0.1.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-packs-raptor-012 \"Permanent link\")\n\n- Fix: prevent RaptorPack infinite recursion (#12008)\n\n### `llama-index-embeddings-huggingface-optimum` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-huggingface-optimum-014 \"Permanent link\")\n\n- Fix(OptimumEmbedding): removing token\\_type\\_ids causing ONNX validation issues\n\n### `llama-index-llms-anthropic` \\[0.1.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-anthropic-017 \"Permanent link\")\n\n- Fix: Anthropic LLM merge consecutive messages with same role (#12013)\n\n### `llama-index-packs-diff-private-simple-dataset` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-packs-diff-private-simple-dataset-010 \"Permanent link\")\n\n- DiffPrivacy ICL Pack - OpenAI Completion LLMs (#11881)\n\n### `llama-index-cli` \\[0.1.11\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-cli-0111 \"Permanent link\")\n\n- Remove llama\\_hub\\_url keyword from download\\_llama\\_dataset of command (#12038)\n\n## \\[2024-03-14\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-03-14 \"Permanent link\")\n\n### `llama-index-core` \\[0.10.20\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01020 \"Permanent link\")\n\n- New `instrumentation` module for observability (#11831)\n- Allow passing in LLM for `CitationQueryEngine` (#11914)\n- Updated keyval docstore to allow changing suffix in addition to namespace (#11873)\n- Add (some) async streaming support to query\\_engine #11949\n\n### `llama-index-embeddings-dashscope` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-dashscope-013 \"Permanent link\")\n\n- Fixed embedding type for query texts (#11901)\n\n### `llama-index-embeddings-premai` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-premai-013 \"Permanent link\")\n\n- Support for premai embeddings (#11954)\n\n### `llama-index-networks` \\[0.2.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-networks-020 \"Permanent link\")\n\n- Added support for network retrievers (#11800)\n\n### `llama-index-llms-anthropic` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-anthropic-016 \"Permanent link\")\n\n- Added support for haiku (#11916)\n\n### `llama-index-llms-mistralai` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-mistralai-016 \"Permanent link\")\n\n- Fixed import error for ChatMessage (#11902)\n\n### `llama-index-llms-openai` \\[0.1.11\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-openai-0111 \"Permanent link\")\n\n- added gpt-35-turbo-0125 for AZURE\\_TURBO\\_MODELS (#11956)\n- fixed error with nontype in logprobs (#11967)\n\n### `llama-index-llms-premai` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-premai-014 \"Permanent link\")\n\n- Support for premai llm (#11954)\n\n### `llama-index-llms-solar` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-solar-013 \"Permanent link\")\n\n- Support for solar as an LLM class (#11710)\n\n### `llama-index-llms-vertex` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-vertex-015 \"Permanent link\")\n\n- Add support for medlm in vertex (#11911)\n\n### `llama-index-readers-goolge` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-goolge-016 \"Permanent link\")\n\n- added README files and query string for google drive reader (#11724)\n\n### `llama-index-readers-file` \\[0.1.11\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-file-0111 \"Permanent link\")\n\n- Updated ImageReader to add `plain_text` option to trigger pytesseract (#11913)\n\n### `llama-index-readers-pathway` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-pathway-013 \"Permanent link\")\n\n- use pure requests to reduce deps, simplify code (#11924)\n\n### `llama-index-retrievers-pathway` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-retrievers-pathway-013 \"Permanent link\")\n\n- use pure requests to reduce deps, simplify code (#11924)\n\n### `llama-index-storage-docstore-mongodb` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-storage-docstore-mongodb-013 \"Permanent link\")\n\n- Allow changing suffix for mongodb docstore (#11873)\n\n### `llama-index-vector-stores-databricks` \\[0.1.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-databricks-011 \"Permanent link\")\n\n- Support for databricks vector search as a vector store (#10754)\n\n### `llama-index-vector-stores-opensearch` \\[0.1.8\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-opensearch-018 \"Permanent link\")\n\n- (re)implement proper delete (#11959)\n\n### `llama-index-vector-stores-postgres` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-postgres-014 \"Permanent link\")\n\n- Fixes for IN filters and OR text search (#11872, #11927)\n\n## \\[2024-03-12\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-03-12 \"Permanent link\")\n\n### `llama-index-cli` \\[0.1.9\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-cli-019 \"Permanent link\")\n\n- Removed chroma as a bundled dep to reduce `llama-index` deps\n\n### `llama-index-core` \\[0.10.19\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01019 \"Permanent link\")\n\n- Introduce retries for rate limits in `OpenAI` llm class (#11867)\n- Added table comments to SQL table schemas in `SQLDatabase` (#11774)\n- Added `LogProb` type to `ChatResponse` object (#11795)\n- Introduced `LabelledSimpleDataset` (#11805)\n- Fixed insert `IndexNode` objects with unserializable objects (#11836)\n- Fixed stream chat type error when writing response to history in `CondenseQuestionChatEngine` (#11856)\n- Improve post-processing for json query engine (#11862)\n\n### `llama-index-embeddings-cohere` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-cohere-014 \"Permanent link\")\n\n- Fixed async kwarg error (#11822)\n\n### `llama-index-embeddings-dashscope` \\[0.1.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-dashscope-012 \"Permanent link\")\n\n- Fixed pydantic import (#11765)\n\n### `llama-index-graph-stores-neo4j` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-graph-stores-neo4j-013 \"Permanent link\")\n\n- Properly close connection after verifying connectivity (#11821)\n\n### `llama-index-llms-cohere` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-cohere-013 \"Permanent link\")\n\n- Add support for new `command-r` model (#11852)\n\n### `llama-index-llms-huggingface` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-huggingface-014 \"Permanent link\")\n\n- Fixed streaming decoding with special tokens (#11807)\n\n### `llama-index-llms-mistralai` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-mistralai-015 \"Permanent link\")\n\n- Added support for latest and open models (#11792)\n\n### `llama-index-tools-finance` \\[0.1.1\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-tools-finance-011 \"Permanent link\")\n\n- Fixed small bug when passing in the API get for stock news (#11772)\n\n### `llama-index-vector-stores-chroma` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-chroma-016 \"Permanent link\")\n\n- Slimmed down chroma deps (#11775)\n\n### `llama-index-vector-stores-lancedb` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-lancedb-013 \"Permanent link\")\n\n- Fixes for deleting (#11825)\n\n### `llama-index-vector-stores-postgres` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-postgres-013 \"Permanent link\")\n\n- Support for nested metadata filters (#11778)\n\n## \\[2024-03-07\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-03-07 \"Permanent link\")\n\n### `llama-index-callbacks-deepeval` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-callbacks-deepeval-013 \"Permanent link\")\n\n- Update import path for callback handler (#11754)\n\n### `llama-index-core` \\[0.10.18\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01018 \"Permanent link\")\n\n- Ensure `LoadAndSearchToolSpec` loads document objects (#11733)\n- Fixed bug for no nodes in `QueryFusionRetriever` (#11759)\n- Allow using different runtime kwargs for different evaluators in `BatchEvalRunner` (#11727)\n- Fixed issues with fsspec + `SimpleDirectoryReader` (#11665)\n- Remove `asyncio.run()` requirement from guideline evaluator (#11719)\n\n### `llama-index-embeddings-voyageai` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-embeddings-voyageai-013 \"Permanent link\")\n\n- Update voyage embeddings to use proper clients (#11721)\n\n### `llama-index-indices-managed-vectara` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-indices-managed-vectara-013 \"Permanent link\")\n\n- Fixed issues with vectara query engine in non-summary mode (#11668)\n\n### `llama-index-llms-mymagic` \\[0.1.5\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-llms-mymagic-015 \"Permanent link\")\n\n- Add `return_output` option for json output with query and response (#11761)\n\n### `llama-index-packs-code-hierarchy` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-packs-code-hierarchy-010 \"Permanent link\")\n\n- Added support for a `CodeHiearchyAgentPack` that allows for agentic traversal of a codebase (#10671)\n\n### `llama-index-packs-cohere-citation-chat` \\[0.1.3\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-packs-cohere-citation-chat-013 \"Permanent link\")\n\n- Added a new llama-pack for citations + chat with cohere (#11697)\n\n### `llama-index-vector-stores-milvus` \\[0.1.6\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-milvus-016 \"Permanent link\")\n\n- Prevent forced `flush()` on document add (#11734)\n\n### `llama-index-vector-stores-opensearch` \\[0.1.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-opensearch-017 \"Permanent link\")\n\n- Small typo in metadata column name (#11751)\n\n### `llama-index-vector-stores-tidbvector` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-tidbvector-010 \"Permanent link\")\n\n- Initial support for TiDB vector store (#11635)\n\n### `llama-index-vector-stores-weaviate` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-weaviate-014 \"Permanent link\")\n\n- Small fix for `int` fields in metadata filters (#11742)\n\n## \\[2024-03-06\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#2024-03-06 \"Permanent link\")\n\nNew format! Going to try out reporting changes per package.\n\n### `llama-index-cli` \\[0.1.8\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-cli-018 \"Permanent link\")\n\n- Update mappings for `upgrade` command (#11699)\n\n### `llama-index-core` \\[0.10.17\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-core-01017 \"Permanent link\")\n\n- add `relative_score` and `dist_based_score` to `QueryFusionRetriever` (#11667)\n- check for `none` in async agent queue (#11669)\n- allow refine template for `BaseSQLTableQueryEngine` (#11378)\n- update mappings for llama-packs (#11699)\n- fixed index error for extracting rel texts in KG index (#11695)\n- return proper response types from synthesizer when no nodes (#11701)\n- Inherit metadata to summaries in DocumentSummaryIndex (#11671)\n- Inherit callback manager in sql query engines (#11662)\n- Fixed bug with agent streaming not being written to chat history (#11675)\n- Fixed a small bug with `none` deltas when streaming a function call with an agent (#11713)\n\n### `llama-index-multi-modal-llms-anthropic` \\[0.1.2\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-multi-modal-llms-anthropic-012 \"Permanent link\")\n\n- Added support for new multi-modal models `haiku` and `sonnet` (#11656)\n\n### `llama-index-packs-finchat` \\[0.1.0\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-packs-finchat-010 \"Permanent link\")\n\n- Added a new llama-pack for hierarchical agents + finance chat (#11387)\n\n### `llama-index-readers-file` \\[0.1.8\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-file-018 \"Permanent link\")\n\n- Added support for checking if NLTK files are already downloaded (#11676)\n\n### `llama-index-readers-json` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-readers-json-014 \"Permanent link\")\n\n- Use the metadata passed in when creating documents (#11626)\n\n### `llama-index-vector-stores-astra-db` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-astra-db-014 \"Permanent link\")\n\n- Update wording in warning message (#11702)\n\n### `llama-index-vector-stores-opensearch` \\[0.1.7\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-opensearch-017_1 \"Permanent link\")\n\n- Avoid calling `nest_asyncio.apply()` in code to avoid confusing errors for users (#11707)\n\n### `llama-index-vector-stores-qdrant` \\[0.1.4\\] [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#llama-index-vector-stores-qdrant-014 \"Permanent link\")\n\n- Catch RPC errors (#11657)\n\n## \\[0.10.16\\] - 2024-03-05 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#01016-2024-03-05 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features \"Permanent link\")\n\n- Anthropic support for new models (#11623, #11612)\n- Easier creation of chat prompts (#11583)\n- Added a raptor retriever llama-pack (#11527)\n- Improve batch cohere embeddings through bedrock (#11572)\n- Added support for vertex AI embeddings (#11561)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits \"Permanent link\")\n\n- Ensure order in async embeddings generation (#11562)\n- Fixed empty metadata for csv reader (#11563)\n- Serializable fix for composable retrievers (#11617)\n- Fixed milvus metadata filter support (#11566)\n- FIxed pydantic import in clickhouse vector store (#11631)\n- Fixed system prompts for gemini/vertext-gemini (#11511)\n\n## \\[0.10.15\\] - 2024-03-01 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#01015-2024-03-01 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_1 \"Permanent link\")\n\n- Added FeishuWikiReader (#11491)\n- Added videodb retriever integration (#11463)\n- Added async to opensearch vector store (#11513)\n- New LangFuse one-click callback handler (#11324)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_1 \"Permanent link\")\n\n- Fixed deadlock issue with async chat streaming (#11548)\n- Improved hidden file check in SimpleDirectoryReader (#11496)\n- Fixed null values in document metadata when using SimpleDirectoryReader (#11501)\n- Fix for sqlite utils in jsonalyze query engine (#11519)\n- Added base url and timeout to ollama multimodal LLM (#11526)\n- Updated duplicate handling in query fusion retriever (#11542)\n- Fixed bug in kg indexx struct updating (#11475)\n\n## \\[0.10.14\\] - 2024-02-28 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#01014-2024-02-28 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_2 \"Permanent link\")\n\n- Released llama-index-networks (#11413)\n- Jina reranker (#11291)\n- Added DuckDuckGo agent search tool (#11386)\n- helper functions for chatml (#10272)\n- added brave search tool for agents (#11468)\n- Added Friendli LLM integration (#11384)\n- metadata only queries for chromadb (#11328)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_2 \"Permanent link\")\n\n- Fixed inheriting llm callback in synthesizers (#11404)\n- Catch delete error in milvus (#11315)\n- Fixed pinecone kwargs issue (#11422)\n- Supabase metadata filtering fix (#11428)\n- api base fix in gemini embeddings (#11393)\n- fix elasticsearch vector store await (#11438)\n- vllm server cuda fix (#11442)\n- fix for passing LLM to context chat engine (#11444)\n- set input types for cohere embeddings (#11288)\n- default value for azure ad token (#10377)\n- added back prompt mixin for react agent (#10610)\n- fixed system roles for gemini (#11481)\n- fixed mean agg pooling returning numpy float values (#11458)\n- improved json path parsing for JSONQueryEngine (#9097)\n\n## \\[0.10.13\\] - 2024-02-26 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#01013-2024-02-26 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_3 \"Permanent link\")\n\n- Added a llama-pack for KodaRetriever, for on-the-fly alpha tuning (#11311)\n- Added support for `mistral-large` (#11398)\n- Last token pooling mode for huggingface embeddings models like SFR-Embedding-Mistral (#11373)\n- Added fsspec support to SimpleDirectoryReader (#11303)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_3 \"Permanent link\")\n\n- Fixed an issue with context window + prompt helper (#11379)\n- Moved OpenSearch vector store to BasePydanticVectorStore (#11400)\n- Fixed function calling in fireworks LLM (#11363)\n- Made cohere embedding types more automatic (#11288)\n- Improve function calling in react agent (#11280)\n- Fixed MockLLM imports (#11376)\n\n## \\[0.10.12\\] - 2024-02-22 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#01012-2024-02-22 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_4 \"Permanent link\")\n\n- Added `llama-index-postprocessor-colbert-rerank` package (#11057)\n- `MyMagicAI` LLM (#11263)\n- `MariaTalk` LLM (#10925)\n- Add retries to github reader (#10980)\n- Added FireworksAI embedding and LLM modules (#10959)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_4 \"Permanent link\")\n\n- Fixed string formatting in weaviate (#11294)\n- Fixed off-by-one error in semantic splitter (#11295)\n- Fixed `download_llama_pack` for multiple files (#11272)\n- Removed `BUILD` files from packages (#11267)\n- Loosened python version reqs for all packages (#11267)\n- Fixed args issue with chromadb (#11104)\n\n## \\[0.10.11\\] - 2024-02-21 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#01011-2024-02-21 \"Permanent link\")\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_5 \"Permanent link\")\n\n- Fixed multi-modal LLM for async acomplete (#11064)\n- Fixed issue with llamaindex-cli imports (#11068)\n\n## \\[0.10.10\\] - 2024-02-20 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#01010-2024-02-20 \"Permanent link\")\n\nI'm still a bit wonky with our publishing process -- apologies. This is just a version\nbump to ensure the changes that were supposed to happen in 0.10.9 actually\ndid get published. (AF)\n\n## \\[0.10.9\\] - 2024-02-20 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0109-2024-02-20 \"Permanent link\")\n\n- add llama-index-cli dependency\n\n## \\[0.10.7\\] - 2024-02-19 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0107-2024-02-19 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_5 \"Permanent link\")\n\n- Added Self-Discover llamapack (#10951)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_6 \"Permanent link\")\n\n- Fixed linting in CICD (#10945)\n- Fixed using remote graph stores (#10971)\n- Added missing LLM kwarg in NoText response synthesizer (#10971)\n- Fixed openai import in rankgpt (#10971)\n- Fixed resolving model name to string in openai embeddings (#10971)\n- Off by one error in sentence window node parser (#10971)\n\n## \\[0.10.6\\] - 2024-02-17 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0106-2024-02-17 \"Permanent link\")\n\nFirst, apologies for missing the changelog the last few versions. Trying to figure out the best process with 400+ packages.\n\nAt some point, each package will have a dedicated changelog.\n\nBut for now, onto the \"master\" changelog.\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_6 \"Permanent link\")\n\n- Added `NomicHFEmbedding` (#10762)\n- Added `MinioReader` (#10744)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_7 \"Permanent link\")\n\n- Various fixes for clickhouse vector store (#10799)\n- Fix index name in neo4j vector store (#10749)\n- Fixes to sagemaker embeddings (#10778)\n- Fixed performance issues when splitting nodes (#10766)\n- Fix non-float values in reranker + b25 (#10930)\n- OpenAI-agent should be a dep of openai program (#10930)\n- Add missing shortcut imports for query pipeline components (#10930)\n- Fix NLTK and tiktoken not being bundled properly with core (#10930)\n- Add back `llama_index.core.__version__` (#10930)\n\n## \\[0.10.3\\] - 2024-02-13 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0103-2024-02-13 \"Permanent link\")\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_8 \"Permanent link\")\n\n- Fixed passing in LLM to `as_chat_engine` (#10605)\n- Fixed system prompt formatting for anthropic (#10603)\n- Fixed elasticsearch vector store error on `__version__` (#10656)\n- Fixed import on openai pydantic program (#10657)\n- Added client back to opensearch vector store exports (#10660)\n- Fixed bug in SimpleDirectoryReader not using file loaders properly (#10655)\n- Added lazy LLM initialization to RankGPT (#10648)\n- Fixed bedrock embedding `from_credentials` passing ing the model name (#10640)\n- Added back recent changes to TelegramReader (#10625)\n\n## \\[0.10.0, 0.10.1\\] - 2024-02-12 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0100-0101-2024-02-12 \"Permanent link\")\n\n### Breaking Changes [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#breaking-changes \"Permanent link\")\n\n- Several changes are introduced. See the [full blog post](https://blog.llamaindex.ai/llamaindex-v0-10-838e735948f8) for complete details.\n\n## \\[0.9.48\\] - 2024-02-12 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0948-2024-02-12 \"Permanent link\")\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_9 \"Permanent link\")\n\n- Add back deprecated API for BedrockEmbdding (#10581)\n\n## \\[0.9.47\\] - 2024-02-11 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0947-2024-02-11 \"Permanent link\")\n\nLast patch before v0.10!\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_7 \"Permanent link\")\n\n- add conditional links to query pipeline (#10520)\n- refactor conditional links + add to cookbook (#10544)\n- agent + query pipeline cleanups (#10563)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_10 \"Permanent link\")\n\n- Add sleep to fix lag in chat stream (#10339)\n- OllamaMultiModal kwargs (#10541)\n- Update Ingestion Pipeline to handle empty documents (#10543)\n- Fixing minor spelling error (#10516)\n- fix elasticsearch async check (#10549)\n- Docs/update slack demo colab (#10534)\n- Adding the possibility to use the IN operator for PGVectorStore (#10547)\n- fix agent reset (#10562)\n- Fix MD duplicated Node id from multiple docs (#10564)\n\n## \\[0.9.46\\] - 2024-02-08 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0946-2024-02-08 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_8 \"Permanent link\")\n\n- Update pooling strategy for embedding models (#10536)\n- Add Multimodal Video RAG example (#10530)\n- Add SECURITY.md (#10531)\n- Move agent module guide up one-level (#10519)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_11 \"Permanent link\")\n\n- Deeplake fixes (#10529)\n- Add Cohere section for llamaindex (#10523)\n- Fix md element (#10510)\n\n## \\[0.9.45.post1\\] - 2024-02-07 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0945post1-2024-02-07 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_9 \"Permanent link\")\n\n- Upgraded deeplake vector database to use BasePydanticVectorStore (#10504)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_12 \"Permanent link\")\n\n- Fix MD parser for inconsistency tables (#10488)\n- Fix ImportError for pypdf in MetadataExtractionSEC.ipynb (#10491)\n\n## \\[0.9.45\\] - 2024-02-07 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0945-2024-02-07 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_10 \"Permanent link\")\n\n- Refactor: add AgentRunner.from\\_llm method (#10452)\n- Support custom prompt formatting for non-chat LLMS (#10466)\n- Bump cryptography from 41.0.7 to 42.0.0 (#10467)\n- Add persist and load method for Colbert Index (#10477)\n- Allow custom agent to take in user inputs (#10450)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_13 \"Permanent link\")\n\n- remove exporter from arize-phoenix global callback handler (#10465)\n- Fixing Dashscope qwen llm bug (#10471)\n- Fix: calling AWS Bedrock models (#10443)\n- Update Azure AI Search (fka Azure Cognitive Search) vector store integration to latest client SDK 11.4.0 stable + updating jupyter notebook sample (#10416)\n- fix some imports (#10485)\n\n## \\[0.9.44\\] - 2024-02-05 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0944-2024-02-05 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_11 \"Permanent link\")\n\n- ollama vision cookbook (#10438)\n- Support Gemini \"transport\" configuration (#10457)\n- Add Upstash Vector (#10451)\n\n## \\[0.9.43\\] - 2024-02-03 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0943-2024-02-03 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_12 \"Permanent link\")\n\n- Add multi-modal ollama (#10434)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_14 \"Permanent link\")\n\n- update base class for astradb (#10435)\n\n## \\[0.9.42.post1\\] - 2024-02-02 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0942post1-2024-02-02 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_13 \"Permanent link\")\n\n- Add Async support for Base nodes parser (#10418)\n\n## \\[0.9.42\\] - 2024-02-02 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0942-2024-02-02 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_14 \"Permanent link\")\n\n- Add support for `gpt-3.5-turbo-0125` (#10412)\n- Added `create-llama` support to rag cli (#10405)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_15 \"Permanent link\")\n\n- Fixed minor bugs in lance-db vector store (#10404)\n- Fixed streaming bug in ollama (#10407)\n\n## \\[0.9.41\\] - 2024-02-01 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0941-2024-02-01 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_15 \"Permanent link\")\n\n- Nomic Embedding (#10388)\n- Dashvector support sparse vector (#10386)\n- Table QA with MarkDownParser and Benchmarking (#10382)\n- Simple web page reader (#10395)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_16 \"Permanent link\")\n\n- fix full node content in KeywordExtractor (#10398)\n\n## \\[0.9.40\\] - 2024-01-30 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0940-2024-01-30 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_16 \"Permanent link\")\n\n- Improve and fix bugs for MarkdownElementNodeParser (#10340)\n- Fixed and improve Perplexity support for new models (#10319)\n- Ensure system\\_prompt is passed to Perplexity LLM (#10326)\n- Extended BaseRetrievalEvaluator to include an optional PostProcessor (#10321)\n\n## \\[0.9.39\\] - 2024-01-26 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0939-2024-01-26 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_17 \"Permanent link\")\n\n- Support for new GPT Turbo Models (#10291)\n- Support Multiple docs for Sentence Transformer Fine tuning(#10297)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_17 \"Permanent link\")\n\n- Marvin imports fixed (#9864)\n\n## \\[0.9.38\\] - 2024-01-25 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0938-2024-01-25 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_18 \"Permanent link\")\n\n- Support for new OpenAI v3 embedding models (#10279)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_18 \"Permanent link\")\n\n- Extra checks on sparse embeddings for qdrant (#10275)\n\n## \\[0.9.37\\] - 2024-01-24 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0937-2024-01-24 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_19 \"Permanent link\")\n\n- Added a RAG CLI utility (#10193)\n- Added a textai vector store (#10240)\n- Added a Postgresql based docstore and index store (#10233)\n- specify tool spec in tool specs (#10263)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_19 \"Permanent link\")\n\n- Fixed serialization error in ollama chat (#10230)\n- Added missing fields to `SentenceTransformerRerank` (#10225)\n- Fixed title extraction (#10209, #10226)\n- nit: make chainable output parser more exposed in library/docs (#10262)\n- :bug: summary index not carrying over excluded metadata keys (#10259)\n\n## \\[0.9.36\\] - 2024-01-23 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0936-2024-01-23 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_20 \"Permanent link\")\n\n- Added support for `SageMakerEmbedding` (#10207)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_20 \"Permanent link\")\n\n- Fix duplicated `file_id` on openai assistant (#10223)\n- Fix circular dependencies for programs (#10222)\n- Run `TitleExtractor` on groups of nodes from the same parent document (#10209)\n- Improve vectara auto-retrieval (#10195)\n\n## \\[0.9.35\\] - 2024-01-22 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0935-2024-01-22 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_21 \"Permanent link\")\n\n- `beautifulsoup4` dependency to new optional extra `html` (#10156)\n- make `BaseNode.hash` an `@property` (#10163)\n- Neutrino (#10150)\n- feat: JSONalyze Query Engine (#10067)\n- \\[wip\\] add custom hybrid retriever notebook (#10164)\n- add from\\_collection method to ChromaVectorStore class (#10167)\n- CLI experiment v0: ask (#10168)\n- make react agent prompts more editable (#10154)\n- Add agent query pipeline (#10180)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_21 \"Permanent link\")\n\n- Update supabase vecs metadata filter function to support multiple fields (#10133)\n- Bugfix/code improvement for LanceDB integration (#10144)\n- `beautifulsoup4` optional dependency (#10156)\n- Fix qdrant aquery hybrid search (#10159)\n- make hash a @property (#10163)\n- fix: bug on poetry install of llama-index\\[postgres\\] (#10171)\n- \\[doc\\] update jaguar vector store documentation (#10179)\n- Remove use of not-launched finish\\_message (#10188)\n- Updates to Lantern vector stores docs (#10192)\n- fix typo in multi\\_document\\_agents.ipynb (#10196)\n\n## \\[0.9.34\\] - 2024-01-19 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0934-2024-01-19 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_22 \"Permanent link\")\n\n- Added SageMakerEndpointLLM (#10140)\n- Added support for Qdrant filters (#10136)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_22 \"Permanent link\")\n\n- Update bedrock utils for Claude 2:1 (#10139)\n- BugFix: deadlocks using multiprocessing (#10125)\n\n## \\[0.9.33\\] - 2024-01-17 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0933-2024-01-17 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_23 \"Permanent link\")\n\n- Added RankGPT as a postprocessor (#10054)\n- Ensure backwards compatibility with new Pinecone client version bifucation (#9995)\n- Recursive retriever all the things (#10019)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_23 \"Permanent link\")\n\n- BugFix: When using markdown element parser on a table containing comma (#9926)\n- extend auto-retrieval notebook (#10065)\n- Updated the Attribute name in llm\\_generators (#10070)\n- jaguar vector store add text\\_tag to add\\_kwargs in add() (#10057)\n\n## \\[0.9.32\\] - 2024-01-16 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0932-2024-01-16 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_24 \"Permanent link\")\n\n- added query-time row retrieval + fix nits with query pipeline over structured data (#10061)\n- ReActive Agents w/ Context + updated stale link (#10058)\n\n## \\[0.9.31\\] - 2024-01-15 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0931-2024-01-15 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_25 \"Permanent link\")\n\n- Added selectors and routers to query pipeline (#9979)\n- Added sparse-only search to qdrant vector store (#10041)\n- Added Tonic evaluators (#10000)\n- Adding async support to firestore docstore (#9983)\n- Implement mongodb docstore `put_all` method (#10014)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_24 \"Permanent link\")\n\n- Properly truncate sql results based on `max_string_length` (#10015)\n- Fixed `node.resolve_image()` for base64 strings (#10026)\n- Fixed cohere system prompt role (#10020)\n- Remove redundant token counting operation in SentenceSplitter (#10053)\n\n## \\[0.9.30\\] - 2024-01-11 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0930-2024-01-11 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_26 \"Permanent link\")\n\n- Implements a Node Parser using embeddings for Semantic Splitting (#9988)\n- Add Anyscale Embedding model support (#9470)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_25 \"Permanent link\")\n\n- nit: fix pandas get prompt (#10001)\n- Fix: Token counting bug (#9912)\n- Bump jinja2 from 3.1.2 to 3.1.3 (#9997)\n- Fix corner case for qdrant hybrid search (#9993)\n- Bugfix: sphinx generation errors (#9944)\n- Fix: `language` used before assignment in `CodeSplitter` (#9987)\n- fix inconsistent name \"text\\_parser\" in section \"Use a Text Splitter\u2026 (#9980)\n- :bug: fixing batch size (#9982)\n- add auto-async execution to query pipelines (#9967)\n- :bug: fixing init (#9977)\n- Parallel Loading with SimpleDirectoryReader (#9965)\n- do not force delete an index in milvus (#9974)\n\n## \\[0.9.29\\] - 2024-01-10 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0929-2024-01-10 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_27 \"Permanent link\")\n\n- Added support for together.ai models (#9962)\n- Added support for batch redis/firestore kvstores, async firestore kvstore (#9827)\n- Parallelize `IngestionPipeline.run()` (#9920)\n- Added new query pipeline components: function, argpack, kwargpack (#9952)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_26 \"Permanent link\")\n\n- Updated optional langchain imports to avoid warnings (#9964)\n- Raise an error if empty nodes are embedded (#9953)\n\n## \\[0.9.28\\] - 2024-01-09 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0928-2024-01-09 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_28 \"Permanent link\")\n\n- Added support for Nvidia TenorRT LLM (#9842)\n- Allow `tool_choice` to be set during agent construction (#9924)\n- Added streaming support for `QueryPipeline` (#9919)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_27 \"Permanent link\")\n\n- Set consistent doc-ids for llama-index readers (#9923, #9916)\n- Remove unneeded model inputs for HuggingFaceEmbedding (#9922)\n- Propagate `tool_choice` flag to downstream APIs (#9901)\n- Add `chat_store_key` to chat memory `from_defaults()` (#9928)\n\n## \\[0.9.27\\] - 2024-01-08 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0927-2024-01-08 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_29 \"Permanent link\")\n\n- add query pipeline (#9908)\n- Feature: Azure Multi Modal (fixes: #9471) (#9843)\n- add postgres docker (#9906)\n- Vectara auto\\_retriever (#9865)\n- Redis Chat Store support (#9880)\n- move more classes to core (#9871)\n\n### Bug Fixes / Nits / Smaller Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits-smaller-features \"Permanent link\")\n\n- Propagate `tool_choice` flag to downstream APIs (#9901)\n- filter out negative indexes from faiss query (#9907)\n- added NE filter for qdrant payloads (#9897)\n- Fix incorrect id assignment in MyScale query result (#9900)\n- Qdrant Text Match Filter (#9895)\n- Fusion top k for hybrid search (#9894)\n- Fix (#9867) sync\\_to\\_async to avoid blocking during asynchronous calls (#9869)\n- A single node passed into compute\\_scores returns as a float (#9866)\n- Remove extra linting steps (#9878)\n- add vectara links (#9886)\n\n## \\[0.9.26\\] - 2024-01-05 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0926-2024-01-05 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_30 \"Permanent link\")\n\n- Added a `BaseChatStore` and `SimpleChatStore` abstraction for dedicated chat memory storage (#9863)\n- Enable custom `tree_sitter` parser to be passed into `CodeSplitter` (#9845)\n- Created a `BaseAutoRetriever` base class, to allow other retrievers to extend to auto modes (#9846)\n- Added support for Nvidia Triton LLM (#9488)\n- Added `DeepEval` one-click observability (#9801)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_28 \"Permanent link\")\n\n- Updated the guidance integration to work with the latest version (#9830)\n- Made text storage optional for doctores/ingestion pipeline (#9847)\n- Added missing `sphinx-automodapi` dependency for docs (#9852)\n- Return actual node ids in weaviate query results (#9854)\n- Added prompt formatting to LangChainLLM (#9844)\n\n## \\[0.9.25\\] - 2024-01-03 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0925-2024-01-03 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_31 \"Permanent link\")\n\n- Added concurrancy limits for dataset generation (#9779)\n- New `deepeval` one-click observability handler (#9801)\n- Added jaguar vector store (#9754)\n- Add beta multimodal ReAct agent (#9807)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_29 \"Permanent link\")\n\n- Changed default batch size for OpenAI embeddings to 100 (#9805)\n- Use batch size properly for qdrant upserts (#9814)\n- `_verify_source_safety` uses AST, not regexes, for proper safety checks (#9789)\n- use provided LLM in element node parsers (#9776)\n- updated legacy vectordb loading function to be more robust (#9773)\n- Use provided http client in AzureOpenAI (#9772)\n\n## \\[0.9.24\\] - 2023-12-30 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0924-2023-12-30 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_32 \"Permanent link\")\n\n- Add reranker for BEIR evaluation (#9743)\n- Add Pathway integration. (#9719)\n- custom agents implementation + notebook (#9746)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_30 \"Permanent link\")\n\n- fix beam search for vllm: add missing parameter (#9741)\n- Fix alpha for hrbrid search (#9742)\n- fix token counter (#9744)\n- BM25 tokenizer lowercase (#9745)\n\n## \\[0.9.23\\] - 2023-12-28 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0923-2023-12-28 \"Permanent link\")\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_31 \"Permanent link\")\n\n- docs: fixes qdrant\\_hybrid.ipynb typos (#9729)\n- make llm completion program more general (#9731)\n- Refactor MM Vector store and Index for empty collection (#9717)\n- Adding IF statement to check for Schema using \"Select\" (#9712)\n- allow skipping module loading in `download_module` and `download_llama_pack` (#9734)\n\n## \\[0.9.22\\] - 2023-12-26 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0922-2023-12-26 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_33 \"Permanent link\")\n\n- Added `.iter_data()` method to `SimpleDirectoryReader` (#9658)\n- Added async support to `Ollama` LLM (#9689)\n- Expanding pinecone filter support for `in` and `not in` (#9683)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_32 \"Permanent link\")\n\n- Improve BM25Retriever performance (#9675)\n- Improved qdrant hybrid search error handling (#9707)\n- Fixed `None` handling in `ChromaVectorStore` (#9697)\n- Fixed postgres schema creation if not existing (#9712)\n\n## \\[0.9.21\\] - 2023-12-23 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0921-2023-12-23 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_34 \"Permanent link\")\n\n- Added zilliz cloud as a managed index (#9605)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_33 \"Permanent link\")\n\n- Bedrock client and LLM fixes (#9671, #9646)\n\n## \\[0.9.20\\] - 2023-12-21 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0920-2023-12-21 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_35 \"Permanent link\")\n\n- Added `insert_batch_size` to limit number of embeddings held in memory when creating an index, defaults to 2048 (#9630)\n- Improve auto-retrieval (#9647)\n- Configurable Node ID Generating Function (#9574)\n- Introduced action input parser (#9575)\n- qdrant sparse vector support (#9644)\n- Introduced upserts and delete in ingestion pipeline (#9643)\n- Add Zilliz Cloud Pipeline as a Managed Index (#9605)\n- Add support for Google Gemini models via VertexAI (#9624)\n- support allowing additional metadata filters on autoretriever (#9662)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_34 \"Permanent link\")\n\n- Fix pip install commands in LM Format Enforcer notebooks (#9648)\n- Fixing some more links and documentations (#9633)\n- some bedrock nits and fixes (#9646)\n\n## \\[0.9.19\\] - 2023-12-20 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0919-2023-12-20 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_36 \"Permanent link\")\n\n- new llama datasets `LabelledEvaluatorDataset` & `LabelledPairwiseEvaluatorDataset` (#9531)\n\n## \\[0.9.18\\] - 2023-12-20 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0918-2023-12-20 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_37 \"Permanent link\")\n\n- multi-doc auto-retrieval guide (#9631)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_35 \"Permanent link\")\n\n- fix(vllm): make Vllm's 'complete' method behave the same as other LLM class (#9634)\n- FIx Doc links and other documentation issue (#9632)\n\n## \\[0.9.17\\] - 2023-12-19 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0917-2023-12-19 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_38 \"Permanent link\")\n\n- \\[example\\] adding user feedback (#9601)\n- FEATURE: Cohere ReRank Relevancy Metric for Retrieval Eval (#9495)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_36 \"Permanent link\")\n\n- Fix Gemini Chat Mode (#9599)\n- Fixed `types-protobuf` from being a primary dependency (#9595)\n- Adding an optional auth token to the TextEmbeddingInference class (#9606)\n- fix: out of index get latest tool call (#9608)\n- fix(azure\\_openai.py): add missing return to subclass override (#9598)\n- fix mix up b/w 'formatted' and 'format' params for ollama api call (#9594)\n\n## \\[0.9.16\\] - 2023-12-18 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0916-2023-12-18 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_39 \"Permanent link\")\n\n- agent refactor: step-wise execution (#9584)\n- Add OpenRouter, with Mixtral demo (#9464)\n- Add hybrid search to neo4j vector store (#9530)\n- Add support for auth service accounts for Google Semantic Retriever (#9545)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_37 \"Permanent link\")\n\n- Fixed missing `default=None` for `LLM.system_prompt` (#9504)\n- Fix #9580 : Incorporate metadata properly (#9582)\n- Integrations: Gradient\\[Embeddings,LLM\\] - sdk-upgrade (#9528)\n- Add mixtral 8x7b model to anyscale available models (#9573)\n- Gemini Model Checks (#9563)\n- Update OpenAI fine-tuning with latest changes (#9564)\n- fix/Reintroduce `WHERE` filter to the Sparse Query for PgVectorStore (#9529)\n- Update Ollama API to ollama v0.1.16 (#9558)\n- ollama: strip invalid `formatted` option (#9555)\n- add a device in optimum push #9541 (#9554)\n- Title vs content difference for Gemini Embedding (#9547)\n- fix pydantic fields to float (#9542)\n\n## \\[0.9.15\\] - 2023-12-13 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0915-2023-12-13 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_40 \"Permanent link\")\n\n- Added full support for Google Gemini text+vision models (#9452)\n- Added new Google Semantic Retriever (#9440)\n- added `from_existing()` method + async support to OpenAI assistants (#9367)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_38 \"Permanent link\")\n\n- Fixed huggingface LLM system prompt and messages to prompt (#9463)\n- Fixed ollama additional kwargs usage (#9455)\n\n## \\[0.9.14\\] - 2023-12-11 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0914-2023-12-11 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_41 \"Permanent link\")\n\n- Add MistralAI LLM (#9444)\n- Add MistralAI Embeddings (#9441)\n- Add `Ollama` Embedding class (#9341)\n- Add `FlagEmbeddingReranker` for reranking (#9285)\n- feat: PgVectorStore support advanced metadata filtering (#9377)\n- Added `sql_only` parameter to SQL query engines to avoid executing SQL (#9422)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_39 \"Permanent link\")\n\n- Feat/PgVector Support custom hnsw.ef\\_search and ivfflat.probes (#9420)\n- fix F1 score definition, update copyright year (#9424)\n- Change more than one image input for Replicate Multi-modal models from error to warning (#9360)\n- Removed GPT-Licensed `aiostream` dependency (#9403)\n- Fix result of BedrockEmbedding with Cohere model (#9396)\n- Only capture valid tool names in react agent (#9412)\n- Fixed `top_k` being multiplied by 10 in azure cosmos (#9438)\n- Fixed hybrid search for OpenSearch (#9430)\n\n### Breaking Changes [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#breaking-changes_1 \"Permanent link\")\n\n- Updated the base `LLM` interface to match `LLMPredictor` (#9388)\n- Deprecated `LLMPredictor` (#9388)\n\n## \\[0.9.13\\] - 2023-12-06 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0913-2023-12-06 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_42 \"Permanent link\")\n\n- Added batch prediction support for `LabelledRagDataset` (#9332)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_40 \"Permanent link\")\n\n- Fixed save and load for faiss vector store (#9330)\n\n## \\[0.9.12\\] - 2023-12-05 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0912-2023-12-05 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_43 \"Permanent link\")\n\n- Added an option `reuse_client` to openai/azure to help with async timeouts. Set to `False` to see improvements (#9301)\n- Added support for `vLLM` llm (#9257)\n- Add support for python 3.12 (#9304)\n- Support for `claude-2.1` model name (#9275)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_41 \"Permanent link\")\n\n- Fix embedding format for bedrock cohere embeddings (#9265)\n- Use `delete_kwargs` for filtering in weaviate vector store (#9300)\n- Fixed automatic qdrant client construction (#9267)\n\n## \\[0.9.11\\] - 2023-12-03 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0911-2023-12-03 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_44 \"Permanent link\")\n\n- Make `reference_contexts` optional in `LabelledRagDataset` (#9266)\n- Re-organize `download` module (#9253)\n- Added document management to ingestion pipeline (#9135)\n- Add docs for `LabelledRagDataset` (#9228)\n- Add submission template notebook and other doc updates for `LabelledRagDataset` (#9273)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_42 \"Permanent link\")\n\n- Convert numpy to list for `InstructorEmbedding` (#9255)\n\n## \\[0.9.10\\] - 2023-11-30 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0910-2023-11-30 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_45 \"Permanent link\")\n\n- Advanced Metadata filter for vector stores (#9216)\n- Amazon Bedrock Embeddings New models (#9222)\n- Added PromptLayer callback integration (#9190)\n- Reuse file ids for `OpenAIAssistant` (#9125)\n\n### Breaking Changes / Deprecations [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#breaking-changes-deprecations \"Permanent link\")\n\n- Deprecate ExactMatchFilter (#9216)\n\n## \\[0.9.9\\] - 2023-11-29 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#099-2023-11-29 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_46 \"Permanent link\")\n\n- Add new abstractions for `LlamaDataset`'s (#9165)\n- Add metadata filtering and MMR mode support for `AstraDBVectorStore` (#9193)\n- Allowing newest `scikit-learn` versions (#9213)\n\n### Breaking Changes / Deprecations [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#breaking-changes-deprecations_1 \"Permanent link\")\n\n- Added `LocalAI` demo and began deprecation cycle (#9151)\n- Deprecate `QueryResponseDataset` and `DatasetGenerator` of `evaluation` module (#9165)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_43 \"Permanent link\")\n\n- Fix bug in `download_utils.py` with pointing to wrong repo (#9215)\n- Use `azure_deployment` kwarg in `AzureOpenAILLM` (#9174)\n- Fix similarity score return for `AstraDBVectorStore` Integration (#9193)\n\n## \\[0.9.8\\] - 2023-11-26 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#098-2023-11-26 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_47 \"Permanent link\")\n\n- Add `persist` and `persist_from_dir` methods to `ObjectIndex` that are able to support it (#9064)\n- Added async metadata extraction + pipeline support (#9121)\n- Added back support for start/end char idx in nodes (#9143)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_44 \"Permanent link\")\n\n- Fix for some kwargs not being set properly in global service context (#9137)\n- Small fix for `memory.get()` when system/prefix messages are large (#9149)\n- Minor fixes for global service context (#9137)\n\n## \\[0.9.7\\] - 2023-11-24 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#097-2023-11-24 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_48 \"Permanent link\")\n\n- Add support for `PGVectoRsStore` (#9087)\n- Enforcing `requests>=2.31` for security, while unpinning `urllib3` (#9108)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_45 \"Permanent link\")\n\n- Increased default memory token limit for context chat engine (#9123)\n- Added system prompt to `CondensePlusContextChatEngine` that gets prepended to the `context_prompt` (#9123)\n- Fixed bug in `CondensePlusContextChatEngine` not using chat history properly (#9129)\n\n## \\[0.9.6\\] - 2023-11-22 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#096-2023-11-22 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_49 \"Permanent link\")\n\n- Added `default_headers` argument to openai LLMs (#9090)\n- Added support for `download_llama_pack()` and LlamaPack integrations\n- Added support for `llamaindex-cli` command line tool\n\n### Bug Fixed / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixed-nits \"Permanent link\")\n\n- store normalize as bool for huggingface embedding (#9089)\n\n## \\[0.9.5\\] - 2023-11-21 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#095-2023-11-21 \"Permanent link\")\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_46 \"Permanent link\")\n\n- Fixed bug with AzureOpenAI logic for inferring if stream chunk is a tool call (#9018)\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_50 \"Permanent link\")\n\n- `FastEmbed` embeddings provider (#9043)\n- More precise testing of `OpenAILike` (#9026)\n- Added callback manager to each retriever (#8871)\n- Ability to bypass `max_tokens` inference with `OpenAILike` (#9032)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_47 \"Permanent link\")\n\n- Fixed bug in formatting chat prompt templates when estimating chunk sizes (#9025)\n- Sandboxed Pandas execution, remediate CVE-2023-39662 (#8890)\n- Restored `mypy` for Python 3.8 (#9031)\n- Loosened `dataclasses-json` version range,\nand removes unnecessary `jinja2` extra from `pandas` (#9042)\n\n## \\[0.9.4\\] - 2023-11-19 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#094-2023-11-19 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_51 \"Permanent link\")\n\n- Added `CondensePlusContextChatEngine` (#8949)\n\n### Smaller Features / Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#smaller-features-bug-fixes-nits \"Permanent link\")\n\n- Fixed bug with `OpenAIAgent` inserting errors into chat history (#9000)\n- Fixed various bugs with LiteLLM and the new OpenAI client (#9003)\n- Added context window attribute to perplexity llm (#9012)\n- Add `node_parser` attribute back to service context (#9013)\n- Refactor MM retriever classes (#8998)\n- Fix TextNode instantiation on SupabaseVectorIndexDemo (#8994)\n\n## \\[0.9.3\\] - 2023-11-17 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#093-2023-11-17 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_52 \"Permanent link\")\n\n- Add perplexity LLM integration (#8734)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_48 \"Permanent link\")\n\n- Fix token counting for new openai client (#8981)\n- Fix small pydantic bug in postgres vector db (#8962)\n- Fixed `chunk_overlap` and `doc_id` bugs in `HierarchicalNodeParser` (#8983)\n\n## \\[0.9.2\\] - 2023-11-16 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#092-2023-11-16 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_53 \"Permanent link\")\n\n- Added new notebook guide for Multi-Modal Rag Evaluation (#8945)\n- Added `MultiModalRelevancyEvaluator`, and `MultiModalFaithfulnessEvaluator` (#8945)\n\n## \\[0.9.1\\] - 2023-11-15 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#091-2023-11-15 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_54 \"Permanent link\")\n\n- Added Cohere Reranker fine-tuning (#8859)\n- Support for custom httpx client in `AzureOpenAI` LLM (#8920)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_49 \"Permanent link\")\n\n- Fixed issue with `set_global_service_context` not propagating settings (#8940)\n- Fixed issue with building index with Google Palm embeddings (#8936)\n- Fixed small issue with parsing ImageDocuments/Nodes that have no text (#8938)\n- Fixed issue with large data inserts in Astra DB (#8937)\n- Optimize `QueryEngineTool` for agents (#8933)\n\n## \\[0.9.0\\] - 2023-11-15 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#090-2023-11-15 \"Permanent link\")\n\n### New Features / Breaking Changes / Deprecations [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features-breaking-changes-deprecations \"Permanent link\")\n\n- New `IngestionPipeline` concept for ingesting and transforming data\n- Data ingestion and transforms are now automatically cached\n- Updated interface for node parsing/text splitting/metadata extraction modules\n- Changes to the default tokenizer, as well as customizing the tokenizer\n- Packaging/Installation changes with PyPi (reduced bloat, new install options)\n- More predictable and consistent import paths\n- Plus, in beta: MultiModal RAG Modules for handling text and images!\n- Find more details at: `https://medium.com/@llama_index/719f03282945`\n\n## \\[0.8.69.post1\\] - 2023-11-13 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0869post1-2023-11-13 \"Permanent link\")\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_50 \"Permanent link\")\n\n- Increase max weaivate delete size to max of 10,000 (#8887)\n- Final pickling remnant fix (#8902)\n\n## \\[0.8.69\\] - 2023-11-13 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0869-2023-11-13 \"Permanent link\")\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_51 \"Permanent link\")\n\n- Fixed bug in loading pickled objects (#8880)\n- Fix `custom_path` vs `custom_dir` in `download_loader` (#8865)\n\n## \\[0.8.68\\] - 2023-11-11 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0868-2023-11-11 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_55 \"Permanent link\")\n\n- openai assistant agent + advanced retrieval cookbook (#8863)\n- add retrieval API benchmark (#8850)\n- Add JinaEmbedding class (#8704)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_52 \"Permanent link\")\n\n- Improved default timeouts/retries for OpenAI (#8819)\n- Add back key validation for OpenAI (#8819)\n- Disable automatic LLM/Embedding model downloads, give informative error (#8819)\n- fix openai assistant tool creation + retrieval notebook (#8862)\n- Quick fix Replicate MultiModal example (#8861)\n- fix: paths treated as hidden (#8860)\n- fix Replicate multi-modal LLM + notebook (#8854)\n- Feature/citation metadata (#8722)\n- Fix ImageNode type from NodeWithScore for SimpleMultiModalQueryEngine (#8844)\n\n## \\[0.8.67\\] - 2023-11-10 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0867-2023-11-10 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_56 \"Permanent link\")\n\n- Advanced Multi Modal Retrieval Example and docs (#8822, #8823)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_53 \"Permanent link\")\n\n- Fix retriever node postprocessors for `CitationQueryEngine` (#8818)\n- Fix `cannot pickle 'builtins.CoreBPE' object` in most scenarios (#8835)\n\n## \\[0.8.66\\] - 2023-11-09 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0866-2023-11-09 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_57 \"Permanent link\")\n\n- Support parallel function calling with new OpenAI client in `OpenAIPydanticProgram` (#8793)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_54 \"Permanent link\")\n\n- Fix bug in pydantic programs with new OpenAI client (#8793)\n- Fixed bug with un-listable fsspec objects (#8795)\n\n## \\[0.8.65\\] - 2023-11-08 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0865-2023-11-08 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_58 \"Permanent link\")\n\n- `OpenAIAgent` parallel function calling (#8738)\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_59 \"Permanent link\")\n\n- Properly supporting Hugging Face recommended model (#8784)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_55 \"Permanent link\")\n\n- Fixed missing import for `embeddings.__all__` (#8779)\n\n### Breaking Changes / Deprecations [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#breaking-changes-deprecations_2 \"Permanent link\")\n\n- Use `tool_choice` over `function_call` and `tool` over `functions` in `OpenAI(LLM)` (#8738)\n- Deprecate `to_openai_function` in favor of `to_openai_tool` (#8738)\n\n## \\[0.8.64\\] - 2023-11-06 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0864-2023-11-06 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_60 \"Permanent link\")\n\n- `OpenAIAgent` parallel function calling (#8738)\n- Add AI assistant agent (#8735)\n- OpenAI GPT4v Abstraction (#8719)\n- Add support for `Lantern` VectorStore (#8714)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_56 \"Permanent link\")\n\n- Fix returning zero nodes in elastic search vector store (#8746)\n- Add try/except for `SimpleDirectoryReader` loop to avoid crashing on a single document (#8744)\n- Fix for `deployment_name` in async embeddings (#8748)\n\n## \\[0.8.63\\] - 2023-11-05 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0863-2023-11-05 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_61 \"Permanent link\")\n\n- added native sync and async client support for the lasted `openai` client package (#8712)\n- added support for `AzureOpenAIEmbedding` (#8712)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_57 \"Permanent link\")\n\n- Fixed errors about \"no host supplied\" with `download_loader` (#8723)\n\n### Breaking Changes [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#breaking-changes_2 \"Permanent link\")\n\n- `OpenAIEmbedding` no longer supports azure, moved into the `AzureOpenAIEmbedding` class (#8712)\n\n## \\[0.8.62.post1\\] - 2023-11-05 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0862post1-2023-11-05 \"Permanent link\")\n\n### Breaking Changes [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#breaking-changes_3 \"Permanent link\")\n\n- add new devday models (#8713)\n- moved `max_docs` parameter from constructor to `lazy_load_data()` for `SimpleMongoReader` (#8686)\n\n## \\[0.8.61\\] - 2023-11-05 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0861-2023-11-05 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_62 \"Permanent link\")\n\n- \\[experimental\\] Hyperparameter tuner (#8687)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_58 \"Permanent link\")\n\n- Fix typo error in CohereAIModelName class: cohere light models was missing v3 (#8684)\n- Update deeplake.py (#8683)\n\n## \\[0.8.60\\] - 2023-11-04 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0860-2023-11-04 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_63 \"Permanent link\")\n\n- prompt optimization guide (#8659)\n- VoyageEmbedding (#8634)\n- Multilingual support for `YoutubeTranscriptReader` (#8673)\n- emotion prompt guide (#8674)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_59 \"Permanent link\")\n\n- Adds mistral 7b instruct v0.1 to available anyscale models (#8652)\n- Make pgvector's setup (extension, schema, and table creation) optional (#8656)\n- Allow init of stores\\_text variable for Pinecone vector store (#8633)\n- fix: azure ad support (#8667)\n- Fix nltk bug in multi-threaded environments (#8668)\n- Fix google colab link in cohereai notebook (#8677)\n- passing max\\_tokens to the `Cohere` llm (#8672)\n\n## \\[0.8.59\\] - 2023-11-02 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0859-2023-11-02 \"Permanent link\")\n\n- Deepmemory support (#8625)\n- Add CohereAI embeddings (#8650)\n- Add Azure AD (Microsoft Entra ID) support (#8667)\n\n## \\[0.8.58\\] - 2023-11-02 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0858-2023-11-02 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_64 \"Permanent link\")\n\n- Add `lm-format-enforcer` integration for structured output (#8601)\n- Google Vertex Support (#8626)\n\n## \\[0.8.57\\] - 2023-10-31 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0857-2023-10-31 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_65 \"Permanent link\")\n\n- Add `VoyageAIEmbedding` integration (#8634)\n- Add fine-tuning evaluator notebooks (#8596)\n- Add `SingleStoreDB` integration (#7991)\n- Add support for ChromaDB PersistentClient (#8582)\n- Add DataStax Astra DB support (#8609)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_60 \"Permanent link\")\n\n- Update dataType in Weaviate (#8608)\n- In Knowledge Graph Index with hybrid retriever\\_mode,\n- return the nodes found by keyword search when 'No Relationship found'\n- Fix exceed context length error in chat engines (#8530)\n- Retrieve actual content of all the triplets from KG (#8579)\n- Return the nodes found by Keywords when no relationship is found by embeddings in hybrid retriever\\_mode in `KnowledgeGraphIndex` (#8575)\n- Optimize content of retriever tool and minor bug fix (#8588)\n\n## \\[0.8.56\\] - 2023-10-30 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0856-2023-10-30 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_66 \"Permanent link\")\n\n- Add Amazon `BedrockEmbedding` (#8550)\n- Moves `HuggingFaceEmbedding` to center on `Pooling` enum for pooling (#8467)\n- Add IBM WatsonX LLM support (#8587)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_61 \"Permanent link\")\n\n- \\[Bug\\] Patch Clarifai classes (#8529)\n- fix retries for bedrock llm (#8528)\n- Fix : VectorStore\u2019s QueryResult always returns saved Node as TextNode (#8521)\n- Added default file\\_metadata to get basic metadata that many postprocessors use, for SimpleDirectoryReader (#8486)\n- Handle metadata with None values in chromadb (#8584)\n\n## \\[0.8.55\\] - 2023-10-29 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0855-2023-10-29 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_67 \"Permanent link\")\n\n- allow prompts to take in functions with `function_mappings` (#8548)\n- add advanced prompt + \"prompt engineering for RAG\" notebook (#8555)\n- Leverage Replicate API for serving LLaVa modal (#8539)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_62 \"Permanent link\")\n\n- Update pull request template with google colab support inclusion (#8525)\n\n## \\[0.8.54\\] - 2023-10-28 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0854-2023-10-28 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_68 \"Permanent link\")\n\n- notebook showing how to fine-tune llama2 on structured outputs (#8540)\n- added GradientAIFineTuningHandler\n- added pydantic\\_program\\_mode to ServiceContext\n- Initialize MultiModal Retrieval using LlamaIndex (#8507)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_63 \"Permanent link\")\n\n- Add missing import to `ChatEngine` usage pattern `.md` doc (#8518)\n- :bug: fixed async add (#8531)\n- fix: add the needed CondenseQuestionChatEngine import in the usage\\_pa\u2026 (#8518)\n- Add import LongLLMLinguaPostprocessor for LongLLMLingua.ipynb (#8519)\n\n## \\[0.8.53\\] - 2023-10-27 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0853-2023-10-27 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_69 \"Permanent link\")\n\n- Docs refactor (#8500)\nAn overhaul of the docs organization. Major changes\n- Added a big new \"understanding\" section\n- Added a big new \"optimizing\" section\n- Overhauled Getting Started content\n- Categorized and moved module guides to a single section\n\n## \\[0.8.52\\] - 2023-10-26 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0852-2023-10-26 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_70 \"Permanent link\")\n\n- Add longllmlingua (#8485)\n- Add google colab support for notebooks (#7560)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_64 \"Permanent link\")\n\n- Adapt Cassandra VectorStore constructor DB connection through cassio.init (#8255)\n- Allow configuration of service context and storage context in managed index (#8487)\n\n## \\[0.8.51.post1\\] - 2023-10-25 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0851post1-2023-10-25 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_71 \"Permanent link\")\n\n- Add Llava MultiModal QA examples for Tesla 10k RAG (#8271)\n- fix bug streaming on react chat agent not working as expected (#8459)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_65 \"Permanent link\")\n\n- patch: add selected result to response metadata for router query engines, fix bug (#8483)\n- add Jina AI embeddings notebook + huggingface embedding fix (#8478)\n- add `is_chat_model` to replicate (#8469)\n- Brought back `toml-sort` to `pre-commit` (#8267)\n- Added `LocationConstraint` for local `test_s3_kvstore` (#8263)\n\n## \\[0.8.50\\] - 2023-10-24 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0850-2023-10-24 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_72 \"Permanent link\")\n\n- Expose prompts in different modules (query engines, synthesizers, and more) (#8275)\n\n## \\[0.8.49\\] - 2023-10-23 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0849-2023-10-23 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_73 \"Permanent link\")\n\n- New LLM integrations\n- Support for Hugging Face Inference API's `conversational`, `text_generation`,\nand `feature_extraction` endpoints via `huggingface_hub[inference]` (#8098)\n- Add Amazon Bedrock LLMs (#8223)\n- Add AI21 Labs LLMs (#8233)\n- Add OpenAILike LLM class for OpenAI-compatible api servers (#7973)\n- New / updated vector store integrations\n- Add DashVector (#7772)\n- Add Tencent VectorDB (#8173)\n- Add option for custom Postgres schema on PGVectorStore instead of only allowing public schema (#8080)\n- Add Gradient fine tuning engine (#8208)\n- docs(FAQ): frequently asked questions (#8249)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_66 \"Permanent link\")\n\n- Fix inconsistencies with `ReActAgent.stream_chat` (#8147)\n- Deprecate some functions for GuardrailsOutputParser (#8016)\n- Simplify dependencies (#8236)\n- Bug fixes for LiteLLM (#7885)\n- Update for Predibase LLM (#8211)\n\n## \\[0.8.48\\] - 2023-10-20 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0848-2023-10-20 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_74 \"Permanent link\")\n\n- Add `DELETE` for MyScale vector store (#8159)\n- Add SQL Retriever (#8197)\n- add semantic kernel document format (#8226)\n- Improve MyScale Hybrid Search and Add `DELETE` for MyScale vector store (#8159)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_67 \"Permanent link\")\n\n- Fixed additional kwargs in ReActAgent.from\\_tools() (#8206)\n- Fixed missing spaces in prompt templates (#8190)\n- Remove auto-download of llama2-13B on exception (#8225)\n\n## \\[0.8.47\\] - 2023-10-19 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0847-2023-10-19 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_75 \"Permanent link\")\n\n- add response synthesis to text-to-SQL (#8196)\n- Added support for `LLMRailsEmbedding` (#8169)\n- Inferring MPS device with PyTorch (#8195)\n- Consolidated query/text prepending (#8189)\n\n## \\[0.8.46\\] - 2023-10-18 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0846-2023-10-18 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_76 \"Permanent link\")\n\n- Add fine-tuning router support + embedding selector (#8174)\n- add more document converters (#8156)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_68 \"Permanent link\")\n\n- Add normalization to huggingface embeddings (#8145)\n- Improve MyScale Hybrid Search (#8159)\n- Fixed duplicate `FORMAT_STR` being inside prompt (#8171)\n- Added: support for output\\_kwargs={'max\\_colwidth': xx} for PandasQueryEngine (#8110)\n- Minor fix in the description for an argument in cohere llm (#8163)\n- Fix Firestore client info (#8166)\n\n## \\[0.8.45\\] - 2023-10-13 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0845-2023-10-13 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_77 \"Permanent link\")\n\n- Added support for fine-tuning cross encoders (#7705)\n- Added `QueryFusionRetriever` for merging multiple retrievers + query augmentation (#8100)\n- Added `nb-clean` to `pre-commit` to minimize PR diffs (#8108)\n- Support for `TextEmbeddingInference` embeddings (#8122)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_69 \"Permanent link\")\n\n- Improved the `BM25Retriever` interface to accept `BaseNode` objects (#8096)\n- Fixed bug with `BM25Retriever` tokenizer not working as expected (#8096)\n- Brought mypy to pass in Python 3.8 (#8107)\n- `ReActAgent` adding missing `super().__init__` call (#8125)\n\n## \\[0.8.44\\] - 2023-10-12 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0844-2023-10-12 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_78 \"Permanent link\")\n\n- add pgvector sql query engine (#8087)\n- Added HoneyHive one-click observability (#7944)\n- Add support for both SQLAlchemy V1 and V2 (#8060)\n\n## \\[0.8.43.post1\\] - 2023-10-11 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0843post1-2023-10-11 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_79 \"Permanent link\")\n\n- Moves `codespell` to `pre-commit` (#8040)\n- Added `prettier` for autoformatting extensions besides `.py` (#8072)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_70 \"Permanent link\")\n\n- Fixed forgotten f-str in `HuggingFaceLLM` (#8075)\n- Relaxed numpy/panadas reqs\n\n## \\[0.8.43\\] - 2023-10-10 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0843-2023-10-10 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_80 \"Permanent link\")\n\n- Added support for `GradientEmbedding` embed models (#8050)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_71 \"Permanent link\")\n\n- added `messages_to_prompt` kwarg to `HuggingFaceLLM` (#8054)\n- improved selection and sql parsing for open-source models (#8054)\n- fixed bug when agents hallucinate too many kwargs for a tool (#8054)\n- improved prompts and debugging for selection+question generation (#8056)\n\n## \\[0.8.42\\] - 2023-10-10 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0842-2023-10-10 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_81 \"Permanent link\")\n\n- `LocalAI` more intuitive module-level var names (#8028)\n- Enable `codespell` for markdown docs (#7972)\n- add unstructured table element node parser (#8036)\n- Add: Async upserting for Qdrant vector store (#7968)\n- Add cohere llm (#8023)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_72 \"Permanent link\")\n\n- Parse multi-line outputs in react agent answers (#8029)\n- Add properly named kwargs to keyword `as_retriever` calls (#8011)\n- Updating Reference to RAGAS LlamaIndex Integration (#8035)\n- Vectara bugfix (#8032)\n- Fix: ChromaVectorStore can attempt to add in excess of chromadb batch\u2026 (#8019)\n- Fix get\\_content method in Mbox reader (#8012)\n- Apply kwarg filters in WeaviateVectorStore (#8017)\n- Avoid ZeroDivisionError (#8027)\n- `LocalAI` intuitive module-level var names (#8028)\n- zep/fix: imports & typing (#8030)\n- refactor: use `str.join` (#8020)\n- use proper metadata str for node parsing (#7987)\n\n## \\[0.8.41\\] - 2023-10-07 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0841-2023-10-07 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_82 \"Permanent link\")\n\n- You.com retriever (#8024)\n- Pull fields from mongodb into metadata with `metadata_names` argument (#8001)\n- Simplified `LocalAI.__init__` preserving the same behaviors (#7982)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_73 \"Permanent link\")\n\n- Use longest metadata string for metadata aware text splitting (#7987)\n- Handle lists of strings in mongodb reader (#8002)\n- Removes `OpenAI.class_type` as it was dead code (#7983)\n- Fixing `HuggingFaceLLM.device_map` type hint (#7989)\n\n## \\[0.8.40\\] - 2023-10-05 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0840-2023-10-05 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_83 \"Permanent link\")\n\n- Added support for `Clarifai` LLM (#7967)\n- Add support for function fine-tuning (#7971)\n\n### Breaking Changes [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#breaking-changes_4 \"Permanent link\")\n\n- Update document summary index (#7815)\n- change default retrieval mode to embedding\n- embed summaries into vector store by default at indexing time (instead of calculating embedding on the fly)\n- support configuring top k in llm retriever\n\n## \\[0.8.39\\] - 2023-10-03 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0839-2023-10-03 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_84 \"Permanent link\")\n\n- Added support for pydantic object outputs with query engines (#7893)\n- `ClarifaiEmbedding` class added for embedding support (#7940)\n- Markdown node parser, flat file reader and simple file node parser (#7863)\n- Added support for mongdb atlas `$vectorSearch` (#7866)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_74 \"Permanent link\")\n\n- Adds support for using message metadata in discord reader (#7906)\n- Fix `LocalAI` chat capability without `max_tokens` (#7942)\n- Added `codespell` for automated checking (#7941)\n- `ruff` modernization and autofixes (#7889)\n- Implement own SQLDatabase class (#7929)\n- Update LlamaCPP context\\_params property (#7945)\n- fix duplicate embedding (#7949)\n- Adds `codespell` tool for enforcing good spelling (#7941)\n- Supporting `mypy` local usage with `venv` (#7952)\n- Vectara - minor update (#7954)\n- Avoiding `pydantic` reinstalls in CI (#7956)\n- move tree\\_sitter\\_languages into data\\_requirements.txt (#7955)\n- Add `cache_okay` param to `PGVectorStore` to help suppress TSVector warnings (#7950)\n\n## \\[0.8.38\\] - 2023-10-02 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0838-2023-10-02 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_85 \"Permanent link\")\n\n- Updated `KeywordNodePostprocessor` to use spacy to support more languages (#7894)\n- `LocalAI` supporting global or per-query `/chat/completions` vs `/completions` (#7921)\n- Added notebook on using REBEL + Wikipedia filtering for knowledge graphs (#7919)\n- Added support for `ElasticsearchEmbedding` (#7914)\n\n## \\[0.8.37\\] - 2023-09-30 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0837-2023-09-30 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_86 \"Permanent link\")\n\n- Supporting `LocalAI` LLMs (#7913)\n- Validations protecting against misconfigured chunk sizes (#7917)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_75 \"Permanent link\")\n\n- Simplify NL SQL response to SQL parsing, with expanded NL SQL prompt (#7868)\n- Improve vector store retrieval speed for vectordb integrations (#7876)\n- Added replacing {{ and }}, and fixed JSON parsing recursion (#7888)\n- Nice-ified JSON decoding error (#7891)\n- Nice-ified SQL error from LLM not providing SQL (#7900)\n- Nice-ified `ImportError` for `HuggingFaceLLM` (#7904)\n- eval fixes: fix dataset response generation, add score to evaluators (#7915)\n\n## \\[0.8.36\\] - 2023-09-27 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0836-2023-09-27 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_87 \"Permanent link\")\n\n- add \"build RAG from scratch notebook\" - OSS/local (#7864)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_76 \"Permanent link\")\n\n- Fix elasticsearch hybrid scoring (#7852)\n- Replace `get_color_mapping` and `print_text` Langchain dependency with internal implementation (#7845)\n- Fix async streaming with azure (#7856)\n- Avoid `NotImplementedError()` in sub question generator (#7855)\n- Patch predibase initialization (#7859)\n- Bumped min langchain version and changed prompt imports from langchain (#7862)\n\n## \\[0.8.35\\] - 2023-09-27 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0835-2023-09-27 \"Permanent link\")\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_77 \"Permanent link\")\n\n- Fix dropping textnodes in recursive retriever (#7840)\n- share callback\\_manager between agent and its llm when callback\\_manager is None (#7844)\n- fix pandas query engine (#7847)\n\n## \\[0.8.34\\] - 2023-09-26 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0834-2023-09-26 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_88 \"Permanent link\")\n\n- Added `Konko` LLM support (#7775)\n- Add before/after context sentence (#7821)\n- EverlyAI integration with LlamaIndex through OpenAI library (#7820)\n- add Arize Phoenix tracer to global handlers (#7835)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_78 \"Permanent link\")\n\n- Normalize scores returned from ElasticSearch vector store (#7792)\n- Fixed `refresh_ref_docs()` bug with order of operations (#7664)\n- Delay postgresql connection for `PGVectorStore` until actually needed (#7793)\n- Fix KeyError in delete method of `SimpleVectorStore` related to metadata filters (#7829)\n- Fix KeyError in delete method of `SimpleVectorStore` related to metadata filters (#7831)\n- Addressing PyYAML import error (#7784)\n- ElasticsearchStore: Update User-Agent + Add example docker compose (#7832)\n- `StorageContext.persist` supporting `Path` (#7783)\n- Update ollama.py (#7839)\n- fix bug for self.\\_session\\_pool (#7834)\n\n## \\[0.8.33\\] - 2023-09-25 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0833-2023-09-25 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_89 \"Permanent link\")\n\n- add pairwise evaluator + benchmark auto-merging retriever (#7810)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_79 \"Permanent link\")\n\n- Minor cleanup in embedding class (#7813)\n- Misc updates to `OpenAIEmbedding` (#7811)\n\n## \\[0.8.32\\] - 2023-09-24 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0832-2023-09-24 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_90 \"Permanent link\")\n\n- Added native support for `HuggingFaceEmbedding`, `InstructorEmbedding`, and `OptimumEmbedding` (#7795)\n- Added metadata filtering and hybrid search to MyScale vector store (#7780)\n- Allowing custom text field name for Milvus (#7790)\n- Add support for `vector_store_query_mode` to `VectorIndexAutoRetriever` (#7797)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_80 \"Permanent link\")\n\n- Update `LanceDBVectorStore` to handle score and distance (#7754)\n- Pass LLM to `memory_cls` in `CondenseQuestionChatEngine` (#7785)\n\n## \\[0.8.31\\] - 2023-09-22 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0831-2023-09-22 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_91 \"Permanent link\")\n\n- add pydantic metadata extractor (#7778)\n- Allow users to set the embedding dimensions in azure cognitive vector store (#7734)\n- Add semantic similarity evaluator (#7770)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_81 \"Permanent link\")\n\n- \ud83d\udcdddocs: Update Chatbot Tutorial and Notebook (#7767)\n- Fixed response synthesizers with empty nodes (#7773)\n- Fix `NotImplementedError` in auto vector retriever (#7764)\n- Multiple kwargs values in \"KnowledgeGraphQueryEngine\" bug-fix (#7763)\n- Allow setting azure cognitive search dimensionality (#7734)\n- Pass service context to index for dataset generator (#7748)\n- Fix output parsers for selector templates (#7774)\n- Update Chatbot\\_SEC.ipynb (#7711)\n- linter/typechecker-friendly improvements to cassandra test (#7771)\n- Expose debug option of `PgVectorStore` (#7776)\n- llms/openai: fix Azure OpenAI by considering `prompt_filter_results` field (#7755)\n\n## \\[0.8.30\\] - 2023-09-21 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0830-2023-09-21 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_92 \"Permanent link\")\n\n- Add support for `gpt-3.5-turbo-instruct` (#7729)\n- Add support for `TimescaleVectorStore` (#7727)\n- Added `LongContextReorder` for lost-in-the-middle issues (#7719)\n- Add retrieval evals (#7738)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_82 \"Permanent link\")\n\n- Added node post-processors to async context chat engine (#7731)\n- Added unique index name for postgres tsv column (#7741)\n\n## \\[0.8.29.post1\\] - 2023-09-18 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0829post1-2023-09-18 \"Permanent link\")\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_83 \"Permanent link\")\n\n- Fix langchain import error for embeddings (#7714)\n\n## \\[0.8.29\\] - 2023-09-18 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0829-2023-09-18 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_93 \"Permanent link\")\n\n- Added metadata filtering to the base simple vector store (#7564)\n- add low-level router guide (#7708)\n- Add CustomQueryEngine class (#7703)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_84 \"Permanent link\")\n\n- Fix context window metadata in lite-llm (#7696)\n\n## \\[0.8.28\\] - 2023-09-16 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0828-2023-09-16 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_94 \"Permanent link\")\n\n- Add CorrectnessEvaluator (#7661)\n- Added support for `Ollama` LLMs (#7635)\n- Added `HWPReader` (#7672)\n- Simplified portkey LLM interface (#7669)\n- Added async operation support to `ElasticsearchStore` vector store (#7613)\n- Added support for `LiteLLM` (#7600)\n- Added batch evaluation runner (#7692)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_85 \"Permanent link\")\n\n- Avoid `NotImplementedError` for async langchain embeddings (#7668)\n- Imrpoved reliability of LLM selectors (#7678)\n- Fixed `query_wrapper_prompt` and `system_prompt` for output parsers and completion models (#7678)\n- Fixed node attribute inheritance in citation query engine (#7675)\n\n### Breaking Changes [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#breaking-changes_5 \"Permanent link\")\n\n- Refactor and update `BaseEvaluator` interface to be more consistent (#7661)\n- Use `evaluate` function for generic input\n- Use `evaluate_response` function with `Response` objects from llama index query engine\n- Update existing evaluators with more explicit naming\n- `ResponseEvaluator` -\\> `FaithfulnessEvaluator`\n- `QueryResponseEvaluator` -\\> `RelevancyEvaluator`\n- old names are kept as class aliases for backwards compatibility\n\n## \\[0.8.27\\] - 2023-09-14 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0827-2023-09-14 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_95 \"Permanent link\")\n\n- add low-level tutorial section (#7673)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_86 \"Permanent link\")\n\n- default delta should be a dict (#7665)\n- better query wrapper logic on LLMPredictor (#7667)\n\n## \\[0.8.26\\] - 2023-09-12 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0826-2023-09-12 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_96 \"Permanent link\")\n\n- add non-linear embedding adapter (#7658)\n- Add \"finetune + RAG\" evaluation to knowledge fine-tuning notebook (#7643)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_87 \"Permanent link\")\n\n- Fixed chunk-overlap for sentence splitter (#7590)\n\n## \\[0.8.25\\] - 2023-09-12 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0825-2023-09-12 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_97 \"Permanent link\")\n\n- Added `AGENT_STEP` callback event type (#7652)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_88 \"Permanent link\")\n\n- Allowed `simple` mode to work with `as_chat_engine()` (#7637)\n- Fixed index error in azure streaming (#7646)\n- Removed `pdb` from llama-cpp (#7651)\n\n## \\[0.8.24\\] - 2023-09-11 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0824-2023-09-11 \"Permanent link\")\n\n## New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_98 \"Permanent link\")\n\n- guide: fine-tuning to memorize knowledge (#7626)\n- added ability to customize prompt template for eval modules (#7626)\n\n### Bug Fixes [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes \"Permanent link\")\n\n- Properly detect `llama-cpp-python` version for loading the default GGML or GGUF `llama2-chat-13b` model (#7616)\n- Pass in `summary_template` properly with `RetrieverQueryEngine.from_args()` (#7621)\n- Fix span types in wandb callback (#7631)\n\n## \\[0.8.23\\] - 2023-09-09 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0823-2023-09-09 \"Permanent link\")\n\n### Bug Fixes [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes_1 \"Permanent link\")\n\n- Make sure context and system prompt is included in prompt for first chat for llama2 (#7597)\n- Avoid negative chunk size error in refine process (#7607)\n- Fix relationships for small documents in hierarchical node parser (#7611)\n- Update Anyscale Endpoints integration with full streaming and async support (#7602)\n- Better support of passing credentials as LLM constructor args in `OpenAI`, `AzureOpenAI`, and `Anyscale` (#7602)\n\n### Breaking Changes [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#breaking-changes_6 \"Permanent link\")\n\n- Update milvus vector store to support filters and dynamic schemas (#7286)\n- See the [updated notebook](https://docs.llamaindex.ai/en/stable/examples/vector_stores/MilvusIndexDemo.html) for usage\n- Added NLTK to core dependencies to support the default sentence splitter (#7606)\n\n## \\[0.8.22\\] - 2023-09-07 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0822-2023-09-07 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_99 \"Permanent link\")\n\n- Added support for ElasticSearch Vector Store (#7543)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_89 \"Permanent link\")\n\n- Fixed small `_index` bug in `ElasticSearchReader` (#7570)\n- Fixed bug with prompt helper settings in global service contexts (#7576)\n- Remove newlines from openai embeddings again (#7588)\n- Fixed small bug with setting `query_wrapper_prompt` in the service context (#7585)\n\n### Breaking/Deprecated API Changes [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#breakingdeprecated-api-changes \"Permanent link\")\n\n- Clean up vector store interface to use `BaseNode` instead of `NodeWithEmbedding`\n- For majority of users, this is a no-op change\n- For users directly operating with the `VectorStore` abstraction and manually constructing `NodeWithEmbedding` objects, this is a minor breaking change. Use `TextNode` with `embedding` set directly, instead of `NodeWithEmbedding`.\n\n## \\[0.8.21\\] - 2023-09-06 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0821-2023-09-06 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_100 \"Permanent link\")\n\n- add embedding adapter fine-tuning engine + guide (#7565)\n- Added support for Azure Cognitive Search vector store (#7469)\n- Support delete in supabase (#6951)\n- Added support for Espilla vector store (#7539)\n- Added support for AnyScale LLM (#7497)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_90 \"Permanent link\")\n\n- Default to user-configurable top-k in `VectorIndexAutoRetriever` (#7556)\n- Catch validation errors for structured responses (#7523)\n- Fix streaming refine template (#7561)\n\n## \\[0.8.20\\] - 2023-09-04 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0820-2023-09-04 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_101 \"Permanent link\")\n\n- Added Portkey LLM integration (#7508)\n- Support postgres/pgvector hybrid search (#7501)\n- upgrade recursive retriever node reference notebook (#7537)\n\n## \\[0.8.19\\] - 2023-09-03 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0819-2023-09-03 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_102 \"Permanent link\")\n\n- replace list index with summary index (#7478)\n- rename list index to summary index part 2 (#7531)\n\n## \\[0.8.18\\] - 2023-09-03 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0818-2023-09-03 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_103 \"Permanent link\")\n\n- add agent finetuning guide (#7526)\n\n## \\[0.8.17\\] - 2023-09-02 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0817-2023-09-02 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_104 \"Permanent link\")\n\n- Make (some) loaders serializable (#7498)\n- add node references to recursive retrieval (#7522)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_91 \"Permanent link\")\n\n- Raise informative error when metadata is too large during splitting (#7513)\n- Allow langchain splitter in simple node parser (#7517)\n\n## \\[0.8.16\\] - 2023-09-01 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0816-2023-09-01 \"Permanent link\")\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_92 \"Permanent link\")\n\n- fix link to Marvin notebook in docs (#7504)\n- Ensure metadata is not `None` in `SimpleWebPageReader` (#7499)\n- Fixed KGIndex visualization (#7493)\n- Improved empty response in KG Index (#7493)\n\n## \\[0.8.15\\] - 2023-08-31 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0815-2023-08-31 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_105 \"Permanent link\")\n\n- Added support for `MarvinEntityExtractor` metadata extractor (#7438)\n- Added a url\\_metadata callback to SimpleWebPageReader (#7445)\n- Expanded callback logging events (#7472)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_93 \"Permanent link\")\n\n- Only convert newlines to spaces for text 001 embedding models in OpenAI (#7484)\n- Fix `KnowledgeGraphRagRetriever` for non-nebula indexes (#7488)\n- Support defined embedding dimension in `PGVectorStore` (#7491)\n- Greatly improved similarity calculation speed for the base vector store (#7494)\n\n## \\[0.8.14\\] - 2023-08-30 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0814-2023-08-30 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_106 \"Permanent link\")\n\n- feat: non-kg heterogeneous graph support in Graph RAG (#7459)\n- rag guide (#7480)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_94 \"Permanent link\")\n\n- Improve openai fine-tuned model parsing (#7474)\n- doing some code de-duplication (#7468)\n- support both str and templates for query\\_wrapper\\_prompt in HF LLMs (#7473)\n\n## \\[0.8.13\\] - 2023-08-29 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0813-2023-08-29 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_107 \"Permanent link\")\n\n- Add embedding finetuning (#7452)\n- Added support for RunGPT LLM (#7401)\n- Integration guide and notebook with DeepEval (#7425)\n- Added `VectorIndex` and `VectaraRetriever` as a managed index (#7440)\n- Added support for `to_tool_list` to detect and use async functions (#7282)\n\n## \\[0.8.12\\] - 2023-08-28 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0812-2023-08-28 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_108 \"Permanent link\")\n\n- add openai finetuning class (#7442)\n- Service Context to/from dict (#7395)\n- add finetuning guide (#7429)\n\n### Smaller Features / Nits / Bug Fixes [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#smaller-features-nits-bug-fixes \"Permanent link\")\n\n- Add example how to run FalkorDB docker (#7441)\n- Update root.md to use get\\_response\\_synthesizer expected type. (#7437)\n- Bugfix MonsterAPI Pydantic version v2/v1 support. Doc Update (#7432)\n\n## \\[0.8.11.post3\\] - 2023-08-27 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0811post3-2023-08-27 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_109 \"Permanent link\")\n\n- AutoMergingRetriever (#7420)\n\n## \\[0.8.10.post1\\] - 2023-08-25 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0810post1-2023-08-25 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_110 \"Permanent link\")\n\n- Added support for `MonsterLLM` using MonsterAPI (#7343)\n- Support comments fields in NebulaGraphStore and int type VID (#7402)\n- Added configurable endpoint for DynamoDB (#6777)\n- Add structured answer filtering for Refine response synthesizer (#7317)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_95 \"Permanent link\")\n\n- Use `utf-8` for json file reader (#7390)\n- Fix entity extractor initialization (#7407)\n\n## \\[0.8.9\\] - 2023-08-24 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#089-2023-08-24 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_111 \"Permanent link\")\n\n- Added support for FalkorDB/RedisGraph graph store (#7346)\n- Added directed sub-graph RAG (#7378)\n- Added support for `BM25Retriever` (#7342)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_96 \"Permanent link\")\n\n- Added `max_tokens` to `Xinference` LLM (#7372)\n- Support cache dir creation in multithreaded apps (#7365)\n- Ensure temperature is a float for openai (#7382)\n- Remove duplicate subjects in knowledge graph retriever (#7378)\n- Added support for both pydantic v1 and v2 to allow other apps to move forward (#7394)\n\n### Breaking/Deprecated API Changes [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#breakingdeprecated-api-changes_1 \"Permanent link\")\n\n- Refactor prompt template (#7319)\n- Use `BasePromptTemplate` for generic typing\n- Use `PromptTemplate`, `ChatPromptTemplate`, `SelectorPromptTemplate` as core implementations\n- Use `LangchainPromptTemplate` for compatibility with Langchain prompt templates\n- Fully replace specific prompt classes (e.g. `SummaryPrompt`) with generic `BasePromptTemplate` for typing in codebase.\n- Keep `Prompt` as an alias for `PromptTemplate` for backwards compatibility.\n- BREAKING CHANGE: remove support for `Prompt.from_langchain_prompt`, please use `template=LangchainPromptTemplate(lc_template)` instead.\n\n## \\[0.8.8\\] - 2023-08-23 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#088-2023-08-23 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_112 \"Permanent link\")\n\n- `OpenAIFineTuningHandler` for collecting LLM inputs/outputs for OpenAI fine tuning (#7367)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_97 \"Permanent link\")\n\n- Add support for `claude-instant-1.2` (#7369)\n\n## \\[0.8.7\\] - 2023-08-22 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#087-2023-08-22 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_113 \"Permanent link\")\n\n- Support fine-tuned OpenAI models (#7364)\n- Added support for Cassandra vector store (#6784)\n- Support pydantic fields in tool functions (#7348)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_98 \"Permanent link\")\n\n- Fix infinite looping with forced function call in `OpenAIAgent` (#7363)\n\n## \\[0.8.6\\] - 2023-08-22 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#086-2023-08-22 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_114 \"Permanent link\")\n\n- auto vs. recursive retriever notebook (#7353)\n- Reader and Vector Store for BagelDB with example notebooks (#7311)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_99 \"Permanent link\")\n\n- Use service context for intermediate index in retry source query engine (#7341)\n- temp fix for prompt helper + chat models (#7350)\n- Properly skip unit-tests when packages not installed (#7351)\n\n## \\[0.8.5.post2\\] - 2023-08-20 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#085post2-2023-08-20 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_115 \"Permanent link\")\n\n- Added FireStore docstore/index store support (#7305)\n- add recursive agent notebook (#7330)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_100 \"Permanent link\")\n\n- Fix Azure pydantic error (#7329)\n- fix callback trace ids (make them a context var) (#7331)\n\n## \\[0.8.5.post1\\] - 2023-08-18 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#085post1-2023-08-18 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_116 \"Permanent link\")\n\n- Awadb Vector Store (#7291)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_101 \"Permanent link\")\n\n- Fix bug in OpenAI llm temperature type\n\n## \\[0.8.5\\] - 2023-08-18 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#085-2023-08-18 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_117 \"Permanent link\")\n\n- Expose a system prompt/query wrapper prompt in the service context for open-source LLMs (#6647)\n- Changed default MyScale index format to `MSTG` (#7288)\n- Added tracing to chat engines/agents (#7304)\n- move LLM and embeddings to pydantic (#7289)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_102 \"Permanent link\")\n\n- Fix sentence splitter bug (#7303)\n- Fix sentence splitter infinite loop (#7295)\n\n## \\[0.8.4\\] - 2023-08-17 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#084-2023-08-17 \"Permanent link\")\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_103 \"Permanent link\")\n\n- Improve SQL Query parsing (#7283)\n- Fix loading embed\\_model from global service context (#7284)\n- Limit langchain version until we migrate to pydantic v2 (#7297)\n\n## \\[0.8.3\\] - 2023-08-16 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#083-2023-08-16 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_118 \"Permanent link\")\n\n- Added Knowledge Graph RAG Retriever (#7204)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_104 \"Permanent link\")\n\n- accept `api_key` kwarg in OpenAI LLM class constructor (#7263)\n- Fix to create separate queue instances for separate instances of `StreamingAgentChatResponse` (#7264)\n\n## \\[0.8.2.post1\\] - 2023-08-14 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#082post1-2023-08-14 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_119 \"Permanent link\")\n\n- Added support for Rockset as a vector store (#7111)\n\n### Bug Fixes [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes_2 \"Permanent link\")\n\n- Fixed bug in service context definition that could disable LLM (#7261)\n\n## \\[0.8.2\\] - 2023-08-14 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#082-2023-08-14 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_120 \"Permanent link\")\n\n- Enable the LLM or embedding model to be disabled by setting to `None` in the service context (#7255)\n- Resolve nearly any huggingface embedding model using the `embed_model=\"local:<model_name>\"` syntax (#7255)\n- Async tool-calling support (#7239)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_105 \"Permanent link\")\n\n- Updated supabase kwargs for add and query (#7103)\n- Small tweak to default prompts to allow for more general purpose queries (#7254)\n- Make callback manager optional for `CustomLLM` \\+ docs update (#7257)\n\n## \\[0.8.1\\] - 2023-08-13 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#081-2023-08-13 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_121 \"Permanent link\")\n\n- feat: add node\\_postprocessors to ContextChatEngine (#7232)\n- add ensemble query engine tutorial (#7247)\n\n### Smaller Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#smaller-features \"Permanent link\")\n\n- Allow EMPTY keys for Fastchat/local OpenAI API endpoints (#7224)\n\n## \\[0.8.0\\] - 2023-08-11 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#080-2023-08-11 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_122 \"Permanent link\")\n\n- Added \"LLAMA\\_INDEX\\_CACHE\\_DIR\" to control cached files (#7233)\n- Default to pydantic selectors when possible (#7154, #7223)\n- Remove the need for langchain wrappers on `embed_model` in the service context (#7157)\n- Metadata extractors take an `LLM` object now, in addition to `LLMPredictor` (#7202)\n- Added local mode + fallback to llama.cpp + llama2 (#7200)\n- Added local fallback for embeddings to `BAAI/bge-small-en` (#7200)\n- Added `SentenceWindowNodeParser` \\+ `MetadataReplacementPostProcessor` (#7211)\n\n### Breaking Changes [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#breaking-changes_7 \"Permanent link\")\n\n- Change default LLM to gpt-3.5-turbo from text-davinci-003 (#7223)\n- Change prompts for compact/refine/tree\\_summarize to work better with gpt-3.5-turbo (#7150, #7179, #7223)\n- Increase default LLM temperature to 0.1 (#7180)\n\n## \\[0.7.24.post1\\] - 2023-08-11 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0724post1-2023-08-11 \"Permanent link\")\n\n### Other Changes [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#other-changes \"Permanent link\")\n\n- Reverted #7223 changes to defaults (#7235)\n\n## \\[0.7.24\\] - 2023-08-10 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0724-2023-08-10 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_123 \"Permanent link\")\n\n- Default to pydantic selectors when possible (#7154, #7223)\n- Remove the need for langchain wrappers on `embed_model` in the service context (#7157)\n- Metadata extractors take an `LLM` object now, in addition to `LLMPredictor` (#7202)\n- Added local mode + fallback to llama.cpp + llama2 (#7200)\n- Added local fallback for embeddings to `BAAI/bge-small-en` (#7200)\n- Added `SentenceWindowNodeParser` \\+ `MetadataReplacementPostProcessor` (#7211)\n\n### Breaking Changes [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#breaking-changes_8 \"Permanent link\")\n\n- Change default LLM to gpt-3.5-turbo from text-davinci-003 (#7223)\n- Change prompts for compact/refine/tree\\_summarize to work better with gpt-3.5-turbo (#7150, #7179, #7223)\n- Increase default LLM temperature to 0.1 (#7180)\n\n### Other Changes [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#other-changes_1 \"Permanent link\")\n\n- docs: Improvements to Mendable Search (#7220)\n- Refactor openai agent (#7077)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_106 \"Permanent link\")\n\n- Use `1 - cosine_distance` for pgvector/postgres vector db (#7217)\n- fix metadata formatting and extraction (#7216)\n- fix(readers): Fix non-ASCII JSON Reader bug (#7086)\n- Chore: change PgVectorStore variable name from `sim` to `distance` for clarity (#7226)\n\n## \\[0.7.23\\] - 2023-08-10 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0723-2023-08-10 \"Permanent link\")\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_107 \"Permanent link\")\n\n- Fixed metadata formatting with custom tempalates and inheritance (#7216)\n\n## \\[0.7.23\\] - 2023-08-10 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0723-2023-08-10_1 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_124 \"Permanent link\")\n\n- Add \"one click observability\" page to docs (#7183)\n- Added Xorbits inference for local deployments (#7151)\n- Added Zep vector store integration (#7203)\n- feat/zep vectorstore (#7203)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_108 \"Permanent link\")\n\n- Update the default `EntityExtractor` model (#7209)\n- Make `ChatMemoryBuffer` pickleable (#7205)\n- Refactored `BaseOpenAIAgent` (#7077)\n\n## \\[0.7.22\\] - 2023-08-08 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0722-2023-08-08 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_125 \"Permanent link\")\n\n- add ensemble retriever notebook (#7190)\n- DOCS: added local llama2 notebook (#7146)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_109 \"Permanent link\")\n\n- Fix for `AttributeError: 'OpenAIAgent' object has no attribute 'callback_manager'` by calling super constructor within `BaseOpenAIAgent`\n- Remove backticks from nebula queries (#7192)\n\n## \\[0.7.21\\] - 2023-08-07 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0721-2023-08-07 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_126 \"Permanent link\")\n\n- Added an `EntityExtractor` for metadata extraction (#7163)\n\n## \\[0.7.20\\] - 2023-08-06 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0720-2023-08-06 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_127 \"Permanent link\")\n\n- add router module docs (#7171)\n- add retriever router (#7166)\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_128 \"Permanent link\")\n\n- Added a `RouterRetriever` for routing queries to specific retrievers (#7166)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_110 \"Permanent link\")\n\n- Fix for issue where having multiple concurrent streamed responses from `OpenAIAgent` would result in interleaving of tokens across each response stream. (#7164)\n- fix llms callbacks issue (args\\[0\\] error) (#7165)\n\n## \\[0.7.19\\] - 2023-08-04 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0719-2023-08-04 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_129 \"Permanent link\")\n\n- Added metadata filtering to weaviate (#7130)\n- Added token counting (and all callbacks) to agents and streaming (#7122)\n\n## \\[0.7.18\\] - 2023-08-03 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0718-2023-08-03 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_130 \"Permanent link\")\n\n- Added `to/from_string` and `to/from_dict` methods to memory objects (#7128)\n- Include columns comments from db tables in table info for SQL queries (#7124)\n- Add Neo4j support (#7122)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_111 \"Permanent link\")\n\n- Added `Azure AD` validation support to the `AzureOpenAI` class (#7127)\n- add `flush=True` when printing agent/chat engine response stream (#7129)\n- Added `Azure AD` support to the `AzureOpenAI` class (#7127)\n- Update LLM question generator prompt to mention JSON markdown (#7105)\n- Fixed `astream_chat` in chat engines (#7139)\n\n## \\[0.7.17\\] - 2023-08-02 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0717-2023-08-02 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_131 \"Permanent link\")\n\n- Update `ReActAgent` to support memory modules (minor breaking change since the constructor takes `memory` instead of `chat_history`, but the main `from_tools` method remains backward compatible.) (#7116)\n- Update `ReActAgent` to support streaming (#7119)\n- Added Neo4j graph store and query engine integrations (#7122)\n- add object streaming (#7117)\n\n## \\[0.7.16\\] - 2023-07-30 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0716-2023-07-30 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_132 \"Permanent link\")\n\n- Chat source nodes (#7078)\n\n## \\[0.7.15\\] - 2023-07-29 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0715-2023-07-29 \"Permanent link\")\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_112 \"Permanent link\")\n\n- anthropic api key customization (#7082)\n- Fix broken link to API reference in Contributor Docs (#7080)\n- Update vector store docs (#7076)\n- Update comment (#7073)\n\n## \\[0.7.14\\] - 2023-07-28 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0714-2023-07-28 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_133 \"Permanent link\")\n\n- Added HotpotQADistractor benchmark evaluator (#7034)\n- Add metadata filter and delete support for LanceDB (#7048)\n- Use MetadataFilters in opensearch (#7005)\n- Added support for `KuzuGraphStore` (#6970)\n- Added `kg_triplet_extract_fn` to customize how KGs are built (#7068)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_113 \"Permanent link\")\n\n- Fix string formatting in context chat engine (#7050)\n- Fixed tracing for async events (#7052)\n- Less strict triplet extraction for KGs (#7059)\n- Add configurable limit to KG data retrieved (#7059)\n- Nebula connection improvements (#7059)\n- Bug fix in building source nodes for agent response (#7067)\n\n## \\[0.7.13\\] - 2023-07-26 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0713-2023-07-26 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_134 \"Permanent link\")\n\n- Support function calling api for AzureOpenAI (#7041)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_114 \"Permanent link\")\n\n- tune prompt to get rid of KeyError in SubQ engine (#7039)\n- Fix validation of Azure OpenAI keys (#7042)\n\n## \\[0.7.12\\] - 2023-07-25 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#0712-2023-07-25 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_135 \"Permanent link\")\n\n- Added `kwargs` to `ComposableGraph` for the underlying query engines (#6990)\n- Validate openai key on init (#6940)\n- Added async embeddings and async RetrieverQueryEngine (#6587)\n- Added async `aquery` and `async_add` to PGVectorStore (#7031)\n- Added `.source_nodes` attribute to chat engine and agent responses (#7029)\n- Added `OpenInferenceCallback` for storing generation data in OpenInference format (#6998)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_115 \"Permanent link\")\n\n- Fix achat memory initialization for data agents (#7000)\n- Add `print_response_stream()` to agengt/chat engine response class (#7018)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_116 \"Permanent link\")\n\n- Fix achat memory initialization for data agents (#7000)\n- Add `print_response_stream()` to agengt/chat engine response class (#7018)\n\n## \\[v0.7.11.post1\\] - 2023-07-20 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v0711post1-2023-07-20 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_136 \"Permanent link\")\n\n- Default to pydantic question generation when possible for sub-question query engine (#6979)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_117 \"Permanent link\")\n\n- Fix returned order of messages in large chat memory (#6979)\n\n## \\[v0.7.11\\] - 2023-07-19 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v0711-2023-07-19 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_137 \"Permanent link\")\n\n- Added a `SentenceTransformerRerank` node post-processor for fast local re-ranking (#6934)\n- Add numpy support for evaluating queries in pandas query engine (#6935)\n- Add metadata filtering support for Postgres Vector Storage integration (#6968)\n- Proper llama2 support for agents and query engines (#6969)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_118 \"Permanent link\")\n\n- Added `model_name` to LLMMetadata (#6911)\n- Fallback to retriever service context in query engines (#6911)\n- Fixed `as_chat_engine()` ValueError with extra kwargs (#6971\n\n## \\[v0.7.10.post1\\] - 2023-07-18 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v0710post1-2023-07-18 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_138 \"Permanent link\")\n\n- Add support for Replicate LLM (vicuna & llama 2!)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_119 \"Permanent link\")\n\n- fix streaming for condense chat engine (#6958)\n\n## \\[v0.7.10\\] - 2023-07-17 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v0710-2023-07-17 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_139 \"Permanent link\")\n\n- Add support for chroma v0.4.0 (#6937)\n- Log embedding vectors to callback manager (#6962)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_120 \"Permanent link\")\n\n- add more robust embedding timeouts (#6779)\n- improved connection session management on postgres vector store (#6843)\n\n## \\[v0.7.9\\] - 2023-07-15 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v079-2023-07-15 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_140 \"Permanent link\")\n\n- specify `embed_model=\"local\"` to use default local embbeddings in the service context (#6806)\n- Add async `acall` endpoint to `BasePydanticProgram` (defaults to sync version). Implement for `OpenAIPydanticProgram`\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_121 \"Permanent link\")\n\n- fix null metadata for searching existing vector dbs (#6912)\n- add module guide docs for `SimpleDirectoryReader` (#6916)\n- make sure `CondenseQuestionChatEngine` streaming chat endpoints work even if not explicitly setting `streaming=True` in the underlying query engine.\n\n## \\[v0.7.8\\] - 2023-07-13 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v078-2023-07-13 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_141 \"Permanent link\")\n\n- Added embedding speed benchmark (#6876)\n- Added BEIR retrieval benchmark (#6825)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_122 \"Permanent link\")\n\n- remove toctrees from deprecated\\_terms (#6895)\n- Relax typing dependencies (#6879)\n- docs: modification to evaluation notebook (#6840)\n- raise error if the model does not support functions (#6896)\n- fix(bench embeddings): bug not taking into account string length (#6899)x\n\n## \\[v0.7.7\\] - 2023-07-13 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v077-2023-07-13 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_142 \"Permanent link\")\n\n- Improved milvus consistency support and output fields support (#6452)\n- Added support for knowledge graph querying w/ cypyer+nebula (#6642)\n- Added `Document.example()` to create documents for fast prototyping (#6739)\n- Replace react chat engine to use native reactive agent (#6870)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_123 \"Permanent link\")\n\n- chore: added a help message to makefile (#6861)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_124 \"Permanent link\")\n\n- Fixed support for using SQLTableSchema context\\_str attribute (#6891)\n\n## \\[v0.7.6\\] - 2023-07-12 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v076-2023-07-12 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_143 \"Permanent link\")\n\n- Added sources to agent/chat engine responses (#6854)\n- Added basic chat buffer memory to agents / chat engines (#6857)\n- Adding load and search tool (#6871)\n- Add simple agent benchmark (#6869)\n- add agent docs (#6866)\n- add react agent (#6865)\n\n### Breaking/Deprecated API Changes [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#breakingdeprecated-api-changes_2 \"Permanent link\")\n\n- Replace react chat engine with native react agent (#6870)\n- Set default chat mode to \"best\": use openai agent when possible, otherwise use react agent (#6870)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_125 \"Permanent link\")\n\n- Fixed support for legacy vector store metadata (#6867)\n- fix chroma notebook in docs (#6872)\n- update LC embeddings docs (#6868)\n\n## \\[v0.7.5\\] - 2023-07-11 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v075-2023-07-11 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_144 \"Permanent link\")\n\n- Add `Anthropic` LLM implementation (#6855)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_126 \"Permanent link\")\n\n- Fix indexing error in `SentenceEmbeddingOptimizer` (#6850)\n- fix doc for custom embedding model (#6851)\n- fix(silent error): Add validation to `SimpleDirectoryReader` (#6819)\n- Fix link in docs (#6833)\n- Fixes Azure gpt-35-turbo model not recognized (#6828)\n- Update Chatbot\\_SEC.ipynb (#6808)\n- Rename leftover original name to LlamaIndex (#6792)\n- patch nested traces of the same type (#6791)\n\n## \\[v0.7.4\\] - 2023-07-08 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v074-2023-07-08 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_145 \"Permanent link\")\n\n- `MetadataExtractor` \\- Documnent Metadata Augmentation via LLM-based feature extractors (#6764)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_127 \"Permanent link\")\n\n- fixed passing in query bundle to node postprocessors (#6780)\n- fixed error in callback manager with nested traces (#6791)\n\n## \\[v0.7.3\\] - 2023-07-07 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v073-2023-07-07 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_146 \"Permanent link\")\n\n- Sub question query engine returns source nodes of sub questions in the callback manager (#6745)\n- trulens integration (#6741)\n- Add sources to subquestion engine (#6745)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_128 \"Permanent link\")\n\n- Added/Fixed streaming support to simple and condense chat engines (#6717)\n- fixed `response_mode=\"no_text\"` response synthesizer (#6755)\n- fixed error setting `num_output` and `context_window` in service context (#6766)\n- Fix missing as\\_query\\_engine() in tutorial (#6747)\n- Fixed variable sql\\_query\\_engine in the notebook (#6778)\n- fix required function fields (#6761)\n- Remove usage of stop token in Prompt, SQL gen (#6782)\n\n## \\[v0.7.2\\] - 2023-07-06 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v072-2023-07-06 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_147 \"Permanent link\")\n\n- Support Azure OpenAI (#6718)\n- Support prefix messages (e.g. system prompt) in chat engine and OpenAI agent (#6723)\n- Added `CBEventType.SUB_QUESTIONS` event type for tracking sub question queries/responses (#6716)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_129 \"Permanent link\")\n\n- Fix HF LLM output error (#6737)\n- Add system message support for langchain message templates (#6743)\n- Fixed applying node-postprocessors (#6749)\n- Add missing `CustomLLM` import under `llama_index.llms` (#6752)\n- fix(typo): `get_transformer_tokenizer_fn` (#6729)\n- feat(formatting): `black[jupyter]` (#6732)\n- fix(test): `test_optimizer_chinese` (#6730)\n\n## \\[v0.7.1\\] - 2023-07-05 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v071-2023-07-05 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_148 \"Permanent link\")\n\n- Streaming support for OpenAI agents (#6694)\n- add recursive retriever + notebook example (#6682)\n\n## \\[v0.7.0\\] - 2023-07-04 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v070-2023-07-04 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_149 \"Permanent link\")\n\n- Index creation progress bars (#6583)\n\n### Bug Fixes/ Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_130 \"Permanent link\")\n\n- Improved chat refine template (#6645)\n\n### Breaking/Deprecated API Changes [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#breakingdeprecated-api-changes_3 \"Permanent link\")\n\n- Change `BaseOpenAIAgent` to use `llama_index.llms.OpenAI`. Adjust `chat_history` to use `List[ChatMessage]]` as type.\n- Remove (previously deprecated) `llama_index.langchain_helpers.chain_wrapper` module.\n- Remove (previously deprecated) `llama_index.token_counter.token_counter` module. See [migration guide](https://docs.llamaindex.ai/how_to/callbacks/token_counting_migration.html) for more details on new callback based token counting.\n- Remove `ChatGPTLLMPredictor` and `HuggingFaceLLMPredictor`. See [migration guide](https://docs.llamaindex.ai/how_to/customization/llms_migration_guide.html) for more details on replacements.\n- Remove support for setting `cache` via `LLMPredictor` constructor.\n- Update `BaseChatEngine` interface:\n- adjust `chat_history` to use `List[ChatMessage]]` as type\n- expose `chat_history` state as a property\n- support overriding `chat_history` in `chat` and `achat` endpoints\n- Remove deprecated arguments for `PromptHelper`: `max_input_size`, `embedding_limit`, `max_chunk_overlap`\n- Update all notebooks to use native openai integration (#6696)\n\n## \\[v0.6.38\\] - 2023-07-02 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v0638-2023-07-02 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_150 \"Permanent link\")\n\n- add optional tqdm progress during index creation (#6583)\n- Added async support for \"compact\" and \"refine\" response modes (#6590)\n- \\[feature\\]add transformer tokenize functionalities for optimizer (chinese) (#6659)\n- Add simple benchmark for vector store (#6670)\n- Introduce `llama_index.llms` module, with new `LLM` interface, and `OpenAI`, `HuggingFaceLLM`, `LangChainLLM` implementations. (#6615)\n- Evaporate pydantic program (#6666)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_131 \"Permanent link\")\n\n- Improve metadata/node storage and retrieval for RedisVectorStore (#6678)\n- Fixed node vs. document filtering in vector stores (#6677)\n- add context retrieval agent notebook link to docs (#6660)\n- Allow null values for the 'image' property in the ImageNode class and se\u2026 (#6661)\n- Fix broken links in docs (#6669)\n- update milvus to store node content (#6667)\n\n## \\[v0.6.37\\] - 2023-06-30 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v0637-2023-06-30 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_151 \"Permanent link\")\n\n- add context augmented openai agent (#6655)\n\n## \\[v0.6.36\\] - 2023-06-29 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v0636-2023-06-29 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_152 \"Permanent link\")\n\n- Redis support for index stores and docstores (#6575)\n- DuckDB + SQL query engine notebook (#6628)\n- add notebook showcasing deplot data loader (#6638)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_132 \"Permanent link\")\n\n- More robust JSON parsing from LLM for `SelectionOutputParser` (#6610)\n- bring our loaders back in line with llama-hub (#6630)\n- Remove usage of SQLStructStoreIndex in notebooks (#6585)\n- MD reader: remove html tags and leave linebreaks alone (#6618)\n- bump min langchain version to latest version (#6632)\n- Fix metadata column name in postgres vector store (#6622)\n- Postgres metadata fixes (#6626, #6634)\n- fixed links to dataloaders in contribution.md (#6636)\n- fix: typo in docs in creating custom\\_llm huggingface example (#6639)\n- Updated SelectionOutputParser to handle JSON objects and arrays (#6610)\n- Fixed docstring argument typo (#6652)\n\n## \\[v0.6.35\\] - 2023-06-28 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v0635-2023-06-28 \"Permanent link\")\n\n- refactor structured output + pydantic programs (#6604)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_133 \"Permanent link\")\n\n- Fix serialization for OpenSearch vector stores (#6612)\n- patch docs relationships (#6606)\n- Bug fix for ignoring directories while parsing git repo (#4196)\n- updated Chroma notebook (#6572)\n- Backport old node name (#6614)\n- Add the ability to change chroma implementation (#6601)\n\n## \\[v0.6.34\\] - 2023-06-26 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v0634-2023-06-26 \"Permanent link\")\n\n### Patch Update (v0.6.34.post1) [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#patch-update-v0634post1 \"Permanent link\")\n\n- Patch imports for Document obj for backwards compatibility (#6597)\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_153 \"Permanent link\")\n\n- New `TextNode`/ `Document` object classes based on pydantic (#6586)\n- `TextNode`/ `Document` objects support metadata customization (metadata templates, exclude metadata from LLM or embeddings) (#6586)\n- Nodes no longer require flat metadata dictionaries, unless the vector store you use requires it (#6586)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_134 \"Permanent link\")\n\n- use `NLTK_DATA` env var to control NLTK download location (#6579)\n- \\[discord\\] save author as metadata in group\\_conversations.py (#6592)\n- bs4 -> beautifulsoup4 in requirements (#6582)\n- negate euclidean distance (#6564)\n- add df output parser notebook link to docs (#6581)\n\n### Breaking/Deprecated API Changes [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#breakingdeprecated-api-changes_4 \"Permanent link\")\n\n- `Node` has been renamed to `TextNode` and is imported from `llama_index.schema` (#6586)\n- `TextNode` and `Document` must be instantiated with kwargs: `Document(text=text)` (#6586)\n- `TextNode` (fka `Node`) has a `id_` or `node_id` property, rather than `doc_id` (#6586)\n- `TextNode` and `Document` have a metadata property, which replaces the extra\\_info property (#6586)\n- `TextNode` no longer has a `node_info` property (start/end indexes are accessed directly with `start/end_char_idx` attributes) (#6586)\n\n## \\[v0.6.33\\] - 2023-06-25 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v0633-2023-06-25 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_154 \"Permanent link\")\n\n- Add typesense vector store (#6561)\n- add df output parser (#6576)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_135 \"Permanent link\")\n\n- Track langchain dependency via bridge module. (#6573)\n\n## \\[v0.6.32\\] - 2023-06-23 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v0632-2023-06-23 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_155 \"Permanent link\")\n\n- add object index (#6548)\n- add SQL Schema Node Mapping + SQLTableRetrieverQueryEngine + obj index fixes (#6569)\n- sql refactor (NLSQLTableQueryEngine) (#6529)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_136 \"Permanent link\")\n\n- Update vector\\_stores.md (#6562)\n- Minor `BaseResponseBuilder` interface cleanup (#6557)\n- Refactor TreeSummarize (#6550)\n\n## \\[v0.6.31\\] - 2023-06-22 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v0631-2023-06-22 \"Permanent link\")\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_137 \"Permanent link\")\n\n- properly convert weaviate distance to score (#6545)\n- refactor tree summarize and fix bug to not truncate context (#6550)\n- fix custom KG retrieval notebook nits (#6551)\n\n## \\[v0.6.30\\] - 2023-06-21 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v0630-2023-06-21 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_156 \"Permanent link\")\n\n- multi-selector support in router query engine (#6518)\n- pydantic selector support in router query engine using OpenAI function calling API (#6518)\n- streaming response support in `CondenseQuestionChatEngine` and `SimpleChatEngine` (#6524)\n- metadata filtering support in `QdrantVectorStore` (#6476)\n- add `PGVectorStore` to support postgres with pgvector (#6190)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_138 \"Permanent link\")\n\n- better error handling in the mbox reader (#6248)\n- Fix blank similarity score when using weaviate (#6512)\n- fix for sorted nodes in `PrevNextNodePostprocessor` (#6048)\n\n### Breaking/Deprecated API Changes [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#breakingdeprecated-api-changes_5 \"Permanent link\")\n\n- Refactor PandasQueryEngine to take in df directly, deprecate PandasIndex (#6527)\n\n## \\[v0.6.29\\] - 2023-06-20 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v0629-2023-06-20 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_157 \"Permanent link\")\n\n- query planning tool with OpenAI Function API (#6520)\n- docs: example of kg+vector index (#6497)\n- Set context window sizes for Cohere and AI21(J2 model) (#6485)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_139 \"Permanent link\")\n\n- add default input size for Cohere and AI21 (#6485)\n- docs: replace comma with colon in dict object (#6439)\n- extra space in prompt and error message update (#6443)\n- \\[Issue 6417\\] Fix prompt\\_templates docs page (#6499)\n- Rip out monkey patch and update model to context window mapping (#6490)\n\n## \\[v0.6.28\\] - 2023-06-19 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v0628-2023-06-19 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_158 \"Permanent link\")\n\n- New OpenAI Agent + Query Engine Cookbook (#6496)\n- allow recursive data extraction (pydantic program) (#6503)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_140 \"Permanent link\")\n\n- update mongo interface (#6501)\n- fixes that we forgot to include for openai pydantic program (#6503) (#6504)\n- Fix github pics in Airbyte notebook (#6493)\n\n## \\[v0.6.27\\] - 2023-06-16 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v0627-2023-06-16 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_159 \"Permanent link\")\n\n- Add node doc\\_id filtering to weaviate (#6467)\n- New `TokenCountingCallback` to customize and track embedding, prompt, and completion token usage (#6440)\n- OpenAI Retrieval Function Agent (#6491)\n\n### Breaking/Deprecated API Changes [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#breakingdeprecated-api-changes_6 \"Permanent link\")\n\n- Deprecated current token tracking (llm predictor and embed model will no longer track tokens in the future, please use the `TokenCountingCallback` (#6440)\n- Add maximal marginal relevance to the Simple Vector Store, which can be enabled as a query mode (#6446)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_141 \"Permanent link\")\n\n- `as_chat_engine` properly inherits the current service context (#6470)\n- Use namespace when deleting from pinecone (#6475)\n- Fix paths when using fsspec on windows (#3778)\n- Fix for using custom file readers in `SimpleDirectoryReader` (#6477)\n- Edit MMR Notebook (#6486)\n- FLARE fixes (#6484)\n\n## \\[v0.6.26\\] - 2023-06-14 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v0626-2023-06-14 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_160 \"Permanent link\")\n\n- Add OpenAIAgent and tutorial notebook for \"build your own agent\" (#6461)\n- Add OpenAIPydanticProgram (#6462)\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_142 \"Permanent link\")\n\n- Fix citation engine import (#6456)\n\n## \\[v0.6.25\\] - 2023-06-13 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v0625-2023-06-13 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_161 \"Permanent link\")\n\n- Added FLARE query engine (#6419).\n\n## \\[v0.6.24\\] - 2023-06-12 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v0624-2023-06-12 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_162 \"Permanent link\")\n\n- Added better support for vector store with existing data (e.g. allow configurable text key) for Pinecone and Weaviate. (#6393)\n- Support batched upsert for Pineone (#6393)\n- Added initial [guidance](https://github.com/microsoft/guidance/) integration. Added `GuidancePydanticProgram` for generic structured output generation and `GuidanceQuestionGenerator` for generating sub-questions in `SubQuestionQueryEngine` (#6246).\n\n## \\[v0.6.23\\] - 2023-06-11 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v0623-2023-06-11 \"Permanent link\")\n\n### Bug Fixes / Nits [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes-nits_143 \"Permanent link\")\n\n- Remove hardcoded chunk size for citation query engine (#6408)\n- Mongo demo improvements (#6406)\n- Fix notebook (#6418)\n- Cleanup RetryQuery notebook (#6381)\n\n## \\[v0.6.22\\] - 2023-06-10 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v0622-2023-06-10 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_163 \"Permanent link\")\n\n- Added `SQLJoinQueryEngine` (generalization of `SQLAutoVectorQueryEngine`) (#6265)\n- Added support for graph stores under the hood, and initial support for Nebula KG. More docs coming soon! (#2581)\n- Added guideline evaluator to allow llm to provide feedback based on user guidelines (#4664)\n- Added support for MongoDB Vector stores to enable Atlas knnbeta search (#6379)\n- Added new CitationQueryEngine for inline citations of sources in response text (#6239)\n\n### Bug Fixes [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes_3 \"Permanent link\")\n\n- Fixed bug with `delete_ref_doc` not removing all metadata from the docstore (#6192)\n- FIxed bug with loading existing QDrantVectorStore (#6230)\n\n### Miscellaneous [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#miscellaneous \"Permanent link\")\n\n- Added changelog officially to github repo (#6191)\n\n## \\[v0.6.21\\] - 2023-06-06 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v0621-2023-06-06 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_164 \"Permanent link\")\n\n- SimpleDirectoryReader has new `filename_as_id` flag to automatically set the doc\\_id (useful for `refresh_ref_docs()`)\n- DocArray vector store integration\n- Tair vector store integration\n- Weights and Biases callback handler for tracing and versioning indexes\n- Can initialize indexes directly from a vector store: `index = VectorStoreIndex.from_vector_store(vector_store=vector_store)`\n\n### Bug Fixes [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes_4 \"Permanent link\")\n\n- Fixed multimodal notebook\n- Updated/fixed the SQL tutorial in the docs\n\n### Miscellaneous [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#miscellaneous_1 \"Permanent link\")\n\n- Minor docs updates\n- Added github pull-requset templates\n- Added github issue-forms\n\n## \\[v0.6.20\\] - 2023-06-04 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v0620-2023-06-04 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_165 \"Permanent link\")\n\n- Added new JSONQueryEngine that uses JSON schema to deliver more accurate JSON query answers\n- Metadata support for redis vector-store\n- Added Supabase vector store integration\n\n### Bug Fixes [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#bug-fixes_5 \"Permanent link\")\n\n- Fixed typo in text-to-sql prompt\n\n### Breaking/Deprecated API Changes [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#breakingdeprecated-api-changes_7 \"Permanent link\")\n\n- Removed GPT prefix from indexes (old imports/names are still supported though)\n\n### Miscellaneous [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#miscellaneous_2 \"Permanent link\")\n\n- Major docs updates, brought important modules to the top level\n\n## \\[v0.6.19\\] - 2023-06-02 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v0619-2023-06-02 \"Permanent link\")\n\n### New Features [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#new-features_166 \"Permanent link\")\n\n- Added agent tool abstraction for llama-hub data loaders\n\n### Miscellaneous [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#miscellaneous_3 \"Permanent link\")\n\n- Minor doc updates\n\n## \\[v0.6.18\\] - 2023-06-02 [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#v0618-2023-06-02 \"Permanent link\")\n\n### Miscellaneous [\\#](https://docs.llamaindex.ai/en/stable/CHANGELOG/\\#miscellaneous_4 \"Permanent link\")\n\n- Added `Discover LlamaIndex` video series to the tutorials docs section\n- Minor docs updates\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Changelog - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/CHANGELOG/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/clip/#llama_index.embeddings.clip.ClipEmbedding)\n\n# Clip\n\n## ClipEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/clip/\\#llama_index.embeddings.clip.ClipEmbedding \"Permanent link\")\n\nBases: `MultiModalEmbedding`\n\nCLIP embedding models for encoding text and image for Multi-Modal purpose.\n\nThis class provides an interface to generate embeddings using a model\ndeployed in OpenAI CLIP. At the initialization it requires a model name\nof CLIP.\n\nNote\n\nRequires `clip` package to be available in the PYTHONPATH. It can be installed with\n`pip install git+https://github.com/openai/CLIP.git`.\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-clip/llama_index/embeddings/clip/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>``` | ```<br>class ClipEmbedding(MultiModalEmbedding):<br>    \"\"\"CLIP embedding models for encoding text and image for Multi-Modal purpose.<br>    This class provides an interface to generate embeddings using a model<br>    deployed in OpenAI CLIP. At the initialization it requires a model name<br>    of CLIP.<br>    Note:<br>        Requires `clip` package to be available in the PYTHONPATH. It can be installed with<br>        `pip install git+https://github.com/openai/CLIP.git`.<br>    \"\"\"<br>    embed_batch_size: int = Field(default=DEFAULT_EMBED_BATCH_SIZE, gt=0)<br>    _clip: Any = PrivateAttr()<br>    _model: Any = PrivateAttr()<br>    _preprocess: Any = PrivateAttr()<br>    _device: Any = PrivateAttr()<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"ClipEmbedding\"<br>    def __init__(<br>        self,<br>        *,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        model_name: str = DEFAULT_CLIP_MODEL,<br>        **kwargs: Any,<br>    ):<br>        \"\"\"Initializes the ClipEmbedding class.<br>        During the initialization the `clip` package is imported.<br>        Args:<br>            embed_batch_size (int, optional): The batch size for embedding generation. Defaults to 10,<br>                must be > 0 and <= 100.<br>            model_name (str): The model name of Clip model.<br>        Raises:<br>            ImportError: If the `clip` package is not available in the PYTHONPATH.<br>            ValueError: If the model cannot be fetched from Open AI. or if the embed_batch_size<br>                is not in the range (0, 100].<br>        \"\"\"<br>        if embed_batch_size <= 0:<br>            raise ValueError(f\"Embed batch size {embed_batch_size}  must be > 0.\")<br>        try:<br>            import clip<br>            import torch<br>        except ImportError:<br>            raise ImportError(<br>                \"ClipEmbedding requires `pip install git+https://github.com/openai/CLIP.git` and torch.\"<br>            )<br>        super().__init__(<br>            embed_batch_size=embed_batch_size, model_name=model_name, **kwargs<br>        )<br>        try:<br>            self._device = \"cuda\" if torch.cuda.is_available() else \"cpu\"<br>            is_local_path = os.path.exists(self.model_name)<br>            if not is_local_path and self.model_name not in AVAILABLE_CLIP_MODELS:<br>                raise ValueError(<br>                    f\"Model name {self.model_name} is not available in CLIP.\"<br>                )<br>            self._model, self._preprocess = clip.load(<br>                self.model_name, device=self._device<br>            )<br>        except Exception as e:<br>            logger.error(\"Error while loading clip model.\")<br>            raise ValueError(\"Unable to fetch the requested embeddings model\") from e<br>    # TEXT EMBEDDINGS<br>    async def _aget_query_embedding(self, query: str) -> Embedding:<br>        return self._get_query_embedding(query)<br>    def _get_text_embedding(self, text: str) -> Embedding:<br>        return self._get_text_embeddings([text])[0]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[Embedding]:<br>        results = []<br>        for text in texts:<br>            try:<br>                import clip<br>            except ImportError:<br>                raise ImportError(<br>                    \"ClipEmbedding requires `pip install git+https://github.com/openai/CLIP.git` and torch.\"<br>                )<br>            text_embedding = self._model.encode_text(<br>                clip.tokenize(text).to(self._device)<br>            )<br>            results.append(text_embedding.tolist()[0])<br>        return results<br>    def _get_query_embedding(self, query: str) -> Embedding:<br>        return self._get_text_embedding(query)<br>    # IMAGE EMBEDDINGS<br>    async def _aget_image_embedding(self, img_file_path: ImageType) -> Embedding:<br>        return self._get_image_embedding(img_file_path)<br>    def _get_image_embedding(self, img_file_path: ImageType) -> Embedding:<br>        import torch<br>        with torch.no_grad():<br>            image = (<br>                self._preprocess(Image.open(img_file_path))<br>                .unsqueeze(0)<br>                .to(self._device)<br>            )<br>            return self._model.encode_image(image).tolist()[0]<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Clip - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/clip/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/agent/llm_compiler/#llama_index.agent.llm_compiler.LLMCompilerAgentWorker)\n\n# Llm compiler\n\n## LLMCompilerAgentWorker [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/llm_compiler/\\#llama_index.agent.llm_compiler.LLMCompilerAgentWorker \"Permanent link\")\n\nBases: `BaseAgentWorker`\n\nLLMCompiler Agent Worker.\n\nLLMCompiler is an agent framework that allows async multi-function calling and query planning.\nHere is the implementation.\n\nSource Repo (paper linked): https://github.com/SqueezeAILab/LLMCompiler?tab=readme-ov-file\n\nSource code in `llama-index-integrations/agent/llama-index-agent-llm-compiler/llama_index/agent/llm_compiler/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>``` | ```<br>class LLMCompilerAgentWorker(BaseAgentWorker):<br>    \"\"\"LLMCompiler Agent Worker.<br>    LLMCompiler is an agent framework that allows async multi-function calling and query planning.<br>    Here is the implementation.<br>    Source Repo (paper linked): https://github.com/SqueezeAILab/LLMCompiler?tab=readme-ov-file<br>    \"\"\"<br>    def __init__(<br>        self,<br>        tools: Sequence[BaseTool],<br>        llm: LLM,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        planner_example_prompt_str: Optional[str] = None,<br>        stop: Optional[List[str]] = None,<br>        joiner_prompt: Optional[PromptTemplate] = None,<br>        max_replans: int = 3,<br>    ) -> None:<br>        self.callback_manager = callback_manager or llm.callback_manager<br>        self.planner_example_prompt_str = (<br>            planner_example_prompt_str or PLANNER_EXAMPLE_PROMPT<br>        )<br>        self.system_prompt = generate_llm_compiler_prompt(<br>            tools, example_prompt=self.planner_example_prompt_str<br>        )<br>        self.system_prompt_replan = generate_llm_compiler_prompt(<br>            tools, is_replan=True, example_prompt=self.planner_example_prompt_str<br>        )<br>        self.llm = llm<br>        # TODO: make tool_retriever work<br>        self.tools = tools<br>        self.output_parser = LLMCompilerPlanParser(tools=tools)<br>        self.stop = stop<br>        self.max_replans = max_replans<br>        self.verbose = verbose<br>        # joiner program<br>        self.joiner_prompt = joiner_prompt or PromptTemplate(OUTPUT_PROMPT)<br>        self.joiner_program = LLMTextCompletionProgram.from_defaults(<br>            output_parser=LLMCompilerJoinerParser(),<br>            output_cls=JoinerOutput,<br>            prompt=self.joiner_prompt,<br>            llm=self.llm,<br>            verbose=verbose,<br>        )<br>        # if len(tools) > 0 and tool_retriever is not None:<br>        #     raise ValueError(\"Cannot specify both tools and tool_retriever\")<br>        # elif len(tools) > 0:<br>        #     self._get_tools = lambda _: tools<br>        # elif tool_retriever is not None:<br>        #     tool_retriever_c = cast(ObjectRetriever[BaseTool], tool_retriever)<br>        #     self._get_tools = lambda message: tool_retriever_c.retrieve(message)<br>        # else:<br>        #     self._get_tools = lambda _: []<br>    @classmethod<br>    def from_tools(<br>        cls,<br>        tools: Optional[Sequence[BaseTool]] = None,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        llm: Optional[LLM] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        **kwargs: Any,<br>    ) -> \"LLMCompilerAgentWorker\":<br>        \"\"\"Convenience constructor method from set of of BaseTools (Optional).<br>        Returns:<br>            LLMCompilerAgentWorker: the LLMCompilerAgentWorker instance<br>        \"\"\"<br>        llm = llm or OpenAI(model=DEFAULT_MODEL_NAME)<br>        if callback_manager is not None:<br>            llm.callback_manager = callback_manager<br>        return cls(<br>            tools=tools or [],<br>            tool_retriever=tool_retriever,<br>            llm=llm,<br>            callback_manager=callback_manager,<br>            verbose=verbose,<br>        )<br>    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>        \"\"\"Initialize step from task.\"\"\"<br>        sources: List[ToolOutput] = []<br>        # temporary memory for new messages<br>        new_memory = ChatMemoryBuffer.from_defaults()<br>        # put user message in memory<br>        new_memory.put(ChatMessage(content=task.input, role=MessageRole.USER))<br>        # initialize task state<br>        task_state = {<br>            \"sources\": sources,<br>            \"new_memory\": new_memory,<br>        }<br>        task.extra_state.update(task_state)<br>        return TaskStep(<br>            task_id=task.task_id,<br>            step_id=str(uuid.uuid4()),<br>            input=task.input,<br>            step_state={\"is_replan\": False, \"contexts\": [], \"replans\": 0},<br>        )<br>    def get_tools(self, input: str) -> List[AsyncBaseTool]:<br>        \"\"\"Get tools.\"\"\"<br>        # return [adapt_to_async_tool(t) for t in self._get_tools(input)]<br>        return [adapt_to_async_tool(t) for t in self.tools]<br>    async def arun_llm(<br>        self,<br>        input: str,<br>        previous_context: Optional[str] = None,<br>        is_replan: bool = False,<br>    ) -> ChatResponse:<br>        \"\"\"Run LLM.\"\"\"<br>        if is_replan:<br>            system_prompt = self.system_prompt_replan<br>            assert previous_context is not None, \"previous_context cannot be None\"<br>            human_prompt = f\"Question: {input}\\n{previous_context}\\n\"<br>        else:<br>            system_prompt = self.system_prompt<br>            human_prompt = f\"Question: {input}\"<br>        messages = [<br>            ChatMessage(role=MessageRole.SYSTEM, content=system_prompt),<br>            ChatMessage(role=MessageRole.USER, content=human_prompt),<br>        ]<br>        return await self.llm.achat(messages)<br>    async def ajoin(<br>        self,<br>        input: str,<br>        tasks: Dict[int, LLMCompilerTask],<br>        is_final: bool = False,<br>    ) -> JoinerOutput:<br>        \"\"\"Join answer using LLM/agent.\"\"\"<br>        agent_scratchpad = \"\\n\\n\"<br>        agent_scratchpad += \"\".join(<br>            [<br>                task.get_thought_action_observation(<br>                    include_action=True, include_thought=True<br>                )<br>                for task in tasks.values()<br>                if not task.is_join<br>            ]<br>        )<br>        agent_scratchpad = agent_scratchpad.strip()<br>        output = self.joiner_program(<br>            query_str=input,<br>            context_str=agent_scratchpad,<br>        )<br>        output = cast(JoinerOutput, output)<br>        if self.verbose:<br>            print_text(f\"> Thought: {output.thought}\\n\", color=\"pink\")<br>            print_text(f\"> Answer: {output.answer}\\n\", color=\"pink\")<br>        if is_final:<br>            output.is_replan = False<br>        return output<br>    def _get_task_step_response(<br>        self,<br>        task: Task,<br>        llmc_tasks: Dict[int, LLMCompilerTask],<br>        answer: str,<br>        joiner_thought: str,<br>        step: TaskStep,<br>        is_replan: bool,<br>    ) -> TaskStepOutput:<br>        \"\"\"Get task step response.\"\"\"<br>        agent_answer = AgentChatResponse(response=answer, sources=[])<br>        if not is_replan:<br>            # generate final answer<br>            new_steps = []<br>            # put in memory<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(content=answer, role=MessageRole.ASSISTANT)<br>            )<br>        else:<br>            # Collect contexts for the subsequent replanner<br>            context = generate_context_for_replanner(<br>                tasks=llmc_tasks, joiner_thought=joiner_thought<br>            )<br>            new_contexts = step.step_state[\"contexts\"] + [context]<br>            # TODO: generate new steps<br>            new_steps = [<br>                step.get_next_step(<br>                    step_id=str(uuid.uuid4()),<br>                    input=None,<br>                    step_state={<br>                        \"is_replan\": is_replan,<br>                        \"contexts\": new_contexts,<br>                        \"replans\": step.step_state[\"replans\"] + 1,<br>                    },<br>                )<br>            ]<br>        return TaskStepOutput(<br>            output=agent_answer,<br>            task_step=step,<br>            next_steps=new_steps,<br>            is_last=not is_replan,<br>        )<br>    async def _arun_step(<br>        self,<br>        step: TaskStep,<br>        task: Task,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        if self.verbose:<br>            print(<br>                f\"> Running step {step.step_id} for task {task.task_id}.\\n\"<br>                f\"> Step count: {step.step_state['replans']}\"<br>            )<br>        is_final_iter = (<br>            step.step_state[\"is_replan\"]<br>            and step.step_state[\"replans\"] >= self.max_replans<br>        )<br>        if len(step.step_state[\"contexts\"]) == 0:<br>            formatted_contexts = None<br>        else:<br>            formatted_contexts = format_contexts(step.step_state[\"contexts\"])<br>        llm_response = await self.arun_llm(<br>            task.input,<br>            previous_context=formatted_contexts,<br>            is_replan=step.step_state[\"is_replan\"],<br>        )<br>        if self.verbose:<br>            print_text(f\"> Plan: {llm_response.message.content}\\n\", color=\"pink\")<br>        # return task dict (will generate plan, parse into dictionary)<br>        task_dict = self.output_parser.parse(cast(str, llm_response.message.content))<br>        # execute via task executor<br>        task_fetching_unit = TaskFetchingUnit.from_tasks(<br>            task_dict, verbose=self.verbose<br>        )<br>        await task_fetching_unit.schedule()<br>        ## join tasks - get response<br>        tasks = cast(Dict[int, LLMCompilerTask], task_fetching_unit.tasks)<br>        joiner_output = await self.ajoin(<br>            task.input,<br>            tasks,<br>            is_final=is_final_iter,<br>        )<br>        # get task step response (with new steps planned)<br>        return self._get_task_step_response(<br>            task,<br>            llmc_tasks=tasks,<br>            answer=joiner_output.answer,<br>            joiner_thought=joiner_output.thought,<br>            step=step,<br>            is_replan=joiner_output.is_replan,<br>        )<br>    @trace_method(\"run_step\")<br>    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        return asyncio.run(self.arun_step(step=step, task=task, **kwargs))<br>    @trace_method(\"run_step\")<br>    async def arun_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        return await self._arun_step(step, task)<br>    @trace_method(\"run_step\")<br>    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        # # TODO: figure out if we need a different type for TaskStepOutput<br>        # return self._run_step_stream(step, task)<br>        raise NotImplementedError<br>    @trace_method(\"run_step\")<br>    async def astream_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        raise NotImplementedError<br>        # \"\"\"Run step (async stream).\"\"\"<br>        # return await self._arun_step_stream(step, task)<br>    def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>        \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>        # add new messages to memory<br>        task.memory.put_messages(task.extra_state[\"new_memory\"].get_all())<br>        # reset new memory<br>        task.extra_state[\"new_memory\"].reset()<br>``` |\n\n### from\\_tools`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/llm_compiler/\\#llama_index.agent.llm_compiler.LLMCompilerAgentWorker.from_tools \"Permanent link\")\n\n```\nfrom_tools(tools: Optional[Sequence[BaseTool]] = None, tool_retriever: Optional[ObjectRetriever[BaseTool]] = None, llm: Optional[LLM] = None, callback_manager: Optional[CallbackManager] = None, verbose: bool = False, **kwargs: Any) -> LLMCompilerAgentWorker\n\n```\n\nConvenience constructor method from set of of BaseTools (Optional).\n\n**Returns:**\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `LLMCompilerAgentWorker` | `LLMCompilerAgentWorker` | the LLMCompilerAgentWorker instance |\n\nSource code in `llama-index-integrations/agent/llama-index-agent-llm-compiler/llama_index/agent/llm_compiler/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>``` | ```<br>@classmethod<br>def from_tools(<br>    cls,<br>    tools: Optional[Sequence[BaseTool]] = None,<br>    tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>    llm: Optional[LLM] = None,<br>    callback_manager: Optional[CallbackManager] = None,<br>    verbose: bool = False,<br>    **kwargs: Any,<br>) -> \"LLMCompilerAgentWorker\":<br>    \"\"\"Convenience constructor method from set of of BaseTools (Optional).<br>    Returns:<br>        LLMCompilerAgentWorker: the LLMCompilerAgentWorker instance<br>    \"\"\"<br>    llm = llm or OpenAI(model=DEFAULT_MODEL_NAME)<br>    if callback_manager is not None:<br>        llm.callback_manager = callback_manager<br>    return cls(<br>        tools=tools or [],<br>        tool_retriever=tool_retriever,<br>        llm=llm,<br>        callback_manager=callback_manager,<br>        verbose=verbose,<br>    )<br>``` |\n\n### initialize\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/llm_compiler/\\#llama_index.agent.llm_compiler.LLMCompilerAgentWorker.initialize_step \"Permanent link\")\n\n```\ninitialize_step(task: Task, **kwargs: Any) -> TaskStep\n\n```\n\nInitialize step from task.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-llm-compiler/llama_index/agent/llm_compiler/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>``` | ```<br>def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>    \"\"\"Initialize step from task.\"\"\"<br>    sources: List[ToolOutput] = []<br>    # temporary memory for new messages<br>    new_memory = ChatMemoryBuffer.from_defaults()<br>    # put user message in memory<br>    new_memory.put(ChatMessage(content=task.input, role=MessageRole.USER))<br>    # initialize task state<br>    task_state = {<br>        \"sources\": sources,<br>        \"new_memory\": new_memory,<br>    }<br>    task.extra_state.update(task_state)<br>    return TaskStep(<br>        task_id=task.task_id,<br>        step_id=str(uuid.uuid4()),<br>        input=task.input,<br>        step_state={\"is_replan\": False, \"contexts\": [], \"replans\": 0},<br>    )<br>``` |\n\n### get\\_tools [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/llm_compiler/\\#llama_index.agent.llm_compiler.LLMCompilerAgentWorker.get_tools \"Permanent link\")\n\n```\nget_tools(input: str) -> List[AsyncBaseTool]\n\n```\n\nGet tools.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-llm-compiler/llama_index/agent/llm_compiler/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>239<br>240<br>241<br>242<br>``` | ```<br>def get_tools(self, input: str) -> List[AsyncBaseTool]:<br>    \"\"\"Get tools.\"\"\"<br>    # return [adapt_to_async_tool(t) for t in self._get_tools(input)]<br>    return [adapt_to_async_tool(t) for t in self.tools]<br>``` |\n\n### arun\\_llm`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/llm_compiler/\\#llama_index.agent.llm_compiler.LLMCompilerAgentWorker.arun_llm \"Permanent link\")\n\n```\narun_llm(input: str, previous_context: Optional[str] = None, is_replan: bool = False) -> ChatResponse\n\n```\n\nRun LLM.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-llm-compiler/llama_index/agent/llm_compiler/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>``` | ```<br>async def arun_llm(<br>    self,<br>    input: str,<br>    previous_context: Optional[str] = None,<br>    is_replan: bool = False,<br>) -> ChatResponse:<br>    \"\"\"Run LLM.\"\"\"<br>    if is_replan:<br>        system_prompt = self.system_prompt_replan<br>        assert previous_context is not None, \"previous_context cannot be None\"<br>        human_prompt = f\"Question: {input}\\n{previous_context}\\n\"<br>    else:<br>        system_prompt = self.system_prompt<br>        human_prompt = f\"Question: {input}\"<br>    messages = [<br>        ChatMessage(role=MessageRole.SYSTEM, content=system_prompt),<br>        ChatMessage(role=MessageRole.USER, content=human_prompt),<br>    ]<br>    return await self.llm.achat(messages)<br>``` |\n\n### ajoin`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/llm_compiler/\\#llama_index.agent.llm_compiler.LLMCompilerAgentWorker.ajoin \"Permanent link\")\n\n```\najoin(input: str, tasks: Dict[int, LLMCompilerTask], is_final: bool = False) -> JoinerOutput\n\n```\n\nJoin answer using LLM/agent.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-llm-compiler/llama_index/agent/llm_compiler/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>``` | ```<br>async def ajoin(<br>    self,<br>    input: str,<br>    tasks: Dict[int, LLMCompilerTask],<br>    is_final: bool = False,<br>) -> JoinerOutput:<br>    \"\"\"Join answer using LLM/agent.\"\"\"<br>    agent_scratchpad = \"\\n\\n\"<br>    agent_scratchpad += \"\".join(<br>        [<br>            task.get_thought_action_observation(<br>                include_action=True, include_thought=True<br>            )<br>            for task in tasks.values()<br>            if not task.is_join<br>        ]<br>    )<br>    agent_scratchpad = agent_scratchpad.strip()<br>    output = self.joiner_program(<br>        query_str=input,<br>        context_str=agent_scratchpad,<br>    )<br>    output = cast(JoinerOutput, output)<br>    if self.verbose:<br>        print_text(f\"> Thought: {output.thought}\\n\", color=\"pink\")<br>        print_text(f\"> Answer: {output.answer}\\n\", color=\"pink\")<br>    if is_final:<br>        output.is_replan = False<br>    return output<br>``` |\n\n### run\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/llm_compiler/\\#llama_index.agent.llm_compiler.LLMCompilerAgentWorker.run_step \"Permanent link\")\n\n```\nrun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-llm-compiler/llama_index/agent/llm_compiler/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>398<br>399<br>400<br>401<br>``` | ```<br>@trace_method(\"run_step\")<br>def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step.\"\"\"<br>    return asyncio.run(self.arun_step(step=step, task=task, **kwargs))<br>``` |\n\n### arun\\_step`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/llm_compiler/\\#llama_index.agent.llm_compiler.LLMCompilerAgentWorker.arun_step \"Permanent link\")\n\n```\narun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async).\n\nSource code in `llama-index-integrations/agent/llama-index-agent-llm-compiler/llama_index/agent/llm_compiler/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>403<br>404<br>405<br>406<br>407<br>408<br>``` | ```<br>@trace_method(\"run_step\")<br>async def arun_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async).\"\"\"<br>    return await self._arun_step(step, task)<br>``` |\n\n### stream\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/llm_compiler/\\#llama_index.agent.llm_compiler.LLMCompilerAgentWorker.stream_step \"Permanent link\")\n\n```\nstream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (stream).\n\nSource code in `llama-index-integrations/agent/llama-index-agent-llm-compiler/llama_index/agent/llm_compiler/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>410<br>411<br>412<br>413<br>414<br>415<br>``` | ```<br>@trace_method(\"run_step\")<br>def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step (stream).\"\"\"<br>    # # TODO: figure out if we need a different type for TaskStepOutput<br>    # return self._run_step_stream(step, task)<br>    raise NotImplementedError<br>``` |\n\n### finalize\\_task [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/llm_compiler/\\#llama_index.agent.llm_compiler.LLMCompilerAgentWorker.finalize_task \"Permanent link\")\n\n```\nfinalize_task(task: Task, **kwargs: Any) -> None\n\n```\n\nFinalize task, after all the steps are completed.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-llm-compiler/llama_index/agent/llm_compiler/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>425<br>426<br>427<br>428<br>429<br>430<br>``` | ```<br>def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>    \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>    # add new messages to memory<br>    task.memory.put_messages(task.extra_state[\"new_memory\"].get_all())<br>    # reset new memory<br>    task.extra_state[\"new_memory\"].reset()<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Llm compiler - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/agent/llm_compiler/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/openinference/#llama_index.callbacks.openinference.OpenInferenceCallbackHandler)\n\n# Openinference\n\n## OpenInferenceCallbackHandler [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/openinference/\\#llama_index.callbacks.openinference.OpenInferenceCallbackHandler \"Permanent link\")\n\nBases: `BaseCallbackHandler`\n\nCallback handler for storing generation data in OpenInference format.\nOpenInference is an open standard for capturing and storing AI model\ninferences. It enables production LLMapp servers to seamlessly integrate\nwith LLM observability solutions such as Arize and Phoenix.\n\nFor more information on the specification, see\nhttps://github.com/Arize-ai/open-inference-spec\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-openinference/llama_index/callbacks/openinference/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>``` | ```<br>class OpenInferenceCallbackHandler(BaseCallbackHandler):<br>    \"\"\"Callback handler for storing generation data in OpenInference format.<br>    OpenInference is an open standard for capturing and storing AI model<br>    inferences. It enables production LLMapp servers to seamlessly integrate<br>    with LLM observability solutions such as Arize and Phoenix.<br>    For more information on the specification, see<br>    https://github.com/Arize-ai/open-inference-spec<br>    \"\"\"<br>    def __init__(<br>        self,<br>        callback: Optional[Callable[[List[QueryData], List[NodeData]], None]] = None,<br>    ) -> None:<br>        \"\"\"Initializes the OpenInferenceCallbackHandler.<br>        Args:<br>            callback (Optional[Callable[[List[QueryData], List[NodeData]], None]], optional): A<br>            callback function that will be called when a query trace is<br>            completed, often used for logging or persisting query data.<br>        \"\"\"<br>        super().__init__(event_starts_to_ignore=[], event_ends_to_ignore=[])<br>        self._callback = callback<br>        self._trace_data = TraceData()<br>        self._query_data_buffer: List[QueryData] = []<br>        self._node_data_buffer: List[NodeData] = []<br>    def start_trace(self, trace_id: Optional[str] = None) -> None:<br>        if trace_id == \"query\" or trace_id == \"chat\":<br>            self._trace_data = TraceData()<br>            self._trace_data.query_data.timestamp = datetime.now().isoformat()<br>            self._trace_data.query_data.id = _generate_random_id()<br>    def end_trace(<br>        self,<br>        trace_id: Optional[str] = None,<br>        trace_map: Optional[Dict[str, List[str]]] = None,<br>    ) -> None:<br>        if trace_id == \"query\" or trace_id == \"chat\":<br>            self._query_data_buffer.append(self._trace_data.query_data)<br>            self._node_data_buffer.extend(self._trace_data.node_datas)<br>            self._trace_data = TraceData()<br>            if self._callback is not None:<br>                self._callback(self._query_data_buffer, self._node_data_buffer)<br>    def on_event_start(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        parent_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> str:<br>        if payload is not None:<br>            if event_type is CBEventType.QUERY:<br>                query_text = payload[EventPayload.QUERY_STR]<br>                self._trace_data.query_data.query_text = query_text<br>            elif event_type is CBEventType.LLM:<br>                if prompt := payload.get(EventPayload.PROMPT, None):<br>                    self._trace_data.query_data.llm_prompt = prompt<br>                if messages := payload.get(EventPayload.MESSAGES, None):<br>                    self._trace_data.query_data.llm_messages = [<br>                        (m.role.value, m.content) for m in messages<br>                    ]<br>                    # For chat engines there is no query event and thus the<br>                    # query text will be None, in this case we set the query<br>                    # text to the last message passed to the LLM<br>                    if self._trace_data.query_data.query_text is None:<br>                        self._trace_data.query_data.query_text = messages[-1].content<br>        return event_id<br>    def on_event_end(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> None:<br>        if payload is None:<br>            return<br>        if event_type is CBEventType.RETRIEVE:<br>            for node_with_score in payload[EventPayload.NODES]:<br>                node = node_with_score.node<br>                score = node_with_score.score<br>                self._trace_data.query_data.node_ids.append(node.hash)<br>                self._trace_data.query_data.scores.append(score)<br>                self._trace_data.node_datas.append(<br>                    NodeData(<br>                        id=node.hash,<br>                        node_text=node.text,<br>                    )<br>                )<br>        elif event_type is CBEventType.LLM:<br>            if self._trace_data.query_data.response_text is None:<br>                if response := payload.get(EventPayload.RESPONSE, None):<br>                    if isinstance(response, ChatResponse):<br>                        # If the response is of class ChatResponse the string<br>                        # representation has the format \"<role>: <message>\",<br>                        # but we want just the message<br>                        response_text = response.message.content<br>                    else:<br>                        response_text = str(response)<br>                    self._trace_data.query_data.response_text = response_text<br>                elif completion := payload.get(EventPayload.COMPLETION, None):<br>                    self._trace_data.query_data.response_text = str(completion)<br>        elif event_type is CBEventType.EMBEDDING:<br>            self._trace_data.query_data.query_embedding = payload[<br>                EventPayload.EMBEDDINGS<br>            ][0]<br>    def flush_query_data_buffer(self) -> List[QueryData]:<br>        \"\"\"Clears the query data buffer and returns the data.<br>        Returns:<br>            List[QueryData]: The query data.<br>        \"\"\"<br>        query_data_buffer = self._query_data_buffer<br>        self._query_data_buffer = []<br>        return query_data_buffer<br>    def flush_node_data_buffer(self) -> List[NodeData]:<br>        \"\"\"Clears the node data buffer and returns the data.<br>        Returns:<br>            List[NodeData]: The node data.<br>        \"\"\"<br>        node_data_buffer = self._node_data_buffer<br>        self._node_data_buffer = []<br>        return node_data_buffer<br>``` |\n\n### flush\\_query\\_data\\_buffer [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/openinference/\\#llama_index.callbacks.openinference.OpenInferenceCallbackHandler.flush_query_data_buffer \"Permanent link\")\n\n```\nflush_query_data_buffer() -> List[QueryData]\n\n```\n\nClears the query data buffer and returns the data.\n\n**Returns:**\n\n| Type | Description |\n| --- | --- |\n| `List[QueryData]` | List\\[QueryData\\]: The query data. |\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-openinference/llama_index/callbacks/openinference/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>``` | ```<br>def flush_query_data_buffer(self) -> List[QueryData]:<br>    \"\"\"Clears the query data buffer and returns the data.<br>    Returns:<br>        List[QueryData]: The query data.<br>    \"\"\"<br>    query_data_buffer = self._query_data_buffer<br>    self._query_data_buffer = []<br>    return query_data_buffer<br>``` |\n\n### flush\\_node\\_data\\_buffer [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/openinference/\\#llama_index.callbacks.openinference.OpenInferenceCallbackHandler.flush_node_data_buffer \"Permanent link\")\n\n```\nflush_node_data_buffer() -> List[NodeData]\n\n```\n\nClears the node data buffer and returns the data.\n\n**Returns:**\n\n| Type | Description |\n| --- | --- |\n| `List[NodeData]` | List\\[NodeData\\]: The node data. |\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-openinference/llama_index/callbacks/openinference/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>``` | ```<br>def flush_node_data_buffer(self) -> List[NodeData]:<br>    \"\"\"Clears the node data buffer and returns the data.<br>    Returns:<br>        List[NodeData]: The node data.<br>    \"\"\"<br>    node_data_buffer = self._node_data_buffer<br>    self._node_data_buffer = []<br>    return node_data_buffer<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Openinference - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/openinference/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/agent/coa/#llama_index.agent.coa.CoAAgentWorker)\n\n# Coa\n\n## CoAAgentWorker [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/coa/\\#llama_index.agent.coa.CoAAgentWorker \"Permanent link\")\n\nBases: `BaseAgentWorker`\n\nChain-of-abstraction Agent Worker.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-coa/llama_index/agent/coa/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>``` | ```<br>class CoAAgentWorker(BaseAgentWorker):<br>    \"\"\"Chain-of-abstraction Agent Worker.\"\"\"<br>    def __init__(<br>        self,<br>        llm: LLM,<br>        reasoning_prompt_template: str,<br>        refine_reasoning_prompt_template: str,<br>        output_parser: BaseOutputParser,<br>        tools: Optional[Sequence[BaseTool]] = None,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>    ) -> None:<br>        self.llm = llm<br>        self.callback_manager = callback_manager or llm.callback_manager<br>        if tools is None and tool_retriever is None:<br>            raise ValueError(\"Either tools or tool_retriever must be provided.\")<br>        self.tools = tools<br>        self.tool_retriever = tool_retriever<br>        self.reasoning_prompt_template = reasoning_prompt_template<br>        self.refine_reasoning_prompt_template = refine_reasoning_prompt_template<br>        self.output_parser = output_parser<br>        self.verbose = verbose<br>    @classmethod<br>    def from_tools(<br>        cls,<br>        tools: Optional[Sequence[BaseTool]] = None,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        llm: Optional[LLM] = None,<br>        reasoning_prompt_template: Optional[str] = None,<br>        refine_reasoning_prompt_template: Optional[str] = None,<br>        output_parser: Optional[BaseOutputParser] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        **kwargs: Any,<br>    ) -> \"CoAAgentWorker\":<br>        \"\"\"Convenience constructor method from set of of BaseTools (Optional).<br>        Returns:<br>            LLMCompilerAgentWorker: the LLMCompilerAgentWorker instance<br>        \"\"\"<br>        llm = llm or Settings.llm<br>        if callback_manager is not None:<br>            llm.callback_manager = callback_manager<br>        reasoning_prompt_template = (<br>            reasoning_prompt_template or REASONING_PROMPT_TEMPALTE<br>        )<br>        refine_reasoning_prompt_template = (<br>            refine_reasoning_prompt_template or REFINE_REASONING_PROMPT_TEMPALTE<br>        )<br>        output_parser = output_parser or ChainOfAbstractionParser(verbose=verbose)<br>        return cls(<br>            llm,<br>            reasoning_prompt_template,<br>            refine_reasoning_prompt_template,<br>            output_parser,<br>            tools=tools,<br>            tool_retriever=tool_retriever,<br>            callback_manager=callback_manager,<br>            verbose=verbose,<br>        )<br>    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>        \"\"\"Initialize step from task.\"\"\"<br>        sources: List[ToolOutput] = []<br>        # temporary memory for new messages<br>        new_memory = ChatMemoryBuffer.from_defaults()<br>        # put current history in new memory<br>        messages = task.memory.get(input=task.input)<br>        for message in messages:<br>            new_memory.put(message)<br>        # initialize task state<br>        task_state = {<br>            \"sources\": sources,<br>            \"new_memory\": new_memory,<br>        }<br>        task.extra_state.update(task_state)<br>        return TaskStep(<br>            task_id=task.task_id,<br>            step_id=str(uuid.uuid4()),<br>            input=task.input,<br>            step_state={\"prev_reasoning\": \"\"},<br>        )<br>    def get_tools(self, query_str: str) -> List[AsyncBaseTool]:<br>        \"\"\"Get tools.\"\"\"<br>        if self.tool_retriever:<br>            tools = self.tool_retriever.retrieve(query_str)<br>        else:<br>            tools = self.tools<br>        return [adapt_to_async_tool(t) for t in tools]<br>    async def _arun_step(<br>        self,<br>        step: TaskStep,<br>        task: Task,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        tools = self.get_tools(task.input)<br>        tools_by_name = {tool.metadata.name: tool for tool in tools}<br>        tools_strs = []<br>        for tool in tools:<br>            if isinstance(tool, FunctionTool):<br>                description = tool.metadata.description<br>                # remove function def, we will make our own<br>                if \"def \" in description:<br>                    description = \"\\n\".join(description.split(\"\\n\")[1:])<br>            else:<br>                description = tool.metadata.description<br>            tool_str = json_schema_to_python(<br>                tool.metadata.fn_schema_str, tool.metadata.name, description=description<br>            )<br>            tools_strs.append(tool_str)<br>        prev_reasoning = step.step_state.get(\"prev_reasoning\", \"\")<br>        # show available functions if first step<br>        if self.verbose and not prev_reasoning:<br>            print(f\"==== Available Parsed Functions ====\")<br>            for tool_str in tools_strs:<br>                print(tool_str)<br>        if not prev_reasoning:<br>            # get the reasoning prompt<br>            reasoning_prompt = self.reasoning_prompt_template.format(<br>                functions=\"\\n\".join(tools_strs), question=step.input<br>            )<br>        else:<br>            # get the refine reasoning prompt<br>            reasoning_prompt = self.refine_reasoning_prompt_template.format(<br>                question=step.input, prev_reasoning=prev_reasoning<br>            )<br>        messages = task.extra_state[\"new_memory\"].get()<br>        reasoning_message = ChatMessage(role=\"user\", content=reasoning_prompt)<br>        messages.append(reasoning_message)<br>        # run the reasoning prompt<br>        response = await self.llm.achat(messages)<br>        # print the chain of abstraction if first step<br>        if self.verbose and not prev_reasoning:<br>            print(f\"==== Generated Chain of Abstraction ====\")<br>            print(str(response.message.content))<br>        # parse the output, run functions<br>        parsed_response, tool_sources = await self.output_parser.aparse(<br>            response.message.content, tools_by_name<br>        )<br>        if len(tool_sources) == 0 or prev_reasoning:<br>            is_done = True<br>            new_steps = []<br>            # only add to memory when we are done<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(role=\"user\", content=task.input)<br>            )<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(role=\"assistant\", content=parsed_response)<br>            )<br>        else:<br>            is_done = False<br>            new_steps = [<br>                TaskStep(<br>                    task_id=task.task_id,<br>                    step_id=str(uuid.uuid4()),<br>                    input=task.input,<br>                    step_state={<br>                        \"prev_reasoning\": parsed_response,<br>                    },<br>                )<br>            ]<br>        agent_response = AgentChatResponse(<br>            response=parsed_response, sources=tool_sources<br>        )<br>        return TaskStepOutput(<br>            output=agent_response,<br>            task_step=step,<br>            is_last=is_done,<br>            next_steps=new_steps,<br>        )<br>    @trace_method(\"run_step\")<br>    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        return asyncio.run(self.arun_step(step=step, task=task, **kwargs))<br>    @trace_method(\"run_step\")<br>    async def arun_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        return await self._arun_step(step, task)<br>    @trace_method(\"run_step\")<br>    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        # Streaming isn't really possible, because we need the full response to know if we are done<br>        raise NotImplementedError<br>    @trace_method(\"run_step\")<br>    async def astream_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async stream).\"\"\"<br>        # Streaming isn't really possible, because we need the full response to know if we are done<br>        raise NotImplementedError<br>    def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>        \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>        # add new messages to memory<br>        task.memory.put_messages(task.extra_state[\"new_memory\"].get_all())<br>        # reset new memory<br>        task.extra_state[\"new_memory\"].reset()<br>``` |\n\n### from\\_tools`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/coa/\\#llama_index.agent.coa.CoAAgentWorker.from_tools \"Permanent link\")\n\n```\nfrom_tools(tools: Optional[Sequence[BaseTool]] = None, tool_retriever: Optional[ObjectRetriever[BaseTool]] = None, llm: Optional[LLM] = None, reasoning_prompt_template: Optional[str] = None, refine_reasoning_prompt_template: Optional[str] = None, output_parser: Optional[BaseOutputParser] = None, callback_manager: Optional[CallbackManager] = None, verbose: bool = False, **kwargs: Any) -> CoAAgentWorker\n\n```\n\nConvenience constructor method from set of of BaseTools (Optional).\n\n**Returns:**\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `LLMCompilerAgentWorker` | `CoAAgentWorker` | the LLMCompilerAgentWorker instance |\n\nSource code in `llama-index-integrations/agent/llama-index-agent-coa/llama_index/agent/coa/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>``` | ```<br>@classmethod<br>def from_tools(<br>    cls,<br>    tools: Optional[Sequence[BaseTool]] = None,<br>    tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>    llm: Optional[LLM] = None,<br>    reasoning_prompt_template: Optional[str] = None,<br>    refine_reasoning_prompt_template: Optional[str] = None,<br>    output_parser: Optional[BaseOutputParser] = None,<br>    callback_manager: Optional[CallbackManager] = None,<br>    verbose: bool = False,<br>    **kwargs: Any,<br>) -> \"CoAAgentWorker\":<br>    \"\"\"Convenience constructor method from set of of BaseTools (Optional).<br>    Returns:<br>        LLMCompilerAgentWorker: the LLMCompilerAgentWorker instance<br>    \"\"\"<br>    llm = llm or Settings.llm<br>    if callback_manager is not None:<br>        llm.callback_manager = callback_manager<br>    reasoning_prompt_template = (<br>        reasoning_prompt_template or REASONING_PROMPT_TEMPALTE<br>    )<br>    refine_reasoning_prompt_template = (<br>        refine_reasoning_prompt_template or REFINE_REASONING_PROMPT_TEMPALTE<br>    )<br>    output_parser = output_parser or ChainOfAbstractionParser(verbose=verbose)<br>    return cls(<br>        llm,<br>        reasoning_prompt_template,<br>        refine_reasoning_prompt_template,<br>        output_parser,<br>        tools=tools,<br>        tool_retriever=tool_retriever,<br>        callback_manager=callback_manager,<br>        verbose=verbose,<br>    )<br>``` |\n\n### initialize\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/coa/\\#llama_index.agent.coa.CoAAgentWorker.initialize_step \"Permanent link\")\n\n```\ninitialize_step(task: Task, **kwargs: Any) -> TaskStep\n\n```\n\nInitialize step from task.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-coa/llama_index/agent/coa/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>``` | ```<br>def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>    \"\"\"Initialize step from task.\"\"\"<br>    sources: List[ToolOutput] = []<br>    # temporary memory for new messages<br>    new_memory = ChatMemoryBuffer.from_defaults()<br>    # put current history in new memory<br>    messages = task.memory.get(input=task.input)<br>    for message in messages:<br>        new_memory.put(message)<br>    # initialize task state<br>    task_state = {<br>        \"sources\": sources,<br>        \"new_memory\": new_memory,<br>    }<br>    task.extra_state.update(task_state)<br>    return TaskStep(<br>        task_id=task.task_id,<br>        step_id=str(uuid.uuid4()),<br>        input=task.input,<br>        step_state={\"prev_reasoning\": \"\"},<br>    )<br>``` |\n\n### get\\_tools [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/coa/\\#llama_index.agent.coa.CoAAgentWorker.get_tools \"Permanent link\")\n\n```\nget_tools(query_str: str) -> List[AsyncBaseTool]\n\n```\n\nGet tools.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-coa/llama_index/agent/coa/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>``` | ```<br>def get_tools(self, query_str: str) -> List[AsyncBaseTool]:<br>    \"\"\"Get tools.\"\"\"<br>    if self.tool_retriever:<br>        tools = self.tool_retriever.retrieve(query_str)<br>    else:<br>        tools = self.tools<br>    return [adapt_to_async_tool(t) for t in tools]<br>``` |\n\n### run\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/coa/\\#llama_index.agent.coa.CoAAgentWorker.run_step \"Permanent link\")\n\n```\nrun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-coa/llama_index/agent/coa/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>244<br>245<br>246<br>247<br>``` | ```<br>@trace_method(\"run_step\")<br>def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step.\"\"\"<br>    return asyncio.run(self.arun_step(step=step, task=task, **kwargs))<br>``` |\n\n### arun\\_step`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/coa/\\#llama_index.agent.coa.CoAAgentWorker.arun_step \"Permanent link\")\n\n```\narun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async).\n\nSource code in `llama-index-integrations/agent/llama-index-agent-coa/llama_index/agent/coa/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>249<br>250<br>251<br>252<br>253<br>254<br>``` | ```<br>@trace_method(\"run_step\")<br>async def arun_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async).\"\"\"<br>    return await self._arun_step(step, task)<br>``` |\n\n### stream\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/coa/\\#llama_index.agent.coa.CoAAgentWorker.stream_step \"Permanent link\")\n\n```\nstream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (stream).\n\nSource code in `llama-index-integrations/agent/llama-index-agent-coa/llama_index/agent/coa/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>256<br>257<br>258<br>259<br>260<br>``` | ```<br>@trace_method(\"run_step\")<br>def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step (stream).\"\"\"<br>    # Streaming isn't really possible, because we need the full response to know if we are done<br>    raise NotImplementedError<br>``` |\n\n### astream\\_step`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/coa/\\#llama_index.agent.coa.CoAAgentWorker.astream_step \"Permanent link\")\n\n```\nastream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async stream).\n\nSource code in `llama-index-integrations/agent/llama-index-agent-coa/llama_index/agent/coa/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>``` | ```<br>@trace_method(\"run_step\")<br>async def astream_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async stream).\"\"\"<br>    # Streaming isn't really possible, because we need the full response to know if we are done<br>    raise NotImplementedError<br>``` |\n\n### finalize\\_task [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/coa/\\#llama_index.agent.coa.CoAAgentWorker.finalize_task \"Permanent link\")\n\n```\nfinalize_task(task: Task, **kwargs: Any) -> None\n\n```\n\nFinalize task, after all the steps are completed.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-coa/llama_index/agent/coa/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>270<br>271<br>272<br>273<br>274<br>275<br>``` | ```<br>def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>    \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>    # add new messages to memory<br>    task.memory.put_messages(task.extra_state[\"new_memory\"].get_all())<br>    # reset new memory<br>    task.extra_state[\"new_memory\"].reset()<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Coa - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/agent/coa/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/agent/lats/#llama_index.agent.lats.LATSAgentWorker)\n\n# Lats\n\n## LATSAgentWorker [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/lats/\\#llama_index.agent.lats.LATSAgentWorker \"Permanent link\")\n\nBases: `CustomSimpleAgentWorker`\n\nAgent worker that performs a step of Language Agent Tree Search.\n\nSource paper: https://arxiv.org/pdf/2310.04406v2.pdf.\n\nContinues iterating until there's no errors / task is done.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-lats/llama_index/agent/lats/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>``` | ```<br>class LATSAgentWorker(CustomSimpleAgentWorker):<br>    \"\"\"Agent worker that performs a step of Language Agent Tree Search.<br>    Source paper: https://arxiv.org/pdf/2310.04406v2.pdf.<br>    Continues iterating until there's no errors / task is done.<br>    \"\"\"<br>    num_expansions: int = Field(default=2, description=\"Number of expansions to do.\")<br>    reflection_prompt: PromptTemplate = Field(..., description=\"Reflection prompt.\")<br>    candiate_expansion_prompt: PromptTemplate = Field(<br>        ..., description=\"Candidate expansion prompt.\"<br>    )<br>    max_rollouts: int = Field(<br>        default=5,<br>        description=(<br>            \"Max rollouts. By default, -1 means that we keep going until the first solution is found.\"<br>        ),<br>    )<br>    chat_formatter: ReActChatFormatter = Field(<br>        default_factory=ReActChatFormatter, description=\"Chat formatter.\"<br>    )<br>    def __init__(<br>        self,<br>        tools: List[BaseTool],<br>        llm: Optional[LLM] = None,<br>        num_expansions: int = 2,<br>        max_rollouts: int = 5,<br>        reflection_prompt: Optional[PromptTemplate] = None,<br>        candiate_expansion_prompt: Optional[PromptTemplate] = None,<br>        **kwargs: Any,<br>    ) -> None:<br>        \"\"\"Init params.\"\"\"<br>        # validate that all tools are query engine tools<br>        llm = llm or Settings.llm<br>        super().__init__(<br>            tools=tools,<br>            llm=llm,<br>            num_expansions=num_expansions,<br>            max_rollouts=max_rollouts,<br>            reflection_prompt=reflection_prompt or DEFAULT_REFLECTION_PROMPT,<br>            candiate_expansion_prompt=candiate_expansion_prompt<br>            or DEFAULT_CANDIDATES_PROMPT,<br>            **kwargs,<br>        )<br>    def _initialize_state(self, task: Task, **kwargs: Any) -> Dict[str, Any]:<br>        \"\"\"Initialize state.\"\"\"<br>        # initialize root node<br>        root_node = SearchNode(<br>            current_reasoning=[ObservationReasoningStep(observation=task.input)],<br>            evaluation=Evaluation(score=1),  # evaluation for root node is blank<br>        )<br>        return {\"count\": 0, \"solution_queue\": [], \"root_node\": root_node}<br>    async def _arun_candidate(<br>        self,<br>        node: SearchNode,<br>        task: Task,<br>    ) -> List[BaseReasoningStep]:<br>        \"\"\"Generate candidate for a given node.<br>        Generically we sample the action space to generate new candidate nodes.<br>        Practically since we're using a ReAct powered agent, this means<br>        using generating a ReAct trajectory, running a tool.<br>        \"\"\"<br>        output_parser = ReActOutputParser()<br>        # format react prompt<br>        formatted_prompt = self.chat_formatter.format(<br>            self.tools,<br>            chat_history=task.memory.get(),<br>            current_reasoning=node.current_reasoning,<br>        )<br>        # run LLM<br>        response = await self.llm.achat(formatted_prompt)<br>        # parse output into reasoning step<br>        try:<br>            reasoning_step = output_parser.parse(response.message.content)<br>        except ValueError as e:<br>            reasoning_step = ResponseReasoningStep(<br>                thought=response.message.content,<br>                response=f\"Encountered an error parsing: {e!s}\",<br>            )<br>        # get response or run tool<br>        if reasoning_step.is_done:<br>            reasoning_step = cast(ResponseReasoningStep, reasoning_step)<br>            current_reasoning = [reasoning_step]<br>        else:<br>            reasoning_step = cast(ActionReasoningStep, reasoning_step)<br>            tool_selection = ToolSelection(<br>                tool_id=reasoning_step.action,<br>                tool_name=reasoning_step.action,<br>                tool_kwargs=reasoning_step.action_input,<br>            )<br>            try:<br>                tool_output = await acall_tool_with_selection(<br>                    tool_selection, self.tools, verbose=self.verbose<br>                )<br>            except Exception as e:<br>                tool_output = f\"Encountered error: {e!s}\"<br>            observation_step = ObservationReasoningStep(observation=str(tool_output))<br>            current_reasoning = [reasoning_step, observation_step]<br>        return current_reasoning<br>    async def _aevaluate(<br>        self,<br>        cur_node: SearchNode,<br>        current_reasoning: List[BaseReasoningStep],<br>        input: str,<br>    ) -> float:<br>        \"\"\"Evaluate.\"\"\"<br>        all_reasoning = cur_node.current_reasoning + current_reasoning<br>        history_str = \"\\n\".join([s.get_content() for s in all_reasoning])<br>        evaluation = await self.llm.astructured_predict(<br>            Evaluation,<br>            prompt=self.reflection_prompt,<br>            query=input,<br>            conversation_history=history_str,<br>        )<br>        if self.verbose:<br>            print_text(<br>                f\"> Evaluation for input {input}\\n: {evaluation}\\n\\n\", color=\"pink\"<br>            )<br>        return evaluation<br>    async def _get_next_candidates(<br>        self,<br>        cur_node: SearchNode,<br>        input: str,<br>    ) -> List[str]:<br>        \"\"\"Get next candidates.\"\"\"<br>        # get candidates<br>        history_str = \"\\n\".join([s.get_content() for s in cur_node.current_reasoning])<br>        candidates = await self.llm.astructured_predict(<br>            Candidates,<br>            prompt=self.candiate_expansion_prompt,<br>            query=input,<br>            conversation_history=history_str,<br>            num_candidates=self.num_expansions,<br>        )<br>        candidate_strs = candidates.candidates[: self.num_expansions]<br>        if self.verbose:<br>            print_text(f\"> Got candidates: {candidate_strs}\\n\", color=\"yellow\")<br>        # ensure we have the right number of candidates<br>        if len(candidate_strs) < self.num_expansions:<br>            return (candidate_strs * self.num_expansions)[: self.num_expansions]<br>        else:<br>            return candidate_strs[: self.num_expansions]<br>    def _update_state(<br>        self,<br>        node: SearchNode,<br>        current_reasoning: List[BaseReasoningStep],<br>        evaluation: Evaluation,<br>    ) -> SearchNode:<br>        \"\"\"Update state.\"\"\"<br>        # create child node<br>        new_node = SearchNode(<br>            current_reasoning=node.current_reasoning + current_reasoning,<br>            parent=node,<br>            children=[],<br>            evaluation=evaluation,<br>        )<br>        node.children.append(new_node)<br>        # backpropagate the reward<br>        new_node.backpropagate(evaluation.score)<br>        return new_node<br>    def _run_step(<br>        self, state: Dict[str, Any], task: Task, input: Optional[str] = None<br>    ) -> Tuple[AgentChatResponse, bool]:<br>        \"\"\"Run step.<br>        Returns:<br>            Tuple of (agent_response, is_done)<br>        \"\"\"<br>        return asyncio.run(self._arun_step(state, task, input))<br>    async def _arun_step(<br>        self, state: Dict[str, Any], task: Task, input: Optional[str] = None<br>    ) -> Tuple[AgentChatResponse, bool]:<br>        \"\"\"Run step.<br>        Returns:<br>            Tuple of (agent_response, is_done)<br>        \"\"\"<br>        root_node = state[\"root_node\"]<br>        cur_node = root_node.get_best_leaf()<br>        if self.verbose:<br>            print_text(<br>                f\"> Selecting node to expand: {cur_node.answer}\\n\", color=\"green\"<br>            )<br>        # expand the given node, generate n candidate nodes<br>        new_candidates = await self._get_next_candidates(<br>            cur_node,<br>            task.input,<br>        )<br>        new_nodes = []<br>        for candidate in new_candidates:<br>            new_nodes.append(<br>                self._update_state(<br>                    cur_node,<br>                    [ObservationReasoningStep(observation=candidate)],<br>                    Evaluation(score=1),  # evaluation for candidate node is blank<br>                )<br>            )<br>        # expand the given node, generate n candidates<br>        # for each candidate, run tool, get response<br>        solution_queue: List[SearchNode] = state[\"solution_queue\"]<br>        # first, generate the candidates<br>        candidate_jobs = [<br>            self._arun_candidate(new_node, task) for new_node in new_nodes<br>        ]<br>        all_new_reasoning_steps = await asyncio.gather(*candidate_jobs)<br>        if self.verbose:<br>            for new_reasoning_steps in all_new_reasoning_steps:<br>                out_txt = \"\\n\".join([s.get_content() for s in new_reasoning_steps])<br>                print_text(f\"> Generated new reasoning step: {out_txt}\\n\", color=\"blue\")<br>        # then, evaluate the candidates<br>        eval_jobs = [<br>            self._aevaluate(new_node, new_reasoning_steps, task.input)<br>            for new_node, new_reasoning_steps in zip(new_nodes, all_new_reasoning_steps)<br>        ]<br>        evaluations = await asyncio.gather(*eval_jobs)<br>        # then, update the state<br>        for new_reasoning_steps, cur_new_node, evaluation in zip(<br>            all_new_reasoning_steps, new_nodes, evaluations<br>        ):<br>            new_node = self._update_state(cur_new_node, new_reasoning_steps, evaluation)<br>            if new_node.is_done:<br>                if self.verbose:<br>                    print_text(<br>                        f\"> Found solution node: {new_node.answer}\\n\", color=\"cyan\"<br>                    )<br>                solution_queue.append(new_node)<br>        # check if done<br>        state[\"count\"] += 1<br>        if self.max_rollouts == -1 and solution_queue:<br>            is_done = True<br>        elif self.max_rollouts > 0 and state[\"count\"] >= self.max_rollouts:<br>            is_done = True<br>        else:<br>            is_done = False<br>        # determine response<br>        if solution_queue:<br>            best_solution_node = max(solution_queue, key=lambda x: x.score)<br>            response = best_solution_node.answer<br>        else:<br>            response = \"I am still thinking.\"<br>        if self.verbose:<br>            print_text(f\"> Got final response: {response!s}\\n\", color=\"green\")<br>        # return response<br>        return AgentChatResponse(response=str(response)), is_done<br>    def _finalize_task(self, state: Dict[str, Any], **kwargs) -> None:<br>        \"\"\"Finalize task.\"\"\"<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Lats - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/agent/lats/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/condense_question/#llama_index.core.chat_engine.CondenseQuestionChatEngine)\n\n# Condense question\n\n## CondenseQuestionChatEngine [\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/condense_question/\\#llama_index.core.chat_engine.CondenseQuestionChatEngine \"Permanent link\")\n\nBases: `BaseChatEngine`\n\nCondense Question Chat Engine.\n\nFirst generate a standalone question from conversation context and last message,\nthen query the query engine for a response.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/condense_question.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>``` | ```<br>class CondenseQuestionChatEngine(BaseChatEngine):<br>    \"\"\"<br>    Condense Question Chat Engine.<br>    First generate a standalone question from conversation context and last message,<br>    then query the query engine for a response.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        query_engine: BaseQueryEngine,<br>        condense_question_prompt: BasePromptTemplate,<br>        memory: BaseMemory,<br>        llm: LLM,<br>        verbose: bool = False,<br>        callback_manager: Optional[CallbackManager] = None,<br>    ) -> None:<br>        self._query_engine = query_engine<br>        self._condense_question_prompt = condense_question_prompt<br>        self._memory = memory<br>        self._llm = llm<br>        self._verbose = verbose<br>        self.callback_manager = callback_manager or CallbackManager([])<br>    @classmethod<br>    def from_defaults(<br>        cls,<br>        query_engine: BaseQueryEngine,<br>        condense_question_prompt: Optional[BasePromptTemplate] = None,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        memory: Optional[BaseMemory] = None,<br>        memory_cls: Type[BaseMemory] = ChatMemoryBuffer,<br>        verbose: bool = False,<br>        system_prompt: Optional[str] = None,<br>        prefix_messages: Optional[List[ChatMessage]] = None,<br>        llm: Optional[LLM] = None,<br>        **kwargs: Any,<br>    ) -> \"CondenseQuestionChatEngine\":<br>        \"\"\"Initialize a CondenseQuestionChatEngine from default parameters.\"\"\"<br>        condense_question_prompt = condense_question_prompt or DEFAULT_PROMPT<br>        llm = llm or Settings.llm<br>        chat_history = chat_history or []<br>        memory = memory or memory_cls.from_defaults(chat_history=chat_history, llm=llm)<br>        if system_prompt is not None:<br>            raise NotImplementedError(<br>                \"system_prompt is not supported for CondenseQuestionChatEngine.\"<br>            )<br>        if prefix_messages is not None:<br>            raise NotImplementedError(<br>                \"prefix_messages is not supported for CondenseQuestionChatEngine.\"<br>            )<br>        return cls(<br>            query_engine,<br>            condense_question_prompt,<br>            memory,<br>            llm,<br>            verbose=verbose,<br>            callback_manager=Settings.callback_manager,<br>        )<br>    def _condense_question(<br>        self, chat_history: List[ChatMessage], last_message: str<br>    ) -> str:<br>        \"\"\"<br>        Generate standalone question from conversation context and last message.<br>        \"\"\"<br>        if not chat_history:<br>            # Keep the question as is if there's no conversation context.<br>            return last_message<br>        chat_history_str = messages_to_history_str(chat_history)<br>        logger.debug(chat_history_str)<br>        return self._llm.predict(<br>            self._condense_question_prompt,<br>            question=last_message,<br>            chat_history=chat_history_str,<br>        )<br>    async def _acondense_question(<br>        self, chat_history: List[ChatMessage], last_message: str<br>    ) -> str:<br>        \"\"\"<br>        Generate standalone question from conversation context and last message.<br>        \"\"\"<br>        if not chat_history:<br>            # Keep the question as is if there's no conversation context.<br>            return last_message<br>        chat_history_str = messages_to_history_str(chat_history)<br>        logger.debug(chat_history_str)<br>        return await self._llm.apredict(<br>            self._condense_question_prompt,<br>            question=last_message,<br>            chat_history=chat_history_str,<br>        )<br>    def _get_tool_output_from_response(<br>        self, query: str, response: RESPONSE_TYPE<br>    ) -> ToolOutput:<br>        if isinstance(response, (StreamingResponse, AsyncStreamingResponse)):<br>            return ToolOutput(<br>                content=\"\",<br>                tool_name=\"query_engine\",<br>                raw_input={\"query\": query},<br>                raw_output=response,<br>            )<br>        else:<br>            return ToolOutput(<br>                content=str(response),<br>                tool_name=\"query_engine\",<br>                raw_input={\"query\": query},<br>                raw_output=response,<br>            )<br>    @trace_method(\"chat\")<br>    def chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> AgentChatResponse:<br>        chat_history = chat_history or self._memory.get(input=message)<br>        # Generate standalone question from conversation context and last message<br>        condensed_question = self._condense_question(chat_history, message)<br>        log_str = f\"Querying with: {condensed_question}\"<br>        logger.info(log_str)<br>        if self._verbose:<br>            print(log_str)<br>        # TODO: right now, query engine uses class attribute to configure streaming,<br>        #       we are moving towards separate streaming and non-streaming methods.<br>        #       In the meanwhile, use this hack to toggle streaming.<br>        from llama_index.core.query_engine.retriever_query_engine import (<br>            RetrieverQueryEngine,<br>        )<br>        if isinstance(self._query_engine, RetrieverQueryEngine):<br>            is_streaming = self._query_engine._response_synthesizer._streaming<br>            self._query_engine._response_synthesizer._streaming = False<br>        # Query with standalone question<br>        query_response = self._query_engine.query(condensed_question)<br>        # NOTE: reset streaming flag<br>        if isinstance(self._query_engine, RetrieverQueryEngine):<br>            self._query_engine._response_synthesizer._streaming = is_streaming<br>        tool_output = self._get_tool_output_from_response(<br>            condensed_question, query_response<br>        )<br>        # Record response<br>        self._memory.put(ChatMessage(role=MessageRole.USER, content=message))<br>        self._memory.put(<br>            ChatMessage(role=MessageRole.ASSISTANT, content=str(query_response))<br>        )<br>        return AgentChatResponse(response=str(query_response), sources=[tool_output])<br>    @trace_method(\"chat\")<br>    def stream_chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> StreamingAgentChatResponse:<br>        chat_history = chat_history or self._memory.get(input=message)<br>        # Generate standalone question from conversation context and last message<br>        condensed_question = self._condense_question(chat_history, message)<br>        log_str = f\"Querying with: {condensed_question}\"<br>        logger.info(log_str)<br>        if self._verbose:<br>            print(log_str)<br>        # TODO: right now, query engine uses class attribute to configure streaming,<br>        #       we are moving towards separate streaming and non-streaming methods.<br>        #       In the meanwhile, use this hack to toggle streaming.<br>        from llama_index.core.query_engine.retriever_query_engine import (<br>            RetrieverQueryEngine,<br>        )<br>        if isinstance(self._query_engine, RetrieverQueryEngine):<br>            is_streaming = self._query_engine._response_synthesizer._streaming<br>            self._query_engine._response_synthesizer._streaming = True<br>        # Query with standalone question<br>        query_response = self._query_engine.query(condensed_question)<br>        # NOTE: reset streaming flag<br>        if isinstance(self._query_engine, RetrieverQueryEngine):<br>            self._query_engine._response_synthesizer._streaming = is_streaming<br>        tool_output = self._get_tool_output_from_response(<br>            condensed_question, query_response<br>        )<br>        # Record response<br>        if (<br>            isinstance(query_response, StreamingResponse)<br>            and query_response.response_gen is not None<br>        ):<br>            # override the generator to include writing to chat history<br>            self._memory.put(ChatMessage(role=MessageRole.USER, content=message))<br>            response = StreamingAgentChatResponse(<br>                chat_stream=response_gen_from_query_engine(query_response.response_gen),<br>                sources=[tool_output],<br>            )<br>            thread = Thread(<br>                target=response.write_response_to_history,<br>                args=(self._memory,),<br>            )<br>            thread.start()<br>        else:<br>            raise ValueError(\"Streaming is not enabled. Please use chat() instead.\")<br>        return response<br>    @trace_method(\"chat\")<br>    async def achat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> AgentChatResponse:<br>        chat_history = chat_history or self._memory.get(input=message)<br>        # Generate standalone question from conversation context and last message<br>        condensed_question = await self._acondense_question(chat_history, message)<br>        log_str = f\"Querying with: {condensed_question}\"<br>        logger.info(log_str)<br>        if self._verbose:<br>            print(log_str)<br>        # TODO: right now, query engine uses class attribute to configure streaming,<br>        #       we are moving towards separate streaming and non-streaming methods.<br>        #       In the meanwhile, use this hack to toggle streaming.<br>        from llama_index.core.query_engine.retriever_query_engine import (<br>            RetrieverQueryEngine,<br>        )<br>        if isinstance(self._query_engine, RetrieverQueryEngine):<br>            is_streaming = self._query_engine._response_synthesizer._streaming<br>            self._query_engine._response_synthesizer._streaming = False<br>        # Query with standalone question<br>        query_response = await self._query_engine.aquery(condensed_question)<br>        # NOTE: reset streaming flag<br>        if isinstance(self._query_engine, RetrieverQueryEngine):<br>            self._query_engine._response_synthesizer._streaming = is_streaming<br>        tool_output = self._get_tool_output_from_response(<br>            condensed_question, query_response<br>        )<br>        # Record response<br>        self._memory.put(ChatMessage(role=MessageRole.USER, content=message))<br>        self._memory.put(<br>            ChatMessage(role=MessageRole.ASSISTANT, content=str(query_response))<br>        )<br>        return AgentChatResponse(response=str(query_response), sources=[tool_output])<br>    @trace_method(\"chat\")<br>    async def astream_chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> StreamingAgentChatResponse:<br>        chat_history = chat_history or self._memory.get(input=message)<br>        # Generate standalone question from conversation context and last message<br>        condensed_question = await self._acondense_question(chat_history, message)<br>        log_str = f\"Querying with: {condensed_question}\"<br>        logger.info(log_str)<br>        if self._verbose:<br>            print(log_str)<br>        # TODO: right now, query engine uses class attribute to configure streaming,<br>        #       we are moving towards separate streaming and non-streaming methods.<br>        #       In the meanwhile, use this hack to toggle streaming.<br>        from llama_index.core.query_engine.retriever_query_engine import (<br>            RetrieverQueryEngine,<br>        )<br>        if isinstance(self._query_engine, RetrieverQueryEngine):<br>            is_streaming = self._query_engine._response_synthesizer._streaming<br>            self._query_engine._response_synthesizer._streaming = True<br>        # Query with standalone question<br>        query_response = await self._query_engine.aquery(condensed_question)<br>        # NOTE: reset streaming flag<br>        if isinstance(self._query_engine, RetrieverQueryEngine):<br>            self._query_engine._response_synthesizer._streaming = is_streaming<br>        tool_output = self._get_tool_output_from_response(<br>            condensed_question, query_response<br>        )<br>        # Record response<br>        if isinstance(query_response, AsyncStreamingResponse):<br>            # override the generator to include writing to chat history<br>            # TODO: query engine does not support async generator yet<br>            self._memory.put(ChatMessage(role=MessageRole.USER, content=message))<br>            response = StreamingAgentChatResponse(<br>                achat_stream=aresponse_gen_from_query_engine(<br>                    query_response.async_response_gen()<br>                ),<br>                sources=[tool_output],<br>            )<br>            asyncio.create_task(response.awrite_response_to_history(self._memory))<br>        else:<br>            raise ValueError(\"Streaming is not enabled. Please use achat() instead.\")<br>        return response<br>    def reset(self) -> None:<br>        # Clear chat history<br>        self._memory.reset()<br>    @property<br>    def chat_history(self) -> List[ChatMessage]:<br>        \"\"\"Get chat history.\"\"\"<br>        return self._memory.get_all()<br>``` |\n\n### chat\\_history`property`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/condense_question/\\#llama_index.core.chat_engine.CondenseQuestionChatEngine.chat_history \"Permanent link\")\n\n```\nchat_history: List[ChatMessage]\n\n```\n\nGet chat history.\n\n### from\\_defaults`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/condense_question/\\#llama_index.core.chat_engine.CondenseQuestionChatEngine.from_defaults \"Permanent link\")\n\n```\nfrom_defaults(query_engine: BaseQueryEngine, condense_question_prompt: Optional[BasePromptTemplate] = None, chat_history: Optional[List[ChatMessage]] = None, memory: Optional[BaseMemory] = None, memory_cls: Type[BaseMemory] = ChatMemoryBuffer, verbose: bool = False, system_prompt: Optional[str] = None, prefix_messages: Optional[List[ChatMessage]] = None, llm: Optional[LLM] = None, **kwargs: Any) -> CondenseQuestionChatEngine\n\n```\n\nInitialize a CondenseQuestionChatEngine from default parameters.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/condense_question.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>``` | ```<br>@classmethod<br>def from_defaults(<br>    cls,<br>    query_engine: BaseQueryEngine,<br>    condense_question_prompt: Optional[BasePromptTemplate] = None,<br>    chat_history: Optional[List[ChatMessage]] = None,<br>    memory: Optional[BaseMemory] = None,<br>    memory_cls: Type[BaseMemory] = ChatMemoryBuffer,<br>    verbose: bool = False,<br>    system_prompt: Optional[str] = None,<br>    prefix_messages: Optional[List[ChatMessage]] = None,<br>    llm: Optional[LLM] = None,<br>    **kwargs: Any,<br>) -> \"CondenseQuestionChatEngine\":<br>    \"\"\"Initialize a CondenseQuestionChatEngine from default parameters.\"\"\"<br>    condense_question_prompt = condense_question_prompt or DEFAULT_PROMPT<br>    llm = llm or Settings.llm<br>    chat_history = chat_history or []<br>    memory = memory or memory_cls.from_defaults(chat_history=chat_history, llm=llm)<br>    if system_prompt is not None:<br>        raise NotImplementedError(<br>            \"system_prompt is not supported for CondenseQuestionChatEngine.\"<br>        )<br>    if prefix_messages is not None:<br>        raise NotImplementedError(<br>            \"prefix_messages is not supported for CondenseQuestionChatEngine.\"<br>        )<br>    return cls(<br>        query_engine,<br>        condense_question_prompt,<br>        memory,<br>        llm,<br>        verbose=verbose,<br>        callback_manager=Settings.callback_manager,<br>    )<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Condense question - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/condense_question/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/aim/#llama_index.callbacks.aim.AimCallback)\n\n# Aim\n\n## AimCallback [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/aim/\\#llama_index.callbacks.aim.AimCallback \"Permanent link\")\n\nBases: `BaseCallbackHandler`\n\nAimCallback callback class.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `repo` |  | obj: `str`, optional):<br>Aim repository path or Repo object to which Run object is bound.<br>If skipped, default Repo is used. | `None` |\n| `experiment_name` |  | obj: `str`, optional):<br>Sets Run's `experiment` property. 'default' if not specified.<br>Can be used later to query runs/sequences. | `None` |\n| `system_tracking_interval` |  | obj: `int`, optional):<br>Sets the tracking interval in seconds for system usage<br>metrics (CPU, Memory, etc.). Set to `None` to disable<br>system metrics tracking. | `1` |\n| `log_system_params` |  | obj: `bool`, optional):<br>Enable/Disable logging of system params such as installed packages,<br>git info, environment variables, etc. | `True` |\n| `capture_terminal_logs` |  | obj: `bool`, optional):<br>Enable/Disable terminal stdout logging. | `True` |\n| `event_starts_to_ignore` | `Optional[List[CBEventType]]` | list of event types to ignore when tracking event starts. | `None` |\n| `event_ends_to_ignore` | `Optional[List[CBEventType]]` | list of event types to ignore when tracking event ends. | `None` |\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-aim/llama_index/callbacks/aim/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 13<br> 14<br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>``` | ```<br>class AimCallback(BaseCallbackHandler):<br>    \"\"\"<br>    AimCallback callback class.<br>    Args:<br>        repo (:obj:`str`, optional):<br>            Aim repository path or Repo object to which Run object is bound.<br>            If skipped, default Repo is used.<br>        experiment_name (:obj:`str`, optional):<br>            Sets Run's `experiment` property. 'default' if not specified.<br>            Can be used later to query runs/sequences.<br>        system_tracking_interval (:obj:`int`, optional):<br>            Sets the tracking interval in seconds for system usage<br>            metrics (CPU, Memory, etc.). Set to `None` to disable<br>            system metrics tracking.<br>        log_system_params (:obj:`bool`, optional):<br>            Enable/Disable logging of system params such as installed packages,<br>            git info, environment variables, etc.<br>        capture_terminal_logs (:obj:`bool`, optional):<br>            Enable/Disable terminal stdout logging.<br>        event_starts_to_ignore (Optional[List[CBEventType]]):<br>            list of event types to ignore when tracking event starts.<br>        event_ends_to_ignore (Optional[List[CBEventType]]):<br>            list of event types to ignore when tracking event ends.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        repo: Optional[str] = None,<br>        experiment_name: Optional[str] = None,<br>        system_tracking_interval: Optional[int] = 1,<br>        log_system_params: Optional[bool] = True,<br>        capture_terminal_logs: Optional[bool] = True,<br>        event_starts_to_ignore: Optional[List[CBEventType]] = None,<br>        event_ends_to_ignore: Optional[List[CBEventType]] = None,<br>        run_params: Optional[Dict[str, Any]] = None,<br>    ) -> None:<br>        if Run is None:<br>            raise ModuleNotFoundError(<br>                \"Please install aim to use the AimCallback: 'pip install aim'\"<br>            )<br>        event_starts_to_ignore = (<br>            event_starts_to_ignore if event_starts_to_ignore else []<br>        )<br>        event_ends_to_ignore = event_ends_to_ignore if event_ends_to_ignore else []<br>        super().__init__(<br>            event_starts_to_ignore=event_starts_to_ignore,<br>            event_ends_to_ignore=event_ends_to_ignore,<br>        )<br>        self.repo = repo<br>        self.experiment_name = experiment_name<br>        self.system_tracking_interval = system_tracking_interval<br>        self.log_system_params = log_system_params<br>        self.capture_terminal_logs = capture_terminal_logs<br>        self._run: Optional[Any] = None<br>        self._run_hash = None<br>        self._llm_response_step = 0<br>        self.setup(run_params)<br>    def on_event_start(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        parent_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> str:<br>        \"\"\"<br>        Args:<br>            event_type (CBEventType): event type to store.<br>            payload (Optional[Dict[str, Any]]): payload to store.<br>            event_id (str): event id to store.<br>            parent_id (str): parent event id.<br>        \"\"\"<br>        return \"\"<br>    def on_event_end(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> None:<br>        \"\"\"<br>        Args:<br>            event_type (CBEventType): event type to store.<br>            payload (Optional[Dict[str, Any]]): payload to store.<br>            event_id (str): event id to store.<br>        \"\"\"<br>        if not self._run:<br>            raise ValueError(\"AimCallback failed to init properly.\")<br>        if event_type is CBEventType.LLM and payload:<br>            if EventPayload.PROMPT in payload:<br>                llm_input = str(payload[EventPayload.PROMPT])<br>                llm_output = str(payload[EventPayload.COMPLETION])<br>            else:<br>                message = payload.get(EventPayload.MESSAGES, [])<br>                llm_input = \"\\n\".join([str(x) for x in message])<br>                llm_output = str(payload[EventPayload.RESPONSE])<br>            self._run.track(<br>                Text(llm_input),<br>                name=\"prompt\",<br>                step=self._llm_response_step,<br>                context={\"event_id\": event_id},<br>            )<br>            self._run.track(<br>                Text(llm_output),<br>                name=\"response\",<br>                step=self._llm_response_step,<br>                context={\"event_id\": event_id},<br>            )<br>            self._llm_response_step += 1<br>        elif event_type is CBEventType.CHUNKING and payload:<br>            for chunk_id, chunk in enumerate(payload[EventPayload.CHUNKS]):<br>                self._run.track(<br>                    Text(chunk),<br>                    name=\"chunk\",<br>                    step=self._llm_response_step,<br>                    context={\"chunk_id\": chunk_id, \"event_id\": event_id},<br>                )<br>    @property<br>    def experiment(self) -> Run:<br>        if not self._run:<br>            self.setup()<br>        return self._run<br>    def setup(self, args: Optional[Dict[str, Any]] = None) -> None:<br>        if not self._run:<br>            if self._run_hash:<br>                self._run = Run(<br>                    self._run_hash,<br>                    repo=self.repo,<br>                    system_tracking_interval=self.system_tracking_interval,<br>                    log_system_params=self.log_system_params,<br>                    capture_terminal_logs=self.capture_terminal_logs,<br>                )<br>            else:<br>                self._run = Run(<br>                    repo=self.repo,<br>                    experiment=self.experiment_name,<br>                    system_tracking_interval=self.system_tracking_interval,<br>                    log_system_params=self.log_system_params,<br>                    capture_terminal_logs=self.capture_terminal_logs,<br>                )<br>                self._run_hash = self._run.hash<br>        # Log config parameters<br>        if args:<br>            try:<br>                for key in args:<br>                    self._run.set(key, args[key], strict=False)<br>            except Exception as e:<br>                logger.warning(f\"Aim could not log config parameters -> {e}\")<br>    def __del__(self) -> None:<br>        if self._run and self._run.active:<br>            self._run.close()<br>    def start_trace(self, trace_id: Optional[str] = None) -> None:<br>        pass<br>    def end_trace(<br>        self,<br>        trace_id: Optional[str] = None,<br>        trace_map: Optional[Dict[str, List[str]]] = None,<br>    ) -> None:<br>        pass<br>``` |\n\n### on\\_event\\_start [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/aim/\\#llama_index.callbacks.aim.AimCallback.on_event_start \"Permanent link\")\n\n```\non_event_start(event_type: CBEventType, payload: Optional[Dict[str, Any]] = None, event_id: str = '', parent_id: str = '', **kwargs: Any) -> str\n\n```\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `event_type` | `CBEventType` | event type to store. | _required_ |\n| `payload` | `Optional[Dict[str, Any]]` | payload to store. | `None` |\n| `event_id` | `str` | event id to store. | `''` |\n| `parent_id` | `str` | parent event id. | `''` |\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-aim/llama_index/callbacks/aim/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>89<br>90<br>91<br>``` | ```<br>def on_event_start(<br>    self,<br>    event_type: CBEventType,<br>    payload: Optional[Dict[str, Any]] = None,<br>    event_id: str = \"\",<br>    parent_id: str = \"\",<br>    **kwargs: Any,<br>) -> str:<br>    \"\"\"<br>    Args:<br>        event_type (CBEventType): event type to store.<br>        payload (Optional[Dict[str, Any]]): payload to store.<br>        event_id (str): event id to store.<br>        parent_id (str): parent event id.<br>    \"\"\"<br>    return \"\"<br>``` |\n\n### on\\_event\\_end [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/aim/\\#llama_index.callbacks.aim.AimCallback.on_event_end \"Permanent link\")\n\n```\non_event_end(event_type: CBEventType, payload: Optional[Dict[str, Any]] = None, event_id: str = '', **kwargs: Any) -> None\n\n```\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `event_type` | `CBEventType` | event type to store. | _required_ |\n| `payload` | `Optional[Dict[str, Any]]` | payload to store. | `None` |\n| `event_id` | `str` | event id to store. | `''` |\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-aim/llama_index/callbacks/aim/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>``` | ```<br>def on_event_end(<br>    self,<br>    event_type: CBEventType,<br>    payload: Optional[Dict[str, Any]] = None,<br>    event_id: str = \"\",<br>    **kwargs: Any,<br>) -> None:<br>    \"\"\"<br>    Args:<br>        event_type (CBEventType): event type to store.<br>        payload (Optional[Dict[str, Any]]): payload to store.<br>        event_id (str): event id to store.<br>    \"\"\"<br>    if not self._run:<br>        raise ValueError(\"AimCallback failed to init properly.\")<br>    if event_type is CBEventType.LLM and payload:<br>        if EventPayload.PROMPT in payload:<br>            llm_input = str(payload[EventPayload.PROMPT])<br>            llm_output = str(payload[EventPayload.COMPLETION])<br>        else:<br>            message = payload.get(EventPayload.MESSAGES, [])<br>            llm_input = \"\\n\".join([str(x) for x in message])<br>            llm_output = str(payload[EventPayload.RESPONSE])<br>        self._run.track(<br>            Text(llm_input),<br>            name=\"prompt\",<br>            step=self._llm_response_step,<br>            context={\"event_id\": event_id},<br>        )<br>        self._run.track(<br>            Text(llm_output),<br>            name=\"response\",<br>            step=self._llm_response_step,<br>            context={\"event_id\": event_id},<br>        )<br>        self._llm_response_step += 1<br>    elif event_type is CBEventType.CHUNKING and payload:<br>        for chunk_id, chunk in enumerate(payload[EventPayload.CHUNKS]):<br>            self._run.track(<br>                Text(chunk),<br>                name=\"chunk\",<br>                step=self._llm_response_step,<br>                context={\"chunk_id\": chunk_id, \"event_id\": event_id},<br>            )<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Aim - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/aim/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/#core-callback-classes)\n\n# Core Callback Classes [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#core-callback-classes \"Permanent link\")\n\n## CallbackManager [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.base.CallbackManager \"Permanent link\")\n\nBases: `BaseCallbackHandler`, `ABC`\n\nCallback manager that handles callbacks for events within LlamaIndex.\n\nThe callback manager provides a way to call handlers on event starts/ends.\n\nAdditionally, the callback manager traces the current stack of events.\nIt does this by using a few key attributes.\n\\- trace\\_stack - The current stack of events that have not ended yet.\nWhen an event ends, it's removed from the stack.\nSince this is a contextvar, it is unique to each\nthread/task.\n\\- trace\\_map - A mapping of event ids to their children events.\nOn the start of events, the bottom of the trace stack\nis used as the current parent event for the trace map.\n\\- trace\\_id - A simple name for the current trace, usually denoting the\nentrypoint (query, index\\_construction, insert, etc.)\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `handlers` | `List[BaseCallbackHandler]` | list of handlers to use. | `None` |\n\nUsage\n\nwith callback\\_manager.event(CBEventType.QUERY) as event:\nevent.on\\_start(payload={key, val})\n...\nevent.on\\_end(payload={key, val})\n\nSource code in `llama-index-core/llama_index/core/callbacks/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>``` | ```<br>class CallbackManager(BaseCallbackHandler, ABC):<br>    \"\"\"<br>    Callback manager that handles callbacks for events within LlamaIndex.<br>    The callback manager provides a way to call handlers on event starts/ends.<br>    Additionally, the callback manager traces the current stack of events.<br>    It does this by using a few key attributes.<br>    - trace_stack - The current stack of events that have not ended yet.<br>                    When an event ends, it's removed from the stack.<br>                    Since this is a contextvar, it is unique to each<br>                    thread/task.<br>    - trace_map - A mapping of event ids to their children events.<br>                  On the start of events, the bottom of the trace stack<br>                  is used as the current parent event for the trace map.<br>    - trace_id - A simple name for the current trace, usually denoting the<br>                 entrypoint (query, index_construction, insert, etc.)<br>    Args:<br>        handlers (List[BaseCallbackHandler]): list of handlers to use.<br>    Usage:<br>        with callback_manager.event(CBEventType.QUERY) as event:<br>            event.on_start(payload={key, val})<br>            ...<br>            event.on_end(payload={key, val})<br>    \"\"\"<br>    def __init__(self, handlers: Optional[List[BaseCallbackHandler]] = None):<br>        \"\"\"Initialize the manager with a list of handlers.\"\"\"<br>        from llama_index.core import global_handler<br>        handlers = handlers or []<br>        # add eval handlers based on global defaults<br>        if global_handler is not None:<br>            new_handler = global_handler<br>            # go through existing handlers, check if any are same type as new handler<br>            # if so, error<br>            for existing_handler in handlers:<br>                if isinstance(existing_handler, type(new_handler)):<br>                    raise ValueError(<br>                        \"Cannot add two handlers of the same type \"<br>                        f\"{type(new_handler)} to the callback manager.\"<br>                    )<br>            handlers.append(new_handler)<br>        # if we passed in no handlers, use the global default<br>        if len(handlers) == 0:<br>            from llama_index.core.settings import Settings<br>            # hidden var access to prevent recursion in getter<br>            cb_manager = Settings._callback_manager<br>            if cb_manager is not None:<br>                handlers = cb_manager.handlers<br>        self.handlers: List[BaseCallbackHandler] = handlers<br>        self._trace_map: Dict[str, List[str]] = defaultdict(list)<br>    def on_event_start(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: Optional[str] = None,<br>        parent_id: Optional[str] = None,<br>        **kwargs: Any,<br>    ) -> str:<br>        \"\"\"Run handlers when an event starts and return id of event.\"\"\"<br>        event_id = event_id or str(uuid.uuid4())<br>        # if no trace is running, start a default trace<br>        try:<br>            parent_id = parent_id or global_stack_trace.get()[-1]<br>        except IndexError:<br>            self.start_trace(\"llama-index\")<br>            parent_id = global_stack_trace.get()[-1]<br>        parent_id = cast(str, parent_id)<br>        self._trace_map[parent_id].append(event_id)<br>        for handler in self.handlers:<br>            if event_type not in handler.event_starts_to_ignore:<br>                handler.on_event_start(<br>                    event_type,<br>                    payload,<br>                    event_id=event_id,<br>                    parent_id=parent_id,<br>                    **kwargs,<br>                )<br>        if event_type not in LEAF_EVENTS:<br>            # copy the stack trace to prevent conflicts with threads/coroutines<br>            current_trace_stack = global_stack_trace.get().copy()<br>            current_trace_stack.append(event_id)<br>            global_stack_trace.set(current_trace_stack)<br>        return event_id<br>    def on_event_end(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: Optional[str] = None,<br>        **kwargs: Any,<br>    ) -> None:<br>        \"\"\"Run handlers when an event ends.\"\"\"<br>        event_id = event_id or str(uuid.uuid4())<br>        for handler in self.handlers:<br>            if event_type not in handler.event_ends_to_ignore:<br>                handler.on_event_end(event_type, payload, event_id=event_id, **kwargs)<br>        if event_type not in LEAF_EVENTS:<br>            # copy the stack trace to prevent conflicts with threads/coroutines<br>            current_trace_stack = global_stack_trace.get().copy()<br>            current_trace_stack.pop()<br>            global_stack_trace.set(current_trace_stack)<br>    def add_handler(self, handler: BaseCallbackHandler) -> None:<br>        \"\"\"Add a handler to the callback manager.\"\"\"<br>        self.handlers.append(handler)<br>    def remove_handler(self, handler: BaseCallbackHandler) -> None:<br>        \"\"\"Remove a handler from the callback manager.\"\"\"<br>        self.handlers.remove(handler)<br>    def set_handlers(self, handlers: List[BaseCallbackHandler]) -> None:<br>        \"\"\"Set handlers as the only handlers on the callback manager.\"\"\"<br>        self.handlers = handlers<br>    @contextmanager<br>    def event(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: Optional[str] = None,<br>    ) -> Generator[\"EventContext\", None, None]:<br>        \"\"\"Context manager for lanching and shutdown of events.<br>        Handles sending on_evnt_start and on_event_end to handlers for specified event.<br>        Usage:<br>            with callback_manager.event(CBEventType.QUERY, payload={key, val}) as event:<br>                ...<br>                event.on_end(payload={key, val})  # optional<br>        \"\"\"<br>        # create event context wrapper<br>        event = EventContext(self, event_type, event_id=event_id)<br>        event.on_start(payload=payload)<br>        payload = None<br>        try:<br>            yield event<br>        except Exception as e:<br>            # data already logged to trace?<br>            if not hasattr(e, \"event_added\"):<br>                payload = {EventPayload.EXCEPTION: e}<br>                e.event_added = True  # type: ignore<br>                if not event.finished:<br>                    event.on_end(payload=payload)<br>            raise<br>        finally:<br>            # ensure event is ended<br>            if not event.finished:<br>                event.on_end(payload=payload)<br>    @contextmanager<br>    def as_trace(self, trace_id: str) -> Generator[None, None, None]:<br>        \"\"\"Context manager tracer for lanching and shutdown of traces.\"\"\"<br>        self.start_trace(trace_id=trace_id)<br>        try:<br>            yield<br>        except Exception as e:<br>            # event already added to trace?<br>            if not hasattr(e, \"event_added\"):<br>                self.on_event_start(<br>                    CBEventType.EXCEPTION, payload={EventPayload.EXCEPTION: e}<br>                )<br>                e.event_added = True  # type: ignore<br>            raise<br>        finally:<br>            # ensure trace is ended<br>            self.end_trace(trace_id=trace_id)<br>    def start_trace(self, trace_id: Optional[str] = None) -> None:<br>        \"\"\"Run when an overall trace is launched.\"\"\"<br>        current_trace_stack_ids = global_stack_trace_ids.get().copy()<br>        if trace_id is not None:<br>            if len(current_trace_stack_ids) == 0:<br>                self._reset_trace_events()<br>                for handler in self.handlers:<br>                    handler.start_trace(trace_id=trace_id)<br>                current_trace_stack_ids = [trace_id]<br>            else:<br>                current_trace_stack_ids.append(trace_id)<br>        global_stack_trace_ids.set(current_trace_stack_ids)<br>    def end_trace(<br>        self,<br>        trace_id: Optional[str] = None,<br>        trace_map: Optional[Dict[str, List[str]]] = None,<br>    ) -> None:<br>        \"\"\"Run when an overall trace is exited.\"\"\"<br>        current_trace_stack_ids = global_stack_trace_ids.get().copy()<br>        if trace_id is not None and len(current_trace_stack_ids) > 0:<br>            current_trace_stack_ids.pop()<br>            if len(current_trace_stack_ids) == 0:<br>                for handler in self.handlers:<br>                    handler.end_trace(trace_id=trace_id, trace_map=self._trace_map)<br>                current_trace_stack_ids = []<br>        global_stack_trace_ids.set(current_trace_stack_ids)<br>    def _reset_trace_events(self) -> None:<br>        \"\"\"Helper function to reset the current trace.\"\"\"<br>        self._trace_map = defaultdict(list)<br>        global_stack_trace.set([BASE_TRACE_EVENT])<br>    @property<br>    def trace_map(self) -> Dict[str, List[str]]:<br>        return self._trace_map<br>    @classmethod<br>    def __get_pydantic_core_schema__(<br>        cls, source: Type[Any], handler: GetCoreSchemaHandler<br>    ) -> CoreSchema:<br>        return core_schema.any_schema()<br>    @classmethod<br>    def __get_pydantic_json_schema__(<br>        cls, core_schema: CoreSchema, handler: GetJsonSchemaHandler<br>    ) -> Dict[str, Any]:<br>        json_schema = handler(core_schema)<br>        return handler.resolve_ref_schema(json_schema)<br>``` |\n\n### on\\_event\\_start [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.base.CallbackManager.on_event_start \"Permanent link\")\n\n```\non_event_start(event_type: CBEventType, payload: Optional[Dict[str, Any]] = None, event_id: Optional[str] = None, parent_id: Optional[str] = None, **kwargs: Any) -> str\n\n```\n\nRun handlers when an event starts and return id of event.\n\nSource code in `llama-index-core/llama_index/core/callbacks/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>``` | ```<br>def on_event_start(<br>    self,<br>    event_type: CBEventType,<br>    payload: Optional[Dict[str, Any]] = None,<br>    event_id: Optional[str] = None,<br>    parent_id: Optional[str] = None,<br>    **kwargs: Any,<br>) -> str:<br>    \"\"\"Run handlers when an event starts and return id of event.\"\"\"<br>    event_id = event_id or str(uuid.uuid4())<br>    # if no trace is running, start a default trace<br>    try:<br>        parent_id = parent_id or global_stack_trace.get()[-1]<br>    except IndexError:<br>        self.start_trace(\"llama-index\")<br>        parent_id = global_stack_trace.get()[-1]<br>    parent_id = cast(str, parent_id)<br>    self._trace_map[parent_id].append(event_id)<br>    for handler in self.handlers:<br>        if event_type not in handler.event_starts_to_ignore:<br>            handler.on_event_start(<br>                event_type,<br>                payload,<br>                event_id=event_id,<br>                parent_id=parent_id,<br>                **kwargs,<br>            )<br>    if event_type not in LEAF_EVENTS:<br>        # copy the stack trace to prevent conflicts with threads/coroutines<br>        current_trace_stack = global_stack_trace.get().copy()<br>        current_trace_stack.append(event_id)<br>        global_stack_trace.set(current_trace_stack)<br>    return event_id<br>``` |\n\n### on\\_event\\_end [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.base.CallbackManager.on_event_end \"Permanent link\")\n\n```\non_event_end(event_type: CBEventType, payload: Optional[Dict[str, Any]] = None, event_id: Optional[str] = None, **kwargs: Any) -> None\n\n```\n\nRun handlers when an event ends.\n\nSource code in `llama-index-core/llama_index/core/callbacks/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>``` | ```<br>def on_event_end(<br>    self,<br>    event_type: CBEventType,<br>    payload: Optional[Dict[str, Any]] = None,<br>    event_id: Optional[str] = None,<br>    **kwargs: Any,<br>) -> None:<br>    \"\"\"Run handlers when an event ends.\"\"\"<br>    event_id = event_id or str(uuid.uuid4())<br>    for handler in self.handlers:<br>        if event_type not in handler.event_ends_to_ignore:<br>            handler.on_event_end(event_type, payload, event_id=event_id, **kwargs)<br>    if event_type not in LEAF_EVENTS:<br>        # copy the stack trace to prevent conflicts with threads/coroutines<br>        current_trace_stack = global_stack_trace.get().copy()<br>        current_trace_stack.pop()<br>        global_stack_trace.set(current_trace_stack)<br>``` |\n\n### add\\_handler [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.base.CallbackManager.add_handler \"Permanent link\")\n\n```\nadd_handler(handler: BaseCallbackHandler) -> None\n\n```\n\nAdd a handler to the callback manager.\n\nSource code in `llama-index-core/llama_index/core/callbacks/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>144<br>145<br>146<br>``` | ```<br>def add_handler(self, handler: BaseCallbackHandler) -> None:<br>    \"\"\"Add a handler to the callback manager.\"\"\"<br>    self.handlers.append(handler)<br>``` |\n\n### remove\\_handler [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.base.CallbackManager.remove_handler \"Permanent link\")\n\n```\nremove_handler(handler: BaseCallbackHandler) -> None\n\n```\n\nRemove a handler from the callback manager.\n\nSource code in `llama-index-core/llama_index/core/callbacks/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>148<br>149<br>150<br>``` | ```<br>def remove_handler(self, handler: BaseCallbackHandler) -> None:<br>    \"\"\"Remove a handler from the callback manager.\"\"\"<br>    self.handlers.remove(handler)<br>``` |\n\n### set\\_handlers [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.base.CallbackManager.set_handlers \"Permanent link\")\n\n```\nset_handlers(handlers: List[BaseCallbackHandler]) -> None\n\n```\n\nSet handlers as the only handlers on the callback manager.\n\nSource code in `llama-index-core/llama_index/core/callbacks/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>152<br>153<br>154<br>``` | ```<br>def set_handlers(self, handlers: List[BaseCallbackHandler]) -> None:<br>    \"\"\"Set handlers as the only handlers on the callback manager.\"\"\"<br>    self.handlers = handlers<br>``` |\n\n### event [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.base.CallbackManager.event \"Permanent link\")\n\n```\nevent(event_type: CBEventType, payload: Optional[Dict[str, Any]] = None, event_id: Optional[str] = None) -> Generator[EventContext, None, None]\n\n```\n\nContext manager for lanching and shutdown of events.\n\nHandles sending on\\_evnt\\_start and on\\_event\\_end to handlers for specified event.\n\nUsage\n\nwith callback\\_manager.event(CBEventType.QUERY, payload={key, val}) as event:\n...\nevent.on\\_end(payload={key, val}) # optional\n\nSource code in `llama-index-core/llama_index/core/callbacks/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>``` | ```<br>@contextmanager<br>def event(<br>    self,<br>    event_type: CBEventType,<br>    payload: Optional[Dict[str, Any]] = None,<br>    event_id: Optional[str] = None,<br>) -> Generator[\"EventContext\", None, None]:<br>    \"\"\"Context manager for lanching and shutdown of events.<br>    Handles sending on_evnt_start and on_event_end to handlers for specified event.<br>    Usage:<br>        with callback_manager.event(CBEventType.QUERY, payload={key, val}) as event:<br>            ...<br>            event.on_end(payload={key, val})  # optional<br>    \"\"\"<br>    # create event context wrapper<br>    event = EventContext(self, event_type, event_id=event_id)<br>    event.on_start(payload=payload)<br>    payload = None<br>    try:<br>        yield event<br>    except Exception as e:<br>        # data already logged to trace?<br>        if not hasattr(e, \"event_added\"):<br>            payload = {EventPayload.EXCEPTION: e}<br>            e.event_added = True  # type: ignore<br>            if not event.finished:<br>                event.on_end(payload=payload)<br>        raise<br>    finally:<br>        # ensure event is ended<br>        if not event.finished:<br>            event.on_end(payload=payload)<br>``` |\n\n### as\\_trace [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.base.CallbackManager.as_trace \"Permanent link\")\n\n```\nas_trace(trace_id: str) -> Generator[None, None, None]\n\n```\n\nContext manager tracer for lanching and shutdown of traces.\n\nSource code in `llama-index-core/llama_index/core/callbacks/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>``` | ```<br>@contextmanager<br>def as_trace(self, trace_id: str) -> Generator[None, None, None]:<br>    \"\"\"Context manager tracer for lanching and shutdown of traces.\"\"\"<br>    self.start_trace(trace_id=trace_id)<br>    try:<br>        yield<br>    except Exception as e:<br>        # event already added to trace?<br>        if not hasattr(e, \"event_added\"):<br>            self.on_event_start(<br>                CBEventType.EXCEPTION, payload={EventPayload.EXCEPTION: e}<br>            )<br>            e.event_added = True  # type: ignore<br>        raise<br>    finally:<br>        # ensure trace is ended<br>        self.end_trace(trace_id=trace_id)<br>``` |\n\n### start\\_trace [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.base.CallbackManager.start_trace \"Permanent link\")\n\n```\nstart_trace(trace_id: Optional[str] = None) -> None\n\n```\n\nRun when an overall trace is launched.\n\nSource code in `llama-index-core/llama_index/core/callbacks/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>``` | ```<br>def start_trace(self, trace_id: Optional[str] = None) -> None:<br>    \"\"\"Run when an overall trace is launched.\"\"\"<br>    current_trace_stack_ids = global_stack_trace_ids.get().copy()<br>    if trace_id is not None:<br>        if len(current_trace_stack_ids) == 0:<br>            self._reset_trace_events()<br>            for handler in self.handlers:<br>                handler.start_trace(trace_id=trace_id)<br>            current_trace_stack_ids = [trace_id]<br>        else:<br>            current_trace_stack_ids.append(trace_id)<br>    global_stack_trace_ids.set(current_trace_stack_ids)<br>``` |\n\n### end\\_trace [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.base.CallbackManager.end_trace \"Permanent link\")\n\n```\nend_trace(trace_id: Optional[str] = None, trace_map: Optional[Dict[str, List[str]]] = None) -> None\n\n```\n\nRun when an overall trace is exited.\n\nSource code in `llama-index-core/llama_index/core/callbacks/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>``` | ```<br>def end_trace(<br>    self,<br>    trace_id: Optional[str] = None,<br>    trace_map: Optional[Dict[str, List[str]]] = None,<br>) -> None:<br>    \"\"\"Run when an overall trace is exited.\"\"\"<br>    current_trace_stack_ids = global_stack_trace_ids.get().copy()<br>    if trace_id is not None and len(current_trace_stack_ids) > 0:<br>        current_trace_stack_ids.pop()<br>        if len(current_trace_stack_ids) == 0:<br>            for handler in self.handlers:<br>                handler.end_trace(trace_id=trace_id, trace_map=self._trace_map)<br>            current_trace_stack_ids = []<br>    global_stack_trace_ids.set(current_trace_stack_ids)<br>``` |\n\n## BaseCallbackHandler [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.base_handler.BaseCallbackHandler \"Permanent link\")\n\nBases: `ABC`\n\nBase callback handler that can be used to track event starts and ends.\n\nSource code in `llama-index-core/llama_index/core/callbacks/base_handler.py`\n\n|     |     |\n| --- | --- |\n| ```<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>``` | ```<br>class BaseCallbackHandler(ABC):<br>    \"\"\"Base callback handler that can be used to track event starts and ends.\"\"\"<br>    def __init__(<br>        self,<br>        event_starts_to_ignore: List[CBEventType],<br>        event_ends_to_ignore: List[CBEventType],<br>    ) -> None:<br>        \"\"\"Initialize the base callback handler.\"\"\"<br>        self.event_starts_to_ignore = tuple(event_starts_to_ignore)<br>        self.event_ends_to_ignore = tuple(event_ends_to_ignore)<br>    @abstractmethod<br>    def on_event_start(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        parent_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> str:<br>        \"\"\"Run when an event starts and return id of event.\"\"\"<br>    @abstractmethod<br>    def on_event_end(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> None:<br>        \"\"\"Run when an event ends.\"\"\"<br>    @abstractmethod<br>    def start_trace(self, trace_id: Optional[str] = None) -> None:<br>        \"\"\"Run when an overall trace is launched.\"\"\"<br>    @abstractmethod<br>    def end_trace(<br>        self,<br>        trace_id: Optional[str] = None,<br>        trace_map: Optional[Dict[str, List[str]]] = None,<br>    ) -> None:<br>        \"\"\"Run when an overall trace is exited.\"\"\"<br>``` |\n\n### on\\_event\\_start`abstractmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.base_handler.BaseCallbackHandler.on_event_start \"Permanent link\")\n\n```\non_event_start(event_type: CBEventType, payload: Optional[Dict[str, Any]] = None, event_id: str = '', parent_id: str = '', **kwargs: Any) -> str\n\n```\n\nRun when an event starts and return id of event.\n\nSource code in `llama-index-core/llama_index/core/callbacks/base_handler.py`\n\n|     |     |\n| --- | --- |\n| ```<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>``` | ```<br>@abstractmethod<br>def on_event_start(<br>    self,<br>    event_type: CBEventType,<br>    payload: Optional[Dict[str, Any]] = None,<br>    event_id: str = \"\",<br>    parent_id: str = \"\",<br>    **kwargs: Any,<br>) -> str:<br>    \"\"\"Run when an event starts and return id of event.\"\"\"<br>``` |\n\n### on\\_event\\_end`abstractmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.base_handler.BaseCallbackHandler.on_event_end \"Permanent link\")\n\n```\non_event_end(event_type: CBEventType, payload: Optional[Dict[str, Any]] = None, event_id: str = '', **kwargs: Any) -> None\n\n```\n\nRun when an event ends.\n\nSource code in `llama-index-core/llama_index/core/callbacks/base_handler.py`\n\n|     |     |\n| --- | --- |\n| ```<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>``` | ```<br>@abstractmethod<br>def on_event_end(<br>    self,<br>    event_type: CBEventType,<br>    payload: Optional[Dict[str, Any]] = None,<br>    event_id: str = \"\",<br>    **kwargs: Any,<br>) -> None:<br>    \"\"\"Run when an event ends.\"\"\"<br>``` |\n\n### start\\_trace`abstractmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.base_handler.BaseCallbackHandler.start_trace \"Permanent link\")\n\n```\nstart_trace(trace_id: Optional[str] = None) -> None\n\n```\n\nRun when an overall trace is launched.\n\nSource code in `llama-index-core/llama_index/core/callbacks/base_handler.py`\n\n|     |     |\n| --- | --- |\n| ```<br>45<br>46<br>47<br>``` | ```<br>@abstractmethod<br>def start_trace(self, trace_id: Optional[str] = None) -> None:<br>    \"\"\"Run when an overall trace is launched.\"\"\"<br>``` |\n\n### end\\_trace`abstractmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.base_handler.BaseCallbackHandler.end_trace \"Permanent link\")\n\n```\nend_trace(trace_id: Optional[str] = None, trace_map: Optional[Dict[str, List[str]]] = None) -> None\n\n```\n\nRun when an overall trace is exited.\n\nSource code in `llama-index-core/llama_index/core/callbacks/base_handler.py`\n\n|     |     |\n| --- | --- |\n| ```<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>``` | ```<br>@abstractmethod<br>def end_trace(<br>    self,<br>    trace_id: Optional[str] = None,<br>    trace_map: Optional[Dict[str, List[str]]] = None,<br>) -> None:<br>    \"\"\"Run when an overall trace is exited.\"\"\"<br>``` |\n\nBase schema for callback managers.\n\n## CBEvent`dataclass`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.schema.CBEvent \"Permanent link\")\n\nGeneric class to store event information.\n\nSource code in `llama-index-core/llama_index/core/callbacks/schema.py`\n\n|     |     |\n| --- | --- |\n| ```<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>89<br>``` | ```<br>@dataclass<br>class CBEvent:<br>    \"\"\"Generic class to store event information.\"\"\"<br>    event_type: CBEventType<br>    payload: Optional[Dict[str, Any]] = None<br>    time: str = \"\"<br>    id_: str = \"\"<br>    def __post_init__(self) -> None:<br>        \"\"\"Init time and id if needed.\"\"\"<br>        if not self.time:<br>            self.time = datetime.now().strftime(TIMESTAMP_FORMAT)<br>        if not self.id_:<br>            self.id = str(uuid.uuid4())<br>``` |\n\n## CBEventType [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.schema.CBEventType \"Permanent link\")\n\nBases: `str`, `Enum`\n\nCallback manager event types.\n\n**Attributes:**\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `CHUNKING` |  | Logs for the before and after of text splitting. |\n| `NODE_PARSING` |  | Logs for the documents and the nodes that they are parsed into. |\n| `EMBEDDING` |  | Logs for the number of texts embedded. |\n| `LLM` |  | Logs for the template and response of LLM calls. |\n| `QUERY` |  | Keeps track of the start and end of each query. |\n| `RETRIEVE` |  | Logs for the nodes retrieved for a query. |\n| `SYNTHESIZE` |  | Logs for the result for synthesize calls. |\n| `TREE` |  | Logs for the summary and level of summaries generated. |\n| `SUB_QUESTION` |  | Logs for a generated sub question and answer. |\n\nSource code in `llama-index-core/llama_index/core/callbacks/schema.py`\n\n|     |     |\n| --- | --- |\n| ```<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>``` | ```<br>class CBEventType(str, Enum):<br>    \"\"\"Callback manager event types.<br>    Attributes:<br>        CHUNKING: Logs for the before and after of text splitting.<br>        NODE_PARSING: Logs for the documents and the nodes that they are parsed into.<br>        EMBEDDING: Logs for the number of texts embedded.<br>        LLM: Logs for the template and response of LLM calls.<br>        QUERY: Keeps track of the start and end of each query.<br>        RETRIEVE: Logs for the nodes retrieved for a query.<br>        SYNTHESIZE: Logs for the result for synthesize calls.<br>        TREE: Logs for the summary and level of summaries generated.<br>        SUB_QUESTION: Logs for a generated sub question and answer.<br>    \"\"\"<br>    CHUNKING = \"chunking\"<br>    NODE_PARSING = \"node_parsing\"<br>    EMBEDDING = \"embedding\"<br>    LLM = \"llm\"<br>    QUERY = \"query\"<br>    RETRIEVE = \"retrieve\"<br>    SYNTHESIZE = \"synthesize\"<br>    TREE = \"tree\"<br>    SUB_QUESTION = \"sub_question\"<br>    TEMPLATING = \"templating\"<br>    FUNCTION_CALL = \"function_call\"<br>    RERANKING = \"reranking\"<br>    EXCEPTION = \"exception\"<br>    AGENT_STEP = \"agent_step\"<br>``` |\n\n## EventPayload [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.schema.EventPayload \"Permanent link\")\n\nBases: `str`, `Enum`\n\nSource code in `llama-index-core/llama_index/core/callbacks/schema.py`\n\n|     |     |\n| --- | --- |\n| ```<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>``` | ```<br>class EventPayload(str, Enum):<br>    DOCUMENTS = \"documents\"  # list of documents before parsing<br>    CHUNKS = \"chunks\"  # list of text chunks<br>    NODES = \"nodes\"  # list of nodes<br>    PROMPT = \"formatted_prompt\"  # formatted prompt sent to LLM<br>    MESSAGES = \"messages\"  # list of messages sent to LLM<br>    COMPLETION = \"completion\"  # completion from LLM<br>    RESPONSE = \"response\"  # message response from LLM<br>    QUERY_STR = \"query_str\"  # query used for query engine<br>    SUB_QUESTION = \"sub_question\"  # a sub question & answer + sources<br>    EMBEDDINGS = \"embeddings\"  # list of embeddings<br>    TOP_K = \"top_k\"  # top k nodes retrieved<br>    ADDITIONAL_KWARGS = \"additional_kwargs\"  # additional kwargs for event call<br>    SERIALIZED = \"serialized\"  # serialized object for event caller<br>    FUNCTION_CALL = \"function_call\"  # function call for the LLM<br>    FUNCTION_OUTPUT = \"function_call_response\"  # function call output<br>    TOOL = \"tool\"  # tool used in LLM call<br>    MODEL_NAME = \"model_name\"  # model name used in an event<br>    TEMPLATE = \"template\"  # template used in LLM call<br>    TEMPLATE_VARS = \"template_vars\"  # template variables used in LLM call<br>    SYSTEM_PROMPT = \"system_prompt\"  # system prompt used in LLM call<br>    QUERY_WRAPPER_PROMPT = \"query_wrapper_prompt\"  # query wrapper prompt used in LLM<br>    EXCEPTION = \"exception\"  # exception raised in an event<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Core Callback Classes - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/#llama_index.core.chat_engine.types.ChatResponseMode)\n\n# Index\n\n## ChatResponseMode [\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.ChatResponseMode \"Permanent link\")\n\nBases: `str`, `Enum`\n\nFlag toggling waiting/streaming in `Agent._chat`.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>40<br>41<br>42<br>43<br>44<br>``` | ```<br>class ChatResponseMode(str, Enum):<br>    \"\"\"Flag toggling waiting/streaming in `Agent._chat`.\"\"\"<br>    WAIT = \"wait\"<br>    STREAM = \"stream\"<br>``` |\n\n## AgentChatResponse`dataclass`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.AgentChatResponse \"Permanent link\")\n\nAgent chat response.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>89<br>90<br>91<br>92<br>``` | ```<br>@dataclass<br>class AgentChatResponse:<br>    \"\"\"Agent chat response.\"\"\"<br>    response: str = \"\"<br>    sources: List[ToolOutput] = field(default_factory=list)<br>    source_nodes: List[NodeWithScore] = field(default_factory=list)<br>    is_dummy_stream: bool = False<br>    metadata: Optional[Dict[str, Any]] = None<br>    def set_source_nodes(self) -> None:<br>        if self.sources and not self.source_nodes:<br>            for tool_output in self.sources:<br>                if isinstance(tool_output.raw_output, (Response, StreamingResponse)):<br>                    self.source_nodes.extend(tool_output.raw_output.source_nodes)<br>    def __post_init__(self) -> None:<br>        self.set_source_nodes()<br>    def __str__(self) -> str:<br>        return self.response<br>    @property<br>    def response_gen(self) -> Generator[str, None, None]:<br>        \"\"\"Used for fake streaming, i.e. with tool outputs.\"\"\"<br>        if not self.is_dummy_stream:<br>            raise ValueError(<br>                \"response_gen is only available for streaming responses. \"<br>                \"Set is_dummy_stream=True if you still want a generator.\"<br>            )<br>        for token in self.response.split(\" \"):<br>            yield token + \" \"<br>            time.sleep(0.1)<br>    async def async_response_gen(self) -> AsyncGenerator[str, None]:<br>        \"\"\"Used for fake streaming, i.e. with tool outputs.\"\"\"<br>        if not self.is_dummy_stream:<br>            raise ValueError(<br>                \"response_gen is only available for streaming responses. \"<br>                \"Set is_dummy_stream=True if you still want a generator.\"<br>            )<br>        for token in self.response.split(\" \"):<br>            yield token + \" \"<br>            await asyncio.sleep(0.1)<br>``` |\n\n### response\\_gen`property`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.AgentChatResponse.response_gen \"Permanent link\")\n\n```\nresponse_gen: Generator[str, None, None]\n\n```\n\nUsed for fake streaming, i.e. with tool outputs.\n\n### async\\_response\\_gen`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.AgentChatResponse.async_response_gen \"Permanent link\")\n\n```\nasync_response_gen() -> AsyncGenerator[str, None]\n\n```\n\nUsed for fake streaming, i.e. with tool outputs.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>89<br>90<br>91<br>92<br>``` | ```<br>async def async_response_gen(self) -> AsyncGenerator[str, None]:<br>    \"\"\"Used for fake streaming, i.e. with tool outputs.\"\"\"<br>    if not self.is_dummy_stream:<br>        raise ValueError(<br>            \"response_gen is only available for streaming responses. \"<br>            \"Set is_dummy_stream=True if you still want a generator.\"<br>        )<br>    for token in self.response.split(\" \"):<br>        yield token + \" \"<br>        await asyncio.sleep(0.1)<br>``` |\n\n## StreamingAgentChatResponse`dataclass`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.StreamingAgentChatResponse \"Permanent link\")\n\nStreaming chat response to user and writing to chat history.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>``` | ```<br>@dataclass<br>class StreamingAgentChatResponse:<br>    \"\"\"Streaming chat response to user and writing to chat history.\"\"\"<br>    response: str = \"\"<br>    sources: List[ToolOutput] = field(default_factory=list)<br>    chat_stream: Optional[ChatResponseGen] = None<br>    achat_stream: Optional[ChatResponseAsyncGen] = None<br>    source_nodes: List[NodeWithScore] = field(default_factory=list)<br>    unformatted_response: str = \"\"<br>    queue: Queue = field(default_factory=Queue)<br>    aqueue: Optional[asyncio.Queue] = None<br>    # flag when chat message is a function call<br>    is_function: Optional[bool] = None<br>    # flag when processing done<br>    is_done = False<br>    # signal when a new item is added to the queue<br>    new_item_event: Optional[asyncio.Event] = None<br>    # NOTE: async code uses two events rather than one since it yields<br>    # control when waiting for queue item<br>    # signal when the OpenAI functions stop executing<br>    is_function_false_event: Optional[asyncio.Event] = None<br>    # signal when an OpenAI function is being executed<br>    is_function_not_none_thread_event: Event = field(default_factory=Event)<br>    # Track if an exception occurred<br>    exception: Optional[Exception] = None<br>    def set_source_nodes(self) -> None:<br>        if self.sources and not self.source_nodes:<br>            for tool_output in self.sources:<br>                if isinstance(tool_output.raw_output, (Response, StreamingResponse)):<br>                    self.source_nodes.extend(tool_output.raw_output.source_nodes)<br>    def __post_init__(self) -> None:<br>        self.set_source_nodes()<br>    def __str__(self) -> str:<br>        if self.is_done and not self.queue.empty() and not self.is_function:<br>            while self.queue.queue:<br>                delta = self.queue.queue.popleft()<br>                self.unformatted_response += delta<br>            self.response = self.unformatted_response.strip()<br>        return self.response<br>    def _ensure_async_setup(self) -> None:<br>        if self.aqueue is None:<br>            self.aqueue = asyncio.Queue()<br>        if self.new_item_event is None:<br>            self.new_item_event = asyncio.Event()<br>        if self.is_function_false_event is None:<br>            self.is_function_false_event = asyncio.Event()<br>    def put_in_queue(self, delta: Optional[str]) -> None:<br>        self.queue.put_nowait(delta)<br>        self.is_function_not_none_thread_event.set()<br>    def aput_in_queue(self, delta: Optional[str]) -> None:<br>        assert self.aqueue is not None<br>        assert self.new_item_event is not None<br>        self.aqueue.put_nowait(delta)<br>        self.new_item_event.set()<br>    @dispatcher.span<br>    def write_response_to_history(<br>        self,<br>        memory: BaseMemory,<br>        on_stream_end_fn: Optional[Callable] = None,<br>    ) -> None:<br>        if self.chat_stream is None:<br>            raise ValueError(<br>                \"chat_stream is None. Cannot write to history without chat_stream.\"<br>            )<br>        # try/except to prevent hanging on error<br>        dispatcher.event(StreamChatStartEvent())<br>        try:<br>            final_text = \"\"<br>            for chat in self.chat_stream:<br>                self.is_function = is_function(chat.message)<br>                if chat.delta:<br>                    dispatcher.event(<br>                        StreamChatDeltaReceivedEvent(<br>                            delta=chat.delta,<br>                        )<br>                    )<br>                    self.put_in_queue(chat.delta)<br>                final_text += chat.delta or \"\"<br>            if self.is_function is not None:  # if loop has gone through iteration<br>                # NOTE: this is to handle the special case where we consume some of the<br>                # chat stream, but not all of it (e.g. in react agent)<br>                chat.message.content = final_text.strip()  # final message<br>                memory.put(chat.message)<br>        except Exception as e:<br>            dispatcher.event(StreamChatErrorEvent(exception=e))<br>            self.exception = e<br>            # This act as is_done events for any consumers waiting<br>            self.is_function_not_none_thread_event.set()<br>            # force the queue reader to see the exception<br>            self.put_in_queue(\"\")<br>            raise<br>        dispatcher.event(StreamChatEndEvent())<br>        self.is_done = True<br>        # This act as is_done events for any consumers waiting<br>        self.is_function_not_none_thread_event.set()<br>        if on_stream_end_fn is not None and not self.is_function:<br>            on_stream_end_fn()<br>    @dispatcher.span<br>    async def awrite_response_to_history(<br>        self,<br>        memory: BaseMemory,<br>        on_stream_end_fn: Optional[Callable] = None,<br>    ) -> None:<br>        self._ensure_async_setup()<br>        assert self.aqueue is not None<br>        assert self.is_function_false_event is not None<br>        assert self.new_item_event is not None<br>        if self.achat_stream is None:<br>            raise ValueError(<br>                \"achat_stream is None. Cannot asynchronously write to \"<br>                \"history without achat_stream.\"<br>            )<br>        # try/except to prevent hanging on error<br>        dispatcher.event(StreamChatStartEvent())<br>        try:<br>            final_text = \"\"<br>            async for chat in self.achat_stream:<br>                self.is_function = is_function(chat.message)<br>                if chat.delta:<br>                    dispatcher.event(<br>                        StreamChatDeltaReceivedEvent(<br>                            delta=chat.delta,<br>                        )<br>                    )<br>                    self.aput_in_queue(chat.delta)<br>                final_text += chat.delta or \"\"<br>                self.new_item_event.set()<br>                if self.is_function is False:<br>                    self.is_function_false_event.set()<br>            if self.is_function is not None:  # if loop has gone through iteration<br>                # NOTE: this is to handle the special case where we consume some of the<br>                # chat stream, but not all of it (e.g. in react agent)<br>                chat.message.content = final_text.strip()  # final message<br>                memory.put(chat.message)<br>        except Exception as e:<br>            dispatcher.event(StreamChatErrorEvent(exception=e))<br>            self.exception = e<br>            # These act as is_done events for any consumers waiting<br>            self.is_function_false_event.set()<br>            self.new_item_event.set()<br>            # force the queue reader to see the exception<br>            self.aput_in_queue(\"\")<br>            raise<br>        dispatcher.event(StreamChatEndEvent())<br>        self.is_done = True<br>        # These act as is_done events for any consumers waiting<br>        self.is_function_false_event.set()<br>        self.new_item_event.set()<br>        if on_stream_end_fn is not None and not self.is_function:<br>            on_stream_end_fn()<br>    @property<br>    def response_gen(self) -> Generator[str, None, None]:<br>        while not self.is_done or not self.queue.empty():<br>            if self.exception is not None:<br>                raise self.exception<br>            try:<br>                delta = self.queue.get(block=False)<br>                self.unformatted_response += delta<br>                yield delta<br>            except Empty:<br>                # Queue is empty, but we're not done yet. Sleep for 0 secs to release the GIL and allow other threads to run.<br>                time.sleep(0)<br>        self.response = self.unformatted_response.strip()<br>    async def async_response_gen(self) -> AsyncGenerator[str, None]:<br>        self._ensure_async_setup()<br>        assert self.aqueue is not None<br>        while True:<br>            if not self.aqueue.empty() or not self.is_done:<br>                if self.exception is not None:<br>                    raise self.exception<br>                try:<br>                    delta = await asyncio.wait_for(self.aqueue.get(), timeout=0.1)<br>                except asyncio.TimeoutError:<br>                    if self.is_done:<br>                        break<br>                    continue<br>                if delta is not None:<br>                    self.unformatted_response += delta<br>                    yield delta<br>            else:<br>                break<br>        self.response = self.unformatted_response.strip()<br>    def print_response_stream(self) -> None:<br>        for token in self.response_gen:<br>            print(token, end=\"\", flush=True)<br>    async def aprint_response_stream(self) -> None:<br>        async for token in self.async_response_gen():<br>            print(token, end=\"\", flush=True)<br>``` |\n\n## BaseChatEngine [\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.BaseChatEngine \"Permanent link\")\n\nBases: `DispatcherSpanMixin`, `ABC`\n\nBase Chat Engine.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>``` | ```<br>class BaseChatEngine(DispatcherSpanMixin, ABC):<br>    \"\"\"Base Chat Engine.\"\"\"<br>    @abstractmethod<br>    def reset(self) -> None:<br>        \"\"\"Reset conversation state.\"\"\"<br>    @abstractmethod<br>    def chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> AGENT_CHAT_RESPONSE_TYPE:<br>        \"\"\"Main chat interface.\"\"\"<br>    @abstractmethod<br>    def stream_chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> StreamingAgentChatResponse:<br>        \"\"\"Stream chat interface.\"\"\"<br>    @abstractmethod<br>    async def achat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> AGENT_CHAT_RESPONSE_TYPE:<br>        \"\"\"Async version of main chat interface.\"\"\"<br>    @abstractmethod<br>    async def astream_chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> StreamingAgentChatResponse:<br>        \"\"\"Async version of main chat interface.\"\"\"<br>    def chat_repl(self) -> None:<br>        \"\"\"Enter interactive chat REPL.\"\"\"<br>        print(\"===== Entering Chat REPL =====\")<br>        print('Type \"exit\" to exit.\\n')<br>        self.reset()<br>        message = input(\"Human: \")<br>        while message != \"exit\":<br>            response = self.chat(message)<br>            print(f\"Assistant: {response}\\n\")<br>            message = input(\"Human: \")<br>    def streaming_chat_repl(self) -> None:<br>        \"\"\"Enter interactive chat REPL with streaming responses.\"\"\"<br>        print(\"===== Entering Chat REPL =====\")<br>        print('Type \"exit\" to exit.\\n')<br>        self.reset()<br>        message = input(\"Human: \")<br>        while message != \"exit\":<br>            response = self.stream_chat(message)<br>            print(\"Assistant: \", end=\"\", flush=True)<br>            response.print_response_stream()<br>            print(\"\\n\")<br>            message = input(\"Human: \")<br>    @property<br>    @abstractmethod<br>    def chat_history(self) -> List[ChatMessage]:<br>        pass<br>``` |\n\n### reset`abstractmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.BaseChatEngine.reset \"Permanent link\")\n\n```\nreset() -> None\n\n```\n\nReset conversation state.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>318<br>319<br>320<br>``` | ```<br>@abstractmethod<br>def reset(self) -> None:<br>    \"\"\"Reset conversation state.\"\"\"<br>``` |\n\n### chat`abstractmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.BaseChatEngine.chat \"Permanent link\")\n\n```\nchat(message: str, chat_history: Optional[List[ChatMessage]] = None) -> AGENT_CHAT_RESPONSE_TYPE\n\n```\n\nMain chat interface.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>322<br>323<br>324<br>325<br>326<br>``` | ```<br>@abstractmethod<br>def chat(<br>    self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>) -> AGENT_CHAT_RESPONSE_TYPE:<br>    \"\"\"Main chat interface.\"\"\"<br>``` |\n\n### stream\\_chat`abstractmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.BaseChatEngine.stream_chat \"Permanent link\")\n\n```\nstream_chat(message: str, chat_history: Optional[List[ChatMessage]] = None) -> StreamingAgentChatResponse\n\n```\n\nStream chat interface.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>328<br>329<br>330<br>331<br>332<br>``` | ```<br>@abstractmethod<br>def stream_chat(<br>    self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>) -> StreamingAgentChatResponse:<br>    \"\"\"Stream chat interface.\"\"\"<br>``` |\n\n### achat`abstractmethod``async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.BaseChatEngine.achat \"Permanent link\")\n\n```\nachat(message: str, chat_history: Optional[List[ChatMessage]] = None) -> AGENT_CHAT_RESPONSE_TYPE\n\n```\n\nAsync version of main chat interface.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>334<br>335<br>336<br>337<br>338<br>``` | ```<br>@abstractmethod<br>async def achat(<br>    self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>) -> AGENT_CHAT_RESPONSE_TYPE:<br>    \"\"\"Async version of main chat interface.\"\"\"<br>``` |\n\n### astream\\_chat`abstractmethod``async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.BaseChatEngine.astream_chat \"Permanent link\")\n\n```\nastream_chat(message: str, chat_history: Optional[List[ChatMessage]] = None) -> StreamingAgentChatResponse\n\n```\n\nAsync version of main chat interface.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>340<br>341<br>342<br>343<br>344<br>``` | ```<br>@abstractmethod<br>async def astream_chat(<br>    self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>) -> StreamingAgentChatResponse:<br>    \"\"\"Async version of main chat interface.\"\"\"<br>``` |\n\n### chat\\_repl [\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.BaseChatEngine.chat_repl \"Permanent link\")\n\n```\nchat_repl() -> None\n\n```\n\nEnter interactive chat REPL.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>``` | ```<br>def chat_repl(self) -> None:<br>    \"\"\"Enter interactive chat REPL.\"\"\"<br>    print(\"===== Entering Chat REPL =====\")<br>    print('Type \"exit\" to exit.\\n')<br>    self.reset()<br>    message = input(\"Human: \")<br>    while message != \"exit\":<br>        response = self.chat(message)<br>        print(f\"Assistant: {response}\\n\")<br>        message = input(\"Human: \")<br>``` |\n\n### streaming\\_chat\\_repl [\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.BaseChatEngine.streaming_chat_repl \"Permanent link\")\n\n```\nstreaming_chat_repl() -> None\n\n```\n\nEnter interactive chat REPL with streaming responses.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>``` | ```<br>def streaming_chat_repl(self) -> None:<br>    \"\"\"Enter interactive chat REPL with streaming responses.\"\"\"<br>    print(\"===== Entering Chat REPL =====\")<br>    print('Type \"exit\" to exit.\\n')<br>    self.reset()<br>    message = input(\"Human: \")<br>    while message != \"exit\":<br>        response = self.stream_chat(message)<br>        print(\"Assistant: \", end=\"\", flush=True)<br>        response.print_response_stream()<br>        print(\"\\n\")<br>        message = input(\"Human: \")<br>``` |\n\n## ChatMode [\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.ChatMode \"Permanent link\")\n\nBases: `str`, `Enum`\n\nChat Engine Modes.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>``` | ```<br>class ChatMode(str, Enum):<br>    \"\"\"Chat Engine Modes.\"\"\"<br>    SIMPLE = \"simple\"<br>    \"\"\"Corresponds to `SimpleChatEngine`.<br>    Chat with LLM, without making use of a knowledge base.<br>    \"\"\"<br>    CONDENSE_QUESTION = \"condense_question\"<br>    \"\"\"Corresponds to `CondenseQuestionChatEngine`.<br>    First generate a standalone question from conversation context and last message,<br>    then query the query engine for a response.<br>    \"\"\"<br>    CONTEXT = \"context\"<br>    \"\"\"Corresponds to `ContextChatEngine`.<br>    First retrieve text from the index using the user's message, then use the context<br>    in the system prompt to generate a response.<br>    \"\"\"<br>    CONDENSE_PLUS_CONTEXT = \"condense_plus_context\"<br>    \"\"\"Corresponds to `CondensePlusContextChatEngine`.<br>    First condense a conversation and latest user message to a standalone question.<br>    Then build a context for the standalone question from a retriever,<br>    Then pass the context along with prompt and user message to LLM to generate a response.<br>    \"\"\"<br>    REACT = \"react\"<br>    \"\"\"Corresponds to `ReActAgent`.<br>    Use a ReAct agent loop with query engine tools.<br>    \"\"\"<br>    OPENAI = \"openai\"<br>    \"\"\"Corresponds to `OpenAIAgent`.<br>    Use an OpenAI function calling agent loop.<br>    NOTE: only works with OpenAI models that support function calling API.<br>    \"\"\"<br>    BEST = \"best\"<br>    \"\"\"Select the best chat engine based on the current LLM.<br>    Corresponds to `OpenAIAgent` if using an OpenAI model that supports<br>    function calling API, otherwise, corresponds to `ReActAgent`.<br>    \"\"\"<br>``` |\n\n### SIMPLE`class-attribute``instance-attribute`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.ChatMode.SIMPLE \"Permanent link\")\n\n```\nSIMPLE = 'simple'\n\n```\n\nCorresponds to `SimpleChatEngine`.\n\nChat with LLM, without making use of a knowledge base.\n\n### CONDENSE\\_QUESTION`class-attribute``instance-attribute`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.ChatMode.CONDENSE_QUESTION \"Permanent link\")\n\n```\nCONDENSE_QUESTION = 'condense_question'\n\n```\n\nCorresponds to `CondenseQuestionChatEngine`.\n\nFirst generate a standalone question from conversation context and last message,\nthen query the query engine for a response.\n\n### CONTEXT`class-attribute``instance-attribute`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.ChatMode.CONTEXT \"Permanent link\")\n\n```\nCONTEXT = 'context'\n\n```\n\nCorresponds to `ContextChatEngine`.\n\nFirst retrieve text from the index using the user's message, then use the context\nin the system prompt to generate a response.\n\n### CONDENSE\\_PLUS\\_CONTEXT`class-attribute``instance-attribute`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.ChatMode.CONDENSE_PLUS_CONTEXT \"Permanent link\")\n\n```\nCONDENSE_PLUS_CONTEXT = 'condense_plus_context'\n\n```\n\nCorresponds to `CondensePlusContextChatEngine`.\n\nFirst condense a conversation and latest user message to a standalone question.\nThen build a context for the standalone question from a retriever,\nThen pass the context along with prompt and user message to LLM to generate a response.\n\n### REACT`class-attribute``instance-attribute`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.ChatMode.REACT \"Permanent link\")\n\n```\nREACT = 'react'\n\n```\n\nCorresponds to `ReActAgent`.\n\nUse a ReAct agent loop with query engine tools.\n\n### OPENAI`class-attribute``instance-attribute`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.ChatMode.OPENAI \"Permanent link\")\n\n```\nOPENAI = 'openai'\n\n```\n\nCorresponds to `OpenAIAgent`.\n\nUse an OpenAI function calling agent loop.\n\nNOTE: only works with OpenAI models that support function calling API.\n\n### BEST`class-attribute``instance-attribute`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.ChatMode.BEST \"Permanent link\")\n\n```\nBEST = 'best'\n\n```\n\nSelect the best chat engine based on the current LLM.\n\nCorresponds to `OpenAIAgent` if using an OpenAI model that supports\nfunction calling API, otherwise, corresponds to `ReActAgent`.\n\n## is\\_function [\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.is_function \"Permanent link\")\n\n```\nis_function(message: ChatMessage) -> bool\n\n```\n\nUtility for ChatMessage responses from OpenAI models.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>35<br>36<br>37<br>``` | ```<br>def is_function(message: ChatMessage) -> bool:<br>    \"\"\"Utility for ChatMessage responses from OpenAI models.\"\"\"<br>    return \"tool_calls\" in message.additional_kwargs<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Index - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/bedrock/#llama_index.embeddings.bedrock.BedrockEmbedding)\n\n# Bedrock\n\n## BedrockEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/bedrock/\\#llama_index.embeddings.bedrock.BedrockEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-bedrock/llama_index/embeddings/bedrock/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>``` | ```<br>class BedrockEmbedding(BaseEmbedding):<br>    model_name: str = Field(description=\"The modelId of the Bedrock model to use.\")<br>    profile_name: Optional[str] = Field(<br>        description=\"The name of aws profile to use. If not given, then the default profile is used.\",<br>    )<br>    aws_access_key_id: Optional[str] = Field(description=\"AWS Access Key ID to use\")<br>    aws_secret_access_key: Optional[str] = Field(<br>        description=\"AWS Secret Access Key to use\"<br>    )<br>    aws_session_token: Optional[str] = Field(description=\"AWS Session Token to use\")<br>    region_name: Optional[str] = Field(<br>        description=\"AWS region name to use. Uses region configured in AWS CLI if not passed\",<br>    )<br>    botocore_session: Optional[Any] = Field(<br>        description=\"Use this Botocore session instead of creating a new default one.\",<br>        exclude=True,<br>    )<br>    botocore_config: Optional[Any] = Field(<br>        description=\"Custom configuration object to use instead of the default generated one.\",<br>        exclude=True,<br>    )<br>    max_retries: int = Field(<br>        default=10, description=\"The maximum number of API retries.\", gt=0<br>    )<br>    timeout: float = Field(<br>        default=60.0,<br>        description=\"The timeout for the Bedrock API request in seconds. It will be used for both connect and read timeouts.\",<br>    )<br>    additional_kwargs: Dict[str, Any] = Field(<br>        default_factory=dict, description=\"Additional kwargs for the bedrock client.\"<br>    )<br>    _client: Any = PrivateAttr()<br>    def __init__(<br>        self,<br>        model_name: str = Models.TITAN_EMBEDDING,<br>        profile_name: Optional[str] = None,<br>        aws_access_key_id: Optional[str] = None,<br>        aws_secret_access_key: Optional[str] = None,<br>        aws_session_token: Optional[str] = None,<br>        region_name: Optional[str] = None,<br>        client: Optional[Any] = None,<br>        botocore_session: Optional[Any] = None,<br>        botocore_config: Optional[Any] = None,<br>        additional_kwargs: Optional[Dict[str, Any]] = None,<br>        max_retries: int = 10,<br>        timeout: float = 60.0,<br>        callback_manager: Optional[CallbackManager] = None,<br>        # base class<br>        system_prompt: Optional[str] = None,<br>        messages_to_prompt: Optional[Callable[[Sequence[ChatMessage]], str]] = None,<br>        completion_to_prompt: Optional[Callable[[str], str]] = None,<br>        pydantic_program_mode: PydanticProgramMode = PydanticProgramMode.DEFAULT,<br>        output_parser: Optional[BaseOutputParser] = None,<br>        **kwargs: Any,<br>    ):<br>        additional_kwargs = additional_kwargs or {}<br>        session_kwargs = {<br>            \"profile_name\": profile_name,<br>            \"region_name\": region_name,<br>            \"aws_access_key_id\": aws_access_key_id,<br>            \"aws_secret_access_key\": aws_secret_access_key,<br>            \"aws_session_token\": aws_session_token,<br>            \"botocore_session\": botocore_session,<br>        }<br>        try:<br>            import boto3<br>            from botocore.config import Config<br>            config = (<br>                Config(<br>                    retries={\"max_attempts\": max_retries, \"mode\": \"standard\"},<br>                    connect_timeout=timeout,<br>                    read_timeout=timeout,<br>                )<br>                if botocore_config is None<br>                else botocore_config<br>            )<br>            session = boto3.Session(**session_kwargs)<br>        except ImportError:<br>            raise ImportError(<br>                \"boto3 package not found, install with\" \"'pip install boto3'\"<br>            )<br>        super().__init__(<br>            model_name=model_name,<br>            max_retries=max_retries,<br>            timeout=timeout,<br>            botocore_config=config,<br>            profile_name=profile_name,<br>            aws_access_key_id=aws_access_key_id,<br>            aws_secret_access_key=aws_secret_access_key,<br>            aws_session_token=aws_session_token,<br>            region_name=region_name,<br>            botocore_session=botocore_session,<br>            additional_kwargs=additional_kwargs,<br>            callback_manager=callback_manager,<br>            system_prompt=system_prompt,<br>            messages_to_prompt=messages_to_prompt,<br>            completion_to_prompt=completion_to_prompt,<br>            pydantic_program_mode=pydantic_program_mode,<br>            output_parser=output_parser,<br>            **kwargs,<br>        )<br>        # Prior to general availability, custom boto3 wheel files were<br>        # distributed that used the bedrock service to invokeModel.<br>        # This check prevents any services still using those wheel files<br>        # from breaking<br>        if client is not None:<br>            self._client = client<br>        elif \"bedrock-runtime\" in session.get_available_services():<br>            self._client = session.client(\"bedrock-runtime\", config=config)<br>        else:<br>            self._client = session.client(\"bedrock\", config=config)<br>    @staticmethod<br>    def list_supported_models() -> Dict[str, List[str]]:<br>        list_models = {}<br>        for provider in PROVIDERS:<br>            list_models[provider.value] = [<br>                m.value for m in Models if provider.value in m.value<br>            ]<br>        return list_models<br>    @classmethod<br>    def class_name(self) -> str:<br>        return \"BedrockEmbedding\"<br>    @deprecated(<br>        version=\"0.9.48\",<br>        reason=(<br>            \"Use the provided kwargs in the constructor, \"<br>            \"set_credentials will be removed in future releases.\"<br>        ),<br>        action=\"once\",<br>    )<br>    def set_credentials(<br>        self,<br>        aws_region: Optional[str] = None,<br>        aws_access_key_id: Optional[str] = None,<br>        aws_secret_access_key: Optional[str] = None,<br>        aws_session_token: Optional[str] = None,<br>        aws_profile: Optional[str] = None,<br>    ) -> None:<br>        aws_region = aws_region or os.getenv(\"AWS_REGION\")<br>        aws_access_key_id = aws_access_key_id or os.getenv(\"AWS_ACCESS_KEY_ID\")<br>        aws_secret_access_key = aws_secret_access_key or os.getenv(<br>            \"AWS_SECRET_ACCESS_KEY\"<br>        )<br>        aws_session_token = aws_session_token or os.getenv(\"AWS_SESSION_TOKEN\")<br>        if aws_region is None:<br>            warnings.warn(<br>                \"AWS_REGION not found. Set environment variable AWS_REGION or set aws_region\"<br>            )<br>        if aws_access_key_id is None:<br>            warnings.warn(<br>                \"AWS_ACCESS_KEY_ID not found. Set environment variable AWS_ACCESS_KEY_ID or set aws_access_key_id\"<br>            )<br>            assert aws_access_key_id is not None<br>        if aws_secret_access_key is None:<br>            warnings.warn(<br>                \"AWS_SECRET_ACCESS_KEY not found. Set environment variable AWS_SECRET_ACCESS_KEY or set aws_secret_access_key\"<br>            )<br>            assert aws_secret_access_key is not None<br>        if aws_session_token is None:<br>            warnings.warn(<br>                \"AWS_SESSION_TOKEN not found. Set environment variable AWS_SESSION_TOKEN or set aws_session_token\"<br>            )<br>            assert aws_session_token is not None<br>        session_kwargs = {<br>            \"profile_name\": aws_profile,<br>            \"region_name\": aws_region,<br>            \"aws_access_key_id\": aws_access_key_id,<br>            \"aws_secret_access_key\": aws_secret_access_key,<br>            \"aws_session_token\": aws_session_token,<br>        }<br>        try:<br>            import boto3<br>            session = boto3.Session(**session_kwargs)<br>        except ImportError:<br>            raise ImportError(<br>                \"boto3 package not found, install with\" \"'pip install boto3'\"<br>            )<br>        if \"bedrock-runtime\" in session.get_available_services():<br>            self._client = session.client(\"bedrock-runtime\")<br>        else:<br>            self._client = session.client(\"bedrock\")<br>    @classmethod<br>    @deprecated(<br>        version=\"0.9.48\",<br>        reason=(<br>            \"Use the provided kwargs in the constructor, \"<br>            \"set_credentials will be removed in future releases.\"<br>        ),<br>        action=\"once\",<br>    )<br>    def from_credentials(<br>        cls,<br>        model_name: str = Models.TITAN_EMBEDDING,<br>        aws_region: Optional[str] = None,<br>        aws_access_key_id: Optional[str] = None,<br>        aws_secret_access_key: Optional[str] = None,<br>        aws_session_token: Optional[str] = None,<br>        aws_profile: Optional[str] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>    ) -> \"BedrockEmbedding\":<br>        \"\"\"<br>        Instantiate using AWS credentials.<br>        Args:<br>            model_name (str) : Name of the model<br>            aws_access_key_id (str): AWS access key ID<br>            aws_secret_access_key (str): AWS secret access key<br>            aws_session_token (str): AWS session token<br>            aws_region (str): AWS region where the service is located<br>            aws_profile (str): AWS profile, when None, default profile is chosen automatically<br>        Example:<br>                .. code-block:: python<br>                    from llama_index.embeddings import BedrockEmbedding<br>                    # Define the model name<br>                    model_name = \"your_model_name\"<br>                    embeddings = BedrockEmbedding.from_credentials(<br>                        model_name,<br>                        aws_access_key_id,<br>                        aws_secret_access_key,<br>                        aws_session_token,<br>                        aws_region,<br>                        aws_profile,<br>                    )<br>        \"\"\"<br>        session_kwargs = {<br>            \"profile_name\": aws_profile,<br>            \"region_name\": aws_region,<br>            \"aws_access_key_id\": aws_access_key_id,<br>            \"aws_secret_access_key\": aws_secret_access_key,<br>            \"aws_session_token\": aws_session_token,<br>        }<br>        try:<br>            import boto3<br>            session = boto3.Session(**session_kwargs)<br>        except ImportError:<br>            raise ImportError(<br>                \"boto3 package not found, install with\" \"'pip install boto3'\"<br>            )<br>        if \"bedrock-runtime\" in session.get_available_services():<br>            client = session.client(\"bedrock-runtime\")<br>        else:<br>            client = session.client(\"bedrock\")<br>        return cls(<br>            client=client,<br>            model=model_name,<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            verbose=verbose,<br>        )<br>    def _get_embedding(<br>        self, payload: Union[str, List[str]], type: Literal[\"text\", \"query\"]<br>    ) -> Union[Embedding, List[Embedding]]:<br>        \"\"\"Get the embedding for the given payload.<br>        Args:<br>            payload (Union[str, List[str]]): The text or list of texts for which the embeddings are to be obtained.<br>            type (Literal[&quot;text&quot;, &quot;query&quot;]): The type of the payload. It can be either \"text\" or \"query\".<br>        Returns:<br>            Union[Embedding, List[Embedding]]: The embedding or list of embeddings for the given payload. If the payload is a list of strings, then the response will be a list of embeddings.<br>        \"\"\"<br>        if self._client is None:<br>            self.set_credentials()<br>        if self._client is None:<br>            raise ValueError(\"Client not set\")<br>        provider = self.model_name.split(\".\")[0]<br>        request_body = self._get_request_body(provider, payload, type)<br>        response = self._client.invoke_model(<br>            body=request_body,<br>            modelId=self.model_name,<br>            accept=\"application/json\",<br>            contentType=\"application/json\",<br>        )<br>        resp = json.loads(response.get(\"body\").read().decode(\"utf-8\"))<br>        identifiers = PROVIDER_SPECIFIC_IDENTIFIERS.get(provider, None)<br>        if identifiers is None:<br>            raise ValueError(\"Provider not supported\")<br>        return identifiers[\"get_embeddings_func\"](resp, isinstance(payload, list))<br>    def _get_query_embedding(self, query: str) -> Embedding:<br>        return self._get_embedding(query, \"query\")<br>    def _get_text_embedding(self, text: str) -> Embedding:<br>        return self._get_embedding(text, \"text\")<br>    def _get_text_embeddings(self, texts: List[str]) -> List[Embedding]:<br>        provider = self.model_name.split(\".\")[0]<br>        if provider == PROVIDERS.COHERE:<br>            return self._get_embedding(texts, \"text\")<br>        return super()._get_text_embeddings(texts)<br>    def _get_request_body(<br>        self,<br>        provider: str,<br>        payload: Union[str, List[str]],<br>        input_type: Literal[\"text\", \"query\"],<br>    ) -> Any:<br>        \"\"\"Build the request body as per the provider.<br>        Currently supported providers are amazon, cohere.<br>        amazon:<br>            Sample Payload of type str<br>            \"Hello World!\"<br>        cohere:<br>            Sample Payload of type dict of following format<br>            {<br>                'texts': [\"This is a test document\", \"This is another document\"],<br>                'input_type': 'search_document'<br>            }<br>        \"\"\"<br>        if provider == PROVIDERS.AMAZON:<br>            if isinstance(payload, list):<br>                raise ValueError(\"Amazon provider does not support list of texts\")<br>            titan_body_request = {\"inputText\": payload}<br>            # Titan Embedding V2.0 has additional body parameters to check.<br>            if \"dimensions\" in self.additional_kwargs:<br>                if self.model_name == Models.TITAN_EMBEDDING_V2_0:<br>                    titan_body_request[\"dimensions\"] = self.additional_kwargs[<br>                        \"dimensions\"<br>                    ]<br>                else:<br>                    raise ValueError(<br>                        \"'dimensions' param not supported outside of 'titan-embed-text-v2:0' model.\"<br>                    )<br>            if \"normalize\" in self.additional_kwargs:<br>                if self.model_name == Models.TITAN_EMBEDDING_V2_0:<br>                    titan_body_request[\"normalize\"] = self.additional_kwargs[<br>                        \"normalize\"<br>                    ]<br>                else:<br>                    raise ValueError(<br>                        \"'normalize' param not supported outside of 'titan-embed-text-v2:0' model.\"<br>                    )<br>            request_body = json.dumps(titan_body_request)<br>        elif provider == PROVIDERS.COHERE:<br>            input_types = {<br>                \"text\": \"search_document\",<br>                \"query\": \"search_query\",<br>            }<br>            payload = [payload] if isinstance(payload, str) else payload<br>            payload = [p[:2048] if len(p) > 2048 else p for p in payload]<br>            request_body = json.dumps(<br>                {<br>                    \"texts\": payload,<br>                    \"input_type\": input_types[input_type],<br>                }<br>            )<br>        else:<br>            raise ValueError(\"Provider not supported\")<br>        return request_body<br>    async def _aget_query_embedding(self, query: str) -> Embedding:<br>        return self._get_embedding(query, \"query\")<br>    async def _aget_text_embedding(self, text: str) -> Embedding:<br>        return self._get_embedding(text, \"text\")<br>``` |\n\n### from\\_credentials`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/bedrock/\\#llama_index.embeddings.bedrock.BedrockEmbedding.from_credentials \"Permanent link\")\n\n```\nfrom_credentials(model_name: str = Models.TITAN_EMBEDDING, aws_region: Optional[str] = None, aws_access_key_id: Optional[str] = None, aws_secret_access_key: Optional[str] = None, aws_session_token: Optional[str] = None, aws_profile: Optional[str] = None, embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE, callback_manager: Optional[CallbackManager] = None, verbose: bool = False) -> BedrockEmbedding\n\n```\n\nInstantiate using AWS credentials.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model_name` | `str) ` | Name of the model | `TITAN_EMBEDDING` |\n| `aws_access_key_id` | `str` | AWS access key ID | `None` |\n| `aws_secret_access_key` | `str` | AWS secret access key | `None` |\n| `aws_session_token` | `str` | AWS session token | `None` |\n| `aws_region` | `str` | AWS region where the service is located | `None` |\n| `aws_profile` | `str` | AWS profile, when None, default profile is chosen automatically | `None` |\n\nExample\n\n.. code-block:: python\n\n```\nfrom llama_index.embeddings import BedrockEmbedding\n\n# Define the model name\nmodel_name = \"your_model_name\"\n\nembeddings = BedrockEmbedding.from_credentials(\n    model_name,\n    aws_access_key_id,\n    aws_secret_access_key,\n    aws_session_token,\n    aws_region,\n    aws_profile,\n)\n\n```\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-bedrock/llama_index/embeddings/bedrock/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>``` | ```<br>@classmethod<br>@deprecated(<br>    version=\"0.9.48\",<br>    reason=(<br>        \"Use the provided kwargs in the constructor, \"<br>        \"set_credentials will be removed in future releases.\"<br>    ),<br>    action=\"once\",<br>)<br>def from_credentials(<br>    cls,<br>    model_name: str = Models.TITAN_EMBEDDING,<br>    aws_region: Optional[str] = None,<br>    aws_access_key_id: Optional[str] = None,<br>    aws_secret_access_key: Optional[str] = None,<br>    aws_session_token: Optional[str] = None,<br>    aws_profile: Optional[str] = None,<br>    embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>    callback_manager: Optional[CallbackManager] = None,<br>    verbose: bool = False,<br>) -> \"BedrockEmbedding\":<br>    \"\"\"<br>    Instantiate using AWS credentials.<br>    Args:<br>        model_name (str) : Name of the model<br>        aws_access_key_id (str): AWS access key ID<br>        aws_secret_access_key (str): AWS secret access key<br>        aws_session_token (str): AWS session token<br>        aws_region (str): AWS region where the service is located<br>        aws_profile (str): AWS profile, when None, default profile is chosen automatically<br>    Example:<br>            .. code-block:: python<br>                from llama_index.embeddings import BedrockEmbedding<br>                # Define the model name<br>                model_name = \"your_model_name\"<br>                embeddings = BedrockEmbedding.from_credentials(<br>                    model_name,<br>                    aws_access_key_id,<br>                    aws_secret_access_key,<br>                    aws_session_token,<br>                    aws_region,<br>                    aws_profile,<br>                )<br>    \"\"\"<br>    session_kwargs = {<br>        \"profile_name\": aws_profile,<br>        \"region_name\": aws_region,<br>        \"aws_access_key_id\": aws_access_key_id,<br>        \"aws_secret_access_key\": aws_secret_access_key,<br>        \"aws_session_token\": aws_session_token,<br>    }<br>    try:<br>        import boto3<br>        session = boto3.Session(**session_kwargs)<br>    except ImportError:<br>        raise ImportError(<br>            \"boto3 package not found, install with\" \"'pip install boto3'\"<br>        )<br>    if \"bedrock-runtime\" in session.get_available_services():<br>        client = session.client(\"bedrock-runtime\")<br>    else:<br>        client = session.client(\"bedrock\")<br>    return cls(<br>        client=client,<br>        model=model_name,<br>        embed_batch_size=embed_batch_size,<br>        callback_manager=callback_manager,<br>        verbose=verbose,<br>    )<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Bedrock - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/bedrock/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/agent/introspective/#llama_index.agent.introspective.IntrospectiveAgentWorker)\n\n# Introspective\n\n## IntrospectiveAgentWorker [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/introspective/\\#llama_index.agent.introspective.IntrospectiveAgentWorker \"Permanent link\")\n\nBases: `BaseAgentWorker`\n\nIntrospective Agent Worker.\n\nThis agent worker implements the Reflection AI agentic pattern. It does\nso by merely delegating the work to two other agents in a purely\ndeterministic fashion.\n\nThe task this agent performs (again via delegation) is to generate a response\nto a query and perform reflection and correction on the response. This\nagent delegates the task to (optionally) first a `main_agent_worker` that\ngenerates the initial response to the query. This initial response is then\npassed to the `reflective_agent_worker` to perform the reflection and\ncorrection of the initial response. Optionally, the `main_agent_worker`\ncan be skipped if none is provided. In this case, the users input query\nwill be assumed to contain the original response that needs to go thru\nreflection and correction.\n\n**Attributes:**\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `reflective_agent_worker` | `BaseAgentWorker` | Reflective agent responsible for<br>performing reflection and correction of the initial response. |\n| `main_agent_worker` | `Optional[BaseAgentWorker]` | Main agent responsible<br>for generating an initial response to the user query. Defaults to None.<br>If None, the user input is assumed as the initial response. |\n| `verbose` | `bool` | Whether execution should be verbose. Defaults to False. |\n| `callback_manager` | `Optional[CallbackManager]` | Callback manager. Defaults to None. |\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>``` | ```<br>class IntrospectiveAgentWorker(BaseAgentWorker):<br>    \"\"\"Introspective Agent Worker.<br>    This agent worker implements the Reflection AI agentic pattern. It does<br>    so by merely delegating the work to two other agents in a purely<br>    deterministic fashion.<br>    The task this agent performs (again via delegation) is to generate a response<br>    to a query and perform reflection and correction on the response. This<br>    agent delegates the task to (optionally) first a `main_agent_worker` that<br>    generates the initial response to the query. This initial response is then<br>    passed to the `reflective_agent_worker` to perform the reflection and<br>    correction of the initial response. Optionally, the `main_agent_worker`<br>    can be skipped if none is provided. In this case, the users input query<br>    will be assumed to contain the original response that needs to go thru<br>    reflection and correction.<br>    Attributes:<br>        reflective_agent_worker (BaseAgentWorker): Reflective agent responsible for<br>            performing reflection and correction of the initial response.<br>        main_agent_worker (Optional[BaseAgentWorker], optional): Main agent responsible<br>            for generating an initial response to the user query. Defaults to None.<br>            If None, the user input is assumed as the initial response.<br>        verbose (bool, optional): Whether execution should be verbose. Defaults to False.<br>        callback_manager (Optional[CallbackManager], optional): Callback manager. Defaults to None.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        reflective_agent_worker: BaseAgentWorker,<br>        main_agent_worker: Optional[BaseAgentWorker] = None,<br>        verbose: bool = False,<br>        callback_manager: Optional[CallbackManager] = None,<br>    ) -> None:<br>        \"\"\"Init params.\"\"\"<br>        self._verbose = verbose<br>        self._main_agent_worker = main_agent_worker<br>        self._reflective_agent_worker = reflective_agent_worker<br>        self.callback_manager = callback_manager or CallbackManager([])<br>    @classmethod<br>    def from_defaults(<br>        cls,<br>        reflective_agent_worker: BaseAgentWorker,<br>        main_agent_worker: Optional[BaseAgentWorker] = None,<br>        verbose: bool = False,<br>        callback_manager: Optional[CallbackManager] = None,<br>        **kwargs: Any,<br>    ) -> \"IntrospectiveAgentWorker\":<br>        \"\"\"Create an IntrospectiveAgentWorker from args.<br>        Similar to `from_defaults` in other classes, this method will<br>        infer defaults for a variety of parameters, including the LLM,<br>        if they are not specified.<br>        \"\"\"<br>        return cls(<br>            main_agent_worker=main_agent_worker,<br>            reflective_agent_worker=reflective_agent_worker,<br>            verbose=verbose,<br>            callback_manager=callback_manager,<br>            **kwargs,<br>        )<br>    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>        \"\"\"Initialize step from task.\"\"\"<br>        # temporary memory for new messages<br>        main_memory = ChatMemoryBuffer.from_defaults()<br>        reflective_memory = ChatMemoryBuffer.from_defaults()<br>        # put current history in new memory<br>        messages = task.memory.get()<br>        for message in messages:<br>            main_memory.put(message)<br>        # initialize task state<br>        task_state = {<br>            \"main\": {<br>                \"memory\": main_memory,<br>                \"sources\": [],<br>            },<br>            \"reflection\": {\"memory\": reflective_memory, \"sources\": []},<br>        }<br>        task.extra_state.update(task_state)<br>        return TaskStep(<br>            task_id=task.task_id,<br>            step_id=str(uuid.uuid4()),<br>            input=task.input,<br>        )<br>    def get_all_messages(self, task: Task) -> List[ChatMessage]:<br>        return (<br>            +task.memory.get()<br>            + task.extra_state[\"main\"][\"memory\"].get_all()<br>            + task.extra_state[\"reflection\"][\"memory\"].get_all()<br>        )<br>    @trace_method(\"run_step\")<br>    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        # run main agent<br>        if self._main_agent_worker is not None:<br>            main_agent_messages = task.extra_state[\"main\"][\"memory\"].get()<br>            main_agent = self._main_agent_worker.as_agent(<br>                chat_history=main_agent_messages<br>            )<br>            main_agent_response = main_agent.chat(task.input)<br>            original_response = main_agent_response.response<br>            task.extra_state[\"main\"][\"sources\"] = main_agent_response.sources<br>            task.extra_state[\"main\"][\"memory\"] = main_agent.memory<br>        else:<br>            pass<br>        # run reflective agent<br>        reflective_agent_messages = task.extra_state[\"main\"][\"memory\"].get()<br>        reflective_agent = self._reflective_agent_worker.as_agent(<br>            chat_history=reflective_agent_messages<br>        )<br>        # NOTE: atm you *need* to pass an input string to `chat`, even if the memory is already<br>        # preloaded. Input will be concatenated on top of chat history from memory<br>        # which will be used to generate the response.<br>        # TODO: make agent interface more flexible<br>        reflective_agent_response = reflective_agent.chat(original_response)<br>        task.extra_state[\"reflection\"][\"sources\"] = reflective_agent_response.sources<br>        task.extra_state[\"reflection\"][\"memory\"] = reflective_agent.memory<br>        agent_response = AgentChatResponse(<br>            response=str(reflective_agent_response.response),<br>            sources=task.extra_state[\"main\"][\"sources\"]<br>            + task.extra_state[\"reflection\"][\"sources\"],<br>        )<br>        return TaskStepOutput(<br>            output=agent_response,<br>            task_step=step,<br>            is_last=True,<br>            next_steps=[],<br>        )<br>    @trace_method(\"run_step\")<br>    async def arun_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        # run main agent if one is supplied otherwise assume user input<br>        # is the original response to be reflected on and subsequently corrected<br>        if self._main_agent_worker is not None:<br>            main_agent_messages = task.extra_state[\"main\"][\"memory\"].get()<br>            main_agent = self._main_agent_worker.as_agent(<br>                chat_history=main_agent_messages, verbose=self._verbose<br>            )<br>            main_agent_response = await main_agent.achat(task.input)<br>            original_response = main_agent_response.response<br>            task.extra_state[\"main\"][\"sources\"] = main_agent_response.sources<br>            task.extra_state[\"main\"][\"memory\"] = main_agent.memory<br>        else:<br>            add_user_step_to_memory(<br>                step, task.extra_state[\"main\"][\"memory\"], verbose=self._verbose<br>            )<br>            original_response = step.input<br>            # fictitious agent's initial response (to get reflection/correction cycle started)<br>            task.extra_state[\"main\"][\"memory\"].put(<br>                ChatMessage(content=original_response, role=\"assistant\")<br>            )<br>        # run reflective agent<br>        reflective_agent_messages = task.extra_state[\"main\"][\"memory\"].get()<br>        reflective_agent = self._reflective_agent_worker.as_agent(<br>            chat_history=reflective_agent_messages, verbose=self._verbose<br>        )<br>        reflective_agent_response = await reflective_agent.achat(original_response)<br>        task.extra_state[\"reflection\"][\"sources\"] = reflective_agent_response.sources<br>        task.extra_state[\"reflection\"][\"memory\"] = reflective_agent.memory<br>        agent_response = AgentChatResponse(<br>            response=str(reflective_agent_response.response),<br>            sources=task.extra_state[\"main\"][\"sources\"]<br>            + task.extra_state[\"reflection\"][\"sources\"],<br>        )<br>        return TaskStepOutput(<br>            output=agent_response,<br>            task_step=step,<br>            is_last=True,<br>            next_steps=[],<br>        )<br>    @trace_method(\"run_step\")<br>    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        raise NotImplementedError(\"Stream not supported for introspective agent\")<br>    @trace_method(\"run_step\")<br>    async def astream_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async stream).\"\"\"<br>        raise NotImplementedError(\"Stream not supported for introspective agent\")<br>    def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>        \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>        # add new messages to memory<br>        main_memory = task.extra_state[\"main\"][<br>            \"memory\"<br>        ].get_all()  # contains initial response as final message<br>        final_corrected_message = task.extra_state[\"reflection\"][\"memory\"].get_all()[-1]<br>        # swap main workers response with the reflected/corrected one<br>        finalized_task_memory = main_memory[:-1] + [final_corrected_message]<br>        task.memory.set(finalized_task_memory)<br>``` |\n\n### from\\_defaults`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/introspective/\\#llama_index.agent.introspective.IntrospectiveAgentWorker.from_defaults \"Permanent link\")\n\n```\nfrom_defaults(reflective_agent_worker: BaseAgentWorker, main_agent_worker: Optional[BaseAgentWorker] = None, verbose: bool = False, callback_manager: Optional[CallbackManager] = None, **kwargs: Any) -> IntrospectiveAgentWorker\n\n```\n\nCreate an IntrospectiveAgentWorker from args.\n\nSimilar to `from_defaults` in other classes, this method will\ninfer defaults for a variety of parameters, including the LLM,\nif they are not specified.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>``` | ```<br>@classmethod<br>def from_defaults(<br>    cls,<br>    reflective_agent_worker: BaseAgentWorker,<br>    main_agent_worker: Optional[BaseAgentWorker] = None,<br>    verbose: bool = False,<br>    callback_manager: Optional[CallbackManager] = None,<br>    **kwargs: Any,<br>) -> \"IntrospectiveAgentWorker\":<br>    \"\"\"Create an IntrospectiveAgentWorker from args.<br>    Similar to `from_defaults` in other classes, this method will<br>    infer defaults for a variety of parameters, including the LLM,<br>    if they are not specified.<br>    \"\"\"<br>    return cls(<br>        main_agent_worker=main_agent_worker,<br>        reflective_agent_worker=reflective_agent_worker,<br>        verbose=verbose,<br>        callback_manager=callback_manager,<br>        **kwargs,<br>    )<br>``` |\n\n### initialize\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/introspective/\\#llama_index.agent.introspective.IntrospectiveAgentWorker.initialize_step \"Permanent link\")\n\n```\ninitialize_step(task: Task, **kwargs: Any) -> TaskStep\n\n```\n\nInitialize step from task.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>``` | ```<br>def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>    \"\"\"Initialize step from task.\"\"\"<br>    # temporary memory for new messages<br>    main_memory = ChatMemoryBuffer.from_defaults()<br>    reflective_memory = ChatMemoryBuffer.from_defaults()<br>    # put current history in new memory<br>    messages = task.memory.get()<br>    for message in messages:<br>        main_memory.put(message)<br>    # initialize task state<br>    task_state = {<br>        \"main\": {<br>            \"memory\": main_memory,<br>            \"sources\": [],<br>        },<br>        \"reflection\": {\"memory\": reflective_memory, \"sources\": []},<br>    }<br>    task.extra_state.update(task_state)<br>    return TaskStep(<br>        task_id=task.task_id,<br>        step_id=str(uuid.uuid4()),<br>        input=task.input,<br>    )<br>``` |\n\n### run\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/introspective/\\#llama_index.agent.introspective.IntrospectiveAgentWorker.run_step \"Permanent link\")\n\n```\nrun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>``` | ```<br>@trace_method(\"run_step\")<br>def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step.\"\"\"<br>    # run main agent<br>    if self._main_agent_worker is not None:<br>        main_agent_messages = task.extra_state[\"main\"][\"memory\"].get()<br>        main_agent = self._main_agent_worker.as_agent(<br>            chat_history=main_agent_messages<br>        )<br>        main_agent_response = main_agent.chat(task.input)<br>        original_response = main_agent_response.response<br>        task.extra_state[\"main\"][\"sources\"] = main_agent_response.sources<br>        task.extra_state[\"main\"][\"memory\"] = main_agent.memory<br>    else:<br>        pass<br>    # run reflective agent<br>    reflective_agent_messages = task.extra_state[\"main\"][\"memory\"].get()<br>    reflective_agent = self._reflective_agent_worker.as_agent(<br>        chat_history=reflective_agent_messages<br>    )<br>    # NOTE: atm you *need* to pass an input string to `chat`, even if the memory is already<br>    # preloaded. Input will be concatenated on top of chat history from memory<br>    # which will be used to generate the response.<br>    # TODO: make agent interface more flexible<br>    reflective_agent_response = reflective_agent.chat(original_response)<br>    task.extra_state[\"reflection\"][\"sources\"] = reflective_agent_response.sources<br>    task.extra_state[\"reflection\"][\"memory\"] = reflective_agent.memory<br>    agent_response = AgentChatResponse(<br>        response=str(reflective_agent_response.response),<br>        sources=task.extra_state[\"main\"][\"sources\"]<br>        + task.extra_state[\"reflection\"][\"sources\"],<br>    )<br>    return TaskStepOutput(<br>        output=agent_response,<br>        task_step=step,<br>        is_last=True,<br>        next_steps=[],<br>    )<br>``` |\n\n### arun\\_step`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/introspective/\\#llama_index.agent.introspective.IntrospectiveAgentWorker.arun_step \"Permanent link\")\n\n```\narun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async).\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>``` | ```<br>@trace_method(\"run_step\")<br>async def arun_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async).\"\"\"<br>    # run main agent if one is supplied otherwise assume user input<br>    # is the original response to be reflected on and subsequently corrected<br>    if self._main_agent_worker is not None:<br>        main_agent_messages = task.extra_state[\"main\"][\"memory\"].get()<br>        main_agent = self._main_agent_worker.as_agent(<br>            chat_history=main_agent_messages, verbose=self._verbose<br>        )<br>        main_agent_response = await main_agent.achat(task.input)<br>        original_response = main_agent_response.response<br>        task.extra_state[\"main\"][\"sources\"] = main_agent_response.sources<br>        task.extra_state[\"main\"][\"memory\"] = main_agent.memory<br>    else:<br>        add_user_step_to_memory(<br>            step, task.extra_state[\"main\"][\"memory\"], verbose=self._verbose<br>        )<br>        original_response = step.input<br>        # fictitious agent's initial response (to get reflection/correction cycle started)<br>        task.extra_state[\"main\"][\"memory\"].put(<br>            ChatMessage(content=original_response, role=\"assistant\")<br>        )<br>    # run reflective agent<br>    reflective_agent_messages = task.extra_state[\"main\"][\"memory\"].get()<br>    reflective_agent = self._reflective_agent_worker.as_agent(<br>        chat_history=reflective_agent_messages, verbose=self._verbose<br>    )<br>    reflective_agent_response = await reflective_agent.achat(original_response)<br>    task.extra_state[\"reflection\"][\"sources\"] = reflective_agent_response.sources<br>    task.extra_state[\"reflection\"][\"memory\"] = reflective_agent.memory<br>    agent_response = AgentChatResponse(<br>        response=str(reflective_agent_response.response),<br>        sources=task.extra_state[\"main\"][\"sources\"]<br>        + task.extra_state[\"reflection\"][\"sources\"],<br>    )<br>    return TaskStepOutput(<br>        output=agent_response,<br>        task_step=step,<br>        is_last=True,<br>        next_steps=[],<br>    )<br>``` |\n\n### stream\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/introspective/\\#llama_index.agent.introspective.IntrospectiveAgentWorker.stream_step \"Permanent link\")\n\n```\nstream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (stream).\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>214<br>215<br>216<br>217<br>``` | ```<br>@trace_method(\"run_step\")<br>def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step (stream).\"\"\"<br>    raise NotImplementedError(\"Stream not supported for introspective agent\")<br>``` |\n\n### astream\\_step`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/introspective/\\#llama_index.agent.introspective.IntrospectiveAgentWorker.astream_step \"Permanent link\")\n\n```\nastream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async stream).\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>219<br>220<br>221<br>222<br>223<br>224<br>``` | ```<br>@trace_method(\"run_step\")<br>async def astream_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async stream).\"\"\"<br>    raise NotImplementedError(\"Stream not supported for introspective agent\")<br>``` |\n\n### finalize\\_task [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/introspective/\\#llama_index.agent.introspective.IntrospectiveAgentWorker.finalize_task \"Permanent link\")\n\n```\nfinalize_task(task: Task, **kwargs: Any) -> None\n\n```\n\nFinalize task, after all the steps are completed.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>``` | ```<br>def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>    \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>    # add new messages to memory<br>    main_memory = task.extra_state[\"main\"][<br>        \"memory\"<br>    ].get_all()  # contains initial response as final message<br>    final_corrected_message = task.extra_state[\"reflection\"][\"memory\"].get_all()[-1]<br>    # swap main workers response with the reflected/corrected one<br>    finalized_task_memory = main_memory[:-1] + [final_corrected_message]<br>    task.memory.set(finalized_task_memory)<br>``` |\n\n## SelfReflectionAgentWorker [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/introspective/\\#llama_index.agent.introspective.SelfReflectionAgentWorker \"Permanent link\")\n\nBases: `BaseModel`, `BaseAgentWorker`\n\nSelf Reflection Agent Worker.\n\nThis agent performs a reflection without any tools on a given response\nand subsequently performs correction. It should be noted that this reflection\nimplementation has been inspired by two works:\n\n1. Reflexion: Language Agents with Verbal Reinforcement Learning, by Shinn et al. (2023)\n    (https://arxiv.org/pdf/2303.11366.pdf)\n2. CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing, by Gou et al. (2024)\n    (https://arxiv.org/pdf/2305.11738.pdf)\n\nThis agent performs cycles of reflection and correction on an initial response\nuntil a satisfactory correction has been generated or a max number of cycles\nhas been reached. To perform reflection, this agent utilizes a user-specified\nLLM along with a PydanticProgram (thru structured\\_predict) to generate a structured\noutput that contains an LLM generated reflection of the current response. After reflection,\nthe same user-specified LLM is used again but this time with another PydanticProgram\nto generate a structured output that contains an LLM generated corrected\nversion of the current response against the priorly generated reflection.\n\nAttr\n\nmax\\_iterations (int, optional): The max number of reflection & correction.\nDefaults to DEFAULT\\_MAX\\_ITERATIONS.\ncallback\\_manager (Optional\\[CallbackManager\\], optional): Callback manager.\nDefaults to None.\nllm (Optional\\[LLM\\], optional): The LLM used to perform reflection and correction.\nMust be an OpenAI LLM at this time. Defaults to None.\nverbose (bool, optional): Whether execution should be verbose. Defaults to False.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/reflective/self_reflection.py`\n\n|     |     |\n| --- | --- |\n| ```<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>472<br>473<br>``` | ```<br>class SelfReflectionAgentWorker(BaseModel, BaseAgentWorker):<br>    \"\"\"Self Reflection Agent Worker.<br>    This agent performs a reflection without any tools on a given response<br>    and subsequently performs correction. It should be noted that this reflection<br>    implementation has been inspired by two works:<br>    1. Reflexion: Language Agents with Verbal Reinforcement Learning, by Shinn et al. (2023)<br>        (https://arxiv.org/pdf/2303.11366.pdf)<br>    2. CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing, by Gou et al. (2024)<br>       (https://arxiv.org/pdf/2305.11738.pdf)<br>    This agent performs cycles of reflection and correction on an initial response<br>    until a satisfactory correction has been generated or a max number of cycles<br>    has been reached. To perform reflection, this agent utilizes a user-specified<br>    LLM along with a PydanticProgram (thru structured_predict) to generate a structured<br>    output that contains an LLM generated reflection of the current response. After reflection,<br>    the same user-specified LLM is used again but this time with another PydanticProgram<br>    to generate a structured output that contains an LLM generated corrected<br>    version of the current response against the priorly generated reflection.<br>    Attr:<br>        max_iterations (int, optional): The max number of reflection & correction.<br>            Defaults to DEFAULT_MAX_ITERATIONS.<br>        callback_manager (Optional[CallbackManager], optional): Callback manager.<br>            Defaults to None.<br>        llm (Optional[LLM], optional): The LLM used to perform reflection and correction.<br>            Must be an OpenAI LLM at this time. Defaults to None.<br>        verbose (bool, optional): Whether execution should be verbose. Defaults to False.<br>    \"\"\"<br>    callback_manager: CallbackManager = Field(default=CallbackManager([]))<br>    max_iterations: int = Field(default=DEFAULT_MAX_ITERATIONS)<br>    _llm: LLM = PrivateAttr()<br>    _verbose: bool = PrivateAttr()<br>    class Config:<br>        arbitrary_types_allowed = True<br>    def __init__(<br>        self,<br>        max_iterations: int = DEFAULT_MAX_ITERATIONS,<br>        callback_manager: Optional[CallbackManager] = None,<br>        llm: Optional[LLM] = None,<br>        verbose: bool = False,<br>        **kwargs: Any,<br>    ) -> None:<br>        \"\"\"__init__.\"\"\"<br>        super().__init__(<br>            callback_manager=callback_manager or CallbackManager([]),<br>            max_iterations=max_iterations,<br>            **kwargs,<br>        )<br>        self._llm = llm<br>        self._verbose = verbose<br>    @classmethod<br>    def from_defaults(<br>        cls,<br>        llm: Optional[LLM] = None,<br>        max_iterations: int = DEFAULT_MAX_ITERATIONS,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        **kwargs: Any,<br>    ) -> \"SelfReflectionAgentWorker\":<br>        \"\"\"Convenience constructor.\"\"\"<br>        if llm is None:<br>            try:<br>                from llama_index.llms.openai import OpenAI<br>            except ImportError:<br>                raise ImportError(<br>                    \"Missing OpenAI LLMs. Please run `pip install llama-index-llms-openai`.\"<br>                )<br>            llm = OpenAI(model=\"gpt-4-turbo-preview\", temperature=0)<br>        return cls(<br>            llm=llm,<br>            max_iterations=max_iterations,<br>            callback_manager=callback_manager,<br>            verbose=verbose,<br>            **kwargs,<br>        )<br>    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>        \"\"\"Initialize step from task.\"\"\"<br>        # temporary memory for new messages<br>        new_memory = ChatMemoryBuffer.from_defaults()<br>        # put current history in new memory<br>        messages = task.memory.get()<br>        for message in messages:<br>            new_memory.put(message)<br>        # inject new input into memory<br>        new_memory.put(ChatMessage(content=task.input, role=MessageRole.USER))<br>        # initialize task state<br>        task_state = {<br>            \"new_memory\": new_memory,<br>            \"sources\": [],<br>        }<br>        task.extra_state.update(task_state)<br>        return TaskStep(<br>            task_id=task.task_id,<br>            step_id=str(uuid.uuid4()),<br>            input=task.input,<br>            step_state={\"count\": 0},<br>        )<br>    def _remove_correction_str_prefix(self, correct_msg: str) -> str:<br>        \"\"\"Helper function to format correction message for final response.\"\"\"<br>        return correct_msg.replace(CORRECT_RESPONSE_PREFIX, \"\")<br>    @dispatcher.span<br>    def _reflect(<br>        self, chat_history: List[ChatMessage]<br>    ) -> Tuple[Reflection, ChatMessage]:<br>        \"\"\"Reflect on the trajectory.\"\"\"<br>        reflection = self._llm.structured_predict(<br>            Reflection,<br>            PromptTemplate(REFLECTION_PROMPT_TEMPLATE),<br>            chat_history=messages_to_prompt(chat_history),<br>        )<br>        if self._verbose:<br>            print(f\"> Reflection: {reflection.model_dump()}\")<br>        # end state: return user message<br>        reflection_output_str = (<br>            f\"Is Done: {reflection.is_done}\\nCritique: {reflection.feedback}\"<br>        )<br>        critique = REFLECTION_RESPONSE_TEMPLATE.format(<br>            reflection_output=reflection_output_str<br>        )<br>        return reflection, ChatMessage.from_str(critique, role=\"user\")<br>    @dispatcher.span<br>    def _correct(self, input_str: str, critique: str) -> ChatMessage:<br>        correction = self._llm.structured_predict(<br>            Correction,<br>            PromptTemplate(CORRECT_PROMPT_TEMPLATE),<br>            input_str=input_str,<br>            feedback=critique,<br>        )<br>        correct_response_str = CORRECT_RESPONSE_FSTRING.format(<br>            correction=correction.correction<br>        )<br>        if self._verbose:<br>            print(f\"Correction: {correction.correction}\", flush=True)<br>        return ChatMessage.from_str(correct_response_str, role=\"assistant\")<br>    @dispatcher.span<br>    @trace_method(\"run_step\")<br>    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        state = step.step_state<br>        state[\"count\"] += 1<br>        # new_memory should at the very least contain the user input<br>        messages = task.extra_state[\"new_memory\"].get()<br>        prev_correct_str = messages[-1].content<br>        prev_correct_str_without_prefix = self._remove_correction_str_prefix(<br>            prev_correct_str<br>        )<br>        # reflect phase<br>        reflection, reflection_msg = self._reflect(chat_history=messages)<br>        is_done = reflection.is_done<br>        critique_msg = ChatMessage(role=MessageRole.USER, content=reflection_msg)<br>        task.extra_state[\"new_memory\"].put(critique_msg)<br>        # correction phase<br>        if is_done:<br>            # no correction to be made prev correction is sufficient<br>            agent_response = AgentChatResponse(<br>                response=prev_correct_str_without_prefix,<br>                sources=task.extra_state[\"sources\"],<br>            )<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(<br>                    role=MessageRole.ASSISTANT,<br>                    content=prev_correct_str_without_prefix,<br>                )<br>            )<br>            new_steps = []<br>        else:<br>            # generate a new correction<br>            correct_msg = self._correct(<br>                input_str=prev_correct_str_without_prefix,<br>                critique=reflection_msg.content,<br>            )<br>            correct_str_without_prefix = self._remove_correction_str_prefix(<br>                correct_msg.content<br>            )<br>            if self.max_iterations == state[\"count\"]:<br>                # this will be the last iteration<br>                task.extra_state[\"new_memory\"].put(<br>                    ChatMessage(<br>                        role=MessageRole.ASSISTANT,<br>                        content=correct_str_without_prefix,<br>                    )<br>                )<br>                agent_response = AgentChatResponse(response=correct_str_without_prefix)<br>                new_steps = []<br>            else:<br>                # another round of reflection/correction will take place<br>                task.extra_state[\"new_memory\"].put(correct_msg)<br>                agent_response = AgentChatResponse(response=str(correct_msg))<br>                new_steps = [<br>                    step.get_next_step(<br>                        step_id=str(uuid.uuid4()),<br>                        # NOTE: input is unused<br>                        input=None,<br>                        step_state=state,<br>                    )<br>                ]<br>        return TaskStepOutput(<br>            output=agent_response,<br>            task_step=step,<br>            is_last=is_done | (self.max_iterations == state[\"count\"]),<br>            next_steps=new_steps,<br>        )<br>    # Async methods<br>    @dispatcher.span<br>    async def _areflect(<br>        self, chat_history: List[ChatMessage]<br>    ) -> Tuple[Reflection, ChatMessage]:<br>        \"\"\"Reflect on the trajectory.\"\"\"<br>        reflection = await self._llm.astructured_predict(<br>            Reflection,<br>            PromptTemplate(REFLECTION_PROMPT_TEMPLATE),<br>            chat_history=messages_to_prompt(chat_history),<br>        )<br>        if self._verbose:<br>            print(f\"> Reflection: {reflection.model_dump()}\")<br>        # end state: return user message<br>        reflection_output_str = (<br>            f\"Is Done: {reflection.is_done}\\nCritique: {reflection.feedback}\"<br>        )<br>        critique = REFLECTION_RESPONSE_TEMPLATE.format(<br>            reflection_output=reflection_output_str<br>        )<br>        return reflection, ChatMessage.from_str(critique, role=\"user\")<br>    @dispatcher.span<br>    async def _acorrect(self, input_str: str, critique: str) -> ChatMessage:<br>        correction = await self._llm.astructured_predict(<br>            Correction,<br>            PromptTemplate(CORRECT_PROMPT_TEMPLATE),<br>            input_str=input_str,<br>            feedback=critique,<br>        )<br>        correct_response_str = CORRECT_RESPONSE_FSTRING.format(<br>            correction=correction.correction<br>        )<br>        if self._verbose:<br>            print(f\"Correction: {correction.correction}\", flush=True)<br>        return ChatMessage.from_str(correct_response_str, role=\"assistant\")<br>    @dispatcher.span<br>    @trace_method(\"run_step\")<br>    async def arun_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        state = step.step_state<br>        state[\"count\"] += 1<br>        messages = task.extra_state[\"new_memory\"].get()<br>        prev_correct_str = messages[-1].content<br>        prev_correct_str_without_prefix = self._remove_correction_str_prefix(<br>            prev_correct_str<br>        )<br>        # reflect<br>        reflection, reflection_msg = await self._areflect(chat_history=messages)<br>        is_done = reflection.is_done<br>        critique_msg = ChatMessage(role=MessageRole.USER, content=reflection_msg)<br>        task.extra_state[\"new_memory\"].put(critique_msg)<br>        # correction phase<br>        if is_done:<br>            # no correction to be made prev correction is sufficient<br>            agent_response = AgentChatResponse(<br>                response=prev_correct_str_without_prefix,<br>                sources=task.extra_state[\"sources\"],<br>            )<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(<br>                    role=MessageRole.ASSISTANT,<br>                    content=prev_correct_str_without_prefix,<br>                )<br>            )<br>            new_steps = []<br>        else:<br>            # generate a new correction<br>            correct_msg = await self._acorrect(<br>                input_str=prev_correct_str_without_prefix,<br>                critique=reflection_msg.content,<br>            )<br>            correct_str_without_prefix = self._remove_correction_str_prefix(<br>                correct_msg.content<br>            )<br>            if self.max_iterations == state[\"count\"]:<br>                # this will be the last iteration<br>                task.extra_state[\"new_memory\"].put(<br>                    ChatMessage(<br>                        role=MessageRole.ASSISTANT,<br>                        content=correct_str_without_prefix,<br>                    )<br>                )<br>                agent_response = AgentChatResponse(response=correct_str_without_prefix)<br>                new_steps = []<br>            else:<br>                # another round of reflection/correction will take place<br>                task.extra_state[\"new_memory\"].put(correct_msg)<br>                agent_response = AgentChatResponse(response=str(correct_msg))<br>                new_steps = [<br>                    step.get_next_step(<br>                        step_id=str(uuid.uuid4()),<br>                        # NOTE: input is unused<br>                        input=None,<br>                        step_state=state,<br>                    )<br>                ]<br>        return TaskStepOutput(<br>            output=agent_response,<br>            task_step=step,<br>            is_last=is_done | (self.max_iterations == state[\"count\"]),<br>            next_steps=new_steps,<br>        )<br>    # Stream methods<br>    @dispatcher.span<br>    @trace_method(\"run_step\")<br>    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        raise NotImplementedError(\"Stream not supported for self reflection agent\")<br>    @dispatcher.span<br>    @trace_method(\"run_step\")<br>    async def astream_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async stream).\"\"\"<br>        raise NotImplementedError(\"Stream not supported for self reflection agent\")<br>    def get_all_messages(self, task: Task) -> List[ChatMessage]:<br>        return (<br>            self.prefix_messages<br>            + task.memory.get()<br>            + task.extra_state[\"new_memory\"].get_all()<br>        )<br>    def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>        \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>        # add new messages to memory<br>        task.memory.set(task.extra_state[\"new_memory\"].get_all())<br>        # reset new memory<br>        task.extra_state[\"new_memory\"].reset()<br>``` |\n\n### from\\_defaults`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/introspective/\\#llama_index.agent.introspective.SelfReflectionAgentWorker.from_defaults \"Permanent link\")\n\n```\nfrom_defaults(llm: Optional[LLM] = None, max_iterations: int = DEFAULT_MAX_ITERATIONS, callback_manager: Optional[CallbackManager] = None, verbose: bool = False, **kwargs: Any) -> SelfReflectionAgentWorker\n\n```\n\nConvenience constructor.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/reflective/self_reflection.py`\n\n|     |     |\n| --- | --- |\n| ```<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>``` | ```<br>@classmethod<br>def from_defaults(<br>    cls,<br>    llm: Optional[LLM] = None,<br>    max_iterations: int = DEFAULT_MAX_ITERATIONS,<br>    callback_manager: Optional[CallbackManager] = None,<br>    verbose: bool = False,<br>    **kwargs: Any,<br>) -> \"SelfReflectionAgentWorker\":<br>    \"\"\"Convenience constructor.\"\"\"<br>    if llm is None:<br>        try:<br>            from llama_index.llms.openai import OpenAI<br>        except ImportError:<br>            raise ImportError(<br>                \"Missing OpenAI LLMs. Please run `pip install llama-index-llms-openai`.\"<br>            )<br>        llm = OpenAI(model=\"gpt-4-turbo-preview\", temperature=0)<br>    return cls(<br>        llm=llm,<br>        max_iterations=max_iterations,<br>        callback_manager=callback_manager,<br>        verbose=verbose,<br>        **kwargs,<br>    )<br>``` |\n\n### initialize\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/introspective/\\#llama_index.agent.introspective.SelfReflectionAgentWorker.initialize_step \"Permanent link\")\n\n```\ninitialize_step(task: Task, **kwargs: Any) -> TaskStep\n\n```\n\nInitialize step from task.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/reflective/self_reflection.py`\n\n|     |     |\n| --- | --- |\n| ```<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>``` | ```<br>def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>    \"\"\"Initialize step from task.\"\"\"<br>    # temporary memory for new messages<br>    new_memory = ChatMemoryBuffer.from_defaults()<br>    # put current history in new memory<br>    messages = task.memory.get()<br>    for message in messages:<br>        new_memory.put(message)<br>    # inject new input into memory<br>    new_memory.put(ChatMessage(content=task.input, role=MessageRole.USER))<br>    # initialize task state<br>    task_state = {<br>        \"new_memory\": new_memory,<br>        \"sources\": [],<br>    }<br>    task.extra_state.update(task_state)<br>    return TaskStep(<br>        task_id=task.task_id,<br>        step_id=str(uuid.uuid4()),<br>        input=task.input,<br>        step_state={\"count\": 0},<br>    )<br>``` |\n\n### run\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/introspective/\\#llama_index.agent.introspective.SelfReflectionAgentWorker.run_step \"Permanent link\")\n\n```\nrun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/reflective/self_reflection.py`\n\n|     |     |\n| --- | --- |\n| ```<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>``` | ```<br>@dispatcher.span<br>@trace_method(\"run_step\")<br>def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step.\"\"\"<br>    state = step.step_state<br>    state[\"count\"] += 1<br>    # new_memory should at the very least contain the user input<br>    messages = task.extra_state[\"new_memory\"].get()<br>    prev_correct_str = messages[-1].content<br>    prev_correct_str_without_prefix = self._remove_correction_str_prefix(<br>        prev_correct_str<br>    )<br>    # reflect phase<br>    reflection, reflection_msg = self._reflect(chat_history=messages)<br>    is_done = reflection.is_done<br>    critique_msg = ChatMessage(role=MessageRole.USER, content=reflection_msg)<br>    task.extra_state[\"new_memory\"].put(critique_msg)<br>    # correction phase<br>    if is_done:<br>        # no correction to be made prev correction is sufficient<br>        agent_response = AgentChatResponse(<br>            response=prev_correct_str_without_prefix,<br>            sources=task.extra_state[\"sources\"],<br>        )<br>        task.extra_state[\"new_memory\"].put(<br>            ChatMessage(<br>                role=MessageRole.ASSISTANT,<br>                content=prev_correct_str_without_prefix,<br>            )<br>        )<br>        new_steps = []<br>    else:<br>        # generate a new correction<br>        correct_msg = self._correct(<br>            input_str=prev_correct_str_without_prefix,<br>            critique=reflection_msg.content,<br>        )<br>        correct_str_without_prefix = self._remove_correction_str_prefix(<br>            correct_msg.content<br>        )<br>        if self.max_iterations == state[\"count\"]:<br>            # this will be the last iteration<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(<br>                    role=MessageRole.ASSISTANT,<br>                    content=correct_str_without_prefix,<br>                )<br>            )<br>            agent_response = AgentChatResponse(response=correct_str_without_prefix)<br>            new_steps = []<br>        else:<br>            # another round of reflection/correction will take place<br>            task.extra_state[\"new_memory\"].put(correct_msg)<br>            agent_response = AgentChatResponse(response=str(correct_msg))<br>            new_steps = [<br>                step.get_next_step(<br>                    step_id=str(uuid.uuid4()),<br>                    # NOTE: input is unused<br>                    input=None,<br>                    step_state=state,<br>                )<br>            ]<br>    return TaskStepOutput(<br>        output=agent_response,<br>        task_step=step,<br>        is_last=is_done | (self.max_iterations == state[\"count\"]),<br>        next_steps=new_steps,<br>    )<br>``` |\n\n### arun\\_step`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/introspective/\\#llama_index.agent.introspective.SelfReflectionAgentWorker.arun_step \"Permanent link\")\n\n```\narun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async).\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/reflective/self_reflection.py`\n\n|     |     |\n| --- | --- |\n| ```<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>``` | ```<br>@dispatcher.span<br>@trace_method(\"run_step\")<br>async def arun_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async).\"\"\"<br>    state = step.step_state<br>    state[\"count\"] += 1<br>    messages = task.extra_state[\"new_memory\"].get()<br>    prev_correct_str = messages[-1].content<br>    prev_correct_str_without_prefix = self._remove_correction_str_prefix(<br>        prev_correct_str<br>    )<br>    # reflect<br>    reflection, reflection_msg = await self._areflect(chat_history=messages)<br>    is_done = reflection.is_done<br>    critique_msg = ChatMessage(role=MessageRole.USER, content=reflection_msg)<br>    task.extra_state[\"new_memory\"].put(critique_msg)<br>    # correction phase<br>    if is_done:<br>        # no correction to be made prev correction is sufficient<br>        agent_response = AgentChatResponse(<br>            response=prev_correct_str_without_prefix,<br>            sources=task.extra_state[\"sources\"],<br>        )<br>        task.extra_state[\"new_memory\"].put(<br>            ChatMessage(<br>                role=MessageRole.ASSISTANT,<br>                content=prev_correct_str_without_prefix,<br>            )<br>        )<br>        new_steps = []<br>    else:<br>        # generate a new correction<br>        correct_msg = await self._acorrect(<br>            input_str=prev_correct_str_without_prefix,<br>            critique=reflection_msg.content,<br>        )<br>        correct_str_without_prefix = self._remove_correction_str_prefix(<br>            correct_msg.content<br>        )<br>        if self.max_iterations == state[\"count\"]:<br>            # this will be the last iteration<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(<br>                    role=MessageRole.ASSISTANT,<br>                    content=correct_str_without_prefix,<br>                )<br>            )<br>            agent_response = AgentChatResponse(response=correct_str_without_prefix)<br>            new_steps = []<br>        else:<br>            # another round of reflection/correction will take place<br>            task.extra_state[\"new_memory\"].put(correct_msg)<br>            agent_response = AgentChatResponse(response=str(correct_msg))<br>            new_steps = [<br>                step.get_next_step(<br>                    step_id=str(uuid.uuid4()),<br>                    # NOTE: input is unused<br>                    input=None,<br>                    step_state=state,<br>                )<br>            ]<br>    return TaskStepOutput(<br>        output=agent_response,<br>        task_step=step,<br>        is_last=is_done | (self.max_iterations == state[\"count\"]),<br>        next_steps=new_steps,<br>    )<br>``` |\n\n### stream\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/introspective/\\#llama_index.agent.introspective.SelfReflectionAgentWorker.stream_step \"Permanent link\")\n\n```\nstream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (stream).\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/reflective/self_reflection.py`\n\n|     |     |\n| --- | --- |\n| ```<br>447<br>448<br>449<br>450<br>451<br>``` | ```<br>@dispatcher.span<br>@trace_method(\"run_step\")<br>def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step (stream).\"\"\"<br>    raise NotImplementedError(\"Stream not supported for self reflection agent\")<br>``` |\n\n### astream\\_step`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/introspective/\\#llama_index.agent.introspective.SelfReflectionAgentWorker.astream_step \"Permanent link\")\n\n```\nastream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async stream).\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/reflective/self_reflection.py`\n\n|     |     |\n| --- | --- |\n| ```<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>``` | ```<br>@dispatcher.span<br>@trace_method(\"run_step\")<br>async def astream_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async stream).\"\"\"<br>    raise NotImplementedError(\"Stream not supported for self reflection agent\")<br>``` |\n\n### finalize\\_task [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/introspective/\\#llama_index.agent.introspective.SelfReflectionAgentWorker.finalize_task \"Permanent link\")\n\n```\nfinalize_task(task: Task, **kwargs: Any) -> None\n\n```\n\nFinalize task, after all the steps are completed.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/reflective/self_reflection.py`\n\n|     |     |\n| --- | --- |\n| ```<br>468<br>469<br>470<br>471<br>472<br>473<br>``` | ```<br>def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>    \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>    # add new messages to memory<br>    task.memory.set(task.extra_state[\"new_memory\"].get_all())<br>    # reset new memory<br>    task.extra_state[\"new_memory\"].reset()<br>``` |\n\n## ToolInteractiveReflectionAgentWorker [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/introspective/\\#llama_index.agent.introspective.ToolInteractiveReflectionAgentWorker \"Permanent link\")\n\nBases: `BaseModel`, `BaseAgentWorker`\n\nTool-Interactive Reflection Agent Worker.\n\nThis agent worker implements the CRITIC reflection framework introduced\nby Gou, Zhibin, et al. (2024) ICLR. (source: https://arxiv.org/pdf/2305.11738)\n\nCRITIC stands for `Correcting with tool-interactive critiquing`. It works\nby performing a reflection on a response to a task/query using external tools\n(e.g., fact checking using a Google search tool) and subsequently using\nthe critique to generate a corrected response. It cycles thru tool-interactive\nreflection and correction until a specific stopping criteria has been met\nor a max number of iterations has been reached.\n\nThis agent delegates the critique subtask to a user-supplied `critique_agent_worker`\nthat is of `FunctionCallingAgentWorker` type i.e. it uses tools to perform\ntasks. For correction, it uses a user-specified `correction_llm` with a\nPydanticProgram (determined dynamically with llm.structured\\_predict)\nin order to produce a structured output, namely `Correction` that\ncontains the correction generated by the `correction_llm`.\n\n**Attributes:**\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `critique_agent_worker` | `FunctionCallingAgentWorker` | Critique agent responsible<br>for performing the critique reflection. |\n| `critique_template` | `str` | The template containing instructions for how the<br>Critique agent should perform the reflection. |\n| `max_iterations` | `int` | The max number of reflection & correction<br>cycles permitted. Defaults to DEFAULT\\_MAX\\_ITERATIONS = 5. |\n| `stopping_callable` | `Optional[StoppingCallable]` | An optional stopping<br>condition that operates over the critique reflection string and returns<br>a boolean to determine if the latest correction is sufficient. Defaults to None. |\n| `correction_llm` | `Optional[LLM]` | The LLM used for producing corrected<br>responses against a critique or reflection. Defaults to None. |\n| `callback_manager` | `Optional[CallbackManager]` | Callback manager. Defaults to None. |\n| `verbose` | `bool` | Whether execution should be verbose. Defaults to False. |\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/reflective/tool_interactive_reflection.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>``` | ```<br>class ToolInteractiveReflectionAgentWorker(BaseModel, BaseAgentWorker):<br>    \"\"\"Tool-Interactive Reflection Agent Worker.<br>    This agent worker implements the CRITIC reflection framework introduced<br>    by Gou, Zhibin, et al. (2024) ICLR. (source: https://arxiv.org/pdf/2305.11738)<br>    CRITIC stands for `Correcting with tool-interactive critiquing`. It works<br>    by performing a reflection on a response to a task/query using external tools<br>    (e.g., fact checking using a Google search tool) and subsequently using<br>    the critique to generate a corrected response. It cycles thru tool-interactive<br>    reflection and correction until a specific stopping criteria has been met<br>    or a max number of iterations has been reached.<br>    This agent delegates the critique subtask to a user-supplied `critique_agent_worker`<br>    that is of `FunctionCallingAgentWorker` type i.e. it uses tools to perform<br>    tasks. For correction, it uses a user-specified `correction_llm` with a<br>    PydanticProgram (determined dynamically with llm.structured_predict)<br>    in order to produce a structured output, namely `Correction` that<br>    contains the correction generated by the `correction_llm`.<br>    Attributes:<br>        critique_agent_worker (FunctionCallingAgentWorker): Critique agent responsible<br>            for performing the critique reflection.<br>        critique_template (str): The template containing instructions for how the<br>            Critique agent should perform the reflection.<br>        max_iterations (int, optional): The max number of reflection & correction<br>            cycles permitted. Defaults to DEFAULT_MAX_ITERATIONS = 5.<br>        stopping_callable (Optional[StoppingCallable], optional): An optional stopping<br>            condition that operates over the critique reflection string and returns<br>            a boolean to determine if the latest correction is sufficient. Defaults to None.<br>        correction_llm (Optional[LLM], optional): The LLM used for producing corrected<br>            responses against a critique or reflection. Defaults to None.<br>        callback_manager (Optional[CallbackManager], optional): Callback manager. Defaults to None.<br>        verbose (bool, optional): Whether execution should be verbose. Defaults to False.<br>    \"\"\"<br>    callback_manager: CallbackManager = Field(default=CallbackManager([]))<br>    max_iterations: int = Field(default=DEFAULT_MAX_ITERATIONS)<br>    stopping_callable: Optional[StoppingCallable] = Field(<br>        default=None,<br>        description=\"Optional function that operates on critique string to see if no more corrections are needed.\",<br>    )<br>    _critique_agent_worker: FunctionCallingAgentWorker = PrivateAttr()<br>    _critique_template: str = PrivateAttr()<br>    _correction_llm: LLM = PrivateAttr()<br>    _verbose: bool = PrivateAttr()<br>    class Config:<br>        arbitrary_types_allowed = True<br>    def __init__(<br>        self,<br>        critique_agent_worker: FunctionCallingAgentWorker,<br>        critique_template: str,<br>        max_iterations: int = DEFAULT_MAX_ITERATIONS,<br>        stopping_callable: Optional[StoppingCallable] = None,<br>        correction_llm: Optional[LLM] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        **kwargs: Any,<br>    ) -> None:<br>        \"\"\"__init__.\"\"\"<br>        super().__init__(<br>            callback_manager=callback_manager,<br>            max_iterations=max_iterations,<br>            stopping_callable=stopping_callable,<br>            **kwargs,<br>        )<br>        self._critique_agent_worker = critique_agent_worker<br>        self._critique_template = critique_template<br>        self._verbose = verbose<br>        self._correction_llm = correction_llm<br>    @classmethod<br>    def from_defaults(<br>        cls,<br>        critique_agent_worker: FunctionCallingAgentWorker,<br>        critique_template: str,<br>        correction_llm: Optional[LLM] = None,<br>        max_iterations: int = DEFAULT_MAX_ITERATIONS,<br>        stopping_callable: Optional[StoppingCallable] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        **kwargs: Any,<br>    ) -> \"ToolInteractiveReflectionAgentWorker\":<br>        \"\"\"Convenience constructor method from set of of BaseTools (Optional).\"\"\"<br>        if correction_llm is None:<br>            try:<br>                from llama_index.llms.openai import OpenAI<br>            except ImportError:<br>                raise ImportError(<br>                    \"Missing OpenAI LLMs. Please run `pip install llama-index-llms-openai`.\"<br>                )<br>            correction_llm = OpenAI(model=\"gpt-4-turbo-preview\", temperature=0)<br>        return cls(<br>            critique_agent_worker=critique_agent_worker,<br>            critique_template=critique_template,<br>            correction_llm=correction_llm,<br>            max_iterations=max_iterations,<br>            stopping_callable=stopping_callable,<br>            callback_manager=callback_manager or CallbackManager([]),<br>            verbose=verbose,<br>            **kwargs,<br>        )<br>    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>        \"\"\"Initialize step from task.\"\"\"<br>        # temporary memory for new messages<br>        new_memory = ChatMemoryBuffer.from_defaults()<br>        # put current history in new memory<br>        messages = task.memory.get()<br>        for message in messages:<br>            new_memory.put(message)<br>        # inject new input into memory<br>        new_memory.put(ChatMessage(content=task.input, role=MessageRole.USER))<br>        # initialize task state<br>        task_state = {<br>            \"new_memory\": new_memory,<br>            \"sources\": [],<br>        }<br>        task.extra_state.update(task_state)<br>        return TaskStep(<br>            task_id=task.task_id,<br>            step_id=str(uuid.uuid4()),<br>            input=task.input,<br>            step_state={\"count\": 0},<br>        )<br>    def _remove_correction_str_prefix(self, correct_msg: str) -> str:<br>        \"\"\"Helper function to format correction message for final response.\"\"\"<br>        return correct_msg.replace(CORRECT_RESPONSE_PREFIX, \"\")<br>    @dispatcher.span<br>    def _critique(self, input_str: str) -> AgentChatResponse:<br>        agent = self._critique_agent_worker.as_agent(verbose=self._verbose)<br>        critique = agent.chat(self._critique_template.format(input_str=input_str))<br>        if self._verbose:<br>            print(f\"Critique: {critique.response}\", flush=True)<br>        return critique<br>    @dispatcher.span<br>    def _correct(self, input_str: str, critique: str) -> ChatMessage:<br>        correction = self._correction_llm.structured_predict(<br>            Correction,<br>            PromptTemplate(CORRECT_PROMPT_TEMPLATE),<br>            input_str=input_str,<br>            critique=critique,<br>        )<br>        correct_response_str = CORRECT_RESPONSE_FSTRING.format(<br>            correction=correction.correction<br>        )<br>        if self._verbose:<br>            print(f\"Correction: {correction.correction}\", flush=True)<br>        return ChatMessage.from_str(correct_response_str, role=\"assistant\")<br>    @dispatcher.span<br>    @trace_method(\"run_step\")<br>    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        state = step.step_state<br>        state[\"count\"] += 1<br>        messages = task.extra_state[\"new_memory\"].get()<br>        prev_correct_str = messages[-1].content<br>        prev_correct_str_without_prefix = self._remove_correction_str_prefix(<br>            prev_correct_str<br>        )<br>        # critique phase<br>        critique_response = self._critique(input_str=prev_correct_str_without_prefix)<br>        task.extra_state[\"sources\"].extend(critique_response.sources)<br>        is_done = False<br>        if self.stopping_callable:<br>            is_done = self.stopping_callable(critique_str=critique_response.response)<br>        critique_msg = ChatMessage(<br>            role=MessageRole.USER, content=critique_response.response<br>        )<br>        task.extra_state[\"new_memory\"].put(critique_msg)<br>        # correction phase<br>        if is_done:<br>            # no correction to be made prev correction is sufficient<br>            agent_response = AgentChatResponse(<br>                response=prev_correct_str_without_prefix,<br>                sources=task.extra_state[\"sources\"],<br>            )<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(<br>                    role=MessageRole.ASSISTANT, content=prev_correct_str_without_prefix<br>                )<br>            )<br>            new_steps = []<br>        else:<br>            # generate a new correction<br>            correct_msg = self._correct(<br>                input_str=prev_correct_str_without_prefix,<br>                critique=critique_response.response,<br>            )<br>            correct_str_without_prefix = self._remove_correction_str_prefix(<br>                correct_msg.content<br>            )<br>            # reached max iterations, no further reflection/correction cycles<br>            if self.max_iterations == state[\"count\"]:<br>                task.extra_state[\"new_memory\"].put(<br>                    ChatMessage(<br>                        role=MessageRole.ASSISTANT, content=correct_str_without_prefix<br>                    )<br>                )<br>                agent_response = AgentChatResponse(response=correct_str_without_prefix)<br>                new_steps = []<br>            else:<br>                # another round of reflection/correction will take place<br>                task.extra_state[\"new_memory\"].put(correct_msg)<br>                agent_response = AgentChatResponse(<br>                    response=str(correct_msg), sources=critique_response.sources<br>                )<br>                new_steps = [<br>                    step.get_next_step(<br>                        step_id=str(uuid.uuid4()),<br>                        # NOTE: input is unused<br>                        input=None,<br>                        step_state=state,<br>                    )<br>                ]<br>        return TaskStepOutput(<br>            output=agent_response,<br>            task_step=step,<br>            is_last=is_done | (self.max_iterations == state[\"count\"]),<br>            next_steps=new_steps,<br>        )<br>    # Async Methods<br>    @dispatcher.span<br>    async def _acritique(self, input_str: str) -> AgentChatResponse:<br>        agent = self._critique_agent_worker.as_agent(verbose=self._verbose)<br>        critique = await agent.achat(<br>            self._critique_template.format(input_str=input_str)<br>        )<br>        if self._verbose:<br>            print(f\"Critique: {critique.response}\", flush=True)<br>        return critique<br>    @dispatcher.span<br>    async def _acorrect(self, input_str: str, critique: str) -> ChatMessage:<br>        correction = await self._correction_llm.astructured_predict(<br>            Correction,<br>            PromptTemplate(CORRECT_PROMPT_TEMPLATE),<br>            input_str=input_str,<br>            critique=critique,<br>        )<br>        correct_response_str = CORRECT_RESPONSE_FSTRING.format(<br>            correction=correction.correction<br>        )<br>        if self._verbose:<br>            print(f\"Correction: {correction.correction}\", flush=True)<br>        return ChatMessage.from_str(correct_response_str, role=\"assistant\")<br>    @dispatcher.span<br>    @trace_method(\"run_step\")<br>    async def arun_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        state = step.step_state<br>        state[\"count\"] += 1<br>        messages = task.extra_state[\"new_memory\"].get()<br>        prev_correct_str = messages[-1].content<br>        prev_correct_str_without_prefix = self._remove_correction_str_prefix(<br>            prev_correct_str<br>        )<br>        # critique phase<br>        critique_response = await self._acritique(<br>            input_str=prev_correct_str_without_prefix<br>        )<br>        task.extra_state[\"sources\"].extend(critique_response.sources)<br>        is_done = False<br>        if self.stopping_callable:<br>            is_done = self.stopping_callable(critique_str=critique_response.response)<br>        critique_msg = ChatMessage(<br>            role=MessageRole.USER, content=critique_response.response<br>        )<br>        task.extra_state[\"new_memory\"].put(critique_msg)<br>        # correction phase<br>        if is_done:<br>            # no correction to be made prev correction is sufficient<br>            agent_response = AgentChatResponse(<br>                response=prev_correct_str_without_prefix,<br>                sources=task.extra_state[\"sources\"],<br>            )<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(<br>                    role=MessageRole.ASSISTANT, content=prev_correct_str_without_prefix<br>                )<br>            )<br>            new_steps = []<br>        else:<br>            # generate a new correction<br>            correct_msg = await self._acorrect(<br>                input_str=prev_correct_str_without_prefix,<br>                critique=critique_response.response,<br>            )<br>            correct_str_without_prefix = self._remove_correction_str_prefix(<br>                correct_msg.content<br>            )<br>            # reached max iterations, no further reflection/correction cycles<br>            if self.max_iterations == state[\"count\"]:<br>                task.extra_state[\"new_memory\"].put(<br>                    ChatMessage(<br>                        role=MessageRole.ASSISTANT, content=correct_str_without_prefix<br>                    )<br>                )<br>                agent_response = AgentChatResponse(response=correct_str_without_prefix)<br>                new_steps = []<br>            else:<br>                # another round of reflection/correction will take place<br>                task.extra_state[\"new_memory\"].put(correct_msg)<br>                agent_response = AgentChatResponse(<br>                    response=str(correct_msg), sources=critique_response.sources<br>                )<br>                new_steps = [<br>                    step.get_next_step(<br>                        step_id=str(uuid.uuid4()),<br>                        # NOTE: input is unused<br>                        input=None,<br>                        step_state=state,<br>                    )<br>                ]<br>        return TaskStepOutput(<br>            output=agent_response,<br>            task_step=step,<br>            is_last=is_done | (self.max_iterations == state[\"count\"]),<br>            next_steps=new_steps,<br>        )<br>    # Steam methods<br>    @dispatcher.span<br>    @trace_method(\"run_step\")<br>    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        raise NotImplementedError(<br>            \"Stream not supported for tool-interactive reflection agent\"<br>        )<br>    @dispatcher.span<br>    @trace_method(\"run_step\")<br>    async def astream_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async stream).\"\"\"<br>        raise NotImplementedError(<br>            \"Stream not supported for tool-interactive reflection agent\"<br>        )<br>    def get_all_messages(self, task: Task) -> List[ChatMessage]:<br>        return (<br>            self.prefix_messages<br>            + task.memory.get()<br>            + task.extra_state[\"new_memory\"].get_all()<br>        )<br>    def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>        \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>        # add new messages to memory<br>        task.memory.set(task.extra_state[\"new_memory\"].get_all())<br>        # reset new memory<br>        task.extra_state[\"new_memory\"].reset()<br>``` |\n\n### from\\_defaults`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/introspective/\\#llama_index.agent.introspective.ToolInteractiveReflectionAgentWorker.from_defaults \"Permanent link\")\n\n```\nfrom_defaults(critique_agent_worker: FunctionCallingAgentWorker, critique_template: str, correction_llm: Optional[LLM] = None, max_iterations: int = DEFAULT_MAX_ITERATIONS, stopping_callable: Optional[StoppingCallable] = None, callback_manager: Optional[CallbackManager] = None, verbose: bool = False, **kwargs: Any) -> ToolInteractiveReflectionAgentWorker\n\n```\n\nConvenience constructor method from set of of BaseTools (Optional).\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/reflective/tool_interactive_reflection.py`\n\n|     |     |\n| --- | --- |\n| ```<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>``` | ```<br>@classmethod<br>def from_defaults(<br>    cls,<br>    critique_agent_worker: FunctionCallingAgentWorker,<br>    critique_template: str,<br>    correction_llm: Optional[LLM] = None,<br>    max_iterations: int = DEFAULT_MAX_ITERATIONS,<br>    stopping_callable: Optional[StoppingCallable] = None,<br>    callback_manager: Optional[CallbackManager] = None,<br>    verbose: bool = False,<br>    **kwargs: Any,<br>) -> \"ToolInteractiveReflectionAgentWorker\":<br>    \"\"\"Convenience constructor method from set of of BaseTools (Optional).\"\"\"<br>    if correction_llm is None:<br>        try:<br>            from llama_index.llms.openai import OpenAI<br>        except ImportError:<br>            raise ImportError(<br>                \"Missing OpenAI LLMs. Please run `pip install llama-index-llms-openai`.\"<br>            )<br>        correction_llm = OpenAI(model=\"gpt-4-turbo-preview\", temperature=0)<br>    return cls(<br>        critique_agent_worker=critique_agent_worker,<br>        critique_template=critique_template,<br>        correction_llm=correction_llm,<br>        max_iterations=max_iterations,<br>        stopping_callable=stopping_callable,<br>        callback_manager=callback_manager or CallbackManager([]),<br>        verbose=verbose,<br>        **kwargs,<br>    )<br>``` |\n\n### initialize\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/introspective/\\#llama_index.agent.introspective.ToolInteractiveReflectionAgentWorker.initialize_step \"Permanent link\")\n\n```\ninitialize_step(task: Task, **kwargs: Any) -> TaskStep\n\n```\n\nInitialize step from task.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/reflective/tool_interactive_reflection.py`\n\n|     |     |\n| --- | --- |\n| ```<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>``` | ```<br>def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>    \"\"\"Initialize step from task.\"\"\"<br>    # temporary memory for new messages<br>    new_memory = ChatMemoryBuffer.from_defaults()<br>    # put current history in new memory<br>    messages = task.memory.get()<br>    for message in messages:<br>        new_memory.put(message)<br>    # inject new input into memory<br>    new_memory.put(ChatMessage(content=task.input, role=MessageRole.USER))<br>    # initialize task state<br>    task_state = {<br>        \"new_memory\": new_memory,<br>        \"sources\": [],<br>    }<br>    task.extra_state.update(task_state)<br>    return TaskStep(<br>        task_id=task.task_id,<br>        step_id=str(uuid.uuid4()),<br>        input=task.input,<br>        step_state={\"count\": 0},<br>    )<br>``` |\n\n### run\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/introspective/\\#llama_index.agent.introspective.ToolInteractiveReflectionAgentWorker.run_step \"Permanent link\")\n\n```\nrun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/reflective/tool_interactive_reflection.py`\n\n|     |     |\n| --- | --- |\n| ```<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>``` | ```<br>@dispatcher.span<br>@trace_method(\"run_step\")<br>def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step.\"\"\"<br>    state = step.step_state<br>    state[\"count\"] += 1<br>    messages = task.extra_state[\"new_memory\"].get()<br>    prev_correct_str = messages[-1].content<br>    prev_correct_str_without_prefix = self._remove_correction_str_prefix(<br>        prev_correct_str<br>    )<br>    # critique phase<br>    critique_response = self._critique(input_str=prev_correct_str_without_prefix)<br>    task.extra_state[\"sources\"].extend(critique_response.sources)<br>    is_done = False<br>    if self.stopping_callable:<br>        is_done = self.stopping_callable(critique_str=critique_response.response)<br>    critique_msg = ChatMessage(<br>        role=MessageRole.USER, content=critique_response.response<br>    )<br>    task.extra_state[\"new_memory\"].put(critique_msg)<br>    # correction phase<br>    if is_done:<br>        # no correction to be made prev correction is sufficient<br>        agent_response = AgentChatResponse(<br>            response=prev_correct_str_without_prefix,<br>            sources=task.extra_state[\"sources\"],<br>        )<br>        task.extra_state[\"new_memory\"].put(<br>            ChatMessage(<br>                role=MessageRole.ASSISTANT, content=prev_correct_str_without_prefix<br>            )<br>        )<br>        new_steps = []<br>    else:<br>        # generate a new correction<br>        correct_msg = self._correct(<br>            input_str=prev_correct_str_without_prefix,<br>            critique=critique_response.response,<br>        )<br>        correct_str_without_prefix = self._remove_correction_str_prefix(<br>            correct_msg.content<br>        )<br>        # reached max iterations, no further reflection/correction cycles<br>        if self.max_iterations == state[\"count\"]:<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(<br>                    role=MessageRole.ASSISTANT, content=correct_str_without_prefix<br>                )<br>            )<br>            agent_response = AgentChatResponse(response=correct_str_without_prefix)<br>            new_steps = []<br>        else:<br>            # another round of reflection/correction will take place<br>            task.extra_state[\"new_memory\"].put(correct_msg)<br>            agent_response = AgentChatResponse(<br>                response=str(correct_msg), sources=critique_response.sources<br>            )<br>            new_steps = [<br>                step.get_next_step(<br>                    step_id=str(uuid.uuid4()),<br>                    # NOTE: input is unused<br>                    input=None,<br>                    step_state=state,<br>                )<br>            ]<br>    return TaskStepOutput(<br>        output=agent_response,<br>        task_step=step,<br>        is_last=is_done | (self.max_iterations == state[\"count\"]),<br>        next_steps=new_steps,<br>    )<br>``` |\n\n### arun\\_step`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/introspective/\\#llama_index.agent.introspective.ToolInteractiveReflectionAgentWorker.arun_step \"Permanent link\")\n\n```\narun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async).\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/reflective/tool_interactive_reflection.py`\n\n|     |     |\n| --- | --- |\n| ```<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>``` | ```<br>@dispatcher.span<br>@trace_method(\"run_step\")<br>async def arun_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async).\"\"\"<br>    state = step.step_state<br>    state[\"count\"] += 1<br>    messages = task.extra_state[\"new_memory\"].get()<br>    prev_correct_str = messages[-1].content<br>    prev_correct_str_without_prefix = self._remove_correction_str_prefix(<br>        prev_correct_str<br>    )<br>    # critique phase<br>    critique_response = await self._acritique(<br>        input_str=prev_correct_str_without_prefix<br>    )<br>    task.extra_state[\"sources\"].extend(critique_response.sources)<br>    is_done = False<br>    if self.stopping_callable:<br>        is_done = self.stopping_callable(critique_str=critique_response.response)<br>    critique_msg = ChatMessage(<br>        role=MessageRole.USER, content=critique_response.response<br>    )<br>    task.extra_state[\"new_memory\"].put(critique_msg)<br>    # correction phase<br>    if is_done:<br>        # no correction to be made prev correction is sufficient<br>        agent_response = AgentChatResponse(<br>            response=prev_correct_str_without_prefix,<br>            sources=task.extra_state[\"sources\"],<br>        )<br>        task.extra_state[\"new_memory\"].put(<br>            ChatMessage(<br>                role=MessageRole.ASSISTANT, content=prev_correct_str_without_prefix<br>            )<br>        )<br>        new_steps = []<br>    else:<br>        # generate a new correction<br>        correct_msg = await self._acorrect(<br>            input_str=prev_correct_str_without_prefix,<br>            critique=critique_response.response,<br>        )<br>        correct_str_without_prefix = self._remove_correction_str_prefix(<br>            correct_msg.content<br>        )<br>        # reached max iterations, no further reflection/correction cycles<br>        if self.max_iterations == state[\"count\"]:<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(<br>                    role=MessageRole.ASSISTANT, content=correct_str_without_prefix<br>                )<br>            )<br>            agent_response = AgentChatResponse(response=correct_str_without_prefix)<br>            new_steps = []<br>        else:<br>            # another round of reflection/correction will take place<br>            task.extra_state[\"new_memory\"].put(correct_msg)<br>            agent_response = AgentChatResponse(<br>                response=str(correct_msg), sources=critique_response.sources<br>            )<br>            new_steps = [<br>                step.get_next_step(<br>                    step_id=str(uuid.uuid4()),<br>                    # NOTE: input is unused<br>                    input=None,<br>                    step_state=state,<br>                )<br>            ]<br>    return TaskStepOutput(<br>        output=agent_response,<br>        task_step=step,<br>        is_last=is_done | (self.max_iterations == state[\"count\"]),<br>        next_steps=new_steps,<br>    )<br>``` |\n\n### stream\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/introspective/\\#llama_index.agent.introspective.ToolInteractiveReflectionAgentWorker.stream_step \"Permanent link\")\n\n```\nstream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (stream).\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/reflective/tool_interactive_reflection.py`\n\n|     |     |\n| --- | --- |\n| ```<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>``` | ```<br>@dispatcher.span<br>@trace_method(\"run_step\")<br>def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step (stream).\"\"\"<br>    raise NotImplementedError(<br>        \"Stream not supported for tool-interactive reflection agent\"<br>    )<br>``` |\n\n### astream\\_step`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/introspective/\\#llama_index.agent.introspective.ToolInteractiveReflectionAgentWorker.astream_step \"Permanent link\")\n\n```\nastream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async stream).\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/reflective/tool_interactive_reflection.py`\n\n|     |     |\n| --- | --- |\n| ```<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>``` | ```<br>@dispatcher.span<br>@trace_method(\"run_step\")<br>async def astream_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async stream).\"\"\"<br>    raise NotImplementedError(<br>        \"Stream not supported for tool-interactive reflection agent\"<br>    )<br>``` |\n\n### finalize\\_task [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/introspective/\\#llama_index.agent.introspective.ToolInteractiveReflectionAgentWorker.finalize_task \"Permanent link\")\n\n```\nfinalize_task(task: Task, **kwargs: Any) -> None\n\n```\n\nFinalize task, after all the steps are completed.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/reflective/tool_interactive_reflection.py`\n\n|     |     |\n| --- | --- |\n| ```<br>448<br>449<br>450<br>451<br>452<br>453<br>``` | ```<br>def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>    \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>    # add new messages to memory<br>    task.memory.set(task.extra_state[\"new_memory\"].get_all())<br>    # reset new memory<br>    task.extra_state[\"new_memory\"].reset()<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Introspective - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/agent/introspective/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/deepinfra/#llama_index.embeddings.deepinfra.DeepInfraEmbeddingModel)\n\n# Deepinfra\n\n## DeepInfraEmbeddingModel [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/deepinfra/\\#llama_index.embeddings.deepinfra.DeepInfraEmbeddingModel \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nA wrapper class for accessing embedding models available via the DeepInfra API. This class allows for easy integration\nof DeepInfra embeddings into your projects, supporting both synchronous and asynchronous retrieval of text embeddings.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model_id` | `str` | Identifier for the model to be used for embeddings. Defaults to 'sentence-transformers/clip-ViT-B-32'. | `DEFAULT_MODEL_ID` |\n| `normalize` | `bool` | Flag to normalize embeddings post retrieval. Defaults to False. | `False` |\n| `api_token` | `str` | DeepInfra API token. If not provided, | `None` |\n\n**Examples:**\n\n```\n>>> from llama_index.embeddings.deepinfra import DeepInfraEmbeddingModel\n>>> model = DeepInfraEmbeddingModel()\n>>> print(model.get_text_embedding(\"Hello, world!\"))\n[0.1, 0.2, 0.3, ...]\n\n```\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-deepinfra/llama_index/embeddings/deepinfra/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>``` | ```<br>class DeepInfraEmbeddingModel(BaseEmbedding):<br>    \"\"\"<br>    A wrapper class for accessing embedding models available via the DeepInfra API. This class allows for easy integration<br>    of DeepInfra embeddings into your projects, supporting both synchronous and asynchronous retrieval of text embeddings.<br>    Args:<br>        model_id (str): Identifier for the model to be used for embeddings. Defaults to 'sentence-transformers/clip-ViT-B-32'.<br>        normalize (bool): Flag to normalize embeddings post retrieval. Defaults to False.<br>        api_token (str): DeepInfra API token. If not provided,<br>        the token is fetched from the environment variable 'DEEPINFRA_API_TOKEN'.<br>    Examples:<br>        >>> from llama_index.embeddings.deepinfra import DeepInfraEmbeddingModel<br>        >>> model = DeepInfraEmbeddingModel()<br>        >>> print(model.get_text_embedding(\"Hello, world!\"))<br>        [0.1, 0.2, 0.3, ...]<br>    \"\"\"<br>    \"\"\"model_id can be obtained from the DeepInfra website.\"\"\"<br>    _model_id: str = PrivateAttr()<br>    \"\"\"normalize flag to normalize embeddings post retrieval.\"\"\"<br>    _normalize: bool = PrivateAttr()<br>    \"\"\"api_token should be obtained from the DeepInfra website.\"\"\"<br>    _api_token: str = PrivateAttr()<br>    \"\"\"query_prefix is used to add a prefix to queries.\"\"\"<br>    _query_prefix: str = PrivateAttr()<br>    \"\"\"text_prefix is used to add a prefix to texts.\"\"\"<br>    _text_prefix: str = PrivateAttr()<br>    def __init__(<br>        self,<br>        model_id: str = DEFAULT_MODEL_ID,<br>        normalize: bool = False,<br>        api_token: str = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        query_prefix: str = \"\",<br>        text_prefix: str = \"\",<br>        embed_batch_size: int = MAX_BATCH_SIZE,<br>    ) -> None:<br>        \"\"\"<br>        Init params.<br>        \"\"\"<br>        super().__init__(<br>            callback_manager=callback_manager, embed_batch_size=embed_batch_size<br>        )<br>        self._model_id = model_id<br>        self._normalize = normalize<br>        self._api_token = api_token or os.getenv(ENV_VARIABLE, None)<br>        self._query_prefix = query_prefix<br>        self._text_prefix = text_prefix<br>    def _post(self, data: List[str]) -> List[List[float]]:<br>        \"\"\"<br>        Sends a POST request to the DeepInfra Inference API with the given data and returns the API response.<br>        Input data is chunked into batches to avoid exceeding the maximum batch size (1024).<br>        Args:<br>            data (List[str]): A list of strings to be embedded.<br>        Returns:<br>            dict: A dictionary containing embeddings from the API.<br>        \"\"\"<br>        url = self.get_url()<br>        chunked_data = _chunk(data, self.embed_batch_size)<br>        embeddings = []<br>        for chunk in chunked_data:<br>            response = requests.post(<br>                url,<br>                json={<br>                    \"inputs\": chunk,<br>                },<br>                headers=self._get_headers(),<br>            )<br>            response.raise_for_status()<br>            embeddings.extend(response.json()[\"embeddings\"])<br>        return embeddings<br>    def get_url(self):<br>        \"\"\"<br>        Get DeepInfra API URL.<br>        \"\"\"<br>        return f\"{INFERENCE_URL}/{self._model_id}\"<br>    async def _apost(self, data: List[str]) -> List[List[float]]:<br>        \"\"\"<br>        Sends a POST request to the DeepInfra Inference API with the given data and returns the API response.<br>        Input data is chunked into batches to avoid exceeding the maximum batch size (1024).<br>        Args:<br>            data (List[str]): A list of strings to be embedded.<br>        Output:<br>            List[float]: A list of embeddings from the API.<br>        \"\"\"<br>        url = self.get_url()<br>        chunked_data = _chunk(data, self.embed_batch_size)<br>        embeddings = []<br>        for chunk in chunked_data:<br>            async with aiohttp.ClientSession() as session:<br>                async with session.post(<br>                    url,<br>                    json={<br>                        \"inputs\": chunk,<br>                    },<br>                    headers=self._get_headers(),<br>                ) as resp:<br>                    response = await resp.json()<br>                    embeddings.extend(response[\"embeddings\"])<br>        return embeddings<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"<br>        Get query embedding.<br>        \"\"\"<br>        return self._post(self._add_query_prefix([query]))[0]<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"<br>        Async get query embedding.<br>        \"\"\"<br>        response = await self._apost(self._add_query_prefix([query]))<br>        return response[0]<br>    def _get_query_embeddings(self, queries: List[str]) -> List[List[float]]:<br>        \"\"\"<br>        Get query embeddings.<br>        \"\"\"<br>        return self._post(self._add_query_prefix(queries))<br>    async def _aget_query_embeddings(self, queries: List[str]) -> List[List[float]]:<br>        \"\"\"<br>        Async get query embeddings.<br>        \"\"\"<br>        return await self._apost(self._add_query_prefix(queries))<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"<br>        Get text embedding.<br>        \"\"\"<br>        return self._post(self._add_text_prefix([text]))[0]<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"<br>        Async get text embedding.<br>        \"\"\"<br>        response = await self._apost(self._add_text_prefix([text]))<br>        return response[0]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"<br>        Get text embedding.<br>        \"\"\"<br>        return self._post(self._add_text_prefix(texts))<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"<br>        Async get text embeddings.<br>        \"\"\"<br>        return await self._apost(self._add_text_prefix(texts))<br>    def _add_query_prefix(self, queries: List[str]) -> List[str]:<br>        \"\"\"<br>        Add query prefix to queries.<br>        \"\"\"<br>        return (<br>            [self._query_prefix + query for query in queries]<br>            if self._query_prefix<br>            else queries<br>        )<br>    def _add_text_prefix(self, texts: List[str]) -> List[str]:<br>        \"\"\"<br>        Add text prefix to texts.<br>        \"\"\"<br>        return (<br>            [self._text_prefix + text for text in texts] if self._text_prefix else texts<br>        )<br>    def _get_headers(self) -> dict:<br>        \"\"\"<br>        Get headers.<br>        \"\"\"<br>        return {<br>            \"Authorization\": f\"Bearer {self._api_token}\",<br>            \"Content-Type\": \"application/json\",<br>            \"User-Agent\": USER_AGENT,<br>        }<br>``` |\n\n### get\\_url [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/deepinfra/\\#llama_index.embeddings.deepinfra.DeepInfraEmbeddingModel.get_url \"Permanent link\")\n\n```\nget_url()\n\n```\n\nGet DeepInfra API URL.\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-deepinfra/llama_index/embeddings/deepinfra/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>104<br>105<br>106<br>107<br>108<br>``` | ```<br>def get_url(self):<br>    \"\"\"<br>    Get DeepInfra API URL.<br>    \"\"\"<br>    return f\"{INFERENCE_URL}/{self._model_id}\"<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Deepinfra - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/deepinfra/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/langfuse/#llama_index.callbacks.langfuse.langfuse_callback_handler)\n\n# Langfuse\n\n## langfuse\\_callback\\_handler [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/langfuse/\\#llama_index.callbacks.langfuse.langfuse_callback_handler \"Permanent link\")\n\n```\nlangfuse_callback_handler(**eval_params: Any) -> BaseCallbackHandler\n\n```\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-langfuse/llama_index/callbacks/langfuse/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 8<br> 9<br>10<br>11<br>``` | ```<br>def langfuse_callback_handler(**eval_params: Any) -> BaseCallbackHandler:<br>    return LlamaIndexCallbackHandler(<br>        **eval_params, sdk_integration=\"llama-index_set-global-handler\"<br>    )<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Langfuse - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/langfuse/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/alephalpha/#llama_index.embeddings.alephalpha.AlephAlphaEmbedding)\n\n# Alephalpha\n\n## AlephAlphaEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/alephalpha/\\#llama_index.embeddings.alephalpha.AlephAlphaEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nAlephAlphaEmbedding uses the Aleph Alpha API to generate embeddings for text.\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-alephalpha/llama_index/embeddings/alephalpha/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>``` | ```<br>class AlephAlphaEmbedding(BaseEmbedding):<br>    \"\"\"AlephAlphaEmbedding uses the Aleph Alpha API to generate embeddings for text.\"\"\"<br>    model: str = Field(<br>        default=DEFAULT_ALEPHALPHA_MODEL, description=\"The Aleph Alpha model to use.\"<br>    )<br>    token: str = Field(default=None, description=\"The Aleph Alpha API token.\")<br>    representation: Optional[str] = Field(<br>        default=SemanticRepresentation.Query,<br>        description=\"The representation type to use for generating embeddings.\",<br>    )<br>    compress_to_size: Optional[int] = Field(<br>        default=None,<br>        description=\"The size to compress the embeddings to.\",<br>        gt=0,<br>    )<br>    base_url: Optional[str] = Field(<br>        default=DEFAULT_ALEPHALPHA_HOST, description=\"The hostname of the API base_url.\"<br>    )<br>    timeout: Optional[float] = Field(<br>        default=None, description=\"The timeout to use in seconds.\", gte=0<br>    )<br>    max_retries: int = Field(<br>        default=10, description=\"The maximum number of API retries.\", gte=0<br>    )<br>    normalize: Optional[bool] = Field(<br>        default=False, description=\"Return normalized embeddings.\"<br>    )<br>    hosting: Optional[str] = Field(default=None, description=\"The hosting to use.\")<br>    nice: bool = Field(default=False, description=\"Whether to be nice to the API.\")<br>    verify_ssl: bool = Field(default=True, description=\"Whether to verify SSL.\")<br>    additional_kwargs: Dict[str, Any] = Field(<br>        default_factory=dict, description=\"Additional kwargs for the Aleph Alpha API.\"<br>    )<br>    # Instance variables initialized via Pydantic's mechanism<br>    _client: Any = PrivateAttr()<br>    _aclient: Any = PrivateAttr()<br>    def __init__(<br>        self,<br>        model: str = DEFAULT_ALEPHALPHA_MODEL,<br>        token: Optional[str] = None,<br>        representation: Optional[str] = None,<br>        base_url: Optional[str] = DEFAULT_ALEPHALPHA_HOST,<br>        hosting: Optional[str] = None,<br>        timeout: Optional[float] = None,<br>        max_retries: int = 10,<br>        nice: bool = False,<br>        verify_ssl: bool = True,<br>        additional_kwargs: Optional[Dict[str, Any]] = None,<br>    ):<br>        \"\"\"<br>        A class representation for generating embeddings using the AlephAlpha API.<br>        Args:<br>            token: The token to use for the AlephAlpha API.<br>            model: The model to use for generating embeddings.<br>            base_url: The base URL of the AlephAlpha API.<br>            nice: Whether to use the \"nice\" mode for the AlephAlpha API.<br>            additional_kwargs: Additional kwargs for the AlephAlpha API.<br>        \"\"\"<br>        additional_kwargs = additional_kwargs or {}<br>        super().__init__(<br>            model=model,<br>            representation=representation,<br>            base_url=base_url,<br>            token=token,<br>            nice=nice,<br>            additional_kwargs=additional_kwargs,<br>        )<br>        self.token = get_from_param_or_env(\"aa_token\", token, \"AA_TOKEN\", \"\")<br>        if representation is not None and isinstance(representation, str):<br>            try:<br>                representation_enum = SemanticRepresentation[<br>                    representation.capitalize()<br>                ]<br>            except KeyError:<br>                raise ValueError(<br>                    f\"{representation} is not a valid representation type. Available types are: {list(SemanticRepresentation.__members__.keys())}\"<br>                )<br>            self.representation = representation_enum<br>        else:<br>            self.representation = representation<br>        self._client = None<br>        self._aclient = None<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"AlephAlphaEmbedding\"<br>    def _get_credential_kwargs(self) -> Dict[str, Any]:<br>        return {<br>            \"token\": self.token,<br>            \"host\": self.base_url,<br>            \"hosting\": self.hosting,<br>            \"request_timeout_seconds\": self.timeout,<br>            \"total_retries\": self.max_retries,<br>            \"nice\": self.nice,<br>            \"verify_ssl\": self.verify_ssl,<br>        }<br>    def _get_client(self) -> Client:<br>        if self._client is None:<br>            self._client = Client(**self._get_credential_kwargs())<br>        return self._client<br>    def _get_aclient(self) -> AsyncClient:<br>        if self._aclient is None:<br>            self._aclient = AsyncClient(**self._get_credential_kwargs())<br>        return self._aclient<br>    def _get_embedding(self, text: str, representation: str) -> List[float]:<br>        \"\"\"Embed sentence using AlephAlpha.\"\"\"<br>        client = self._get_client()<br>        request = SemanticEmbeddingRequest(<br>            prompt=Prompt.from_text(text),<br>            representation=representation or self.representation,<br>            compress_to_size=self.compress_to_size,<br>            normalize=self.normalize,<br>        )<br>        result = client.semantic_embed(request=request, model=self.model)<br>        return result.embedding<br>    async def _aget_embedding(self, text: str, representation: str) -> List[float]:<br>        \"\"\"Get embedding async.\"\"\"<br>        aclient = self._get_aclient()<br>        request = SemanticEmbeddingRequest(<br>            prompt=Prompt.from_text(text),<br>            representation=representation or self.representation,<br>            compress_to_size=self.compress_to_size,<br>            normalize=self.normalize,<br>        )<br>        result = await aclient.semantic_embed(request=request, model=self.model)<br>        return result.embedding<br>    def _get_embeddings(<br>        self, texts: List[str], representation: str<br>    ) -> List[List[float]]:<br>        \"\"\"Embed sentences using AlephAlpha.\"\"\"<br>        client = self._get_client()<br>        request = BatchSemanticEmbeddingRequest(<br>            prompts=[Prompt.from_text(text) for text in texts],<br>            representation=representation or self.representation,<br>            compress_to_size=self.compress_to_size,<br>            normalize=self.normalize,<br>        )<br>        result: BatchSemanticEmbeddingResponse = client.batch_semantic_embed(<br>            request=request, model=self.model<br>        )<br>        return result.embeddings<br>    async def _aget_embeddings(<br>        self, texts: List[str], representation: str<br>    ) -> List[List[float]]:<br>        \"\"\"Get embeddings async.\"\"\"<br>        aclient = self._get_aclient()<br>        request = BatchSemanticEmbeddingRequest(<br>            prompts=[Prompt.from_text(text) for text in texts],<br>            representation=representation or self.representation,<br>            compress_to_size=self.compress_to_size,<br>            normalize=self.normalize,<br>        )<br>        result: BatchSemanticEmbeddingResponse = await aclient.batch_semantic_embed(<br>            request=request, model=self.model<br>        )<br>        return result.embeddings<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding. For query embeddings, representation='query'.\"\"\"<br>        return self._get_embedding(query, SemanticRepresentation.Query)<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding async. For query embeddings, representation='query'.\"\"\"<br>        return self._aget_embedding(query, SemanticRepresentation.Query)<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding. For text embeddings, representation='document'.\"\"\"<br>        return self._get_embedding(text, SemanticRepresentation.Document)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding async.\"\"\"<br>        return self._aget_embedding(text, SemanticRepresentation.Document)<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        return self._get_embeddings(texts, SemanticRepresentation.Document)<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings async.\"\"\"<br>        return self._aget_embeddings(texts, SemanticRepresentation.Document)<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Alephalpha - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/alephalpha/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/token_counter/#llama_index.core.callbacks.token_counting.TokenCountingHandler)\n\n# Token counter\n\n## TokenCountingHandler [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/token_counter/\\#llama_index.core.callbacks.token_counting.TokenCountingHandler \"Permanent link\")\n\nBases: `PythonicallyPrintingBaseHandler`\n\nCallback handler for counting tokens in LLM and Embedding events.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `tokenizer` | `Optional[Callable[[str], List]]` | Tokenizer to use. Defaults to the global tokenizer<br>(see llama\\_index.core.utils.globals\\_helper). | `None` |\n| `event_starts_to_ignore` | `Optional[List[CBEventType]]` | List of event types to ignore at the start of a trace. | `None` |\n| `event_ends_to_ignore` | `Optional[List[CBEventType]]` | List of event types to ignore at the end of a trace. | `None` |\n\nSource code in `llama-index-core/llama_index/core/callbacks/token_counting.py`\n\n|     |     |\n| --- | --- |\n| ```<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>``` | ```<br>class TokenCountingHandler(PythonicallyPrintingBaseHandler):<br>    \"\"\"Callback handler for counting tokens in LLM and Embedding events.<br>    Args:<br>        tokenizer:<br>            Tokenizer to use. Defaults to the global tokenizer<br>            (see llama_index.core.utils.globals_helper).<br>        event_starts_to_ignore: List of event types to ignore at the start of a trace.<br>        event_ends_to_ignore: List of event types to ignore at the end of a trace.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        tokenizer: Optional[Callable[[str], List]] = None,<br>        event_starts_to_ignore: Optional[List[CBEventType]] = None,<br>        event_ends_to_ignore: Optional[List[CBEventType]] = None,<br>        verbose: bool = False,<br>        logger: Optional[logging.Logger] = None,<br>    ) -> None:<br>        self.llm_token_counts: List[TokenCountingEvent] = []<br>        self.embedding_token_counts: List[TokenCountingEvent] = []<br>        self.tokenizer = tokenizer or get_tokenizer()<br>        self._token_counter = TokenCounter(tokenizer=self.tokenizer)<br>        self._verbose = verbose<br>        super().__init__(<br>            event_starts_to_ignore=event_starts_to_ignore or [],<br>            event_ends_to_ignore=event_ends_to_ignore or [],<br>            logger=logger,<br>        )<br>    def start_trace(self, trace_id: Optional[str] = None) -> None:<br>        return<br>    def end_trace(<br>        self,<br>        trace_id: Optional[str] = None,<br>        trace_map: Optional[Dict[str, List[str]]] = None,<br>    ) -> None:<br>        return<br>    def on_event_start(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        parent_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> str:<br>        return event_id<br>    def on_event_end(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> None:<br>        \"\"\"Count the LLM or Embedding tokens as needed.\"\"\"<br>        if (<br>            event_type == CBEventType.LLM<br>            and event_type not in self.event_ends_to_ignore<br>            and payload is not None<br>        ):<br>            self.llm_token_counts.append(<br>                get_llm_token_counts(<br>                    token_counter=self._token_counter,<br>                    payload=payload,<br>                    event_id=event_id,<br>                )<br>            )<br>            if self._verbose:<br>                self._print(<br>                    \"LLM Prompt Token Usage: \"<br>                    f\"{self.llm_token_counts[-1].prompt_token_count}\\n\"<br>                    \"LLM Completion Token Usage: \"<br>                    f\"{self.llm_token_counts[-1].completion_token_count}\",<br>                )<br>        elif (<br>            event_type == CBEventType.EMBEDDING<br>            and event_type not in self.event_ends_to_ignore<br>            and payload is not None<br>        ):<br>            total_chunk_tokens = 0<br>            for chunk in payload.get(EventPayload.CHUNKS, []):<br>                self.embedding_token_counts.append(<br>                    TokenCountingEvent(<br>                        event_id=event_id,<br>                        prompt=chunk,<br>                        prompt_token_count=self._token_counter.get_string_tokens(chunk),<br>                        completion=\"\",<br>                        completion_token_count=0,<br>                    )<br>                )<br>                total_chunk_tokens += self.embedding_token_counts[-1].total_token_count<br>            if self._verbose:<br>                self._print(f\"Embedding Token Usage: {total_chunk_tokens}\")<br>    @property<br>    def total_llm_token_count(self) -> int:<br>        \"\"\"Get the current total LLM token count.\"\"\"<br>        return sum([x.total_token_count for x in self.llm_token_counts])<br>    @property<br>    def prompt_llm_token_count(self) -> int:<br>        \"\"\"Get the current total LLM prompt token count.\"\"\"<br>        return sum([x.prompt_token_count for x in self.llm_token_counts])<br>    @property<br>    def completion_llm_token_count(self) -> int:<br>        \"\"\"Get the current total LLM completion token count.\"\"\"<br>        return sum([x.completion_token_count for x in self.llm_token_counts])<br>    @property<br>    def total_embedding_token_count(self) -> int:<br>        \"\"\"Get the current total Embedding token count.\"\"\"<br>        return sum([x.total_token_count for x in self.embedding_token_counts])<br>    def reset_counts(self) -> None:<br>        \"\"\"Reset the token counts.\"\"\"<br>        self.llm_token_counts = []<br>        self.embedding_token_counts = []<br>``` |\n\n### total\\_llm\\_token\\_count`property`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/token_counter/\\#llama_index.core.callbacks.token_counting.TokenCountingHandler.total_llm_token_count \"Permanent link\")\n\n```\ntotal_llm_token_count: int\n\n```\n\nGet the current total LLM token count.\n\n### prompt\\_llm\\_token\\_count`property`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/token_counter/\\#llama_index.core.callbacks.token_counting.TokenCountingHandler.prompt_llm_token_count \"Permanent link\")\n\n```\nprompt_llm_token_count: int\n\n```\n\nGet the current total LLM prompt token count.\n\n### completion\\_llm\\_token\\_count`property`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/token_counter/\\#llama_index.core.callbacks.token_counting.TokenCountingHandler.completion_llm_token_count \"Permanent link\")\n\n```\ncompletion_llm_token_count: int\n\n```\n\nGet the current total LLM completion token count.\n\n### total\\_embedding\\_token\\_count`property`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/token_counter/\\#llama_index.core.callbacks.token_counting.TokenCountingHandler.total_embedding_token_count \"Permanent link\")\n\n```\ntotal_embedding_token_count: int\n\n```\n\nGet the current total Embedding token count.\n\n### on\\_event\\_end [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/token_counter/\\#llama_index.core.callbacks.token_counting.TokenCountingHandler.on_event_end \"Permanent link\")\n\n```\non_event_end(event_type: CBEventType, payload: Optional[Dict[str, Any]] = None, event_id: str = '', **kwargs: Any) -> None\n\n```\n\nCount the LLM or Embedding tokens as needed.\n\nSource code in `llama-index-core/llama_index/core/callbacks/token_counting.py`\n\n|     |     |\n| --- | --- |\n| ```<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>``` | ```<br>def on_event_end(<br>    self,<br>    event_type: CBEventType,<br>    payload: Optional[Dict[str, Any]] = None,<br>    event_id: str = \"\",<br>    **kwargs: Any,<br>) -> None:<br>    \"\"\"Count the LLM or Embedding tokens as needed.\"\"\"<br>    if (<br>        event_type == CBEventType.LLM<br>        and event_type not in self.event_ends_to_ignore<br>        and payload is not None<br>    ):<br>        self.llm_token_counts.append(<br>            get_llm_token_counts(<br>                token_counter=self._token_counter,<br>                payload=payload,<br>                event_id=event_id,<br>            )<br>        )<br>        if self._verbose:<br>            self._print(<br>                \"LLM Prompt Token Usage: \"<br>                f\"{self.llm_token_counts[-1].prompt_token_count}\\n\"<br>                \"LLM Completion Token Usage: \"<br>                f\"{self.llm_token_counts[-1].completion_token_count}\",<br>            )<br>    elif (<br>        event_type == CBEventType.EMBEDDING<br>        and event_type not in self.event_ends_to_ignore<br>        and payload is not None<br>    ):<br>        total_chunk_tokens = 0<br>        for chunk in payload.get(EventPayload.CHUNKS, []):<br>            self.embedding_token_counts.append(<br>                TokenCountingEvent(<br>                    event_id=event_id,<br>                    prompt=chunk,<br>                    prompt_token_count=self._token_counter.get_string_tokens(chunk),<br>                    completion=\"\",<br>                    completion_token_count=0,<br>                )<br>            )<br>            total_chunk_tokens += self.embedding_token_counts[-1].total_token_count<br>        if self._verbose:<br>            self._print(f\"Embedding Token Usage: {total_chunk_tokens}\")<br>``` |\n\n### reset\\_counts [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/token_counter/\\#llama_index.core.callbacks.token_counting.TokenCountingHandler.reset_counts \"Permanent link\")\n\n```\nreset_counts() -> None\n\n```\n\nReset the token counts.\n\nSource code in `llama-index-core/llama_index/core/callbacks/token_counting.py`\n\n|     |     |\n| --- | --- |\n| ```<br>260<br>261<br>262<br>263<br>``` | ```<br>def reset_counts(self) -> None:<br>    \"\"\"Reset the token counts.\"\"\"<br>    self.llm_token_counts = []<br>    self.embedding_token_counts = []<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Token counter - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/token_counter/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/context/#llama_index.core.chat_engine.ContextChatEngine)\n\n# Context\n\n## ContextChatEngine [\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/context/\\#llama_index.core.chat_engine.ContextChatEngine \"Permanent link\")\n\nBases: `BaseChatEngine`\n\nContext Chat Engine.\n\nUses a retriever to retrieve a context, set the context in the system prompt,\nand then uses an LLM to generate a response, for a fluid chat experience.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/context.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>``` | ```<br>class ContextChatEngine(BaseChatEngine):<br>    \"\"\"<br>    Context Chat Engine.<br>    Uses a retriever to retrieve a context, set the context in the system prompt,<br>    and then uses an LLM to generate a response, for a fluid chat experience.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        retriever: BaseRetriever,<br>        llm: LLM,<br>        memory: BaseMemory,<br>        prefix_messages: List[ChatMessage],<br>        node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,<br>        context_template: Optional[str] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>    ) -> None:<br>        self._retriever = retriever<br>        self._llm = llm<br>        self._memory = memory<br>        self._prefix_messages = prefix_messages<br>        self._node_postprocessors = node_postprocessors or []<br>        self._context_template = context_template or DEFAULT_CONTEXT_TEMPLATE<br>        self.callback_manager = callback_manager or CallbackManager([])<br>        for node_postprocessor in self._node_postprocessors:<br>            node_postprocessor.callback_manager = self.callback_manager<br>    @classmethod<br>    def from_defaults(<br>        cls,<br>        retriever: BaseRetriever,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        memory: Optional[BaseMemory] = None,<br>        system_prompt: Optional[str] = None,<br>        prefix_messages: Optional[List[ChatMessage]] = None,<br>        node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,<br>        context_template: Optional[str] = None,<br>        llm: Optional[LLM] = None,<br>        **kwargs: Any,<br>    ) -> \"ContextChatEngine\":<br>        \"\"\"Initialize a ContextChatEngine from default parameters.\"\"\"<br>        llm = llm or Settings.llm<br>        chat_history = chat_history or []<br>        memory = memory or ChatMemoryBuffer.from_defaults(<br>            chat_history=chat_history, token_limit=llm.metadata.context_window - 256<br>        )<br>        if system_prompt is not None:<br>            if prefix_messages is not None:<br>                raise ValueError(<br>                    \"Cannot specify both system_prompt and prefix_messages\"<br>                )<br>            prefix_messages = [<br>                ChatMessage(content=system_prompt, role=llm.metadata.system_role)<br>            ]<br>        prefix_messages = prefix_messages or []<br>        node_postprocessors = node_postprocessors or []<br>        return cls(<br>            retriever,<br>            llm=llm,<br>            memory=memory,<br>            prefix_messages=prefix_messages,<br>            node_postprocessors=node_postprocessors,<br>            callback_manager=Settings.callback_manager,<br>            context_template=context_template,<br>        )<br>    def _generate_context(self, message: str) -> Tuple[str, List[NodeWithScore]]:<br>        \"\"\"Generate context information from a message.\"\"\"<br>        nodes = self._retriever.retrieve(message)<br>        for postprocessor in self._node_postprocessors:<br>            nodes = postprocessor.postprocess_nodes(<br>                nodes, query_bundle=QueryBundle(message)<br>            )<br>        context_str = \"\\n\\n\".join(<br>            [n.node.get_content(metadata_mode=MetadataMode.LLM).strip() for n in nodes]<br>        )<br>        return self._context_template.format(context_str=context_str), nodes<br>    async def _agenerate_context(self, message: str) -> Tuple[str, List[NodeWithScore]]:<br>        \"\"\"Generate context information from a message.\"\"\"<br>        nodes = await self._retriever.aretrieve(message)<br>        for postprocessor in self._node_postprocessors:<br>            nodes = postprocessor.postprocess_nodes(<br>                nodes, query_bundle=QueryBundle(message)<br>            )<br>        context_str = \"\\n\\n\".join(<br>            [n.node.get_content(metadata_mode=MetadataMode.LLM).strip() for n in nodes]<br>        )<br>        return self._context_template.format(context_str=context_str), nodes<br>    def _get_prefix_messages_with_context(self, context_str: str) -> List[ChatMessage]:<br>        \"\"\"Get the prefix messages with context.\"\"\"<br>        # ensure we grab the user-configured system prompt<br>        system_prompt = \"\"<br>        prefix_messages = self._prefix_messages<br>        if (<br>            len(self._prefix_messages) != 0<br>            and self._prefix_messages[0].role == MessageRole.SYSTEM<br>        ):<br>            system_prompt = str(self._prefix_messages[0].content)<br>            prefix_messages = self._prefix_messages[1:]<br>        context_str_w_sys_prompt = system_prompt.strip() + \"\\n\" + context_str<br>        return [<br>            ChatMessage(<br>                content=context_str_w_sys_prompt, role=self._llm.metadata.system_role<br>            ),<br>            *prefix_messages,<br>        ]<br>    @trace_method(\"chat\")<br>    def chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        prev_chunks: Optional[List[NodeWithScore]] = None,<br>    ) -> AgentChatResponse:<br>        if chat_history is not None:<br>            self._memory.set(chat_history)<br>        self._memory.put(ChatMessage(content=message, role=\"user\"))<br>        context_str_template, nodes = self._generate_context(message)<br>        # If the fetched context is completely empty<br>        if len(nodes) == 0 and prev_chunks is not None:<br>            context_str = \"\\n\\n\".join(<br>                [<br>                    n.node.get_content(metadata_mode=MetadataMode.LLM).strip()<br>                    for n in prev_chunks<br>                ]<br>            )<br>            # Create a new context string template by using previous nodes<br>            context_str_template = self._context_template.format(<br>                context_str=context_str<br>            )<br>        prefix_messages = self._get_prefix_messages_with_context(context_str_template)<br>        if hasattr(self._memory, \"tokenizer_fn\"):<br>            prefix_messages_token_count = len(<br>                self._memory.tokenizer_fn(<br>                    \" \".join([(m.content or \"\") for m in prefix_messages])<br>                )<br>            )<br>        else:<br>            prefix_messages_token_count = 0<br>        all_messages = prefix_messages + self._memory.get(<br>            initial_token_count=prefix_messages_token_count<br>        )<br>        chat_response = self._llm.chat(all_messages)<br>        ai_message = chat_response.message<br>        self._memory.put(ai_message)<br>        return AgentChatResponse(<br>            response=str(chat_response.message.content),<br>            sources=[<br>                ToolOutput(<br>                    tool_name=\"retriever\",<br>                    content=str(prefix_messages[0]),<br>                    raw_input={\"message\": message},<br>                    raw_output=prefix_messages[0],<br>                )<br>            ],<br>            source_nodes=nodes,<br>        )<br>    @trace_method(\"chat\")<br>    def stream_chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        prev_chunks: Optional[List[NodeWithScore]] = None,<br>    ) -> StreamingAgentChatResponse:<br>        if chat_history is not None:<br>            self._memory.set(chat_history)<br>        self._memory.put(ChatMessage(content=message, role=\"user\"))<br>        context_str_template, nodes = self._generate_context(message)<br>        # If the fetched context is completely empty<br>        if len(nodes) == 0 and prev_chunks is not None:<br>            context_str = \"\\n\\n\".join(<br>                [<br>                    n.node.get_content(metadata_mode=MetadataMode.LLM).strip()<br>                    for n in prev_chunks<br>                ]<br>            )<br>            # Create a new context string template by using previous nodes<br>            context_str_template = self._context_template.format(<br>                context_str=context_str<br>            )<br>        prefix_messages = self._get_prefix_messages_with_context(context_str_template)<br>        if hasattr(self._memory, \"tokenizer_fn\"):<br>            initial_token_count = len(<br>                self._memory.tokenizer_fn(<br>                    \" \".join([(m.content or \"\") for m in prefix_messages])<br>                )<br>            )<br>        else:<br>            initial_token_count = 0<br>        all_messages = prefix_messages + self._memory.get(<br>            initial_token_count=initial_token_count<br>        )<br>        chat_response = StreamingAgentChatResponse(<br>            chat_stream=self._llm.stream_chat(all_messages),<br>            sources=[<br>                ToolOutput(<br>                    tool_name=\"retriever\",<br>                    content=str(prefix_messages[0]),<br>                    raw_input={\"message\": message},<br>                    raw_output=prefix_messages[0],<br>                )<br>            ],<br>            source_nodes=nodes,<br>        )<br>        thread = Thread(<br>            target=chat_response.write_response_to_history, args=(self._memory,)<br>        )<br>        thread.start()<br>        return chat_response<br>    @trace_method(\"chat\")<br>    async def achat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        prev_chunks: Optional[Sequence[NodeWithScore]] = None,<br>    ) -> AgentChatResponse:<br>        if chat_history is not None:<br>            self._memory.set(chat_history)<br>        self._memory.put(ChatMessage(content=message, role=\"user\"))<br>        context_str_template, nodes = await self._agenerate_context(message)<br>        # If the fetched context is completely empty<br>        if len(nodes) == 0 and prev_chunks is not None:<br>            context_str = \"\\n\\n\".join(<br>                [<br>                    n.node.get_content(metadata_mode=MetadataMode.LLM).strip()<br>                    for n in prev_chunks<br>                ]<br>            )<br>            # Create a new context string template by using previous nodes<br>            context_str_template = self._context_template.format(<br>                context_str=context_str<br>            )<br>        prefix_messages = self._get_prefix_messages_with_context(context_str_template)<br>        if hasattr(self._memory, \"tokenizer_fn\"):<br>            initial_token_count = len(<br>                self._memory.tokenizer_fn(<br>                    \" \".join([(m.content or \"\") for m in prefix_messages])<br>                )<br>            )<br>        else:<br>            initial_token_count = 0<br>        all_messages = prefix_messages + self._memory.get(<br>            initial_token_count=initial_token_count<br>        )<br>        chat_response = await self._llm.achat(all_messages)<br>        ai_message = chat_response.message<br>        self._memory.put(ai_message)<br>        return AgentChatResponse(<br>            response=str(chat_response.message.content),<br>            sources=[<br>                ToolOutput(<br>                    tool_name=\"retriever\",<br>                    content=str(prefix_messages[0]),<br>                    raw_input={\"message\": message},<br>                    raw_output=prefix_messages[0],<br>                )<br>            ],<br>            source_nodes=nodes,<br>        )<br>    @trace_method(\"chat\")<br>    async def astream_chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        prev_chunks: Optional[Sequence[NodeWithScore]] = None,<br>    ) -> StreamingAgentChatResponse:<br>        if chat_history is not None:<br>            self._memory.set(chat_history)<br>        self._memory.put(ChatMessage(content=message, role=\"user\"))<br>        context_str_template, nodes = await self._agenerate_context(message)<br>        # If the fetched context is completely empty<br>        if len(nodes) == 0 and prev_chunks is not None:<br>            context_str = \"\\n\\n\".join(<br>                [<br>                    n.node.get_content(metadata_mode=MetadataMode.LLM).strip()<br>                    for n in prev_chunks<br>                ]<br>            )<br>            # Create a new context string template by using previous nodes<br>            context_str_template = self._context_template.format(<br>                context_str=context_str<br>            )<br>        prefix_messages = self._get_prefix_messages_with_context(context_str_template)<br>        if hasattr(self._memory, \"tokenizer_fn\"):<br>            initial_token_count = len(<br>                self._memory.tokenizer_fn(<br>                    \" \".join([(m.content or \"\") for m in prefix_messages])<br>                )<br>            )<br>        else:<br>            initial_token_count = 0<br>        all_messages = prefix_messages + self._memory.get(<br>            initial_token_count=initial_token_count<br>        )<br>        chat_response = StreamingAgentChatResponse(<br>            achat_stream=await self._llm.astream_chat(all_messages),<br>            sources=[<br>                ToolOutput(<br>                    tool_name=\"retriever\",<br>                    content=str(prefix_messages[0]),<br>                    raw_input={\"message\": message},<br>                    raw_output=prefix_messages[0],<br>                )<br>            ],<br>            source_nodes=nodes,<br>        )<br>        asyncio.create_task(chat_response.awrite_response_to_history(self._memory))<br>        return chat_response<br>    def reset(self) -> None:<br>        self._memory.reset()<br>    @property<br>    def chat_history(self) -> List[ChatMessage]:<br>        \"\"\"Get chat history.\"\"\"<br>        return self._memory.get_all()<br>``` |\n\n### chat\\_history`property`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/context/\\#llama_index.core.chat_engine.ContextChatEngine.chat_history \"Permanent link\")\n\n```\nchat_history: List[ChatMessage]\n\n```\n\nGet chat history.\n\n### from\\_defaults`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/context/\\#llama_index.core.chat_engine.ContextChatEngine.from_defaults \"Permanent link\")\n\n```\nfrom_defaults(retriever: BaseRetriever, chat_history: Optional[List[ChatMessage]] = None, memory: Optional[BaseMemory] = None, system_prompt: Optional[str] = None, prefix_messages: Optional[List[ChatMessage]] = None, node_postprocessors: Optional[List[BaseNodePostprocessor]] = None, context_template: Optional[str] = None, llm: Optional[LLM] = None, **kwargs: Any) -> ContextChatEngine\n\n```\n\nInitialize a ContextChatEngine from default parameters.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/context.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>``` | ```<br>@classmethod<br>def from_defaults(<br>    cls,<br>    retriever: BaseRetriever,<br>    chat_history: Optional[List[ChatMessage]] = None,<br>    memory: Optional[BaseMemory] = None,<br>    system_prompt: Optional[str] = None,<br>    prefix_messages: Optional[List[ChatMessage]] = None,<br>    node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,<br>    context_template: Optional[str] = None,<br>    llm: Optional[LLM] = None,<br>    **kwargs: Any,<br>) -> \"ContextChatEngine\":<br>    \"\"\"Initialize a ContextChatEngine from default parameters.\"\"\"<br>    llm = llm or Settings.llm<br>    chat_history = chat_history or []<br>    memory = memory or ChatMemoryBuffer.from_defaults(<br>        chat_history=chat_history, token_limit=llm.metadata.context_window - 256<br>    )<br>    if system_prompt is not None:<br>        if prefix_messages is not None:<br>            raise ValueError(<br>                \"Cannot specify both system_prompt and prefix_messages\"<br>            )<br>        prefix_messages = [<br>            ChatMessage(content=system_prompt, role=llm.metadata.system_role)<br>        ]<br>    prefix_messages = prefix_messages or []<br>    node_postprocessors = node_postprocessors or []<br>    return cls(<br>        retriever,<br>        llm=llm,<br>        memory=memory,<br>        prefix_messages=prefix_messages,<br>        node_postprocessors=node_postprocessors,<br>        callback_manager=Settings.callback_manager,<br>        context_template=context_template,<br>    )<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Context - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/context/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/#llama_index.core.embeddings.BaseEmbedding)\n\n# Index\n\n## BaseEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/\\#llama_index.core.embeddings.BaseEmbedding \"Permanent link\")\n\nBases: `TransformComponent`, `DispatcherSpanMixin`\n\nBase class for embeddings.\n\nSource code in `llama-index-core/llama_index/core/base/embeddings/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br>``` | ```<br>class BaseEmbedding(TransformComponent, DispatcherSpanMixin):<br>    \"\"\"Base class for embeddings.\"\"\"<br>    model_config = ConfigDict(<br>        protected_namespaces=(\"pydantic_model_\",), arbitrary_types_allowed=True<br>    )<br>    model_name: str = Field(<br>        default=\"unknown\", description=\"The name of the embedding model.\"<br>    )<br>    embed_batch_size: int = Field(<br>        default=DEFAULT_EMBED_BATCH_SIZE,<br>        description=\"The batch size for embedding calls.\",<br>        gt=0,<br>        le=2048,<br>    )<br>    callback_manager: CallbackManager = Field(<br>        default_factory=lambda: CallbackManager([]), exclude=True<br>    )<br>    num_workers: Optional[int] = Field(<br>        default=None,<br>        description=\"The number of workers to use for async embedding calls.\",<br>    )<br>    @field_validator(\"callback_manager\")<br>    @classmethod<br>    def check_callback_manager(cls, v: CallbackManager) -> CallbackManager:<br>        if v is None:<br>            return CallbackManager([])<br>        return v<br>    @abstractmethod<br>    def _get_query_embedding(self, query: str) -> Embedding:<br>        \"\"\"<br>        Embed the input query synchronously.<br>        Subclasses should implement this method. Reference get_query_embedding's<br>        docstring for more information.<br>        \"\"\"<br>    @abstractmethod<br>    async def _aget_query_embedding(self, query: str) -> Embedding:<br>        \"\"\"<br>        Embed the input query asynchronously.<br>        Subclasses should implement this method. Reference get_query_embedding's<br>        docstring for more information.<br>        \"\"\"<br>    @dispatcher.span<br>    def get_query_embedding(self, query: str) -> Embedding:<br>        \"\"\"<br>        Embed the input query.<br>        When embedding a query, depending on the model, a special instruction<br>        can be prepended to the raw query string. For example, \"Represent the<br>        question for retrieving supporting documents: \". If you're curious,<br>        other examples of predefined instructions can be found in<br>        embeddings/huggingface_utils.py.<br>        \"\"\"<br>        model_dict = self.to_dict()<br>        model_dict.pop(\"api_key\", None)<br>        dispatcher.event(<br>            EmbeddingStartEvent(<br>                model_dict=model_dict,<br>            )<br>        )<br>        with self.callback_manager.event(<br>            CBEventType.EMBEDDING, payload={EventPayload.SERIALIZED: self.to_dict()}<br>        ) as event:<br>            query_embedding = self._get_query_embedding(query)<br>            event.on_end(<br>                payload={<br>                    EventPayload.CHUNKS: [query],<br>                    EventPayload.EMBEDDINGS: [query_embedding],<br>                },<br>            )<br>        dispatcher.event(<br>            EmbeddingEndEvent(<br>                chunks=[query],<br>                embeddings=[query_embedding],<br>            )<br>        )<br>        return query_embedding<br>    @dispatcher.span<br>    async def aget_query_embedding(self, query: str) -> Embedding:<br>        \"\"\"Get query embedding.\"\"\"<br>        model_dict = self.to_dict()<br>        model_dict.pop(\"api_key\", None)<br>        dispatcher.event(<br>            EmbeddingStartEvent(<br>                model_dict=model_dict,<br>            )<br>        )<br>        with self.callback_manager.event(<br>            CBEventType.EMBEDDING, payload={EventPayload.SERIALIZED: self.to_dict()}<br>        ) as event:<br>            query_embedding = await self._aget_query_embedding(query)<br>            event.on_end(<br>                payload={<br>                    EventPayload.CHUNKS: [query],<br>                    EventPayload.EMBEDDINGS: [query_embedding],<br>                },<br>            )<br>        dispatcher.event(<br>            EmbeddingEndEvent(<br>                chunks=[query],<br>                embeddings=[query_embedding],<br>            )<br>        )<br>        return query_embedding<br>    def get_agg_embedding_from_queries(<br>        self,<br>        queries: List[str],<br>        agg_fn: Optional[Callable[..., Embedding]] = None,<br>    ) -> Embedding:<br>        \"\"\"Get aggregated embedding from multiple queries.\"\"\"<br>        query_embeddings = [self.get_query_embedding(query) for query in queries]<br>        agg_fn = agg_fn or mean_agg<br>        return agg_fn(query_embeddings)<br>    async def aget_agg_embedding_from_queries(<br>        self,<br>        queries: List[str],<br>        agg_fn: Optional[Callable[..., Embedding]] = None,<br>    ) -> Embedding:<br>        \"\"\"Async get aggregated embedding from multiple queries.\"\"\"<br>        query_embeddings = [await self.aget_query_embedding(query) for query in queries]<br>        agg_fn = agg_fn or mean_agg<br>        return agg_fn(query_embeddings)<br>    @abstractmethod<br>    def _get_text_embedding(self, text: str) -> Embedding:<br>        \"\"\"<br>        Embed the input text synchronously.<br>        Subclasses should implement this method. Reference get_text_embedding's<br>        docstring for more information.<br>        \"\"\"<br>    async def _aget_text_embedding(self, text: str) -> Embedding:<br>        \"\"\"<br>        Embed the input text asynchronously.<br>        Subclasses can implement this method if there is a true async<br>        implementation. Reference get_text_embedding's docstring for more<br>        information.<br>        \"\"\"<br>        # Default implementation just falls back on _get_text_embedding<br>        return self._get_text_embedding(text)<br>    def _get_text_embeddings(self, texts: List[str]) -> List[Embedding]:<br>        \"\"\"<br>        Embed the input sequence of text synchronously.<br>        Subclasses can implement this method if batch queries are supported.<br>        \"\"\"<br>        # Default implementation just loops over _get_text_embedding<br>        return [self._get_text_embedding(text) for text in texts]<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[Embedding]:<br>        \"\"\"<br>        Embed the input sequence of text asynchronously.<br>        Subclasses can implement this method if batch queries are supported.<br>        \"\"\"<br>        return await asyncio.gather(<br>            *[self._aget_text_embedding(text) for text in texts]<br>        )<br>    @dispatcher.span<br>    def get_text_embedding(self, text: str) -> Embedding:<br>        \"\"\"<br>        Embed the input text.<br>        When embedding text, depending on the model, a special instruction<br>        can be prepended to the raw text string. For example, \"Represent the<br>        document for retrieval: \". If you're curious, other examples of<br>        predefined instructions can be found in embeddings/huggingface_utils.py.<br>        \"\"\"<br>        model_dict = self.to_dict()<br>        model_dict.pop(\"api_key\", None)<br>        dispatcher.event(<br>            EmbeddingStartEvent(<br>                model_dict=model_dict,<br>            )<br>        )<br>        with self.callback_manager.event(<br>            CBEventType.EMBEDDING, payload={EventPayload.SERIALIZED: self.to_dict()}<br>        ) as event:<br>            text_embedding = self._get_text_embedding(text)<br>            event.on_end(<br>                payload={<br>                    EventPayload.CHUNKS: [text],<br>                    EventPayload.EMBEDDINGS: [text_embedding],<br>                }<br>            )<br>        dispatcher.event(<br>            EmbeddingEndEvent(<br>                chunks=[text],<br>                embeddings=[text_embedding],<br>            )<br>        )<br>        return text_embedding<br>    @dispatcher.span<br>    async def aget_text_embedding(self, text: str) -> Embedding:<br>        \"\"\"Async get text embedding.\"\"\"<br>        model_dict = self.to_dict()<br>        model_dict.pop(\"api_key\", None)<br>        dispatcher.event(<br>            EmbeddingStartEvent(<br>                model_dict=model_dict,<br>            )<br>        )<br>        with self.callback_manager.event(<br>            CBEventType.EMBEDDING, payload={EventPayload.SERIALIZED: self.to_dict()}<br>        ) as event:<br>            text_embedding = await self._aget_text_embedding(text)<br>            event.on_end(<br>                payload={<br>                    EventPayload.CHUNKS: [text],<br>                    EventPayload.EMBEDDINGS: [text_embedding],<br>                }<br>            )<br>        dispatcher.event(<br>            EmbeddingEndEvent(<br>                chunks=[text],<br>                embeddings=[text_embedding],<br>            )<br>        )<br>        return text_embedding<br>    @dispatcher.span<br>    def get_text_embedding_batch(<br>        self,<br>        texts: List[str],<br>        show_progress: bool = False,<br>        **kwargs: Any,<br>    ) -> List[Embedding]:<br>        \"\"\"Get a list of text embeddings, with batching.\"\"\"<br>        cur_batch: List[str] = []<br>        result_embeddings: List[Embedding] = []<br>        queue_with_progress = enumerate(<br>            get_tqdm_iterable(texts, show_progress, \"Generating embeddings\")<br>        )<br>        model_dict = self.to_dict()<br>        model_dict.pop(\"api_key\", None)<br>        for idx, text in queue_with_progress:<br>            cur_batch.append(text)<br>            if idx == len(texts) - 1 or len(cur_batch) == self.embed_batch_size:<br>                # flush<br>                dispatcher.event(<br>                    EmbeddingStartEvent(<br>                        model_dict=model_dict,<br>                    )<br>                )<br>                with self.callback_manager.event(<br>                    CBEventType.EMBEDDING,<br>                    payload={EventPayload.SERIALIZED: self.to_dict()},<br>                ) as event:<br>                    embeddings = self._get_text_embeddings(cur_batch)<br>                    result_embeddings.extend(embeddings)<br>                    event.on_end(<br>                        payload={<br>                            EventPayload.CHUNKS: cur_batch,<br>                            EventPayload.EMBEDDINGS: embeddings,<br>                        },<br>                    )<br>                dispatcher.event(<br>                    EmbeddingEndEvent(<br>                        chunks=cur_batch,<br>                        embeddings=embeddings,<br>                    )<br>                )<br>                cur_batch = []<br>        return result_embeddings<br>    @dispatcher.span<br>    async def aget_text_embedding_batch(<br>        self, texts: List[str], show_progress: bool = False<br>    ) -> List[Embedding]:<br>        \"\"\"Asynchronously get a list of text embeddings, with batching.\"\"\"<br>        num_workers = self.num_workers<br>        model_dict = self.to_dict()<br>        model_dict.pop(\"api_key\", None)<br>        cur_batch: List[str] = []<br>        callback_payloads: List[Tuple[str, List[str]]] = []<br>        result_embeddings: List[Embedding] = []<br>        embeddings_coroutines: List[Coroutine] = []<br>        for idx, text in enumerate(texts):<br>            cur_batch.append(text)<br>            if idx == len(texts) - 1 or len(cur_batch) == self.embed_batch_size:<br>                # flush<br>                dispatcher.event(<br>                    EmbeddingStartEvent(<br>                        model_dict=model_dict,<br>                    )<br>                )<br>                event_id = self.callback_manager.on_event_start(<br>                    CBEventType.EMBEDDING,<br>                    payload={EventPayload.SERIALIZED: self.to_dict()},<br>                )<br>                callback_payloads.append((event_id, cur_batch))<br>                embeddings_coroutines.append(self._aget_text_embeddings(cur_batch))<br>                cur_batch = []<br>        # flatten the results of asyncio.gather, which is a list of embeddings lists<br>        nested_embeddings = []<br>        if num_workers and num_workers > 1:<br>            nested_embeddings = await run_jobs(<br>                embeddings_coroutines,<br>                show_progress=show_progress,<br>                workers=self.num_workers,<br>                desc=\"Generating embeddings\",<br>            )<br>        else:<br>            if show_progress:<br>                try:<br>                    from tqdm.asyncio import tqdm_asyncio<br>                    nested_embeddings = await tqdm_asyncio.gather(<br>                        *embeddings_coroutines,<br>                        total=len(embeddings_coroutines),<br>                        desc=\"Generating embeddings\",<br>                    )<br>                except ImportError:<br>                    nested_embeddings = await asyncio.gather(*embeddings_coroutines)<br>            else:<br>                nested_embeddings = await asyncio.gather(*embeddings_coroutines)<br>        result_embeddings = [<br>            embedding for embeddings in nested_embeddings for embedding in embeddings<br>        ]<br>        for (event_id, text_batch), embeddings in zip(<br>            callback_payloads, nested_embeddings<br>        ):<br>            dispatcher.event(<br>                EmbeddingEndEvent(<br>                    chunks=text_batch,<br>                    embeddings=embeddings,<br>                )<br>            )<br>            self.callback_manager.on_event_end(<br>                CBEventType.EMBEDDING,<br>                payload={<br>                    EventPayload.CHUNKS: text_batch,<br>                    EventPayload.EMBEDDINGS: embeddings,<br>                },<br>                event_id=event_id,<br>            )<br>        return result_embeddings<br>    def similarity(<br>        self,<br>        embedding1: Embedding,<br>        embedding2: Embedding,<br>        mode: SimilarityMode = SimilarityMode.DEFAULT,<br>    ) -> float:<br>        \"\"\"Get embedding similarity.\"\"\"<br>        return similarity(embedding1=embedding1, embedding2=embedding2, mode=mode)<br>    def __call__(self, nodes: Sequence[BaseNode], **kwargs: Any) -> Sequence[BaseNode]:<br>        embeddings = self.get_text_embedding_batch(<br>            [node.get_content(metadata_mode=MetadataMode.EMBED) for node in nodes],<br>            **kwargs,<br>        )<br>        for node, embedding in zip(nodes, embeddings):<br>            node.embedding = embedding<br>        return nodes<br>    async def acall(<br>        self, nodes: Sequence[BaseNode], **kwargs: Any<br>    ) -> Sequence[BaseNode]:<br>        embeddings = await self.aget_text_embedding_batch(<br>            [node.get_content(metadata_mode=MetadataMode.EMBED) for node in nodes],<br>            **kwargs,<br>        )<br>        for node, embedding in zip(nodes, embeddings):<br>            node.embedding = embedding<br>        return nodes<br>``` |\n\n### get\\_query\\_embedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/\\#llama_index.core.embeddings.BaseEmbedding.get_query_embedding \"Permanent link\")\n\n```\nget_query_embedding(query: str) -> Embedding\n\n```\n\nEmbed the input query.\n\nWhen embedding a query, depending on the model, a special instruction\ncan be prepended to the raw query string. For example, \"Represent the\nquestion for retrieving supporting documents: \". If you're curious,\nother examples of predefined instructions can be found in\nembeddings/huggingface\\_utils.py.\n\nSource code in `llama-index-core/llama_index/core/base/embeddings/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>``` | ```<br>@dispatcher.span<br>def get_query_embedding(self, query: str) -> Embedding:<br>    \"\"\"<br>    Embed the input query.<br>    When embedding a query, depending on the model, a special instruction<br>    can be prepended to the raw query string. For example, \"Represent the<br>    question for retrieving supporting documents: \". If you're curious,<br>    other examples of predefined instructions can be found in<br>    embeddings/huggingface_utils.py.<br>    \"\"\"<br>    model_dict = self.to_dict()<br>    model_dict.pop(\"api_key\", None)<br>    dispatcher.event(<br>        EmbeddingStartEvent(<br>            model_dict=model_dict,<br>        )<br>    )<br>    with self.callback_manager.event(<br>        CBEventType.EMBEDDING, payload={EventPayload.SERIALIZED: self.to_dict()}<br>    ) as event:<br>        query_embedding = self._get_query_embedding(query)<br>        event.on_end(<br>            payload={<br>                EventPayload.CHUNKS: [query],<br>                EventPayload.EMBEDDINGS: [query_embedding],<br>            },<br>        )<br>    dispatcher.event(<br>        EmbeddingEndEvent(<br>            chunks=[query],<br>            embeddings=[query_embedding],<br>        )<br>    )<br>    return query_embedding<br>``` |\n\n### aget\\_query\\_embedding`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/\\#llama_index.core.embeddings.BaseEmbedding.aget_query_embedding \"Permanent link\")\n\n```\naget_query_embedding(query: str) -> Embedding\n\n```\n\nGet query embedding.\n\nSource code in `llama-index-core/llama_index/core/base/embeddings/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>``` | ```<br>@dispatcher.span<br>async def aget_query_embedding(self, query: str) -> Embedding:<br>    \"\"\"Get query embedding.\"\"\"<br>    model_dict = self.to_dict()<br>    model_dict.pop(\"api_key\", None)<br>    dispatcher.event(<br>        EmbeddingStartEvent(<br>            model_dict=model_dict,<br>        )<br>    )<br>    with self.callback_manager.event(<br>        CBEventType.EMBEDDING, payload={EventPayload.SERIALIZED: self.to_dict()}<br>    ) as event:<br>        query_embedding = await self._aget_query_embedding(query)<br>        event.on_end(<br>            payload={<br>                EventPayload.CHUNKS: [query],<br>                EventPayload.EMBEDDINGS: [query_embedding],<br>            },<br>        )<br>    dispatcher.event(<br>        EmbeddingEndEvent(<br>            chunks=[query],<br>            embeddings=[query_embedding],<br>        )<br>    )<br>    return query_embedding<br>``` |\n\n### get\\_agg\\_embedding\\_from\\_queries [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/\\#llama_index.core.embeddings.BaseEmbedding.get_agg_embedding_from_queries \"Permanent link\")\n\n```\nget_agg_embedding_from_queries(queries: List[str], agg_fn: Optional[Callable[..., Embedding]] = None) -> Embedding\n\n```\n\nGet aggregated embedding from multiple queries.\n\nSource code in `llama-index-core/llama_index/core/base/embeddings/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>``` | ```<br>def get_agg_embedding_from_queries(<br>    self,<br>    queries: List[str],<br>    agg_fn: Optional[Callable[..., Embedding]] = None,<br>) -> Embedding:<br>    \"\"\"Get aggregated embedding from multiple queries.\"\"\"<br>    query_embeddings = [self.get_query_embedding(query) for query in queries]<br>    agg_fn = agg_fn or mean_agg<br>    return agg_fn(query_embeddings)<br>``` |\n\n### aget\\_agg\\_embedding\\_from\\_queries`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/\\#llama_index.core.embeddings.BaseEmbedding.aget_agg_embedding_from_queries \"Permanent link\")\n\n```\naget_agg_embedding_from_queries(queries: List[str], agg_fn: Optional[Callable[..., Embedding]] = None) -> Embedding\n\n```\n\nAsync get aggregated embedding from multiple queries.\n\nSource code in `llama-index-core/llama_index/core/base/embeddings/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>``` | ```<br>async def aget_agg_embedding_from_queries(<br>    self,<br>    queries: List[str],<br>    agg_fn: Optional[Callable[..., Embedding]] = None,<br>) -> Embedding:<br>    \"\"\"Async get aggregated embedding from multiple queries.\"\"\"<br>    query_embeddings = [await self.aget_query_embedding(query) for query in queries]<br>    agg_fn = agg_fn or mean_agg<br>    return agg_fn(query_embeddings)<br>``` |\n\n### get\\_text\\_embedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/\\#llama_index.core.embeddings.BaseEmbedding.get_text_embedding \"Permanent link\")\n\n```\nget_text_embedding(text: str) -> Embedding\n\n```\n\nEmbed the input text.\n\nWhen embedding text, depending on the model, a special instruction\ncan be prepended to the raw text string. For example, \"Represent the\ndocument for retrieval: \". If you're curious, other examples of\npredefined instructions can be found in embeddings/huggingface\\_utils.py.\n\nSource code in `llama-index-core/llama_index/core/base/embeddings/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>``` | ```<br>@dispatcher.span<br>def get_text_embedding(self, text: str) -> Embedding:<br>    \"\"\"<br>    Embed the input text.<br>    When embedding text, depending on the model, a special instruction<br>    can be prepended to the raw text string. For example, \"Represent the<br>    document for retrieval: \". If you're curious, other examples of<br>    predefined instructions can be found in embeddings/huggingface_utils.py.<br>    \"\"\"<br>    model_dict = self.to_dict()<br>    model_dict.pop(\"api_key\", None)<br>    dispatcher.event(<br>        EmbeddingStartEvent(<br>            model_dict=model_dict,<br>        )<br>    )<br>    with self.callback_manager.event(<br>        CBEventType.EMBEDDING, payload={EventPayload.SERIALIZED: self.to_dict()}<br>    ) as event:<br>        text_embedding = self._get_text_embedding(text)<br>        event.on_end(<br>            payload={<br>                EventPayload.CHUNKS: [text],<br>                EventPayload.EMBEDDINGS: [text_embedding],<br>            }<br>        )<br>    dispatcher.event(<br>        EmbeddingEndEvent(<br>            chunks=[text],<br>            embeddings=[text_embedding],<br>        )<br>    )<br>    return text_embedding<br>``` |\n\n### aget\\_text\\_embedding`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/\\#llama_index.core.embeddings.BaseEmbedding.aget_text_embedding \"Permanent link\")\n\n```\naget_text_embedding(text: str) -> Embedding\n\n```\n\nAsync get text embedding.\n\nSource code in `llama-index-core/llama_index/core/base/embeddings/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>``` | ```<br>@dispatcher.span<br>async def aget_text_embedding(self, text: str) -> Embedding:<br>    \"\"\"Async get text embedding.\"\"\"<br>    model_dict = self.to_dict()<br>    model_dict.pop(\"api_key\", None)<br>    dispatcher.event(<br>        EmbeddingStartEvent(<br>            model_dict=model_dict,<br>        )<br>    )<br>    with self.callback_manager.event(<br>        CBEventType.EMBEDDING, payload={EventPayload.SERIALIZED: self.to_dict()}<br>    ) as event:<br>        text_embedding = await self._aget_text_embedding(text)<br>        event.on_end(<br>            payload={<br>                EventPayload.CHUNKS: [text],<br>                EventPayload.EMBEDDINGS: [text_embedding],<br>            }<br>        )<br>    dispatcher.event(<br>        EmbeddingEndEvent(<br>            chunks=[text],<br>            embeddings=[text_embedding],<br>        )<br>    )<br>    return text_embedding<br>``` |\n\n### get\\_text\\_embedding\\_batch [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/\\#llama_index.core.embeddings.BaseEmbedding.get_text_embedding_batch \"Permanent link\")\n\n```\nget_text_embedding_batch(texts: List[str], show_progress: bool = False, **kwargs: Any) -> List[Embedding]\n\n```\n\nGet a list of text embeddings, with batching.\n\nSource code in `llama-index-core/llama_index/core/base/embeddings/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>``` | ```<br>@dispatcher.span<br>def get_text_embedding_batch(<br>    self,<br>    texts: List[str],<br>    show_progress: bool = False,<br>    **kwargs: Any,<br>) -> List[Embedding]:<br>    \"\"\"Get a list of text embeddings, with batching.\"\"\"<br>    cur_batch: List[str] = []<br>    result_embeddings: List[Embedding] = []<br>    queue_with_progress = enumerate(<br>        get_tqdm_iterable(texts, show_progress, \"Generating embeddings\")<br>    )<br>    model_dict = self.to_dict()<br>    model_dict.pop(\"api_key\", None)<br>    for idx, text in queue_with_progress:<br>        cur_batch.append(text)<br>        if idx == len(texts) - 1 or len(cur_batch) == self.embed_batch_size:<br>            # flush<br>            dispatcher.event(<br>                EmbeddingStartEvent(<br>                    model_dict=model_dict,<br>                )<br>            )<br>            with self.callback_manager.event(<br>                CBEventType.EMBEDDING,<br>                payload={EventPayload.SERIALIZED: self.to_dict()},<br>            ) as event:<br>                embeddings = self._get_text_embeddings(cur_batch)<br>                result_embeddings.extend(embeddings)<br>                event.on_end(<br>                    payload={<br>                        EventPayload.CHUNKS: cur_batch,<br>                        EventPayload.EMBEDDINGS: embeddings,<br>                    },<br>                )<br>            dispatcher.event(<br>                EmbeddingEndEvent(<br>                    chunks=cur_batch,<br>                    embeddings=embeddings,<br>                )<br>            )<br>            cur_batch = []<br>    return result_embeddings<br>``` |\n\n### aget\\_text\\_embedding\\_batch`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/\\#llama_index.core.embeddings.BaseEmbedding.aget_text_embedding_batch \"Permanent link\")\n\n```\naget_text_embedding_batch(texts: List[str], show_progress: bool = False) -> List[Embedding]\n\n```\n\nAsynchronously get a list of text embeddings, with batching.\n\nSource code in `llama-index-core/llama_index/core/base/embeddings/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>``` | ```<br>@dispatcher.span<br>async def aget_text_embedding_batch(<br>    self, texts: List[str], show_progress: bool = False<br>) -> List[Embedding]:<br>    \"\"\"Asynchronously get a list of text embeddings, with batching.\"\"\"<br>    num_workers = self.num_workers<br>    model_dict = self.to_dict()<br>    model_dict.pop(\"api_key\", None)<br>    cur_batch: List[str] = []<br>    callback_payloads: List[Tuple[str, List[str]]] = []<br>    result_embeddings: List[Embedding] = []<br>    embeddings_coroutines: List[Coroutine] = []<br>    for idx, text in enumerate(texts):<br>        cur_batch.append(text)<br>        if idx == len(texts) - 1 or len(cur_batch) == self.embed_batch_size:<br>            # flush<br>            dispatcher.event(<br>                EmbeddingStartEvent(<br>                    model_dict=model_dict,<br>                )<br>            )<br>            event_id = self.callback_manager.on_event_start(<br>                CBEventType.EMBEDDING,<br>                payload={EventPayload.SERIALIZED: self.to_dict()},<br>            )<br>            callback_payloads.append((event_id, cur_batch))<br>            embeddings_coroutines.append(self._aget_text_embeddings(cur_batch))<br>            cur_batch = []<br>    # flatten the results of asyncio.gather, which is a list of embeddings lists<br>    nested_embeddings = []<br>    if num_workers and num_workers > 1:<br>        nested_embeddings = await run_jobs(<br>            embeddings_coroutines,<br>            show_progress=show_progress,<br>            workers=self.num_workers,<br>            desc=\"Generating embeddings\",<br>        )<br>    else:<br>        if show_progress:<br>            try:<br>                from tqdm.asyncio import tqdm_asyncio<br>                nested_embeddings = await tqdm_asyncio.gather(<br>                    *embeddings_coroutines,<br>                    total=len(embeddings_coroutines),<br>                    desc=\"Generating embeddings\",<br>                )<br>            except ImportError:<br>                nested_embeddings = await asyncio.gather(*embeddings_coroutines)<br>        else:<br>            nested_embeddings = await asyncio.gather(*embeddings_coroutines)<br>    result_embeddings = [<br>        embedding for embeddings in nested_embeddings for embedding in embeddings<br>    ]<br>    for (event_id, text_batch), embeddings in zip(<br>        callback_payloads, nested_embeddings<br>    ):<br>        dispatcher.event(<br>            EmbeddingEndEvent(<br>                chunks=text_batch,<br>                embeddings=embeddings,<br>            )<br>        )<br>        self.callback_manager.on_event_end(<br>            CBEventType.EMBEDDING,<br>            payload={<br>                EventPayload.CHUNKS: text_batch,<br>                EventPayload.EMBEDDINGS: embeddings,<br>            },<br>            event_id=event_id,<br>        )<br>    return result_embeddings<br>``` |\n\n### similarity [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/\\#llama_index.core.embeddings.BaseEmbedding.similarity \"Permanent link\")\n\n```\nsimilarity(embedding1: Embedding, embedding2: Embedding, mode: SimilarityMode = SimilarityMode.DEFAULT) -> float\n\n```\n\nGet embedding similarity.\n\nSource code in `llama-index-core/llama_index/core/base/embeddings/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>``` | ```<br>def similarity(<br>    self,<br>    embedding1: Embedding,<br>    embedding2: Embedding,<br>    mode: SimilarityMode = SimilarityMode.DEFAULT,<br>) -> float:<br>    \"\"\"Get embedding similarity.\"\"\"<br>    return similarity(embedding1=embedding1, embedding2=embedding2, mode=mode)<br>``` |\n\n## resolve\\_embed\\_model [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/\\#llama_index.core.embeddings.resolve_embed_model \"Permanent link\")\n\n```\nresolve_embed_model(embed_model: Optional[EmbedType] = None, callback_manager: Optional[CallbackManager] = None) -> BaseEmbedding\n\n```\n\nResolve embed model.\n\nSource code in `llama-index-core/llama_index/core/embeddings/utils.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>``` | ```<br>def resolve_embed_model(<br>    embed_model: Optional[EmbedType] = None,<br>    callback_manager: Optional[CallbackManager] = None,<br>) -> BaseEmbedding:<br>    \"\"\"Resolve embed model.\"\"\"<br>    from llama_index.core.settings import Settings<br>    try:<br>        from llama_index.core.bridge.langchain import Embeddings as LCEmbeddings<br>    except ImportError:<br>        LCEmbeddings = None  # type: ignore<br>    if embed_model == \"default\":<br>        if os.getenv(\"IS_TESTING\"):<br>            embed_model = MockEmbedding(embed_dim=8)<br>            embed_model.callback_manager = callback_manager or Settings.callback_manager<br>            return embed_model<br>        try:<br>            from llama_index.embeddings.openai import (<br>                OpenAIEmbedding,<br>            )  # pants: no-infer-dep<br>            from llama_index.embeddings.openai.utils import (<br>                validate_openai_api_key,<br>            )  # pants: no-infer-dep<br>            embed_model = OpenAIEmbedding()<br>            validate_openai_api_key(embed_model.api_key)  # type: ignore<br>        except ImportError:<br>            raise ImportError(<br>                \"`llama-index-embeddings-openai` package not found, \"<br>                \"please run `pip install llama-index-embeddings-openai`\"<br>            )<br>        except ValueError as e:<br>            raise ValueError(<br>                \"\\n******\\n\"<br>                \"Could not load OpenAI embedding model. \"<br>                \"If you intended to use OpenAI, please check your OPENAI_API_KEY.\\n\"<br>                \"Original error:\\n\"<br>                f\"{e!s}\"<br>                \"\\nConsider using embed_model='local'.\\n\"<br>                \"Visit our documentation for more embedding options: \"<br>                \"https://docs.llamaindex.ai/en/stable/module_guides/models/\"<br>                \"embeddings.html#modules\"<br>                \"\\n******\"<br>            )<br>    # for image multi-modal embeddings<br>    elif isinstance(embed_model, str) and embed_model.startswith(\"clip\"):<br>        try:<br>            from llama_index.embeddings.clip import ClipEmbedding  # pants: no-infer-dep<br>            clip_model_name = (<br>                embed_model.split(\":\")[1] if \":\" in embed_model else \"ViT-B/32\"<br>            )<br>            embed_model = ClipEmbedding(model_name=clip_model_name)<br>        except ImportError as e:<br>            raise ImportError(<br>                \"`llama-index-embeddings-clip` package not found, \"<br>                \"please run `pip install llama-index-embeddings-clip` and `pip install git+https://github.com/openai/CLIP.git`\"<br>            )<br>    if isinstance(embed_model, str):<br>        try:<br>            from llama_index.embeddings.huggingface import (<br>                HuggingFaceEmbedding,<br>            )  # pants: no-infer-dep<br>            splits = embed_model.split(\":\", 1)<br>            is_local = splits[0]<br>            model_name = splits[1] if len(splits) > 1 else None<br>            if is_local != \"local\":<br>                raise ValueError(<br>                    \"embed_model must start with str 'local' or of type BaseEmbedding\"<br>                )<br>            cache_folder = os.path.join(get_cache_dir(), \"models\")<br>            os.makedirs(cache_folder, exist_ok=True)<br>            embed_model = HuggingFaceEmbedding(<br>                model_name=model_name, cache_folder=cache_folder<br>            )<br>        except ImportError:<br>            raise ImportError(<br>                \"`llama-index-embeddings-huggingface` package not found, \"<br>                \"please run `pip install llama-index-embeddings-huggingface`\"<br>            )<br>    if LCEmbeddings is not None and isinstance(embed_model, LCEmbeddings):<br>        try:<br>            from llama_index.embeddings.langchain import (<br>                LangchainEmbedding,<br>            )  # pants: no-infer-dep<br>            embed_model = LangchainEmbedding(embed_model)<br>        except ImportError as e:<br>            raise ImportError(<br>                \"`llama-index-embeddings-langchain` package not found, \"<br>                \"please run `pip install llama-index-embeddings-langchain`\"<br>            )<br>    if embed_model is None:<br>        print(\"Embeddings have been explicitly disabled. Using MockEmbedding.\")<br>        embed_model = MockEmbedding(embed_dim=1)<br>    assert isinstance(embed_model, BaseEmbedding)<br>    embed_model.callback_manager = callback_manager or Settings.callback_manager<br>    return embed_model<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Index - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai_legacy/#llama_index.agent.openai_legacy.ContextRetrieverOpenAIAgent)\n\n# Openai legacy\n\n## ContextRetrieverOpenAIAgent [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai_legacy/\\#llama_index.agent.openai_legacy.ContextRetrieverOpenAIAgent \"Permanent link\")\n\nBases: `BaseOpenAIAgent`\n\nContextRetriever OpenAI Agent.\n\nThis agent performs retrieval from BaseRetriever before\ncalling the LLM. Allows it to augment user message with context.\n\nNOTE: this is a beta feature, function interfaces might change.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `tools` | `List[BaseTool]` | A list of tools. | _required_ |\n| `retriever` | `BaseRetriever` | A retriever. | _required_ |\n| `qa_prompt` | `Optional[PromptTemplate]` | A QA prompt. | _required_ |\n| `context_separator` | `str` | A context separator. | _required_ |\n| `llm` | `Optional[OpenAI]` | An OpenAI LLM. | _required_ |\n| `chat_history` | `Optional[List[ChatMessage]]` | A chat history. | _required_ |\n| `prefix_messages` | `List[ChatMessage]` | List\\[ChatMessage\\]: A list of prefix messages. | _required_ |\n| `verbose` | `bool` | Whether to print debug statements. | `False` |\n| `max_function_calls` | `int` | Maximum number of function calls. | `DEFAULT_MAX_FUNCTION_CALLS` |\n| `callback_manager` | `Optional[CallbackManager]` | A callback manager. | `None` |\n\nSource code in `llama-index-integrations/agent/llama-index-agent-openai-legacy/llama_index/agent/openai_legacy/context_retriever_agent.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>``` | ```<br>class ContextRetrieverOpenAIAgent(BaseOpenAIAgent):<br>    \"\"\"ContextRetriever OpenAI Agent.<br>    This agent performs retrieval from BaseRetriever before<br>    calling the LLM. Allows it to augment user message with context.<br>    NOTE: this is a beta feature, function interfaces might change.<br>    Args:<br>        tools (List[BaseTool]): A list of tools.<br>        retriever (BaseRetriever): A retriever.<br>        qa_prompt (Optional[PromptTemplate]): A QA prompt.<br>        context_separator (str): A context separator.<br>        llm (Optional[OpenAI]): An OpenAI LLM.<br>        chat_history (Optional[List[ChatMessage]]): A chat history.<br>        prefix_messages: List[ChatMessage]: A list of prefix messages.<br>        verbose (bool): Whether to print debug statements.<br>        max_function_calls (int): Maximum number of function calls.<br>        callback_manager (Optional[CallbackManager]): A callback manager.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        tools: List[BaseTool],<br>        retriever: BaseRetriever,<br>        qa_prompt: PromptTemplate,<br>        context_separator: str,<br>        llm: OpenAI,<br>        memory: BaseMemory,<br>        prefix_messages: List[ChatMessage],<br>        verbose: bool = False,<br>        max_function_calls: int = DEFAULT_MAX_FUNCTION_CALLS,<br>        callback_manager: Optional[CallbackManager] = None,<br>    ) -> None:<br>        super().__init__(<br>            llm=llm,<br>            memory=memory,<br>            prefix_messages=prefix_messages,<br>            verbose=verbose,<br>            max_function_calls=max_function_calls,<br>            callback_manager=callback_manager,<br>        )<br>        self._tools = tools<br>        self._qa_prompt = qa_prompt<br>        self._retriever = retriever<br>        self._context_separator = context_separator<br>    @classmethod<br>    def from_tools_and_retriever(<br>        cls,<br>        tools: List[BaseTool],<br>        retriever: BaseRetriever,<br>        qa_prompt: Optional[PromptTemplate] = None,<br>        context_separator: str = \"\\n\",<br>        llm: Optional[LLM] = None,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        memory: Optional[BaseMemory] = None,<br>        memory_cls: Type[BaseMemory] = ChatMemoryBuffer,<br>        verbose: bool = False,<br>        max_function_calls: int = DEFAULT_MAX_FUNCTION_CALLS,<br>        callback_manager: Optional[CallbackManager] = None,<br>        system_prompt: Optional[str] = None,<br>        prefix_messages: Optional[List[ChatMessage]] = None,<br>    ) -> \"ContextRetrieverOpenAIAgent\":<br>        \"\"\"Create a ContextRetrieverOpenAIAgent from a retriever.<br>        Args:<br>            retriever (BaseRetriever): A retriever.<br>            qa_prompt (Optional[PromptTemplate]): A QA prompt.<br>            context_separator (str): A context separator.<br>            llm (Optional[OpenAI]): An OpenAI LLM.<br>            chat_history (Optional[ChatMessageHistory]): A chat history.<br>            verbose (bool): Whether to print debug statements.<br>            max_function_calls (int): Maximum number of function calls.<br>            callback_manager (Optional[CallbackManager]): A callback manager.<br>        \"\"\"<br>        qa_prompt = qa_prompt or DEFAULT_QA_PROMPT<br>        chat_history = chat_history or []<br>        llm = llm or Settings.llm<br>        if not isinstance(llm, OpenAI):<br>            raise ValueError(\"llm must be a OpenAI instance\")<br>        if callback_manager is not None:<br>            llm.callback_manager = callback_manager<br>        memory = memory or memory_cls.from_defaults(chat_history=chat_history, llm=llm)<br>        if not is_function_calling_model(llm.model):<br>            raise ValueError(<br>                f\"Model name {llm.model} does not support function calling API.\"<br>            )<br>        if system_prompt is not None:<br>            if prefix_messages is not None:<br>                raise ValueError(<br>                    \"Cannot specify both system_prompt and prefix_messages\"<br>                )<br>            prefix_messages = [ChatMessage(content=system_prompt, role=\"system\")]<br>        prefix_messages = prefix_messages or []<br>        return cls(<br>            tools=tools,<br>            retriever=retriever,<br>            qa_prompt=qa_prompt,<br>            context_separator=context_separator,<br>            llm=llm,<br>            memory=memory,<br>            prefix_messages=prefix_messages,<br>            verbose=verbose,<br>            max_function_calls=max_function_calls,<br>            callback_manager=callback_manager,<br>        )<br>    def _get_tools(self, message: str) -> List[BaseTool]:<br>        \"\"\"Get tools.\"\"\"<br>        return self._tools<br>    def _build_formatted_message(self, message: str) -> str:<br>        # augment user message<br>        retrieved_nodes_w_scores: List[NodeWithScore] = self._retriever.retrieve(<br>            message<br>        )<br>        retrieved_nodes = [node.node for node in retrieved_nodes_w_scores]<br>        retrieved_texts = [node.get_content() for node in retrieved_nodes]<br>        # format message<br>        context_str = self._context_separator.join(retrieved_texts)<br>        return self._qa_prompt.format(context_str=context_str, query_str=message)<br>    def chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Union[str, dict] = \"auto\",<br>    ) -> AgentChatResponse:<br>        \"\"\"Chat.\"\"\"<br>        formatted_message = self._build_formatted_message(message)<br>        if self._verbose:<br>            print_text(formatted_message + \"\\n\", color=\"yellow\")<br>        return super().chat(<br>            formatted_message, chat_history=chat_history, tool_choice=tool_choice<br>        )<br>    async def achat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Union[str, dict] = \"auto\",<br>    ) -> AgentChatResponse:<br>        \"\"\"Chat.\"\"\"<br>        formatted_message = self._build_formatted_message(message)<br>        if self._verbose:<br>            print_text(formatted_message + \"\\n\", color=\"yellow\")<br>        return await super().achat(<br>            formatted_message, chat_history=chat_history, tool_choice=tool_choice<br>        )<br>    def get_tools(self, message: str) -> List[BaseTool]:<br>        \"\"\"Get tools.\"\"\"<br>        return self._get_tools(message)<br>``` |\n\n### from\\_tools\\_and\\_retriever`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai_legacy/\\#llama_index.agent.openai_legacy.ContextRetrieverOpenAIAgent.from_tools_and_retriever \"Permanent link\")\n\n```\nfrom_tools_and_retriever(tools: List[BaseTool], retriever: BaseRetriever, qa_prompt: Optional[PromptTemplate] = None, context_separator: str = '\\n', llm: Optional[LLM] = None, chat_history: Optional[List[ChatMessage]] = None, memory: Optional[BaseMemory] = None, memory_cls: Type[BaseMemory] = ChatMemoryBuffer, verbose: bool = False, max_function_calls: int = DEFAULT_MAX_FUNCTION_CALLS, callback_manager: Optional[CallbackManager] = None, system_prompt: Optional[str] = None, prefix_messages: Optional[List[ChatMessage]] = None) -> ContextRetrieverOpenAIAgent\n\n```\n\nCreate a ContextRetrieverOpenAIAgent from a retriever.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `retriever` | `BaseRetriever` | A retriever. | _required_ |\n| `qa_prompt` | `Optional[PromptTemplate]` | A QA prompt. | `None` |\n| `context_separator` | `str` | A context separator. | `'\\n'` |\n| `llm` | `Optional[OpenAI]` | An OpenAI LLM. | `None` |\n| `chat_history` | `Optional[ChatMessageHistory]` | A chat history. | `None` |\n| `verbose` | `bool` | Whether to print debug statements. | `False` |\n| `max_function_calls` | `int` | Maximum number of function calls. | `DEFAULT_MAX_FUNCTION_CALLS` |\n| `callback_manager` | `Optional[CallbackManager]` | A callback manager. | `None` |\n\nSource code in `llama-index-integrations/agent/llama-index-agent-openai-legacy/llama_index/agent/openai_legacy/context_retriever_agent.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>``` | ```<br>@classmethod<br>def from_tools_and_retriever(<br>    cls,<br>    tools: List[BaseTool],<br>    retriever: BaseRetriever,<br>    qa_prompt: Optional[PromptTemplate] = None,<br>    context_separator: str = \"\\n\",<br>    llm: Optional[LLM] = None,<br>    chat_history: Optional[List[ChatMessage]] = None,<br>    memory: Optional[BaseMemory] = None,<br>    memory_cls: Type[BaseMemory] = ChatMemoryBuffer,<br>    verbose: bool = False,<br>    max_function_calls: int = DEFAULT_MAX_FUNCTION_CALLS,<br>    callback_manager: Optional[CallbackManager] = None,<br>    system_prompt: Optional[str] = None,<br>    prefix_messages: Optional[List[ChatMessage]] = None,<br>) -> \"ContextRetrieverOpenAIAgent\":<br>    \"\"\"Create a ContextRetrieverOpenAIAgent from a retriever.<br>    Args:<br>        retriever (BaseRetriever): A retriever.<br>        qa_prompt (Optional[PromptTemplate]): A QA prompt.<br>        context_separator (str): A context separator.<br>        llm (Optional[OpenAI]): An OpenAI LLM.<br>        chat_history (Optional[ChatMessageHistory]): A chat history.<br>        verbose (bool): Whether to print debug statements.<br>        max_function_calls (int): Maximum number of function calls.<br>        callback_manager (Optional[CallbackManager]): A callback manager.<br>    \"\"\"<br>    qa_prompt = qa_prompt or DEFAULT_QA_PROMPT<br>    chat_history = chat_history or []<br>    llm = llm or Settings.llm<br>    if not isinstance(llm, OpenAI):<br>        raise ValueError(\"llm must be a OpenAI instance\")<br>    if callback_manager is not None:<br>        llm.callback_manager = callback_manager<br>    memory = memory or memory_cls.from_defaults(chat_history=chat_history, llm=llm)<br>    if not is_function_calling_model(llm.model):<br>        raise ValueError(<br>            f\"Model name {llm.model} does not support function calling API.\"<br>        )<br>    if system_prompt is not None:<br>        if prefix_messages is not None:<br>            raise ValueError(<br>                \"Cannot specify both system_prompt and prefix_messages\"<br>            )<br>        prefix_messages = [ChatMessage(content=system_prompt, role=\"system\")]<br>    prefix_messages = prefix_messages or []<br>    return cls(<br>        tools=tools,<br>        retriever=retriever,<br>        qa_prompt=qa_prompt,<br>        context_separator=context_separator,<br>        llm=llm,<br>        memory=memory,<br>        prefix_messages=prefix_messages,<br>        verbose=verbose,<br>        max_function_calls=max_function_calls,<br>        callback_manager=callback_manager,<br>    )<br>``` |\n\n### chat [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai_legacy/\\#llama_index.agent.openai_legacy.ContextRetrieverOpenAIAgent.chat \"Permanent link\")\n\n```\nchat(message: str, chat_history: Optional[List[ChatMessage]] = None, tool_choice: Union[str, dict] = 'auto') -> AgentChatResponse\n\n```\n\nChat.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-openai-legacy/llama_index/agent/openai_legacy/context_retriever_agent.py`\n\n|     |     |\n| --- | --- |\n| ```<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>``` | ```<br>def chat(<br>    self,<br>    message: str,<br>    chat_history: Optional[List[ChatMessage]] = None,<br>    tool_choice: Union[str, dict] = \"auto\",<br>) -> AgentChatResponse:<br>    \"\"\"Chat.\"\"\"<br>    formatted_message = self._build_formatted_message(message)<br>    if self._verbose:<br>        print_text(formatted_message + \"\\n\", color=\"yellow\")<br>    return super().chat(<br>        formatted_message, chat_history=chat_history, tool_choice=tool_choice<br>    )<br>``` |\n\n### achat`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai_legacy/\\#llama_index.agent.openai_legacy.ContextRetrieverOpenAIAgent.achat \"Permanent link\")\n\n```\nachat(message: str, chat_history: Optional[List[ChatMessage]] = None, tool_choice: Union[str, dict] = 'auto') -> AgentChatResponse\n\n```\n\nChat.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-openai-legacy/llama_index/agent/openai_legacy/context_retriever_agent.py`\n\n|     |     |\n| --- | --- |\n| ```<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>``` | ```<br>async def achat(<br>    self,<br>    message: str,<br>    chat_history: Optional[List[ChatMessage]] = None,<br>    tool_choice: Union[str, dict] = \"auto\",<br>) -> AgentChatResponse:<br>    \"\"\"Chat.\"\"\"<br>    formatted_message = self._build_formatted_message(message)<br>    if self._verbose:<br>        print_text(formatted_message + \"\\n\", color=\"yellow\")<br>    return await super().achat(<br>        formatted_message, chat_history=chat_history, tool_choice=tool_choice<br>    )<br>``` |\n\n### get\\_tools [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai_legacy/\\#llama_index.agent.openai_legacy.ContextRetrieverOpenAIAgent.get_tools \"Permanent link\")\n\n```\nget_tools(message: str) -> List[BaseTool]\n\n```\n\nGet tools.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-openai-legacy/llama_index/agent/openai_legacy/context_retriever_agent.py`\n\n|     |     |\n| --- | --- |\n| ```<br>197<br>198<br>199<br>``` | ```<br>def get_tools(self, message: str) -> List[BaseTool]:<br>    \"\"\"Get tools.\"\"\"<br>    return self._get_tools(message)<br>``` |\n\n## FnRetrieverOpenAIAgent [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai_legacy/\\#llama_index.agent.openai_legacy.FnRetrieverOpenAIAgent \"Permanent link\")\n\nBases: `OpenAIAgent`\n\nFunction Retriever OpenAI Agent.\n\nUses our object retriever module to retrieve openai agent.\n\nNOTE: This is deprecated, you can just use the base `OpenAIAgent` class by\nspecifying the following:\n\n```\nagent = OpenAIAgent.from_tools(tool_retriever=retriever, ...)\n\n```\n\nSource code in `llama-index-integrations/agent/llama-index-agent-openai-legacy/llama_index/agent/openai_legacy/retriever_openai_agent.py`\n\n|     |     |\n| --- | --- |\n| ```<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>``` | ````<br>class FnRetrieverOpenAIAgent(OpenAIAgent):<br>    \"\"\"Function Retriever OpenAI Agent.<br>    Uses our object retriever module to retrieve openai agent.<br>    NOTE: This is deprecated, you can just use the base `OpenAIAgent` class by<br>    specifying the following:<br>    ```<br>    agent = OpenAIAgent.from_tools(tool_retriever=retriever, ...)<br>    ```<br>    \"\"\"<br>    @classmethod<br>    def from_retriever(<br>        cls, retriever: ObjectRetriever[BaseTool], **kwargs: Any<br>    ) -> \"FnRetrieverOpenAIAgent\":<br>        return cast(<br>            FnRetrieverOpenAIAgent, cls.from_tools(tool_retriever=retriever, **kwargs)<br>        )<br>```` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Openai legacy - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/agent/openai_legacy/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/wandb/#llama_index.callbacks.wandb.WandbCallbackHandler)\n\n# Wandb\n\n## WandbCallbackHandler [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/wandb/\\#llama_index.callbacks.wandb.WandbCallbackHandler \"Permanent link\")\n\nBases: `BaseCallbackHandler`\n\nCallback handler that logs events to wandb.\n\nNOTE: this is a beta feature. The usage within our codebase, and the interface\nmay change.\n\nUse the `WandbCallbackHandler` to log trace events to wandb. This handler is\nuseful for debugging and visualizing the trace events. It captures the payload of\nthe events and logs them to wandb. The handler also tracks the start and end of\nevents. This is particularly useful for debugging your LLM calls.\n\nThe `WandbCallbackHandler` can also be used to log the indices and graphs to wandb\nusing the `persist_index` method. This will save the indexes as artifacts in wandb.\nThe `load_storage_context` method can be used to load the indexes from wandb\nartifacts. This method will return a `StorageContext` object that can be used to\nbuild the index, using `load_index_from_storage`, `load_indices_from_storage` or\n`load_graph_from_storage` functions.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `event_starts_to_ignore` | `Optional[List[CBEventType]]` | list of event types to<br>ignore when tracking event starts. | `None` |\n| `event_ends_to_ignore` | `Optional[List[CBEventType]]` | list of event types to<br>ignore when tracking event ends. | `None` |\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-wandb/llama_index/callbacks/wandb/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>472<br>473<br>474<br>475<br>476<br>477<br>478<br>479<br>480<br>481<br>482<br>483<br>484<br>485<br>486<br>487<br>488<br>489<br>490<br>491<br>492<br>493<br>494<br>495<br>496<br>497<br>498<br>499<br>500<br>501<br>502<br>503<br>504<br>505<br>506<br>507<br>508<br>509<br>510<br>511<br>512<br>513<br>514<br>515<br>516<br>517<br>518<br>519<br>520<br>521<br>522<br>523<br>524<br>525<br>526<br>527<br>528<br>529<br>530<br>531<br>532<br>533<br>534<br>535<br>536<br>537<br>538<br>539<br>540<br>541<br>542<br>543<br>544<br>545<br>546<br>547<br>548<br>549<br>550<br>551<br>552<br>553<br>554<br>555<br>556<br>557<br>558<br>559<br>560<br>561<br>562<br>563<br>564<br>565<br>566<br>567<br>568<br>569<br>570<br>``` | ```<br>class WandbCallbackHandler(BaseCallbackHandler):<br>    \"\"\"Callback handler that logs events to wandb.<br>    NOTE: this is a beta feature. The usage within our codebase, and the interface<br>    may change.<br>    Use the `WandbCallbackHandler` to log trace events to wandb. This handler is<br>    useful for debugging and visualizing the trace events. It captures the payload of<br>    the events and logs them to wandb. The handler also tracks the start and end of<br>    events. This is particularly useful for debugging your LLM calls.<br>    The `WandbCallbackHandler` can also be used to log the indices and graphs to wandb<br>    using the `persist_index` method. This will save the indexes as artifacts in wandb.<br>    The `load_storage_context` method can be used to load the indexes from wandb<br>    artifacts. This method will return a `StorageContext` object that can be used to<br>    build the index, using `load_index_from_storage`, `load_indices_from_storage` or<br>    `load_graph_from_storage` functions.<br>    Args:<br>        event_starts_to_ignore (Optional[List[CBEventType]]): list of event types to<br>            ignore when tracking event starts.<br>        event_ends_to_ignore (Optional[List[CBEventType]]): list of event types to<br>            ignore when tracking event ends.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        run_args: Optional[WandbRunArgs] = None,<br>        tokenizer: Optional[Callable[[str], List]] = None,<br>        event_starts_to_ignore: Optional[List[CBEventType]] = None,<br>        event_ends_to_ignore: Optional[List[CBEventType]] = None,<br>    ) -> None:<br>        try:<br>            import wandb<br>            from wandb.sdk.data_types import trace_tree<br>            self._wandb = wandb<br>            self._trace_tree = trace_tree<br>        except ImportError:<br>            raise ImportError(<br>                \"WandbCallbackHandler requires wandb. \"<br>                \"Please install it with `pip install wandb`.\"<br>            )<br>        from llama_index.core.indices import (<br>            ComposableGraph,<br>            GPTEmptyIndex,<br>            GPTKeywordTableIndex,<br>            GPTRAKEKeywordTableIndex,<br>            GPTSimpleKeywordTableIndex,<br>            GPTSQLStructStoreIndex,<br>            GPTTreeIndex,<br>            GPTVectorStoreIndex,<br>            SummaryIndex,<br>        )<br>        self._IndexType = (<br>            ComposableGraph,<br>            GPTKeywordTableIndex,<br>            GPTSimpleKeywordTableIndex,<br>            GPTRAKEKeywordTableIndex,<br>            SummaryIndex,<br>            GPTEmptyIndex,<br>            GPTTreeIndex,<br>            GPTVectorStoreIndex,<br>            GPTSQLStructStoreIndex,<br>        )<br>        self._run_args = run_args<br>        # Check if a W&B run is already initialized; if not, initialize one<br>        self._ensure_run(should_print_url=(self._wandb.run is None))  # type: ignore[attr-defined]<br>        self._event_pairs_by_id: Dict[str, List[CBEvent]] = defaultdict(list)<br>        self._cur_trace_id: Optional[str] = None<br>        self._trace_map: Dict[str, List[str]] = defaultdict(list)<br>        self.tokenizer = tokenizer or get_tokenizer()<br>        self._token_counter = TokenCounter(tokenizer=self.tokenizer)<br>        event_starts_to_ignore = (<br>            event_starts_to_ignore if event_starts_to_ignore else []<br>        )<br>        event_ends_to_ignore = event_ends_to_ignore if event_ends_to_ignore else []<br>        super().__init__(<br>            event_starts_to_ignore=event_starts_to_ignore,<br>            event_ends_to_ignore=event_ends_to_ignore,<br>        )<br>    def on_event_start(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        parent_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> str:<br>        \"\"\"Store event start data by event type.<br>        Args:<br>            event_type (CBEventType): event type to store.<br>            payload (Optional[Dict[str, Any]]): payload to store.<br>            event_id (str): event id to store.<br>            parent_id (str): parent event id.<br>        \"\"\"<br>        event = CBEvent(event_type, payload=payload, id_=event_id)<br>        self._event_pairs_by_id[event.id_].append(event)<br>        return event.id_<br>    def on_event_end(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> None:<br>        \"\"\"Store event end data by event type.<br>        Args:<br>            event_type (CBEventType): event type to store.<br>            payload (Optional[Dict[str, Any]]): payload to store.<br>            event_id (str): event id to store.<br>        \"\"\"<br>        event = CBEvent(event_type, payload=payload, id_=event_id)<br>        self._event_pairs_by_id[event.id_].append(event)<br>        self._trace_map = defaultdict(list)<br>    def start_trace(self, trace_id: Optional[str] = None) -> None:<br>        \"\"\"Launch a trace.\"\"\"<br>        self._trace_map = defaultdict(list)<br>        self._cur_trace_id = trace_id<br>        self._start_time = datetime.now()<br>    def end_trace(<br>        self,<br>        trace_id: Optional[str] = None,<br>        trace_map: Optional[Dict[str, List[str]]] = None,<br>    ) -> None:<br>        # Ensure W&B run is initialized<br>        self._ensure_run()<br>        self._trace_map = trace_map or defaultdict(list)<br>        self._end_time = datetime.now()<br>        # Log the trace map to wandb<br>        # We can control what trace ids we want to log here.<br>        self.log_trace_tree()<br>        # TODO (ayulockin): Log the LLM token counts to wandb when weave is ready<br>    def log_trace_tree(self) -> None:<br>        \"\"\"Log the trace tree to wandb.\"\"\"<br>        try:<br>            child_nodes = self._trace_map[\"root\"]<br>            root_span = self._convert_event_pair_to_wb_span(<br>                self._event_pairs_by_id[child_nodes[0]],<br>                trace_id=self._cur_trace_id if len(child_nodes) > 1 else None,<br>            )<br>            if len(child_nodes) == 1:<br>                child_nodes = self._trace_map[child_nodes[0]]<br>                root_span = self._build_trace_tree(child_nodes, root_span)<br>            else:<br>                root_span = self._build_trace_tree(child_nodes, root_span)<br>            if root_span:<br>                root_trace = self._trace_tree.WBTraceTree(root_span)<br>                if self._wandb.run:  # type: ignore[attr-defined]<br>                    self._wandb.run.log({\"trace\": root_trace})  # type: ignore[attr-defined]<br>                self._wandb.termlog(\"Logged trace tree to W&B.\")  # type: ignore[attr-defined]<br>        except Exception as e:<br>            print(f\"Failed to log trace tree to W&B: {e}\")<br>            # ignore errors to not break user code<br>    def persist_index(<br>        self, index: \"IndexType\", index_name: str, persist_dir: Union[str, None] = None<br>    ) -> None:<br>        \"\"\"Upload an index to wandb as an artifact. You can learn more about W&B<br>        artifacts here: https://docs.wandb.ai/guides/artifacts.<br>        For the `ComposableGraph` index, the root id is stored as artifact metadata.<br>        Args:<br>            index (IndexType): index to upload.<br>            index_name (str): name of the index. This will be used as the artifact name.<br>            persist_dir (Union[str, None]): directory to persist the index. If None, a<br>                temporary directory will be created and used.<br>        \"\"\"<br>        if persist_dir is None:<br>            persist_dir = f\"{self._wandb.run.dir}/storage\"  # type: ignore<br>            _default_persist_dir = True<br>        if not os.path.exists(persist_dir):<br>            os.makedirs(persist_dir)<br>        if isinstance(index, self._IndexType):<br>            try:<br>                index.storage_context.persist(persist_dir)  # type: ignore<br>                metadata = None<br>                # For the `ComposableGraph` index, store the root id as metadata<br>                if isinstance(index, self._IndexType[0]):<br>                    root_id = index.root_id<br>                    metadata = {\"root_id\": root_id}<br>                self._upload_index_as_wb_artifact(persist_dir, index_name, metadata)<br>            except Exception as e:<br>                # Silently ignore errors to not break user code<br>                self._print_upload_index_fail_message(e)<br>        # clear the default storage dir<br>        if _default_persist_dir:<br>            shutil.rmtree(persist_dir, ignore_errors=True)<br>    def load_storage_context(<br>        self, artifact_url: str, index_download_dir: Union[str, None] = None<br>    ) -> \"StorageContext\":<br>        \"\"\"Download an index from wandb and return a storage context.<br>        Use this storage context to load the index into memory using<br>        `load_index_from_storage`, `load_indices_from_storage` or<br>        `load_graph_from_storage` functions.<br>        Args:<br>            artifact_url (str): url of the artifact to download. The artifact url will<br>                be of the form: `entity/project/index_name:version` and can be found in<br>                the W&B UI.<br>            index_download_dir (Union[str, None]): directory to download the index to.<br>        \"\"\"<br>        from llama_index.core.storage.storage_context import StorageContext<br>        artifact = self._wandb.use_artifact(artifact_url, type=\"storage_context\")  # type: ignore[attr-defined]<br>        artifact_dir = artifact.download(root=index_download_dir)<br>        return StorageContext.from_defaults(persist_dir=artifact_dir)<br>    def _upload_index_as_wb_artifact(<br>        self, dir_path: str, artifact_name: str, metadata: Optional[Dict]<br>    ) -> None:<br>        \"\"\"Utility function to upload a dir to W&B as an artifact.\"\"\"<br>        artifact = self._wandb.Artifact(artifact_name, type=\"storage_context\")  # type: ignore[attr-defined]<br>        if metadata:<br>            artifact.metadata = metadata<br>        artifact.add_dir(dir_path)<br>        self._wandb.run.log_artifact(artifact)  # type: ignore<br>    def _build_trace_tree(<br>        self, events: List[str], span: \"trace_tree.Span\"<br>    ) -> \"trace_tree.Span\":<br>        \"\"\"Build the trace tree from the trace map.\"\"\"<br>        for child_event in events:<br>            child_span = self._convert_event_pair_to_wb_span(<br>                self._event_pairs_by_id[child_event]<br>            )<br>            child_span = self._build_trace_tree(<br>                self._trace_map[child_event], child_span<br>            )<br>            span.add_child_span(child_span)<br>        return span<br>    def _convert_event_pair_to_wb_span(<br>        self,<br>        event_pair: List[CBEvent],<br>        trace_id: Optional[str] = None,<br>    ) -> \"trace_tree.Span\":<br>        \"\"\"Convert a pair of events to a wandb trace tree span.\"\"\"<br>        start_time_ms, end_time_ms = self._get_time_in_ms(event_pair)<br>        if trace_id is None:<br>            event_type = event_pair[0].event_type<br>            span_kind = self._map_event_type_to_span_kind(event_type)<br>        else:<br>            event_type = trace_id  # type: ignore<br>            span_kind = None<br>        wb_span = self._trace_tree.Span(<br>            name=f\"{event_type}\",<br>            span_kind=span_kind,<br>            start_time_ms=start_time_ms,<br>            end_time_ms=end_time_ms,<br>        )<br>        inputs, outputs, wb_span = self._add_payload_to_span(wb_span, event_pair)<br>        wb_span.add_named_result(inputs=inputs, outputs=outputs)  # type: ignore<br>        return wb_span<br>    def _map_event_type_to_span_kind(<br>        self, event_type: CBEventType<br>    ) -> Union[None, \"trace_tree.SpanKind\"]:<br>        \"\"\"Map a CBEventType to a wandb trace tree SpanKind.\"\"\"<br>        if event_type == CBEventType.CHUNKING:<br>            span_kind = None<br>        elif event_type == CBEventType.NODE_PARSING:<br>            span_kind = None<br>        elif event_type == CBEventType.EMBEDDING:<br>            # TODO: add span kind for EMBEDDING when it's available<br>            span_kind = None<br>        elif event_type == CBEventType.LLM:<br>            span_kind = self._trace_tree.SpanKind.LLM<br>        elif event_type == CBEventType.QUERY:<br>            span_kind = self._trace_tree.SpanKind.AGENT<br>        elif event_type == CBEventType.AGENT_STEP:<br>            span_kind = self._trace_tree.SpanKind.AGENT<br>        elif event_type == CBEventType.RETRIEVE:<br>            span_kind = self._trace_tree.SpanKind.TOOL<br>        elif event_type == CBEventType.SYNTHESIZE:<br>            span_kind = self._trace_tree.SpanKind.CHAIN<br>        elif event_type == CBEventType.TREE:<br>            span_kind = self._trace_tree.SpanKind.CHAIN<br>        elif event_type == CBEventType.SUB_QUESTION:<br>            span_kind = self._trace_tree.SpanKind.CHAIN<br>        elif event_type == CBEventType.RERANKING:<br>            span_kind = self._trace_tree.SpanKind.CHAIN<br>        elif event_type == CBEventType.FUNCTION_CALL:<br>            span_kind = self._trace_tree.SpanKind.TOOL<br>        else:<br>            span_kind = None<br>        return span_kind<br>    def _add_payload_to_span(<br>        self, span: \"trace_tree.Span\", event_pair: List[CBEvent]<br>    ) -> Tuple[Optional[Dict[str, Any]], Optional[Dict[str, Any]], \"trace_tree.Span\"]:<br>        \"\"\"Add the event's payload to the span.\"\"\"<br>        assert len(event_pair) == 2<br>        event_type = event_pair[0].event_type<br>        inputs = None<br>        outputs = None<br>        if event_type == CBEventType.NODE_PARSING:<br>            # TODO: disabled full detailed inputs/outputs due to UI lag<br>            inputs, outputs = self._handle_node_parsing_payload(event_pair)<br>        elif event_type == CBEventType.LLM:<br>            inputs, outputs, span = self._handle_llm_payload(event_pair, span)<br>        elif event_type == CBEventType.QUERY:<br>            inputs, outputs = self._handle_query_payload(event_pair)<br>        elif event_type == CBEventType.EMBEDDING:<br>            inputs, outputs = self._handle_embedding_payload(event_pair)<br>        return inputs, outputs, span<br>    def _handle_node_parsing_payload(<br>        self, event_pair: List[CBEvent]<br>    ) -> Tuple[Dict[str, Any], Dict[str, Any]]:<br>        \"\"\"Handle the payload of a NODE_PARSING event.\"\"\"<br>        inputs = event_pair[0].payload<br>        outputs = event_pair[-1].payload<br>        if inputs and EventPayload.DOCUMENTS in inputs:<br>            documents = inputs.pop(EventPayload.DOCUMENTS)<br>            inputs[\"num_documents\"] = len(documents)<br>        if outputs and EventPayload.NODES in outputs:<br>            nodes = outputs.pop(EventPayload.NODES)<br>            outputs[\"num_nodes\"] = len(nodes)<br>        return inputs or {}, outputs or {}<br>    def _handle_llm_payload(<br>        self, event_pair: List[CBEvent], span: \"trace_tree.Span\"<br>    ) -> Tuple[Dict[str, Any], Dict[str, Any], \"trace_tree.Span\"]:<br>        \"\"\"Handle the payload of a LLM event.\"\"\"<br>        inputs = event_pair[0].payload<br>        outputs = event_pair[-1].payload<br>        assert isinstance(inputs, dict) and isinstance(outputs, dict)<br>        # Get `original_template` from Prompt<br>        if EventPayload.PROMPT in inputs:<br>            inputs[EventPayload.PROMPT] = inputs[EventPayload.PROMPT]<br>        # Format messages<br>        if EventPayload.MESSAGES in inputs:<br>            inputs[EventPayload.MESSAGES] = \"\\n\".join(<br>                [str(x) for x in inputs[EventPayload.MESSAGES]]<br>            )<br>        token_counts = get_llm_token_counts(self._token_counter, outputs)<br>        metadata = {<br>            \"formatted_prompt_tokens_count\": token_counts.prompt_token_count,<br>            \"prediction_tokens_count\": token_counts.completion_token_count,<br>            \"total_tokens_used\": token_counts.total_token_count,<br>        }<br>        span.attributes = metadata<br>        # Make `response` part of `outputs`<br>        outputs = {EventPayload.RESPONSE: str(outputs[EventPayload.RESPONSE])}<br>        return inputs, outputs, span<br>    def _handle_query_payload(<br>        self, event_pair: List[CBEvent]<br>    ) -> Tuple[Optional[Dict[str, Any]], Dict[str, Any]]:<br>        \"\"\"Handle the payload of a QUERY event.\"\"\"<br>        inputs = event_pair[0].payload<br>        outputs = event_pair[-1].payload<br>        if outputs:<br>            response_obj = outputs[EventPayload.RESPONSE]<br>            response = str(outputs[EventPayload.RESPONSE])<br>            if type(response).__name__ == \"Response\":<br>                response = response_obj.response<br>            elif type(response).__name__ == \"StreamingResponse\":<br>                response = response_obj.get_response().response<br>        else:<br>            response = \" \"<br>        outputs = {\"response\": response}<br>        return inputs, outputs<br>    def _handle_embedding_payload(<br>        self,<br>        event_pair: List[CBEvent],<br>    ) -> Tuple[Optional[Dict[str, Any]], Dict[str, Any]]:<br>        event_pair[0].payload<br>        outputs = event_pair[-1].payload<br>        chunks = []<br>        if outputs:<br>            chunks = outputs.get(EventPayload.CHUNKS, [])<br>        return {}, {\"num_chunks\": len(chunks)}<br>    def _get_time_in_ms(self, event_pair: List[CBEvent]) -> Tuple[int, int]:<br>        \"\"\"Get the start and end time of an event pair in milliseconds.\"\"\"<br>        start_time = datetime.strptime(event_pair[0].time, TIMESTAMP_FORMAT)<br>        end_time = datetime.strptime(event_pair[1].time, TIMESTAMP_FORMAT)<br>        start_time_in_ms = int(<br>            (start_time - datetime(1970, 1, 1)).total_seconds() * 1000<br>        )<br>        end_time_in_ms = int((end_time - datetime(1970, 1, 1)).total_seconds() * 1000)<br>        return start_time_in_ms, end_time_in_ms<br>    def _ensure_run(self, should_print_url: bool = False) -> None:<br>        \"\"\"Ensures an active W&B run exists.<br>        If not, will start a new run with the provided run_args.<br>        \"\"\"<br>        if self._wandb.run is None:  # type: ignore[attr-defined]<br>            # Make a shallow copy of the run args, so we don't modify the original<br>            run_args = self._run_args or {}  # type: ignore<br>            run_args: dict = {**run_args}  # type: ignore<br>            # Prefer to run in silent mode since W&B has a lot of output<br>            # which can be undesirable when dealing with text-based models.<br>            if \"settings\" not in run_args:  # type: ignore<br>                run_args[\"settings\"] = {\"silent\": True}  # type: ignore<br>            # Start the run and add the stream table<br>            self._wandb.init(**run_args)  # type: ignore[attr-defined]<br>            self._wandb.run._label(repo=\"llama_index\")  # type: ignore<br>            if should_print_url:<br>                self._print_wandb_init_message(<br>                    self._wandb.run.settings.run_url  # type: ignore<br>                )<br>    def _print_wandb_init_message(self, run_url: str) -> None:<br>        \"\"\"Print a message to the terminal when W&B is initialized.\"\"\"<br>        self._wandb.termlog(  # type: ignore[attr-defined]<br>            f\"Streaming LlamaIndex events to W&B at {run_url}\\n\"<br>            \"`WandbCallbackHandler` is currently in beta.\\n\"<br>            \"Please report any issues to https://github.com/wandb/wandb/issues \"<br>            \"with the tag `llamaindex`.\"<br>        )<br>    def _print_upload_index_fail_message(self, e: Exception) -> None:<br>        \"\"\"Print a message to the terminal when uploading the index fails.\"\"\"<br>        self._wandb.termlog(  # type: ignore[attr-defined]<br>            f\"Failed to upload index to W&B with the following error: {e}\\n\"<br>        )<br>    def finish(self) -> None:<br>        \"\"\"Finish the callback handler.\"\"\"<br>        self._wandb.finish()  # type: ignore[attr-defined]<br>``` |\n\n### on\\_event\\_start [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/wandb/\\#llama_index.callbacks.wandb.WandbCallbackHandler.on_event_start \"Permanent link\")\n\n```\non_event_start(event_type: CBEventType, payload: Optional[Dict[str, Any]] = None, event_id: str = '', parent_id: str = '', **kwargs: Any) -> str\n\n```\n\nStore event start data by event type.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `event_type` | `CBEventType` | event type to store. | _required_ |\n| `payload` | `Optional[Dict[str, Any]]` | payload to store. | `None` |\n| `event_id` | `str` | event id to store. | `''` |\n| `parent_id` | `str` | parent event id. | `''` |\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-wandb/llama_index/callbacks/wandb/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>``` | ```<br>def on_event_start(<br>    self,<br>    event_type: CBEventType,<br>    payload: Optional[Dict[str, Any]] = None,<br>    event_id: str = \"\",<br>    parent_id: str = \"\",<br>    **kwargs: Any,<br>) -> str:<br>    \"\"\"Store event start data by event type.<br>    Args:<br>        event_type (CBEventType): event type to store.<br>        payload (Optional[Dict[str, Any]]): payload to store.<br>        event_id (str): event id to store.<br>        parent_id (str): parent event id.<br>    \"\"\"<br>    event = CBEvent(event_type, payload=payload, id_=event_id)<br>    self._event_pairs_by_id[event.id_].append(event)<br>    return event.id_<br>``` |\n\n### on\\_event\\_end [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/wandb/\\#llama_index.callbacks.wandb.WandbCallbackHandler.on_event_end \"Permanent link\")\n\n```\non_event_end(event_type: CBEventType, payload: Optional[Dict[str, Any]] = None, event_id: str = '', **kwargs: Any) -> None\n\n```\n\nStore event end data by event type.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `event_type` | `CBEventType` | event type to store. | _required_ |\n| `payload` | `Optional[Dict[str, Any]]` | payload to store. | `None` |\n| `event_id` | `str` | event id to store. | `''` |\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-wandb/llama_index/callbacks/wandb/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>``` | ```<br>def on_event_end(<br>    self,<br>    event_type: CBEventType,<br>    payload: Optional[Dict[str, Any]] = None,<br>    event_id: str = \"\",<br>    **kwargs: Any,<br>) -> None:<br>    \"\"\"Store event end data by event type.<br>    Args:<br>        event_type (CBEventType): event type to store.<br>        payload (Optional[Dict[str, Any]]): payload to store.<br>        event_id (str): event id to store.<br>    \"\"\"<br>    event = CBEvent(event_type, payload=payload, id_=event_id)<br>    self._event_pairs_by_id[event.id_].append(event)<br>    self._trace_map = defaultdict(list)<br>``` |\n\n### start\\_trace [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/wandb/\\#llama_index.callbacks.wandb.WandbCallbackHandler.start_trace \"Permanent link\")\n\n```\nstart_trace(trace_id: Optional[str] = None) -> None\n\n```\n\nLaunch a trace.\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-wandb/llama_index/callbacks/wandb/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>216<br>217<br>218<br>219<br>220<br>``` | ```<br>def start_trace(self, trace_id: Optional[str] = None) -> None:<br>    \"\"\"Launch a trace.\"\"\"<br>    self._trace_map = defaultdict(list)<br>    self._cur_trace_id = trace_id<br>    self._start_time = datetime.now()<br>``` |\n\n### log\\_trace\\_tree [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/wandb/\\#llama_index.callbacks.wandb.WandbCallbackHandler.log_trace_tree \"Permanent link\")\n\n```\nlog_trace_tree() -> None\n\n```\n\nLog the trace tree to wandb.\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-wandb/llama_index/callbacks/wandb/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>``` | ```<br>def log_trace_tree(self) -> None:<br>    \"\"\"Log the trace tree to wandb.\"\"\"<br>    try:<br>        child_nodes = self._trace_map[\"root\"]<br>        root_span = self._convert_event_pair_to_wb_span(<br>            self._event_pairs_by_id[child_nodes[0]],<br>            trace_id=self._cur_trace_id if len(child_nodes) > 1 else None,<br>        )<br>        if len(child_nodes) == 1:<br>            child_nodes = self._trace_map[child_nodes[0]]<br>            root_span = self._build_trace_tree(child_nodes, root_span)<br>        else:<br>            root_span = self._build_trace_tree(child_nodes, root_span)<br>        if root_span:<br>            root_trace = self._trace_tree.WBTraceTree(root_span)<br>            if self._wandb.run:  # type: ignore[attr-defined]<br>                self._wandb.run.log({\"trace\": root_trace})  # type: ignore[attr-defined]<br>            self._wandb.termlog(\"Logged trace tree to W&B.\")  # type: ignore[attr-defined]<br>    except Exception as e:<br>        print(f\"Failed to log trace tree to W&B: {e}\")<br>``` |\n\n### persist\\_index [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/wandb/\\#llama_index.callbacks.wandb.WandbCallbackHandler.persist_index \"Permanent link\")\n\n```\npersist_index(index: IndexType, index_name: str, persist_dir: Union[str, None] = None) -> None\n\n```\n\nUpload an index to wandb as an artifact. You can learn more about W&B\nartifacts here: https://docs.wandb.ai/guides/artifacts.\n\nFor the `ComposableGraph` index, the root id is stored as artifact metadata.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `index` | `IndexType` | index to upload. | _required_ |\n| `index_name` | `str` | name of the index. This will be used as the artifact name. | _required_ |\n| `persist_dir` | `Union[str, None]` | directory to persist the index. If None, a<br>temporary directory will be created and used. | `None` |\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-wandb/llama_index/callbacks/wandb/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>``` | ```<br>def persist_index(<br>    self, index: \"IndexType\", index_name: str, persist_dir: Union[str, None] = None<br>) -> None:<br>    \"\"\"Upload an index to wandb as an artifact. You can learn more about W&B<br>    artifacts here: https://docs.wandb.ai/guides/artifacts.<br>    For the `ComposableGraph` index, the root id is stored as artifact metadata.<br>    Args:<br>        index (IndexType): index to upload.<br>        index_name (str): name of the index. This will be used as the artifact name.<br>        persist_dir (Union[str, None]): directory to persist the index. If None, a<br>            temporary directory will be created and used.<br>    \"\"\"<br>    if persist_dir is None:<br>        persist_dir = f\"{self._wandb.run.dir}/storage\"  # type: ignore<br>        _default_persist_dir = True<br>    if not os.path.exists(persist_dir):<br>        os.makedirs(persist_dir)<br>    if isinstance(index, self._IndexType):<br>        try:<br>            index.storage_context.persist(persist_dir)  # type: ignore<br>            metadata = None<br>            # For the `ComposableGraph` index, store the root id as metadata<br>            if isinstance(index, self._IndexType[0]):<br>                root_id = index.root_id<br>                metadata = {\"root_id\": root_id}<br>            self._upload_index_as_wb_artifact(persist_dir, index_name, metadata)<br>        except Exception as e:<br>            # Silently ignore errors to not break user code<br>            self._print_upload_index_fail_message(e)<br>    # clear the default storage dir<br>    if _default_persist_dir:<br>        shutil.rmtree(persist_dir, ignore_errors=True)<br>``` |\n\n### load\\_storage\\_context [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/wandb/\\#llama_index.callbacks.wandb.WandbCallbackHandler.load_storage_context \"Permanent link\")\n\n```\nload_storage_context(artifact_url: str, index_download_dir: Union[str, None] = None) -> StorageContext\n\n```\n\nDownload an index from wandb and return a storage context.\n\nUse this storage context to load the index into memory using\n`load_index_from_storage`, `load_indices_from_storage` or\n`load_graph_from_storage` functions.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `artifact_url` | `str` | url of the artifact to download. The artifact url will<br>be of the form: `entity/project/index_name:version` and can be found in<br>the W&B UI. | _required_ |\n| `index_download_dir` | `Union[str, None]` | directory to download the index to. | `None` |\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-wandb/llama_index/callbacks/wandb/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>``` | ```<br>def load_storage_context(<br>    self, artifact_url: str, index_download_dir: Union[str, None] = None<br>) -> \"StorageContext\":<br>    \"\"\"Download an index from wandb and return a storage context.<br>    Use this storage context to load the index into memory using<br>    `load_index_from_storage`, `load_indices_from_storage` or<br>    `load_graph_from_storage` functions.<br>    Args:<br>        artifact_url (str): url of the artifact to download. The artifact url will<br>            be of the form: `entity/project/index_name:version` and can be found in<br>            the W&B UI.<br>        index_download_dir (Union[str, None]): directory to download the index to.<br>    \"\"\"<br>    from llama_index.core.storage.storage_context import StorageContext<br>    artifact = self._wandb.use_artifact(artifact_url, type=\"storage_context\")  # type: ignore[attr-defined]<br>    artifact_dir = artifact.download(root=index_download_dir)<br>    return StorageContext.from_defaults(persist_dir=artifact_dir)<br>``` |\n\n### finish [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/wandb/\\#llama_index.callbacks.wandb.WandbCallbackHandler.finish \"Permanent link\")\n\n```\nfinish() -> None\n\n```\n\nFinish the callback handler.\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-wandb/llama_index/callbacks/wandb/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>568<br>569<br>570<br>``` | ```<br>def finish(self) -> None:<br>    \"\"\"Finish the callback handler.\"\"\"<br>    self._wandb.finish()  # type: ignore[attr-defined]<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Wandb - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/wandb/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/argilla/#llama_index.callbacks.argilla.argilla_callback_handler)\n\n# Argilla\n\n## argilla\\_callback\\_handler [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/argilla/\\#llama_index.callbacks.argilla.argilla_callback_handler \"Permanent link\")\n\n```\nargilla_callback_handler(**kwargs: Any) -> BaseCallbackHandler\n\n```\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-argilla/llama_index/callbacks/argilla/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>``` | ```<br>def argilla_callback_handler(**kwargs: Any) -> BaseCallbackHandler:<br>    try:<br>        # lazy import<br>        from argilla_llama_index import ArgillaCallbackHandler<br>    except ImportError:<br>        raise ImportError(\"Please install Argilla with `pip install argilla`\")<br>    return ArgillaCallbackHandler(**kwargs)<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Argilla - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/argilla/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/cohere/#llama_index.embeddings.cohere.CohereEmbedding)\n\n# Cohere\n\n## CohereEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/cohere/\\#llama_index.embeddings.cohere.CohereEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nCohereEmbedding uses the Cohere API to generate embeddings for text.\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-cohere/llama_index/embeddings/cohere/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>``` | ```<br>class CohereEmbedding(BaseEmbedding):<br>    \"\"\"CohereEmbedding uses the Cohere API to generate embeddings for text.\"\"\"<br>    # Instance variables initialized via Pydantic's mechanism<br>    api_key: str = Field(description=\"The Cohere API key.\")<br>    truncate: str = Field(description=\"Truncation type - START/ END/ NONE\")<br>    input_type: Optional[str] = Field(<br>        description=\"Model Input type. If not provided, search_document and search_query are used when needed.\"<br>    )<br>    embedding_type: str = Field(<br>        description=\"Embedding type. If not provided float embedding_type is used when needed.\"<br>    )<br>    _client: cohere.Client = PrivateAttr()<br>    _async_client: cohere.AsyncClient = PrivateAttr()<br>    _base_url: Optional[str] = PrivateAttr()<br>    _timeout: Optional[float] = PrivateAttr()<br>    _httpx_client: Optional[httpx.Client] = PrivateAttr()<br>    _httpx_async_client: Optional[httpx.AsyncClient] = PrivateAttr()<br>    def __init__(<br>        self,<br>        # deprecated<br>        cohere_api_key: Optional[str] = None,<br>        api_key: Optional[str] = None,<br>        model_name: str = \"embed-english-v3.0\",<br>        truncate: str = \"END\",<br>        input_type: Optional[str] = None,<br>        embedding_type: str = \"float\",<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>        base_url: Optional[str] = None,<br>        timeout: Optional[float] = None,<br>        httpx_client: Optional[httpx.Client] = None,<br>        httpx_async_client: Optional[httpx.AsyncClient] = None,<br>        num_workers: Optional[int] = None,<br>        **kwargs: Any,<br>    ):<br>        \"\"\"<br>        A class representation for generating embeddings using the Cohere API.<br>        Args:<br>            truncate (str): A string indicating the truncation strategy to be applied to input text. Possible values<br>                        are 'START', 'END', or 'NONE'.<br>            input_type (Optional[str]): An optional string that specifies the type of input provided to the model.<br>                                    This is model-dependent and could be one of the following: 'search_query',<br>                                    'search_document', 'classification', or 'clustering'.<br>            model_name (str): The name of the model to be used for generating embeddings. The class ensures that<br>                          this model is supported and that the input type provided is compatible with the model.<br>        \"\"\"<br>        # Validate model_name and input_type<br>        if model_name not in VALID_MODEL_INPUT_TYPES:<br>            raise ValueError(f\"{model_name} is not a valid model name\")<br>        if input_type not in VALID_MODEL_INPUT_TYPES[model_name]:<br>            raise ValueError(<br>                f\"{input_type} is not a valid input type for the provided model.\"<br>            )<br>        if embedding_type not in VALID_MODEL_EMBEDDING_TYPES[model_name]:<br>            raise ValueError(<br>                f\"{embedding_type} is not a embedding type for the provided model.\"<br>            )<br>        if truncate not in VALID_TRUNCATE_OPTIONS:<br>            raise ValueError(f\"truncate must be one of {VALID_TRUNCATE_OPTIONS}\")<br>        super().__init__(<br>            api_key=api_key or cohere_api_key,<br>            model_name=model_name,<br>            input_type=input_type,<br>            embedding_type=embedding_type,<br>            truncate=truncate,<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            num_workers=num_workers,<br>            **kwargs,<br>        )<br>        self._client = None<br>        self._async_client = None<br>        self._base_url = base_url<br>        self._timeout = timeout<br>        self._httpx_client = httpx_client<br>        self._httpx_async_client = httpx_async_client<br>    def _get_client(self) -> cohere.Client:<br>        if self._client is None:<br>            self._client = cohere.Client(<br>                api_key=self.api_key,<br>                client_name=\"llama_index\",<br>                base_url=self._base_url,<br>                timeout=self._timeout,<br>                httpx_client=self._httpx_client,<br>            )<br>        return self._client<br>    def _get_async_client(self) -> cohere.AsyncClient:<br>        if self._async_client is None:<br>            self._async_client = cohere.AsyncClient(<br>                api_key=self.api_key,<br>                client_name=\"llama_index\",<br>                base_url=self._base_url,<br>                timeout=self._timeout,<br>                httpx_client=self._httpx_async_client,<br>            )<br>        return self._async_client<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"CohereEmbedding\"<br>    def _embed(self, texts: List[str], input_type: str) -> List[List[float]]:<br>        \"\"\"Embed sentences using Cohere.\"\"\"<br>        client = self._get_client()<br>        if self.model_name in V3_MODELS:<br>            result = client.embed(<br>                texts=texts,<br>                input_type=self.input_type or input_type,<br>                embedding_types=[self.embedding_type],<br>                model=self.model_name,<br>                truncate=self.truncate,<br>            ).embeddings<br>        else:<br>            result = client.embed(<br>                texts=texts,<br>                model=self.model_name,<br>                embedding_types=[self.embedding_type],<br>                truncate=self.truncate,<br>            ).embeddings<br>        return getattr(result, self.embedding_type, None)<br>    async def _aembed(self, texts: List[str], input_type: str) -> List[List[float]]:<br>        \"\"\"Embed sentences using Cohere.\"\"\"<br>        async_client = self._get_async_client()<br>        if self.model_name in V3_MODELS:<br>            result = (<br>                await async_client.embed(<br>                    texts=texts,<br>                    input_type=self.input_type or input_type,<br>                    embedding_types=[self.embedding_type],<br>                    model=self.model_name,<br>                    truncate=self.truncate,<br>                )<br>            ).embeddings<br>        else:<br>            result = (<br>                await async_client.embed(<br>                    texts=texts,<br>                    model=self.model_name,<br>                    embedding_types=[self.embedding_type],<br>                    truncate=self.truncate,<br>                )<br>            ).embeddings<br>        return getattr(result, self.embedding_type, None)<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding. For query embeddings, input_type='search_query'.\"\"\"<br>        return self._embed([query], input_type=\"search_query\")[0]<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding async. For query embeddings, input_type='search_query'.\"\"\"<br>        return (await self._aembed([query], input_type=\"search_query\"))[0]<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._embed([text], input_type=\"search_document\")[0]<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding async.\"\"\"<br>        return (await self._aembed([text], input_type=\"search_document\"))[0]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        return self._embed(texts, input_type=\"search_document\")<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        return await self._aembed(texts, input_type=\"search_document\")<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Cohere - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/cohere/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/#api-reference)\n\n# API Reference [\\#](https://docs.llamaindex.ai/en/stable/api_reference/\\#api-reference \"Permanent link\")\n\nLlamaIndex provides thorough documentation of modules and integrations used in the framework.\n\nUse the navigation or search to find the classes you are interested in!\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "API Reference - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/elasticsearch/#llama_index.embeddings.elasticsearch.ElasticsearchEmbedding)\n\n# Elasticsearch\n\n## ElasticsearchEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/elasticsearch/\\#llama_index.embeddings.elasticsearch.ElasticsearchEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nElasticsearch embedding models.\n\nThis class provides an interface to generate embeddings using a model deployed\nin an Elasticsearch cluster. It requires an Elasticsearch connection object\nand the model\\_id of the model deployed in the cluster.\n\nIn Elasticsearch you need to have an embedding model loaded and deployed.\n\\- https://www.elastic.co\n/guide/en/elasticsearch/reference/current/infer-trained-model.html\n\\- https://www.elastic.co\n/guide/en/machine-learning/current/ml-nlp-deploy-models.html\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-elasticsearch/llama_index/embeddings/elasticsearch/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 10<br> 11<br> 12<br> 13<br> 14<br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>``` | ```<br>class ElasticsearchEmbedding(BaseEmbedding):<br>    \"\"\"Elasticsearch embedding models.<br>    This class provides an interface to generate embeddings using a model deployed<br>    in an Elasticsearch cluster. It requires an Elasticsearch connection object<br>    and the model_id of the model deployed in the cluster.<br>    In Elasticsearch you need to have an embedding model loaded and deployed.<br>    - https://www.elastic.co<br>        /guide/en/elasticsearch/reference/current/infer-trained-model.html<br>    - https://www.elastic.co<br>        /guide/en/machine-learning/current/ml-nlp-deploy-models.html<br>    \"\"\"  #<br>    _client: Any = PrivateAttr()<br>    model_id: str<br>    input_field: str<br>    def class_name(self) -> str:<br>        return \"ElasticsearchEmbedding\"<br>    def __init__(<br>        self,<br>        client: Any,<br>        model_id: str,<br>        input_field: str = \"text_field\",<br>        **kwargs: Any,<br>    ):<br>        super().__init__(model_id=model_id, input_field=input_field, **kwargs)<br>        self._client = client<br>    @classmethod<br>    def from_es_connection(<br>        cls,<br>        model_id: str,<br>        es_connection: Any,<br>        input_field: str = \"text_field\",<br>    ) -> BaseEmbedding:<br>        \"\"\"<br>        Instantiate embeddings from an existing Elasticsearch connection.<br>        This method provides a way to create an instance of the ElasticsearchEmbedding<br>        class using an existing Elasticsearch connection. The connection object is used<br>        to create an MlClient, which is then used to initialize the<br>        ElasticsearchEmbedding instance.<br>        Args:<br>        model_id (str): The model_id of the model deployed in the Elasticsearch cluster.<br>        es_connection (elasticsearch.Elasticsearch): An existing Elasticsearch<br>            connection object.<br>        input_field (str, optional): The name of the key for the input text field<br>            in the document. Defaults to 'text_field'.<br>        Returns:<br>        ElasticsearchEmbedding: An instance of the ElasticsearchEmbedding class.<br>        Example:<br>            .. code-block:: python<br>                from elasticsearch import Elasticsearch<br>                from llama_index.embeddings.elasticsearch import ElasticsearchEmbedding<br>                # Define the model ID and input field name (if different from default)<br>                model_id = \"your_model_id\"<br>                # Optional, only if different from 'text_field'<br>                input_field = \"your_input_field\"<br>                # Create Elasticsearch connection<br>                es_connection = Elasticsearch(hosts=[\"localhost:9200\"], basic_auth=(\"user\", \"password\"))<br>                # Instantiate ElasticsearchEmbedding using the existing connection<br>                embeddings = ElasticsearchEmbedding.from_es_connection(<br>                    model_id,<br>                    es_connection,<br>                    input_field=input_field,<br>                )<br>        \"\"\"<br>        client = MlClient(es_connection)<br>        return cls(client, model_id, input_field=input_field)<br>    @classmethod<br>    def from_credentials(<br>        cls,<br>        model_id: str,<br>        es_url: str,<br>        es_username: str,<br>        es_password: str,<br>        input_field: str = \"text_field\",<br>    ) -> BaseEmbedding:<br>        \"\"\"Instantiate embeddings from Elasticsearch credentials.<br>        Args:<br>            model_id (str): The model_id of the model deployed in the Elasticsearch<br>                cluster.<br>            input_field (str): The name of the key for the input text field in the<br>                document. Defaults to 'text_field'.<br>            es_url: (str): The Elasticsearch url to connect to.<br>            es_username: (str): Elasticsearch username.<br>            es_password: (str): Elasticsearch password.<br>        Example:<br>            .. code-block:: python<br>                from llama_index.embeddings.bedrock import ElasticsearchEmbedding<br>                # Define the model ID and input field name (if different from default)<br>                model_id = \"your_model_id\"<br>                # Optional, only if different from 'text_field'<br>                input_field = \"your_input_field\"<br>                embeddings = ElasticsearchEmbedding.from_credentials(<br>                    model_id,<br>                    input_field=input_field,<br>                    es_url=\"foo\",<br>                    es_username=\"bar\",<br>                    es_password=\"baz\",<br>                )<br>        \"\"\"<br>        es_connection = Elasticsearch(<br>            hosts=[es_url],<br>            basic_auth=(es_username, es_password),<br>        )<br>        client = MlClient(es_connection)<br>        return cls(client, model_id, input_field=input_field)<br>    def _get_embedding(self, text: str) -> List[float]:<br>        \"\"\"<br>        Generate an embedding for a single query text.<br>        Args:<br>            text (str): The query text to generate an embedding for.<br>        Returns:<br>            List[float]: The embedding for the input query text.<br>        \"\"\"<br>        response = self._client.infer_trained_model(<br>            model_id=self.model_id,<br>            docs=[{self.input_field: text}],<br>        )<br>        return response[\"inference_results\"][0][\"predicted_value\"]<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        return self._get_embedding(text)<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        return self._get_embedding(query)<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        return self._get_query_embedding(query)<br>``` |\n\n### from\\_es\\_connection`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/elasticsearch/\\#llama_index.embeddings.elasticsearch.ElasticsearchEmbedding.from_es_connection \"Permanent link\")\n\n```\nfrom_es_connection(model_id: str, es_connection: Any, input_field: str = 'text_field') -> BaseEmbedding\n\n```\n\nInstantiate embeddings from an existing Elasticsearch connection.\n\nThis method provides a way to create an instance of the ElasticsearchEmbedding\nclass using an existing Elasticsearch connection. The connection object is used\nto create an MlClient, which is then used to initialize the\nElasticsearchEmbedding instance.\n\nArgs:\nmodel\\_id (str): The model\\_id of the model deployed in the Elasticsearch cluster.\nes\\_connection (elasticsearch.Elasticsearch): An existing Elasticsearch\nconnection object.\ninput\\_field (str, optional): The name of the key for the input text field\nin the document. Defaults to 'text\\_field'.\n\nReturns:\nElasticsearchEmbedding: An instance of the ElasticsearchEmbedding class.\n\nExample\n\n.. code-block:: python\n\n```\nfrom elasticsearch import Elasticsearch\n\nfrom llama_index.embeddings.elasticsearch import ElasticsearchEmbedding\n\n# Define the model ID and input field name (if different from default)\nmodel_id = \"your_model_id\"\n# Optional, only if different from 'text_field'\ninput_field = \"your_input_field\"\n\n# Create Elasticsearch connection\nes_connection = Elasticsearch(hosts=[\"localhost:9200\"], basic_auth=(\"user\", \"password\"))\n\n# Instantiate ElasticsearchEmbedding using the existing connection\nembeddings = ElasticsearchEmbedding.from_es_connection(\n    model_id,\n    es_connection,\n    input_field=input_field,\n)\n\n```\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-elasticsearch/llama_index/embeddings/elasticsearch/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>89<br>``` | ```<br>@classmethod<br>def from_es_connection(<br>    cls,<br>    model_id: str,<br>    es_connection: Any,<br>    input_field: str = \"text_field\",<br>) -> BaseEmbedding:<br>    \"\"\"<br>    Instantiate embeddings from an existing Elasticsearch connection.<br>    This method provides a way to create an instance of the ElasticsearchEmbedding<br>    class using an existing Elasticsearch connection. The connection object is used<br>    to create an MlClient, which is then used to initialize the<br>    ElasticsearchEmbedding instance.<br>    Args:<br>    model_id (str): The model_id of the model deployed in the Elasticsearch cluster.<br>    es_connection (elasticsearch.Elasticsearch): An existing Elasticsearch<br>        connection object.<br>    input_field (str, optional): The name of the key for the input text field<br>        in the document. Defaults to 'text_field'.<br>    Returns:<br>    ElasticsearchEmbedding: An instance of the ElasticsearchEmbedding class.<br>    Example:<br>        .. code-block:: python<br>            from elasticsearch import Elasticsearch<br>            from llama_index.embeddings.elasticsearch import ElasticsearchEmbedding<br>            # Define the model ID and input field name (if different from default)<br>            model_id = \"your_model_id\"<br>            # Optional, only if different from 'text_field'<br>            input_field = \"your_input_field\"<br>            # Create Elasticsearch connection<br>            es_connection = Elasticsearch(hosts=[\"localhost:9200\"], basic_auth=(\"user\", \"password\"))<br>            # Instantiate ElasticsearchEmbedding using the existing connection<br>            embeddings = ElasticsearchEmbedding.from_es_connection(<br>                model_id,<br>                es_connection,<br>                input_field=input_field,<br>            )<br>    \"\"\"<br>    client = MlClient(es_connection)<br>    return cls(client, model_id, input_field=input_field)<br>``` |\n\n### from\\_credentials`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/elasticsearch/\\#llama_index.embeddings.elasticsearch.ElasticsearchEmbedding.from_credentials \"Permanent link\")\n\n```\nfrom_credentials(model_id: str, es_url: str, es_username: str, es_password: str, input_field: str = 'text_field') -> BaseEmbedding\n\n```\n\nInstantiate embeddings from Elasticsearch credentials.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model_id` | `str` | The model\\_id of the model deployed in the Elasticsearch<br>cluster. | _required_ |\n| `input_field` | `str` | The name of the key for the input text field in the<br>document. Defaults to 'text\\_field'. | `'text_field'` |\n| `es_url` | `str` | (str): The Elasticsearch url to connect to. | _required_ |\n| `es_username` | `str` | (str): Elasticsearch username. | _required_ |\n| `es_password` | `str` | (str): Elasticsearch password. | _required_ |\n\nExample\n\n.. code-block:: python\n\n```\nfrom llama_index.embeddings.bedrock import ElasticsearchEmbedding\n\n# Define the model ID and input field name (if different from default)\nmodel_id = \"your_model_id\"\n# Optional, only if different from 'text_field'\ninput_field = \"your_input_field\"\n\nembeddings = ElasticsearchEmbedding.from_credentials(\n    model_id,\n    input_field=input_field,\n    es_url=\"foo\",\n    es_username=\"bar\",\n    es_password=\"baz\",\n)\n\n```\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-elasticsearch/llama_index/embeddings/elasticsearch/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>``` | ```<br>@classmethod<br>def from_credentials(<br>    cls,<br>    model_id: str,<br>    es_url: str,<br>    es_username: str,<br>    es_password: str,<br>    input_field: str = \"text_field\",<br>) -> BaseEmbedding:<br>    \"\"\"Instantiate embeddings from Elasticsearch credentials.<br>    Args:<br>        model_id (str): The model_id of the model deployed in the Elasticsearch<br>            cluster.<br>        input_field (str): The name of the key for the input text field in the<br>            document. Defaults to 'text_field'.<br>        es_url: (str): The Elasticsearch url to connect to.<br>        es_username: (str): Elasticsearch username.<br>        es_password: (str): Elasticsearch password.<br>    Example:<br>        .. code-block:: python<br>            from llama_index.embeddings.bedrock import ElasticsearchEmbedding<br>            # Define the model ID and input field name (if different from default)<br>            model_id = \"your_model_id\"<br>            # Optional, only if different from 'text_field'<br>            input_field = \"your_input_field\"<br>            embeddings = ElasticsearchEmbedding.from_credentials(<br>                model_id,<br>                input_field=input_field,<br>                es_url=\"foo\",<br>                es_username=\"bar\",<br>                es_password=\"baz\",<br>            )<br>    \"\"\"<br>    es_connection = Elasticsearch(<br>        hosts=[es_url],<br>        basic_auth=(es_username, es_password),<br>    )<br>    client = MlClient(es_connection)<br>    return cls(client, model_id, input_field=input_field)<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Elasticsearch - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/elasticsearch/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/#llama_index.agent.openai.OpenAIAgent)\n\n# Openai\n\n## OpenAIAgent [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/\\#llama_index.agent.openai.OpenAIAgent \"Permanent link\")\n\nBases: `AgentRunner`\n\nOpenAI agent.\n\nSubclasses AgentRunner with a OpenAIAgentWorker.\n\nFor the legacy implementation see:\n\n```\nfrom llama_index..agent.legacy.openai.base import OpenAIAgent\n\n```\n\nSource code in `llama-index-integrations/agent/llama-index-agent-openai/llama_index/agent/openai/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>``` | ````<br>class OpenAIAgent(AgentRunner):<br>    \"\"\"OpenAI agent.<br>    Subclasses AgentRunner with a OpenAIAgentWorker.<br>    For the legacy implementation see:<br>    ```python<br>    from llama_index..agent.legacy.openai.base import OpenAIAgent<br>    ```<br>    \"\"\"<br>    def __init__(<br>        self,<br>        tools: List[BaseTool],<br>        llm: OpenAI,<br>        memory: BaseMemory,<br>        prefix_messages: List[ChatMessage],<br>        verbose: bool = False,<br>        max_function_calls: int = DEFAULT_MAX_FUNCTION_CALLS,<br>        default_tool_choice: str = \"auto\",<br>        callback_manager: Optional[CallbackManager] = None,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        tool_call_parser: Optional[Callable[[OpenAIToolCall], Dict]] = None,<br>    ) -> None:<br>        \"\"\"Init params.\"\"\"<br>        callback_manager = callback_manager or llm.callback_manager<br>        step_engine = OpenAIAgentWorker.from_tools(<br>            tools=tools,<br>            tool_retriever=tool_retriever,<br>            llm=llm,<br>            verbose=verbose,<br>            max_function_calls=max_function_calls,<br>            callback_manager=callback_manager,<br>            prefix_messages=prefix_messages,<br>            tool_call_parser=tool_call_parser,<br>        )<br>        super().__init__(<br>            step_engine,<br>            memory=memory,<br>            llm=llm,<br>            callback_manager=callback_manager,<br>            default_tool_choice=default_tool_choice,<br>        )<br>    @classmethod<br>    def from_tools(<br>        cls,<br>        tools: Optional[List[BaseTool]] = None,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        llm: Optional[LLM] = None,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        memory: Optional[BaseMemory] = None,<br>        memory_cls: Type[BaseMemory] = ChatMemoryBuffer,<br>        verbose: bool = False,<br>        max_function_calls: int = DEFAULT_MAX_FUNCTION_CALLS,<br>        default_tool_choice: str = \"auto\",<br>        callback_manager: Optional[CallbackManager] = None,<br>        system_prompt: Optional[str] = None,<br>        prefix_messages: Optional[List[ChatMessage]] = None,<br>        tool_call_parser: Optional[Callable[[OpenAIToolCall], Dict]] = None,<br>        **kwargs: Any,<br>    ) -> \"OpenAIAgent\":<br>        \"\"\"Create an OpenAIAgent from a list of tools.<br>        Similar to `from_defaults` in other classes, this method will<br>        infer defaults for a variety of parameters, including the LLM,<br>        if they are not specified.<br>        \"\"\"<br>        tools = tools or []<br>        chat_history = chat_history or []<br>        llm = llm or Settings.llm<br>        if not isinstance(llm, OpenAI):<br>            raise ValueError(\"llm must be a OpenAI instance\")<br>        if callback_manager is not None:<br>            llm.callback_manager = callback_manager<br>        memory = memory or memory_cls.from_defaults(chat_history, llm=llm)<br>        if not llm.metadata.is_function_calling_model:<br>            raise ValueError(<br>                f\"Model name {llm.model} does not support function calling API. \"<br>            )<br>        if system_prompt is not None:<br>            if prefix_messages is not None:<br>                raise ValueError(<br>                    \"Cannot specify both system_prompt and prefix_messages\"<br>                )<br>            prefix_messages = [ChatMessage(content=system_prompt, role=\"system\")]<br>        prefix_messages = prefix_messages or []<br>        return cls(<br>            tools=tools,<br>            tool_retriever=tool_retriever,<br>            llm=llm,<br>            memory=memory,<br>            prefix_messages=prefix_messages,<br>            verbose=verbose,<br>            max_function_calls=max_function_calls,<br>            callback_manager=callback_manager,<br>            default_tool_choice=default_tool_choice,<br>            tool_call_parser=tool_call_parser,<br>        )<br>```` |\n\n### from\\_tools`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/\\#llama_index.agent.openai.OpenAIAgent.from_tools \"Permanent link\")\n\n```\nfrom_tools(tools: Optional[List[BaseTool]] = None, tool_retriever: Optional[ObjectRetriever[BaseTool]] = None, llm: Optional[LLM] = None, chat_history: Optional[List[ChatMessage]] = None, memory: Optional[BaseMemory] = None, memory_cls: Type[BaseMemory] = ChatMemoryBuffer, verbose: bool = False, max_function_calls: int = DEFAULT_MAX_FUNCTION_CALLS, default_tool_choice: str = 'auto', callback_manager: Optional[CallbackManager] = None, system_prompt: Optional[str] = None, prefix_messages: Optional[List[ChatMessage]] = None, tool_call_parser: Optional[Callable[[OpenAIToolCall], Dict]] = None, **kwargs: Any) -> OpenAIAgent\n\n```\n\nCreate an OpenAIAgent from a list of tools.\n\nSimilar to `from_defaults` in other classes, this method will\ninfer defaults for a variety of parameters, including the LLM,\nif they are not specified.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-openai/llama_index/agent/openai/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>``` | ```<br>@classmethod<br>def from_tools(<br>    cls,<br>    tools: Optional[List[BaseTool]] = None,<br>    tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>    llm: Optional[LLM] = None,<br>    chat_history: Optional[List[ChatMessage]] = None,<br>    memory: Optional[BaseMemory] = None,<br>    memory_cls: Type[BaseMemory] = ChatMemoryBuffer,<br>    verbose: bool = False,<br>    max_function_calls: int = DEFAULT_MAX_FUNCTION_CALLS,<br>    default_tool_choice: str = \"auto\",<br>    callback_manager: Optional[CallbackManager] = None,<br>    system_prompt: Optional[str] = None,<br>    prefix_messages: Optional[List[ChatMessage]] = None,<br>    tool_call_parser: Optional[Callable[[OpenAIToolCall], Dict]] = None,<br>    **kwargs: Any,<br>) -> \"OpenAIAgent\":<br>    \"\"\"Create an OpenAIAgent from a list of tools.<br>    Similar to `from_defaults` in other classes, this method will<br>    infer defaults for a variety of parameters, including the LLM,<br>    if they are not specified.<br>    \"\"\"<br>    tools = tools or []<br>    chat_history = chat_history or []<br>    llm = llm or Settings.llm<br>    if not isinstance(llm, OpenAI):<br>        raise ValueError(\"llm must be a OpenAI instance\")<br>    if callback_manager is not None:<br>        llm.callback_manager = callback_manager<br>    memory = memory or memory_cls.from_defaults(chat_history, llm=llm)<br>    if not llm.metadata.is_function_calling_model:<br>        raise ValueError(<br>            f\"Model name {llm.model} does not support function calling API. \"<br>        )<br>    if system_prompt is not None:<br>        if prefix_messages is not None:<br>            raise ValueError(<br>                \"Cannot specify both system_prompt and prefix_messages\"<br>            )<br>        prefix_messages = [ChatMessage(content=system_prompt, role=\"system\")]<br>    prefix_messages = prefix_messages or []<br>    return cls(<br>        tools=tools,<br>        tool_retriever=tool_retriever,<br>        llm=llm,<br>        memory=memory,<br>        prefix_messages=prefix_messages,<br>        verbose=verbose,<br>        max_function_calls=max_function_calls,<br>        callback_manager=callback_manager,<br>        default_tool_choice=default_tool_choice,<br>        tool_call_parser=tool_call_parser,<br>    )<br>``` |\n\n## OpenAIAssistantAgent [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/\\#llama_index.agent.openai.OpenAIAssistantAgent \"Permanent link\")\n\nBases: `BaseAgent`\n\nOpenAIAssistant agent.\n\nWrapper around OpenAI assistant API: https://platform.openai.com/docs/assistants/overview\n\nSource code in `llama-index-integrations/agent/llama-index-agent-openai/llama_index/agent/openai/openai_assistant_agent.py`\n\n|     |     |\n| --- | --- |\n| ```<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>472<br>473<br>474<br>475<br>476<br>477<br>478<br>479<br>480<br>481<br>482<br>483<br>484<br>485<br>486<br>487<br>488<br>489<br>490<br>491<br>492<br>493<br>494<br>495<br>496<br>497<br>498<br>499<br>500<br>501<br>502<br>503<br>504<br>505<br>506<br>507<br>508<br>509<br>510<br>511<br>512<br>513<br>514<br>515<br>516<br>517<br>518<br>519<br>520<br>521<br>522<br>523<br>524<br>525<br>526<br>527<br>528<br>529<br>530<br>531<br>532<br>533<br>534<br>535<br>536<br>537<br>538<br>539<br>540<br>541<br>542<br>543<br>544<br>545<br>546<br>547<br>548<br>549<br>550<br>551<br>552<br>553<br>554<br>555<br>556<br>557<br>558<br>559<br>560<br>561<br>562<br>563<br>564<br>565<br>566<br>567<br>568<br>569<br>570<br>571<br>572<br>573<br>574<br>575<br>576<br>577<br>578<br>579<br>580<br>581<br>582<br>583<br>584<br>585<br>586<br>587<br>588<br>589<br>590<br>591<br>592<br>593<br>594<br>``` | ```<br>class OpenAIAssistantAgent(BaseAgent):<br>    \"\"\"OpenAIAssistant agent.<br>    Wrapper around OpenAI assistant API: https://platform.openai.com/docs/assistants/overview<br>    \"\"\"<br>    def __init__(<br>        self,<br>        client: Any,<br>        assistant: Any,<br>        tools: Optional[List[BaseTool]],<br>        callback_manager: Optional[CallbackManager] = None,<br>        thread_id: Optional[str] = None,<br>        instructions_prefix: Optional[str] = None,<br>        run_retrieve_sleep_time: float = 0.1,<br>        file_dict: Dict[str, str] = {},<br>        verbose: bool = False,<br>    ) -> None:<br>        \"\"\"Init params.\"\"\"<br>        from openai import OpenAI<br>        from openai.types.beta.assistant import Assistant<br>        self._client = cast(OpenAI, client)<br>        self._assistant = cast(Assistant, assistant)<br>        self._tools = tools or []<br>        if thread_id is None:<br>            thread = self._client.beta.threads.create()<br>            thread_id = thread.id<br>        self._thread_id = thread_id<br>        self._instructions_prefix = instructions_prefix<br>        self._run_retrieve_sleep_time = run_retrieve_sleep_time<br>        self._verbose = verbose<br>        self.file_dict = file_dict<br>        self.callback_manager = callback_manager or CallbackManager([])<br>    @classmethod<br>    def from_new(<br>        cls,<br>        name: str,<br>        instructions: str,<br>        tools: Optional[List[BaseTool]] = None,<br>        openai_tools: Optional[List[Dict]] = None,<br>        thread_id: Optional[str] = None,<br>        model: str = \"gpt-4-1106-preview\",<br>        instructions_prefix: Optional[str] = None,<br>        run_retrieve_sleep_time: float = 0.1,<br>        files: Optional[List[str]] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        file_ids: Optional[List[str]] = None,<br>        api_key: Optional[str] = None,<br>    ) -> \"OpenAIAssistantAgent\":<br>        \"\"\"From new assistant.<br>        Args:<br>            name: name of assistant<br>            instructions: instructions for assistant<br>            tools: list of tools<br>            openai_tools: list of openai tools<br>            thread_id: thread id<br>            model: model<br>            run_retrieve_sleep_time: run retrieve sleep time<br>            files: files<br>            instructions_prefix: instructions prefix<br>            callback_manager: callback manager<br>            verbose: verbose<br>            file_ids: list of file ids<br>            api_key: OpenAI API key<br>        \"\"\"<br>        from openai import OpenAI<br>        # this is the set of openai tools<br>        # not to be confused with the tools we pass in for function calling<br>        openai_tools = openai_tools or []<br>        tools = tools or []<br>        tool_fns = [t.metadata.to_openai_tool() for t in tools]<br>        all_openai_tools = openai_tools + tool_fns<br>        # initialize client<br>        client = OpenAI(api_key=api_key)<br>        # process files<br>        files = files or []<br>        file_ids = file_ids or []<br>        file_dict = _process_files(client, files)<br>        # TODO: openai's typing is a bit sus<br>        all_openai_tools = cast(List[Any], all_openai_tools)<br>        assistant = client.beta.assistants.create(<br>            name=name,<br>            instructions=instructions,<br>            tools=cast(List[Any], all_openai_tools),<br>            model=model,<br>        )<br>        return cls(<br>            client,<br>            assistant,<br>            tools,<br>            callback_manager=callback_manager,<br>            thread_id=thread_id,<br>            instructions_prefix=instructions_prefix,<br>            file_dict=file_dict,<br>            run_retrieve_sleep_time=run_retrieve_sleep_time,<br>            verbose=verbose,<br>        )<br>    @classmethod<br>    def from_existing(<br>        cls,<br>        assistant_id: str,<br>        tools: Optional[List[BaseTool]] = None,<br>        thread_id: Optional[str] = None,<br>        instructions_prefix: Optional[str] = None,<br>        run_retrieve_sleep_time: float = 0.1,<br>        callback_manager: Optional[CallbackManager] = None,<br>        api_key: Optional[str] = None,<br>        verbose: bool = False,<br>    ) -> \"OpenAIAssistantAgent\":<br>        \"\"\"From existing assistant id.<br>        Args:<br>            assistant_id: id of assistant<br>            tools: list of BaseTools Assistant can use<br>            thread_id: thread id<br>            run_retrieve_sleep_time: run retrieve sleep time<br>            instructions_prefix: instructions prefix<br>            callback_manager: callback manager<br>            api_key: OpenAI API key<br>            verbose: verbose<br>        \"\"\"<br>        from openai import OpenAI<br>        # initialize client<br>        client = OpenAI(api_key=api_key)<br>        # get assistant<br>        assistant = client.beta.assistants.retrieve(assistant_id)<br>        # assistant.tools is incompatible with BaseTools so have to pass from params<br>        return cls(<br>            client,<br>            assistant,<br>            tools=tools,<br>            callback_manager=callback_manager,<br>            thread_id=thread_id,<br>            instructions_prefix=instructions_prefix,<br>            run_retrieve_sleep_time=run_retrieve_sleep_time,<br>            verbose=verbose,<br>        )<br>    @property<br>    def assistant(self) -> Any:<br>        \"\"\"Get assistant.\"\"\"<br>        return self._assistant<br>    @property<br>    def client(self) -> Any:<br>        \"\"\"Get client.\"\"\"<br>        return self._client<br>    @property<br>    def thread_id(self) -> str:<br>        \"\"\"Get thread id.\"\"\"<br>        return self._thread_id<br>    @property<br>    def files_dict(self) -> Dict[str, str]:<br>        \"\"\"Get files dict.\"\"\"<br>        return self.file_dict<br>    @property<br>    def chat_history(self) -> List[ChatMessage]:<br>        raw_messages = self._client.beta.threads.messages.list(<br>            thread_id=self._thread_id, order=\"asc\"<br>        )<br>        return from_openai_thread_messages(list(raw_messages))<br>    def reset(self) -> None:<br>        \"\"\"Delete and create a new thread.\"\"\"<br>        self._client.beta.threads.delete(self._thread_id)<br>        thread = self._client.beta.threads.create()<br>        thread_id = thread.id<br>        self._thread_id = thread_id<br>    def get_tools(self, message: str) -> List[BaseTool]:<br>        \"\"\"Get tools.\"\"\"<br>        return self._tools<br>    def upload_files(self, files: List[str]) -> Dict[str, Any]:<br>        \"\"\"Upload files.\"\"\"<br>        return _process_files(self._client, files)<br>    def add_message(<br>        self,<br>        message: str,<br>        file_ids: Optional[List[str]] = None,<br>        tools: Optional[List[Dict[str, Any]]] = None,<br>    ) -> Any:<br>        \"\"\"Add message to assistant.\"\"\"<br>        attachments = format_attachments(file_ids=file_ids, tools=tools)<br>        return self._client.beta.threads.messages.create(<br>            thread_id=self._thread_id,<br>            role=\"user\",<br>            content=message,<br>            attachments=attachments,<br>        )<br>    def _run_function_calling(self, run: Any) -> List[ToolOutput]:<br>        \"\"\"Run function calling.\"\"\"<br>        tool_calls = run.required_action.submit_tool_outputs.tool_calls<br>        tool_output_dicts = []<br>        tool_output_objs: List[ToolOutput] = []<br>        for tool_call in tool_calls:<br>            fn_obj = tool_call.function<br>            _, tool_output = call_function(self._tools, fn_obj, verbose=self._verbose)<br>            tool_output_dicts.append(<br>                {\"tool_call_id\": tool_call.id, \"output\": str(tool_output)}<br>            )<br>            tool_output_objs.append(tool_output)<br>        # submit tool outputs<br>        # TODO: openai's typing is a bit sus<br>        self._client.beta.threads.runs.submit_tool_outputs(<br>            thread_id=self._thread_id,<br>            run_id=run.id,<br>            tool_outputs=cast(List[Any], tool_output_dicts),<br>        )<br>        return tool_output_objs<br>    async def _arun_function_calling(self, run: Any) -> List[ToolOutput]:<br>        \"\"\"Run function calling.\"\"\"<br>        tool_calls = run.required_action.submit_tool_outputs.tool_calls<br>        tool_output_dicts = []<br>        tool_output_objs: List[ToolOutput] = []<br>        for tool_call in tool_calls:<br>            fn_obj = tool_call.function<br>            _, tool_output = await acall_function(<br>                self._tools, fn_obj, verbose=self._verbose<br>            )<br>            tool_output_dicts.append(<br>                {\"tool_call_id\": tool_call.id, \"output\": str(tool_output)}<br>            )<br>            tool_output_objs.append(tool_output)<br>        # submit tool outputs<br>        self._client.beta.threads.runs.submit_tool_outputs(<br>            thread_id=self._thread_id,<br>            run_id=run.id,<br>            tool_outputs=cast(List[Any], tool_output_dicts),<br>        )<br>        return tool_output_objs<br>    def run_assistant(<br>        self, instructions_prefix: Optional[str] = None<br>    ) -> Tuple[Any, Dict]:<br>        \"\"\"Run assistant.\"\"\"<br>        instructions_prefix = instructions_prefix or self._instructions_prefix<br>        run = self._client.beta.threads.runs.create(<br>            thread_id=self._thread_id,<br>            assistant_id=self._assistant.id,<br>            instructions=instructions_prefix,<br>        )<br>        from openai.types.beta.threads import Run<br>        run = cast(Run, run)<br>        sources = []<br>        while run.status in [\"queued\", \"in_progress\", \"requires_action\"]:<br>            run = self._client.beta.threads.runs.retrieve(<br>                thread_id=self._thread_id, run_id=run.id<br>            )<br>            if run.status == \"requires_action\":<br>                cur_tool_outputs = self._run_function_calling(run)<br>                sources.extend(cur_tool_outputs)<br>            time.sleep(self._run_retrieve_sleep_time)<br>        if run.status == \"failed\":<br>            raise ValueError(<br>                f\"Run failed with status {run.status}.\\n\" f\"Error: {run.last_error}\"<br>            )<br>        return run, {\"sources\": sources}<br>    async def arun_assistant(<br>        self, instructions_prefix: Optional[str] = None<br>    ) -> Tuple[Any, Dict]:<br>        \"\"\"Run assistant.\"\"\"<br>        instructions_prefix = instructions_prefix or self._instructions_prefix<br>        run = self._client.beta.threads.runs.create(<br>            thread_id=self._thread_id,<br>            assistant_id=self._assistant.id,<br>            instructions=instructions_prefix,<br>        )<br>        from openai.types.beta.threads import Run<br>        run = cast(Run, run)<br>        sources = []<br>        while run.status in [\"queued\", \"in_progress\", \"requires_action\"]:<br>            run = self._client.beta.threads.runs.retrieve(<br>                thread_id=self._thread_id, run_id=run.id<br>            )<br>            if run.status == \"requires_action\":<br>                cur_tool_outputs = await self._arun_function_calling(run)<br>                sources.extend(cur_tool_outputs)<br>            await asyncio.sleep(self._run_retrieve_sleep_time)<br>        if run.status == \"failed\":<br>            raise ValueError(<br>                f\"Run failed with status {run.status}.\\n\" f\"Error: {run.last_error}\"<br>            )<br>        return run, {\"sources\": sources}<br>    @property<br>    def latest_message(self) -> ChatMessage:<br>        \"\"\"Get latest message.\"\"\"<br>        raw_messages = self._client.beta.threads.messages.list(<br>            thread_id=self._thread_id, order=\"desc\"<br>        )<br>        messages = from_openai_thread_messages(list(raw_messages))<br>        return messages[0]<br>    def _chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        function_call: Union[str, dict] = \"auto\",<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>    ) -> AGENT_CHAT_RESPONSE_TYPE:<br>        \"\"\"Main chat interface.\"\"\"<br>        # TODO: since chat interface doesn't expose additional kwargs<br>        # we can't pass in file_ids per message<br>        _added_message_obj = self.add_message(message)<br>        _run, metadata = self.run_assistant(<br>            instructions_prefix=self._instructions_prefix,<br>        )<br>        latest_message = self.latest_message<br>        # get most recent message content<br>        return AgentChatResponse(<br>            response=str(latest_message.content),<br>            sources=metadata[\"sources\"],<br>        )<br>    async def _achat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        function_call: Union[str, dict] = \"auto\",<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>    ) -> AGENT_CHAT_RESPONSE_TYPE:<br>        \"\"\"Asynchronous main chat interface.\"\"\"<br>        self.add_message(message)<br>        run, metadata = await self.arun_assistant(<br>            instructions_prefix=self._instructions_prefix,<br>        )<br>        latest_message = self.latest_message<br>        # get most recent message content<br>        return AgentChatResponse(<br>            response=str(latest_message.content),<br>            sources=metadata[\"sources\"],<br>        )<br>    @trace_method(\"chat\")<br>    def chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        function_call: Union[str, dict] = \"auto\",<br>    ) -> AgentChatResponse:<br>        with self.callback_manager.event(<br>            CBEventType.AGENT_STEP,<br>            payload={EventPayload.MESSAGES: [message]},<br>        ) as e:<br>            chat_response = self._chat(<br>                message, chat_history, function_call, mode=ChatResponseMode.WAIT<br>            )<br>            assert isinstance(chat_response, AgentChatResponse)<br>            e.on_end(payload={EventPayload.RESPONSE: chat_response})<br>        return chat_response<br>    @trace_method(\"chat\")<br>    async def achat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        function_call: Union[str, dict] = \"auto\",<br>    ) -> AgentChatResponse:<br>        with self.callback_manager.event(<br>            CBEventType.AGENT_STEP,<br>            payload={EventPayload.MESSAGES: [message]},<br>        ) as e:<br>            chat_response = await self._achat(<br>                message, chat_history, function_call, mode=ChatResponseMode.WAIT<br>            )<br>            assert isinstance(chat_response, AgentChatResponse)<br>            e.on_end(payload={EventPayload.RESPONSE: chat_response})<br>        return chat_response<br>    @trace_method(\"chat\")<br>    def stream_chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        function_call: Union[str, dict] = \"auto\",<br>    ) -> StreamingAgentChatResponse:<br>        raise NotImplementedError(\"stream_chat not implemented\")<br>    @trace_method(\"chat\")<br>    async def astream_chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        function_call: Union[str, dict] = \"auto\",<br>    ) -> StreamingAgentChatResponse:<br>        raise NotImplementedError(\"astream_chat not implemented\")<br>``` |\n\n### assistant`property`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/\\#llama_index.agent.openai.OpenAIAssistantAgent.assistant \"Permanent link\")\n\n```\nassistant: Any\n\n```\n\nGet assistant.\n\n### client`property`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/\\#llama_index.agent.openai.OpenAIAssistantAgent.client \"Permanent link\")\n\n```\nclient: Any\n\n```\n\nGet client.\n\n### thread\\_id`property`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/\\#llama_index.agent.openai.OpenAIAssistantAgent.thread_id \"Permanent link\")\n\n```\nthread_id: str\n\n```\n\nGet thread id.\n\n### files\\_dict`property`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/\\#llama_index.agent.openai.OpenAIAssistantAgent.files_dict \"Permanent link\")\n\n```\nfiles_dict: Dict[str, str]\n\n```\n\nGet files dict.\n\n### latest\\_message`property`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/\\#llama_index.agent.openai.OpenAIAssistantAgent.latest_message \"Permanent link\")\n\n```\nlatest_message: ChatMessage\n\n```\n\nGet latest message.\n\n### from\\_new`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/\\#llama_index.agent.openai.OpenAIAssistantAgent.from_new \"Permanent link\")\n\n```\nfrom_new(name: str, instructions: str, tools: Optional[List[BaseTool]] = None, openai_tools: Optional[List[Dict]] = None, thread_id: Optional[str] = None, model: str = 'gpt-4-1106-preview', instructions_prefix: Optional[str] = None, run_retrieve_sleep_time: float = 0.1, files: Optional[List[str]] = None, callback_manager: Optional[CallbackManager] = None, verbose: bool = False, file_ids: Optional[List[str]] = None, api_key: Optional[str] = None) -> OpenAIAssistantAgent\n\n```\n\nFrom new assistant.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `name` | `str` | name of assistant | _required_ |\n| `instructions` | `str` | instructions for assistant | _required_ |\n| `tools` | `Optional[List[BaseTool]]` | list of tools | `None` |\n| `openai_tools` | `Optional[List[Dict]]` | list of openai tools | `None` |\n| `thread_id` | `Optional[str]` | thread id | `None` |\n| `model` | `str` | model | `'gpt-4-1106-preview'` |\n| `run_retrieve_sleep_time` | `float` | run retrieve sleep time | `0.1` |\n| `files` | `Optional[List[str]]` | files | `None` |\n| `instructions_prefix` | `Optional[str]` | instructions prefix | `None` |\n| `callback_manager` | `Optional[CallbackManager]` | callback manager | `None` |\n| `verbose` | `bool` | verbose | `False` |\n| `file_ids` | `Optional[List[str]]` | list of file ids | `None` |\n| `api_key` | `Optional[str]` | OpenAI API key | `None` |\n\nSource code in `llama-index-integrations/agent/llama-index-agent-openai/llama_index/agent/openai/openai_assistant_agent.py`\n\n|     |     |\n| --- | --- |\n| ```<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>``` | ```<br>@classmethod<br>def from_new(<br>    cls,<br>    name: str,<br>    instructions: str,<br>    tools: Optional[List[BaseTool]] = None,<br>    openai_tools: Optional[List[Dict]] = None,<br>    thread_id: Optional[str] = None,<br>    model: str = \"gpt-4-1106-preview\",<br>    instructions_prefix: Optional[str] = None,<br>    run_retrieve_sleep_time: float = 0.1,<br>    files: Optional[List[str]] = None,<br>    callback_manager: Optional[CallbackManager] = None,<br>    verbose: bool = False,<br>    file_ids: Optional[List[str]] = None,<br>    api_key: Optional[str] = None,<br>) -> \"OpenAIAssistantAgent\":<br>    \"\"\"From new assistant.<br>    Args:<br>        name: name of assistant<br>        instructions: instructions for assistant<br>        tools: list of tools<br>        openai_tools: list of openai tools<br>        thread_id: thread id<br>        model: model<br>        run_retrieve_sleep_time: run retrieve sleep time<br>        files: files<br>        instructions_prefix: instructions prefix<br>        callback_manager: callback manager<br>        verbose: verbose<br>        file_ids: list of file ids<br>        api_key: OpenAI API key<br>    \"\"\"<br>    from openai import OpenAI<br>    # this is the set of openai tools<br>    # not to be confused with the tools we pass in for function calling<br>    openai_tools = openai_tools or []<br>    tools = tools or []<br>    tool_fns = [t.metadata.to_openai_tool() for t in tools]<br>    all_openai_tools = openai_tools + tool_fns<br>    # initialize client<br>    client = OpenAI(api_key=api_key)<br>    # process files<br>    files = files or []<br>    file_ids = file_ids or []<br>    file_dict = _process_files(client, files)<br>    # TODO: openai's typing is a bit sus<br>    all_openai_tools = cast(List[Any], all_openai_tools)<br>    assistant = client.beta.assistants.create(<br>        name=name,<br>        instructions=instructions,<br>        tools=cast(List[Any], all_openai_tools),<br>        model=model,<br>    )<br>    return cls(<br>        client,<br>        assistant,<br>        tools,<br>        callback_manager=callback_manager,<br>        thread_id=thread_id,<br>        instructions_prefix=instructions_prefix,<br>        file_dict=file_dict,<br>        run_retrieve_sleep_time=run_retrieve_sleep_time,<br>        verbose=verbose,<br>    )<br>``` |\n\n### from\\_existing`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/\\#llama_index.agent.openai.OpenAIAssistantAgent.from_existing \"Permanent link\")\n\n```\nfrom_existing(assistant_id: str, tools: Optional[List[BaseTool]] = None, thread_id: Optional[str] = None, instructions_prefix: Optional[str] = None, run_retrieve_sleep_time: float = 0.1, callback_manager: Optional[CallbackManager] = None, api_key: Optional[str] = None, verbose: bool = False) -> OpenAIAssistantAgent\n\n```\n\nFrom existing assistant id.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `assistant_id` | `str` | id of assistant | _required_ |\n| `tools` | `Optional[List[BaseTool]]` | list of BaseTools Assistant can use | `None` |\n| `thread_id` | `Optional[str]` | thread id | `None` |\n| `run_retrieve_sleep_time` | `float` | run retrieve sleep time | `0.1` |\n| `instructions_prefix` | `Optional[str]` | instructions prefix | `None` |\n| `callback_manager` | `Optional[CallbackManager]` | callback manager | `None` |\n| `api_key` | `Optional[str]` | OpenAI API key | `None` |\n| `verbose` | `bool` | verbose | `False` |\n\nSource code in `llama-index-integrations/agent/llama-index-agent-openai/llama_index/agent/openai/openai_assistant_agent.py`\n\n|     |     |\n| --- | --- |\n| ```<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>``` | ```<br>@classmethod<br>def from_existing(<br>    cls,<br>    assistant_id: str,<br>    tools: Optional[List[BaseTool]] = None,<br>    thread_id: Optional[str] = None,<br>    instructions_prefix: Optional[str] = None,<br>    run_retrieve_sleep_time: float = 0.1,<br>    callback_manager: Optional[CallbackManager] = None,<br>    api_key: Optional[str] = None,<br>    verbose: bool = False,<br>) -> \"OpenAIAssistantAgent\":<br>    \"\"\"From existing assistant id.<br>    Args:<br>        assistant_id: id of assistant<br>        tools: list of BaseTools Assistant can use<br>        thread_id: thread id<br>        run_retrieve_sleep_time: run retrieve sleep time<br>        instructions_prefix: instructions prefix<br>        callback_manager: callback manager<br>        api_key: OpenAI API key<br>        verbose: verbose<br>    \"\"\"<br>    from openai import OpenAI<br>    # initialize client<br>    client = OpenAI(api_key=api_key)<br>    # get assistant<br>    assistant = client.beta.assistants.retrieve(assistant_id)<br>    # assistant.tools is incompatible with BaseTools so have to pass from params<br>    return cls(<br>        client,<br>        assistant,<br>        tools=tools,<br>        callback_manager=callback_manager,<br>        thread_id=thread_id,<br>        instructions_prefix=instructions_prefix,<br>        run_retrieve_sleep_time=run_retrieve_sleep_time,<br>        verbose=verbose,<br>    )<br>``` |\n\n### reset [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/\\#llama_index.agent.openai.OpenAIAssistantAgent.reset \"Permanent link\")\n\n```\nreset() -> None\n\n```\n\nDelete and create a new thread.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-openai/llama_index/agent/openai/openai_assistant_agent.py`\n\n|     |     |\n| --- | --- |\n| ```<br>356<br>357<br>358<br>359<br>360<br>361<br>``` | ```<br>def reset(self) -> None:<br>    \"\"\"Delete and create a new thread.\"\"\"<br>    self._client.beta.threads.delete(self._thread_id)<br>    thread = self._client.beta.threads.create()<br>    thread_id = thread.id<br>    self._thread_id = thread_id<br>``` |\n\n### get\\_tools [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/\\#llama_index.agent.openai.OpenAIAssistantAgent.get_tools \"Permanent link\")\n\n```\nget_tools(message: str) -> List[BaseTool]\n\n```\n\nGet tools.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-openai/llama_index/agent/openai/openai_assistant_agent.py`\n\n|     |     |\n| --- | --- |\n| ```<br>363<br>364<br>365<br>``` | ```<br>def get_tools(self, message: str) -> List[BaseTool]:<br>    \"\"\"Get tools.\"\"\"<br>    return self._tools<br>``` |\n\n### upload\\_files [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/\\#llama_index.agent.openai.OpenAIAssistantAgent.upload_files \"Permanent link\")\n\n```\nupload_files(files: List[str]) -> Dict[str, Any]\n\n```\n\nUpload files.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-openai/llama_index/agent/openai/openai_assistant_agent.py`\n\n|     |     |\n| --- | --- |\n| ```<br>367<br>368<br>369<br>``` | ```<br>def upload_files(self, files: List[str]) -> Dict[str, Any]:<br>    \"\"\"Upload files.\"\"\"<br>    return _process_files(self._client, files)<br>``` |\n\n### add\\_message [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/\\#llama_index.agent.openai.OpenAIAssistantAgent.add_message \"Permanent link\")\n\n```\nadd_message(message: str, file_ids: Optional[List[str]] = None, tools: Optional[List[Dict[str, Any]]] = None) -> Any\n\n```\n\nAdd message to assistant.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-openai/llama_index/agent/openai/openai_assistant_agent.py`\n\n|     |     |\n| --- | --- |\n| ```<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>``` | ```<br>def add_message(<br>    self,<br>    message: str,<br>    file_ids: Optional[List[str]] = None,<br>    tools: Optional[List[Dict[str, Any]]] = None,<br>) -> Any:<br>    \"\"\"Add message to assistant.\"\"\"<br>    attachments = format_attachments(file_ids=file_ids, tools=tools)<br>    return self._client.beta.threads.messages.create(<br>        thread_id=self._thread_id,<br>        role=\"user\",<br>        content=message,<br>        attachments=attachments,<br>    )<br>``` |\n\n### run\\_assistant [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/\\#llama_index.agent.openai.OpenAIAssistantAgent.run_assistant \"Permanent link\")\n\n```\nrun_assistant(instructions_prefix: Optional[str] = None) -> Tuple[Any, Dict]\n\n```\n\nRun assistant.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-openai/llama_index/agent/openai/openai_assistant_agent.py`\n\n|     |     |\n| --- | --- |\n| ```<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>``` | ```<br>def run_assistant(<br>    self, instructions_prefix: Optional[str] = None<br>) -> Tuple[Any, Dict]:<br>    \"\"\"Run assistant.\"\"\"<br>    instructions_prefix = instructions_prefix or self._instructions_prefix<br>    run = self._client.beta.threads.runs.create(<br>        thread_id=self._thread_id,<br>        assistant_id=self._assistant.id,<br>        instructions=instructions_prefix,<br>    )<br>    from openai.types.beta.threads import Run<br>    run = cast(Run, run)<br>    sources = []<br>    while run.status in [\"queued\", \"in_progress\", \"requires_action\"]:<br>        run = self._client.beta.threads.runs.retrieve(<br>            thread_id=self._thread_id, run_id=run.id<br>        )<br>        if run.status == \"requires_action\":<br>            cur_tool_outputs = self._run_function_calling(run)<br>            sources.extend(cur_tool_outputs)<br>        time.sleep(self._run_retrieve_sleep_time)<br>    if run.status == \"failed\":<br>        raise ValueError(<br>            f\"Run failed with status {run.status}.\\n\" f\"Error: {run.last_error}\"<br>        )<br>    return run, {\"sources\": sources}<br>``` |\n\n### arun\\_assistant`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/\\#llama_index.agent.openai.OpenAIAssistantAgent.arun_assistant \"Permanent link\")\n\n```\narun_assistant(instructions_prefix: Optional[str] = None) -> Tuple[Any, Dict]\n\n```\n\nRun assistant.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-openai/llama_index/agent/openai/openai_assistant_agent.py`\n\n|     |     |\n| --- | --- |\n| ```<br>462<br>463<br>464<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>472<br>473<br>474<br>475<br>476<br>477<br>478<br>479<br>480<br>481<br>482<br>483<br>484<br>485<br>486<br>487<br>488<br>489<br>490<br>491<br>``` | ```<br>async def arun_assistant(<br>    self, instructions_prefix: Optional[str] = None<br>) -> Tuple[Any, Dict]:<br>    \"\"\"Run assistant.\"\"\"<br>    instructions_prefix = instructions_prefix or self._instructions_prefix<br>    run = self._client.beta.threads.runs.create(<br>        thread_id=self._thread_id,<br>        assistant_id=self._assistant.id,<br>        instructions=instructions_prefix,<br>    )<br>    from openai.types.beta.threads import Run<br>    run = cast(Run, run)<br>    sources = []<br>    while run.status in [\"queued\", \"in_progress\", \"requires_action\"]:<br>        run = self._client.beta.threads.runs.retrieve(<br>            thread_id=self._thread_id, run_id=run.id<br>        )<br>        if run.status == \"requires_action\":<br>            cur_tool_outputs = await self._arun_function_calling(run)<br>            sources.extend(cur_tool_outputs)<br>        await asyncio.sleep(self._run_retrieve_sleep_time)<br>    if run.status == \"failed\":<br>        raise ValueError(<br>            f\"Run failed with status {run.status}.\\n\" f\"Error: {run.last_error}\"<br>        )<br>    return run, {\"sources\": sources}<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Openai - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/anyscale/#llama_index.embeddings.anyscale.AnyscaleEmbedding)\n\n# Anyscale\n\n## AnyscaleEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/anyscale/\\#llama_index.embeddings.anyscale.AnyscaleEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nAnyscale class for embeddings.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model` | `str` | Model for embedding.<br>Defaults to \"thenlper/gte-large\" | `DEFAULT_MODEL` |\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-anyscale/llama_index/embeddings/anyscale/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>``` | ```<br>class AnyscaleEmbedding(BaseEmbedding):<br>    \"\"\"<br>    Anyscale class for embeddings.<br>    Args:<br>        model (str): Model for embedding.<br>            Defaults to \"thenlper/gte-large\"<br>    \"\"\"<br>    additional_kwargs: Dict[str, Any] = Field(<br>        default_factory=dict, description=\"Additional kwargs for the OpenAI API.\"<br>    )<br>    api_key: str = Field(description=\"The Anyscale API key.\")<br>    api_base: str = Field(description=\"The base URL for Anyscale API.\")<br>    api_version: str = Field(description=\"The version for OpenAI API.\")<br>    max_retries: int = Field(<br>        default=10, description=\"Maximum number of retries.\", gte=0<br>    )<br>    timeout: float = Field(default=60.0, description=\"Timeout for each request.\", gte=0)<br>    default_headers: Optional[Dict[str, str]] = Field(<br>        default=None, description=\"The default headers for API requests.\"<br>    )<br>    reuse_client: bool = Field(<br>        default=True,<br>        description=(<br>            \"Reuse the Anyscale client between requests. When doing anything with large \"<br>            \"volumes of async API calls, setting this to false can improve stability.\"<br>        ),<br>    )<br>    _query_engine: Optional[str] = PrivateAttr()<br>    _text_engine: Optional[str] = PrivateAttr()<br>    _client: Optional[OpenAI] = PrivateAttr()<br>    _aclient: Optional[AsyncOpenAI] = PrivateAttr()<br>    _http_client: Optional[httpx.Client] = PrivateAttr()<br>    def __init__(<br>        self,<br>        model: str = DEFAULT_MODEL,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        additional_kwargs: Optional[Dict[str, Any]] = None,<br>        api_key: Optional[str] = None,<br>        api_base: Optional[str] = DEFAULT_API_BASE,<br>        api_version: Optional[str] = None,<br>        max_retries: int = 10,<br>        timeout: float = 60.0,<br>        reuse_client: bool = True,<br>        callback_manager: Optional[CallbackManager] = None,<br>        default_headers: Optional[Dict[str, str]] = None,<br>        http_client: Optional[httpx.Client] = None,<br>        **kwargs: Any,<br>    ) -> None:<br>        additional_kwargs = additional_kwargs or {}<br>        api_key, api_base, api_version = resolve_anyscale_credentials(<br>            api_key=api_key,<br>            api_base=api_base,<br>            api_version=api_version,<br>        )<br>        if \"model_name\" in kwargs:<br>            model_name = kwargs.pop(\"model_name\")<br>        else:<br>            model_name = model<br>        super().__init__(<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            model_name=model_name,<br>            additional_kwargs=additional_kwargs,<br>            api_key=api_key,<br>            api_base=api_base,<br>            api_version=api_version,<br>            max_retries=max_retries,<br>            reuse_client=reuse_client,<br>            timeout=timeout,<br>            default_headers=default_headers,<br>            **kwargs,<br>        )<br>        self._query_engine = model_name<br>        self._text_engine = model_name<br>        self._client = None<br>        self._aclient = None<br>        self._http_client = http_client<br>    def _get_client(self) -> OpenAI:<br>        if not self.reuse_client:<br>            return OpenAI(**self._get_credential_kwargs())<br>        if self._client is None:<br>            self._client = OpenAI(**self._get_credential_kwargs())<br>        return self._client<br>    def _get_aclient(self) -> AsyncOpenAI:<br>        if not self.reuse_client:<br>            return AsyncOpenAI(**self._get_credential_kwargs())<br>        if self._aclient is None:<br>            self._aclient = AsyncOpenAI(**self._get_credential_kwargs())<br>        return self._aclient<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"AnyscaleEmbedding\"<br>    def _get_credential_kwargs(self) -> Dict[str, Any]:<br>        return {<br>            \"api_key\": self.api_key,<br>            \"base_url\": self.api_base,<br>            \"max_retries\": self.max_retries,<br>            \"timeout\": self.timeout,<br>            \"default_headers\": self.default_headers,<br>            \"http_client\": self._http_client,<br>        }<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        client = self._get_client()<br>        return get_embedding(<br>            client,<br>            query,<br>            engine=self._query_engine,<br>            **self.additional_kwargs,<br>        )<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        aclient = self._get_aclient()<br>        return await aget_embedding(<br>            aclient,<br>            query,<br>            engine=self._query_engine,<br>            **self.additional_kwargs,<br>        )<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        client = self._get_client()<br>        return get_embedding(<br>            client,<br>            text,<br>            engine=self._text_engine,<br>            **self.additional_kwargs,<br>        )<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously get text embedding.\"\"\"<br>        aclient = self._get_aclient()<br>        return await aget_embedding(<br>            aclient,<br>            text,<br>            engine=self._text_engine,<br>            **self.additional_kwargs,<br>        )<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"<br>        Get text embeddings.<br>        By default, this is a wrapper around _get_text_embedding.<br>        Can be overridden for batch queries.<br>        \"\"\"<br>        client = self._get_client()<br>        return get_embeddings(<br>            client,<br>            texts,<br>            engine=self._text_engine,<br>            **self.additional_kwargs,<br>        )<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Asynchronously get text embeddings.\"\"\"<br>        aclient = self._get_aclient()<br>        return await aget_embeddings(<br>            aclient,<br>            texts,<br>            engine=self._text_engine,<br>            **self.additional_kwargs,<br>        )<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Anyscale - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/anyscale/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/clarifai/#llama_index.embeddings.clarifai.ClarifaiEmbedding)\n\n# Clarifai\n\n## ClarifaiEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/clarifai/\\#llama_index.embeddings.clarifai.ClarifaiEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nClarifai embeddings class.\n\nClarifai uses Personal Access Tokens(PAT) to validate requests.\nYou can create and manage PATs under your Clarifai account security settings.\nExport your PAT as an environment variable by running `export CLARIFAI_PAT={PAT}`\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-clarifai/llama_index/embeddings/clarifai/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>``` | ```<br>class ClarifaiEmbedding(BaseEmbedding):<br>    \"\"\"Clarifai embeddings class.<br>    Clarifai uses Personal Access Tokens(PAT) to validate requests.<br>    You can create and manage PATs under your Clarifai account security settings.<br>    Export your PAT as an environment variable by running `export CLARIFAI_PAT={PAT}`<br>    \"\"\"<br>    model_url: Optional[str] = Field(<br>        description=f\"Full URL of the model. e.g. `{EXAMPLE_URL}`\"<br>    )<br>    model_id: Optional[str] = Field(description=\"Model ID.\")<br>    model_version_id: Optional[str] = Field(description=\"Model Version ID.\")<br>    app_id: Optional[str] = Field(description=\"Clarifai application ID of the model.\")<br>    user_id: Optional[str] = Field(description=\"Clarifai user ID of the model.\")<br>    pat: Optional[str] = Field(<br>        description=\"Personal Access Tokens(PAT) to validate requests.\"<br>    )<br>    _model: Any = PrivateAttr()<br>    def __init__(<br>        self,<br>        model_name: Optional[str] = None,<br>        model_url: Optional[str] = None,<br>        model_version_id: Optional[str] = \"\",<br>        app_id: Optional[str] = None,<br>        user_id: Optional[str] = None,<br>        pat: Optional[str] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>    ):<br>        embed_batch_size = min(128, embed_batch_size)<br>        if pat is None and os.environ.get(\"CLARIFAI_PAT\") is not None:<br>            pat = os.environ.get(\"CLARIFAI_PAT\")<br>        if not pat and os.environ.get(\"CLARIFAI_PAT\") is None:<br>            raise ValueError(<br>                \"Set `CLARIFAI_PAT` as env variable or pass `pat` as constructor argument\"<br>            )<br>        if model_url is not None and model_name is not None:<br>            raise ValueError(\"You can only specify one of model_url or model_name.\")<br>        if model_url is None and model_name is None:<br>            raise ValueError(\"You must specify one of model_url or model_name.\")<br>        if model_name is not None:<br>            if app_id is None or user_id is None:<br>                raise ValueError(<br>                    f\"Missing one app ID or user ID of the model: {app_id=}, {user_id=}\"<br>                )<br>            else:<br>                model = Model(<br>                    user_id=user_id,<br>                    app_id=app_id,<br>                    model_id=model_name,<br>                    model_version={\"id\": model_version_id},<br>                    pat=pat,<br>                )<br>        if model_url is not None:<br>            model = Model(model_url, pat=pat)<br>            model_name = model.id<br>        super().__init__(<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            model_name=model_name,<br>        )<br>        self._model = model<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"ClarifaiEmbedding\"<br>    def _embed(self, sentences: List[str]) -> List[List[float]]:<br>        \"\"\"Embed sentences.\"\"\"<br>        try:<br>            from clarifai.client.input import Inputs<br>        except ImportError:<br>            raise ImportError(\"ClarifaiEmbedding requires `pip install clarifai`.\")<br>        embeddings = []<br>        try:<br>            for i in range(0, len(sentences), self.embed_batch_size):<br>                batch = sentences[i : i + self.embed_batch_size]<br>                input_batch = [<br>                    Inputs.get_text_input(input_id=str(id), raw_text=inp)<br>                    for id, inp in enumerate(batch)<br>                ]<br>                predict_response = self._model.predict(input_batch)<br>                embeddings.extend(<br>                    [<br>                        list(output.data.embeddings[0].vector)<br>                        for output in predict_response.outputs<br>                    ]<br>                )<br>        except Exception as e:<br>            logger.error(f\"Predict failed, exception: {e}\")<br>        return embeddings<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self._embed([query])[0]<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding async.\"\"\"<br>        return self._get_query_embedding(query)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding async.\"\"\"<br>        return self._get_text_embedding(text)<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._embed([text])[0]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        return self._embed(texts)<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Clarifai - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/clarifai/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/condense_plus_context/#llama_index.core.chat_engine.CondensePlusContextChatEngine)\n\n# Condense plus context\n\n## CondensePlusContextChatEngine [\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/condense_plus_context/\\#llama_index.core.chat_engine.CondensePlusContextChatEngine \"Permanent link\")\n\nBases: `BaseChatEngine`\n\nCondensed Conversation & Context Chat Engine.\n\nFirst condense a conversation and latest user message to a standalone question\nThen build a context for the standalone question from a retriever,\nThen pass the context along with prompt and user message to LLM to generate a response.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/condense_plus_context.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>``` | ```<br>class CondensePlusContextChatEngine(BaseChatEngine):<br>    \"\"\"<br>    Condensed Conversation & Context Chat Engine.<br>    First condense a conversation and latest user message to a standalone question<br>    Then build a context for the standalone question from a retriever,<br>    Then pass the context along with prompt and user message to LLM to generate a response.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        retriever: BaseRetriever,<br>        llm: LLM,<br>        memory: BaseMemory,<br>        context_prompt: Optional[str] = None,<br>        condense_prompt: Optional[str] = None,<br>        system_prompt: Optional[str] = None,<br>        skip_condense: bool = False,<br>        node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>    ):<br>        self._retriever = retriever<br>        self._llm = llm<br>        self._memory = memory<br>        self._context_prompt_template = (<br>            context_prompt or DEFAULT_CONTEXT_PROMPT_TEMPLATE<br>        )<br>        condense_prompt_str = condense_prompt or DEFAULT_CONDENSE_PROMPT_TEMPLATE<br>        self._condense_prompt_template = PromptTemplate(condense_prompt_str)<br>        self._system_prompt = system_prompt<br>        self._skip_condense = skip_condense<br>        self._node_postprocessors = node_postprocessors or []<br>        self.callback_manager = callback_manager or CallbackManager([])<br>        for node_postprocessor in self._node_postprocessors:<br>            node_postprocessor.callback_manager = self.callback_manager<br>        self._token_counter = TokenCounter()<br>        self._verbose = verbose<br>    @classmethod<br>    def from_defaults(<br>        cls,<br>        retriever: BaseRetriever,<br>        llm: Optional[LLM] = None,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        memory: Optional[BaseMemory] = None,<br>        system_prompt: Optional[str] = None,<br>        context_prompt: Optional[str] = None,<br>        condense_prompt: Optional[str] = None,<br>        skip_condense: bool = False,<br>        node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,<br>        verbose: bool = False,<br>        **kwargs: Any,<br>    ) -> \"CondensePlusContextChatEngine\":<br>        \"\"\"Initialize a CondensePlusContextChatEngine from default parameters.\"\"\"<br>        llm = llm or Settings.llm<br>        chat_history = chat_history or []<br>        memory = memory or ChatMemoryBuffer.from_defaults(<br>            chat_history=chat_history, token_limit=llm.metadata.context_window - 256<br>        )<br>        return cls(<br>            retriever=retriever,<br>            llm=llm,<br>            memory=memory,<br>            context_prompt=context_prompt,<br>            condense_prompt=condense_prompt,<br>            skip_condense=skip_condense,<br>            callback_manager=Settings.callback_manager,<br>            node_postprocessors=node_postprocessors,<br>            system_prompt=system_prompt,<br>            verbose=verbose,<br>        )<br>    def _condense_question(<br>        self, chat_history: List[ChatMessage], latest_message: str<br>    ) -> str:<br>        \"\"\"Condense a conversation history and latest user message to a standalone question.\"\"\"<br>        if self._skip_condense or len(chat_history) == 0:<br>            return latest_message<br>        chat_history_str = messages_to_history_str(chat_history)<br>        logger.debug(chat_history_str)<br>        return self._llm.predict(<br>            self._condense_prompt_template,<br>            question=latest_message,<br>            chat_history=chat_history_str,<br>        )<br>    async def _acondense_question(<br>        self, chat_history: List[ChatMessage], latest_message: str<br>    ) -> str:<br>        \"\"\"Condense a conversation history and latest user message to a standalone question.\"\"\"<br>        if self._skip_condense or len(chat_history) == 0:<br>            return latest_message<br>        chat_history_str = messages_to_history_str(chat_history)<br>        logger.debug(chat_history_str)<br>        return await self._llm.apredict(<br>            self._condense_prompt_template,<br>            question=latest_message,<br>            chat_history=chat_history_str,<br>        )<br>    def _retrieve_context(self, message: str) -> Tuple[str, List[NodeWithScore]]:<br>        \"\"\"Build context for a message from retriever.\"\"\"<br>        nodes = self._retriever.retrieve(message)<br>        for postprocessor in self._node_postprocessors:<br>            nodes = postprocessor.postprocess_nodes(<br>                nodes, query_bundle=QueryBundle(message)<br>            )<br>        context_str = \"\\n\\n\".join(<br>            [n.node.get_content(metadata_mode=MetadataMode.LLM).strip() for n in nodes]<br>        )<br>        return context_str, nodes<br>    async def _aretrieve_context(self, message: str) -> Tuple[str, List[NodeWithScore]]:<br>        \"\"\"Build context for a message from retriever.\"\"\"<br>        nodes = await self._retriever.aretrieve(message)<br>        for postprocessor in self._node_postprocessors:<br>            nodes = postprocessor.postprocess_nodes(<br>                nodes, query_bundle=QueryBundle(message)<br>            )<br>        context_str = \"\\n\\n\".join(<br>            [n.node.get_content(metadata_mode=MetadataMode.LLM).strip() for n in nodes]<br>        )<br>        return context_str, nodes<br>    def _run_c3(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> Tuple[List[ChatMessage], ToolOutput, List[NodeWithScore]]:<br>        if chat_history is not None:<br>            self._memory.set(chat_history)<br>        chat_history = self._memory.get(input=message)<br>        # Condense conversation history and latest message to a standalone question<br>        condensed_question = self._condense_question(chat_history, message)  # type: ignore<br>        logger.info(f\"Condensed question: {condensed_question}\")<br>        if self._verbose:<br>            print(f\"Condensed question: {condensed_question}\")<br>        # Build context for the standalone question from a retriever<br>        context_str, context_nodes = self._retrieve_context(condensed_question)<br>        context_source = ToolOutput(<br>            tool_name=\"retriever\",<br>            content=context_str,<br>            raw_input={\"message\": condensed_question},<br>            raw_output=context_str,<br>        )<br>        logger.debug(f\"Context: {context_str}\")<br>        if self._verbose:<br>            print(f\"Context: {context_str}\")<br>        system_message_content = self._context_prompt_template.format(<br>            context_str=context_str<br>        )<br>        if self._system_prompt:<br>            system_message_content = self._system_prompt + \"\\n\" + system_message_content<br>        system_message = ChatMessage(<br>            content=system_message_content, role=self._llm.metadata.system_role<br>        )<br>        initial_token_count = self._token_counter.estimate_tokens_in_messages(<br>            [system_message]<br>        )<br>        self._memory.put(ChatMessage(content=message, role=MessageRole.USER))<br>        chat_messages = [<br>            system_message,<br>            *self._memory.get(initial_token_count=initial_token_count),<br>        ]<br>        return chat_messages, context_source, context_nodes<br>    async def _arun_c3(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> Tuple[List[ChatMessage], ToolOutput, List[NodeWithScore]]:<br>        if chat_history is not None:<br>            self._memory.set(chat_history)<br>        chat_history = self._memory.get(input=message)<br>        # Condense conversation history and latest message to a standalone question<br>        condensed_question = await self._acondense_question(chat_history, message)  # type: ignore<br>        logger.info(f\"Condensed question: {condensed_question}\")<br>        if self._verbose:<br>            print(f\"Condensed question: {condensed_question}\")<br>        # Build context for the standalone question from a retriever<br>        context_str, context_nodes = await self._aretrieve_context(condensed_question)<br>        context_source = ToolOutput(<br>            tool_name=\"retriever\",<br>            content=context_str,<br>            raw_input={\"message\": condensed_question},<br>            raw_output=context_str,<br>        )<br>        logger.debug(f\"Context: {context_str}\")<br>        if self._verbose:<br>            print(f\"Context: {context_str}\")<br>        system_message_content = self._context_prompt_template.format(<br>            context_str=context_str<br>        )<br>        if self._system_prompt:<br>            system_message_content = self._system_prompt + \"\\n\" + system_message_content<br>        system_message = ChatMessage(<br>            content=system_message_content, role=self._llm.metadata.system_role<br>        )<br>        initial_token_count = self._token_counter.estimate_tokens_in_messages(<br>            [system_message]<br>        )<br>        self._memory.put(ChatMessage(content=message, role=MessageRole.USER))<br>        chat_messages = [<br>            system_message,<br>            *self._memory.get(initial_token_count=initial_token_count),<br>        ]<br>        return chat_messages, context_source, context_nodes<br>    @trace_method(\"chat\")<br>    def chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> AgentChatResponse:<br>        chat_messages, context_source, context_nodes = self._run_c3(<br>            message, chat_history<br>        )<br>        # pass the context, system prompt and user message as chat to LLM to generate a response<br>        chat_response = self._llm.chat(chat_messages)<br>        assistant_message = chat_response.message<br>        self._memory.put(assistant_message)<br>        return AgentChatResponse(<br>            response=str(assistant_message.content),<br>            sources=[context_source],<br>            source_nodes=context_nodes,<br>        )<br>    @trace_method(\"chat\")<br>    def stream_chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> StreamingAgentChatResponse:<br>        chat_messages, context_source, context_nodes = self._run_c3(<br>            message, chat_history<br>        )<br>        # pass the context, system prompt and user message as chat to LLM to generate a response<br>        chat_response = StreamingAgentChatResponse(<br>            chat_stream=self._llm.stream_chat(chat_messages),<br>            sources=[context_source],<br>            source_nodes=context_nodes,<br>        )<br>        thread = Thread(<br>            target=chat_response.write_response_to_history, args=(self._memory,)<br>        )<br>        thread.start()<br>        return chat_response<br>    @trace_method(\"chat\")<br>    async def achat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> AgentChatResponse:<br>        chat_messages, context_source, context_nodes = await self._arun_c3(<br>            message, chat_history<br>        )<br>        # pass the context, system prompt and user message as chat to LLM to generate a response<br>        chat_response = await self._llm.achat(chat_messages)<br>        assistant_message = chat_response.message<br>        self._memory.put(assistant_message)<br>        return AgentChatResponse(<br>            response=str(assistant_message.content),<br>            sources=[context_source],<br>            source_nodes=context_nodes,<br>        )<br>    @trace_method(\"chat\")<br>    async def astream_chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> StreamingAgentChatResponse:<br>        chat_messages, context_source, context_nodes = await self._arun_c3(<br>            message, chat_history<br>        )<br>        # pass the context, system prompt and user message as chat to LLM to generate a response<br>        chat_response = StreamingAgentChatResponse(<br>            achat_stream=await self._llm.astream_chat(chat_messages),<br>            sources=[context_source],<br>            source_nodes=context_nodes,<br>        )<br>        asyncio.create_task(chat_response.awrite_response_to_history(self._memory))<br>        return chat_response<br>    def reset(self) -> None:<br>        # Clear chat history<br>        self._memory.reset()<br>    @property<br>    def chat_history(self) -> List[ChatMessage]:<br>        \"\"\"Get chat history.\"\"\"<br>        return self._memory.get_all()<br>``` |\n\n### chat\\_history`property`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/condense_plus_context/\\#llama_index.core.chat_engine.CondensePlusContextChatEngine.chat_history \"Permanent link\")\n\n```\nchat_history: List[ChatMessage]\n\n```\n\nGet chat history.\n\n### from\\_defaults`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/condense_plus_context/\\#llama_index.core.chat_engine.CondensePlusContextChatEngine.from_defaults \"Permanent link\")\n\n```\nfrom_defaults(retriever: BaseRetriever, llm: Optional[LLM] = None, chat_history: Optional[List[ChatMessage]] = None, memory: Optional[BaseMemory] = None, system_prompt: Optional[str] = None, context_prompt: Optional[str] = None, condense_prompt: Optional[str] = None, skip_condense: bool = False, node_postprocessors: Optional[List[BaseNodePostprocessor]] = None, verbose: bool = False, **kwargs: Any) -> CondensePlusContextChatEngine\n\n```\n\nInitialize a CondensePlusContextChatEngine from default parameters.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/condense_plus_context.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>``` | ```<br>@classmethod<br>def from_defaults(<br>    cls,<br>    retriever: BaseRetriever,<br>    llm: Optional[LLM] = None,<br>    chat_history: Optional[List[ChatMessage]] = None,<br>    memory: Optional[BaseMemory] = None,<br>    system_prompt: Optional[str] = None,<br>    context_prompt: Optional[str] = None,<br>    condense_prompt: Optional[str] = None,<br>    skip_condense: bool = False,<br>    node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,<br>    verbose: bool = False,<br>    **kwargs: Any,<br>) -> \"CondensePlusContextChatEngine\":<br>    \"\"\"Initialize a CondensePlusContextChatEngine from default parameters.\"\"\"<br>    llm = llm or Settings.llm<br>    chat_history = chat_history or []<br>    memory = memory or ChatMemoryBuffer.from_defaults(<br>        chat_history=chat_history, token_limit=llm.metadata.context_window - 256<br>    )<br>    return cls(<br>        retriever=retriever,<br>        llm=llm,<br>        memory=memory,<br>        context_prompt=context_prompt,<br>        condense_prompt=condense_prompt,<br>        skip_condense=skip_condense,<br>        callback_manager=Settings.callback_manager,<br>        node_postprocessors=node_postprocessors,<br>        system_prompt=system_prompt,<br>        verbose=verbose,<br>    )<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Condense plus context - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/condense_plus_context/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/agent/react/#llama_index.core.agent.react.ReActAgent)\n\n# React\n\n## ReActAgent [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/react/\\#llama_index.core.agent.react.ReActAgent \"Permanent link\")\n\nBases: `AgentRunner`\n\nReAct agent.\n\nSubclasses AgentRunner with a ReActAgentWorker.\n\nFor the legacy implementation see:\n\n```\nfrom llama_index.core.agent.legacy.react.base import ReActAgent\n\n```\n\nSource code in `llama-index-core/llama_index/core/agent/react/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>``` | ````<br>class ReActAgent(AgentRunner):<br>    \"\"\"ReAct agent.<br>    Subclasses AgentRunner with a ReActAgentWorker.<br>    For the legacy implementation see:<br>    ```python<br>    from llama_index.core.agent.legacy.react.base import ReActAgent<br>    ```<br>    \"\"\"<br>    def __init__(<br>        self,<br>        tools: Sequence[BaseTool],<br>        llm: LLM,<br>        memory: BaseMemory,<br>        max_iterations: int = 10,<br>        react_chat_formatter: Optional[ReActChatFormatter] = None,<br>        output_parser: Optional[ReActOutputParser] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        context: Optional[str] = None,<br>        handle_reasoning_failure_fn: Optional[<br>            Callable[[CallbackManager, Exception], ToolOutput]<br>        ] = None,<br>    ) -> None:<br>        \"\"\"Init params.\"\"\"<br>        callback_manager = callback_manager or llm.callback_manager<br>        if context and react_chat_formatter:<br>            raise ValueError(\"Cannot provide both context and react_chat_formatter\")<br>        if context:<br>            react_chat_formatter = ReActChatFormatter.from_context(context)<br>        step_engine = ReActAgentWorker.from_tools(<br>            tools=tools,<br>            tool_retriever=tool_retriever,<br>            llm=llm,<br>            max_iterations=max_iterations,<br>            react_chat_formatter=react_chat_formatter,<br>            output_parser=output_parser,<br>            callback_manager=callback_manager,<br>            verbose=verbose,<br>            handle_reasoning_failure_fn=handle_reasoning_failure_fn,<br>        )<br>        super().__init__(<br>            step_engine,<br>            memory=memory,<br>            llm=llm,<br>            callback_manager=callback_manager,<br>            verbose=verbose,<br>        )<br>    @classmethod<br>    def from_tools(<br>        cls,<br>        tools: Optional[List[BaseTool]] = None,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        llm: Optional[LLM] = None,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        memory: Optional[BaseMemory] = None,<br>        memory_cls: Type[BaseMemory] = ChatMemoryBuffer,<br>        max_iterations: int = 10,<br>        react_chat_formatter: Optional[ReActChatFormatter] = None,<br>        output_parser: Optional[ReActOutputParser] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        context: Optional[str] = None,<br>        handle_reasoning_failure_fn: Optional[<br>            Callable[[CallbackManager, Exception], ToolOutput]<br>        ] = None,<br>        **kwargs: Any,<br>    ) -> \"ReActAgent\":<br>        \"\"\"Convenience constructor method from set of BaseTools (Optional).<br>        NOTE: kwargs should have been exhausted by this point. In other words<br>        the various upstream components such as BaseSynthesizer (response synthesizer)<br>        or BaseRetriever should have picked up off their respective kwargs in their<br>        constructions.<br>        If `handle_reasoning_failure_fn` is provided, when LLM fails to follow the response templates specified in<br>        the System Prompt, this function will be called. This function should provide to the Agent, so that the Agent<br>        can have a second chance to fix its mistakes.<br>        To handle the exception yourself, you can provide a function that raises the `Exception`.<br>        Note: If you modified any response template in the System Prompt, you should override the method<br>        `_extract_reasoning_step` in `ReActAgentWorker`.<br>        Returns:<br>            ReActAgent<br>        \"\"\"<br>        llm = llm or Settings.llm<br>        if callback_manager is not None:<br>            llm.callback_manager = callback_manager<br>        memory = memory or memory_cls.from_defaults(<br>            chat_history=chat_history or [], llm=llm<br>        )<br>        return cls(<br>            tools=tools or [],<br>            tool_retriever=tool_retriever,<br>            llm=llm,<br>            memory=memory,<br>            max_iterations=max_iterations,<br>            react_chat_formatter=react_chat_formatter,<br>            output_parser=output_parser,<br>            callback_manager=callback_manager,<br>            verbose=verbose,<br>            context=context,<br>            handle_reasoning_failure_fn=handle_reasoning_failure_fn,<br>        )<br>    def _get_prompt_modules(self) -> PromptMixinType:<br>        \"\"\"Get prompt modules.\"\"\"<br>        return {\"agent_worker\": self.agent_worker}<br>```` |\n\n### from\\_tools`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/react/\\#llama_index.core.agent.react.ReActAgent.from_tools \"Permanent link\")\n\n```\nfrom_tools(tools: Optional[List[BaseTool]] = None, tool_retriever: Optional[ObjectRetriever[BaseTool]] = None, llm: Optional[LLM] = None, chat_history: Optional[List[ChatMessage]] = None, memory: Optional[BaseMemory] = None, memory_cls: Type[BaseMemory] = ChatMemoryBuffer, max_iterations: int = 10, react_chat_formatter: Optional[ReActChatFormatter] = None, output_parser: Optional[ReActOutputParser] = None, callback_manager: Optional[CallbackManager] = None, verbose: bool = False, context: Optional[str] = None, handle_reasoning_failure_fn: Optional[Callable[[CallbackManager, Exception], ToolOutput]] = None, **kwargs: Any) -> ReActAgent\n\n```\n\nConvenience constructor method from set of BaseTools (Optional).\n\nNOTE: kwargs should have been exhausted by this point. In other words\nthe various upstream components such as BaseSynthesizer (response synthesizer)\nor BaseRetriever should have picked up off their respective kwargs in their\nconstructions.\n\nIf `handle_reasoning_failure_fn` is provided, when LLM fails to follow the response templates specified in\nthe System Prompt, this function will be called. This function should provide to the Agent, so that the Agent\ncan have a second chance to fix its mistakes.\nTo handle the exception yourself, you can provide a function that raises the `Exception`.\n\nNote: If you modified any response template in the System Prompt, you should override the method\n`_extract_reasoning_step` in `ReActAgentWorker`.\n\n**Returns:**\n\n| Type | Description |\n| --- | --- |\n| `ReActAgent` | ReActAgent |\n\nSource code in `llama-index-core/llama_index/core/agent/react/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>``` | ```<br>@classmethod<br>def from_tools(<br>    cls,<br>    tools: Optional[List[BaseTool]] = None,<br>    tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>    llm: Optional[LLM] = None,<br>    chat_history: Optional[List[ChatMessage]] = None,<br>    memory: Optional[BaseMemory] = None,<br>    memory_cls: Type[BaseMemory] = ChatMemoryBuffer,<br>    max_iterations: int = 10,<br>    react_chat_formatter: Optional[ReActChatFormatter] = None,<br>    output_parser: Optional[ReActOutputParser] = None,<br>    callback_manager: Optional[CallbackManager] = None,<br>    verbose: bool = False,<br>    context: Optional[str] = None,<br>    handle_reasoning_failure_fn: Optional[<br>        Callable[[CallbackManager, Exception], ToolOutput]<br>    ] = None,<br>    **kwargs: Any,<br>) -> \"ReActAgent\":<br>    \"\"\"Convenience constructor method from set of BaseTools (Optional).<br>    NOTE: kwargs should have been exhausted by this point. In other words<br>    the various upstream components such as BaseSynthesizer (response synthesizer)<br>    or BaseRetriever should have picked up off their respective kwargs in their<br>    constructions.<br>    If `handle_reasoning_failure_fn` is provided, when LLM fails to follow the response templates specified in<br>    the System Prompt, this function will be called. This function should provide to the Agent, so that the Agent<br>    can have a second chance to fix its mistakes.<br>    To handle the exception yourself, you can provide a function that raises the `Exception`.<br>    Note: If you modified any response template in the System Prompt, you should override the method<br>    `_extract_reasoning_step` in `ReActAgentWorker`.<br>    Returns:<br>        ReActAgent<br>    \"\"\"<br>    llm = llm or Settings.llm<br>    if callback_manager is not None:<br>        llm.callback_manager = callback_manager<br>    memory = memory or memory_cls.from_defaults(<br>        chat_history=chat_history or [], llm=llm<br>    )<br>    return cls(<br>        tools=tools or [],<br>        tool_retriever=tool_retriever,<br>        llm=llm,<br>        memory=memory,<br>        max_iterations=max_iterations,<br>        react_chat_formatter=react_chat_formatter,<br>        output_parser=output_parser,<br>        callback_manager=callback_manager,<br>        verbose=verbose,<br>        context=context,<br>        handle_reasoning_failure_fn=handle_reasoning_failure_fn,<br>    )<br>``` |\n\n## ReActAgentWorker [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/react/\\#llama_index.core.agent.react.ReActAgentWorker \"Permanent link\")\n\nBases: `BaseAgentWorker`\n\nOpenAI Agent worker.\n\nSource code in `llama-index-core/llama_index/core/agent/react/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>472<br>473<br>474<br>475<br>476<br>477<br>478<br>479<br>480<br>481<br>482<br>483<br>484<br>485<br>486<br>487<br>488<br>489<br>490<br>491<br>492<br>493<br>494<br>495<br>496<br>497<br>498<br>499<br>500<br>501<br>502<br>503<br>504<br>505<br>506<br>507<br>508<br>509<br>510<br>511<br>512<br>513<br>514<br>515<br>516<br>517<br>518<br>519<br>520<br>521<br>522<br>523<br>524<br>525<br>526<br>527<br>528<br>529<br>530<br>531<br>532<br>533<br>534<br>535<br>536<br>537<br>538<br>539<br>540<br>541<br>542<br>543<br>544<br>545<br>546<br>547<br>548<br>549<br>550<br>551<br>552<br>553<br>554<br>555<br>556<br>557<br>558<br>559<br>560<br>561<br>562<br>563<br>564<br>565<br>566<br>567<br>568<br>569<br>570<br>571<br>572<br>573<br>574<br>575<br>576<br>577<br>578<br>579<br>580<br>581<br>582<br>583<br>584<br>585<br>586<br>587<br>588<br>589<br>590<br>591<br>592<br>593<br>594<br>595<br>596<br>597<br>598<br>599<br>600<br>601<br>602<br>603<br>604<br>605<br>606<br>607<br>608<br>609<br>610<br>611<br>612<br>613<br>614<br>615<br>616<br>617<br>618<br>619<br>620<br>621<br>622<br>623<br>624<br>625<br>626<br>627<br>628<br>629<br>630<br>631<br>632<br>633<br>634<br>635<br>636<br>637<br>638<br>639<br>640<br>641<br>642<br>643<br>644<br>645<br>646<br>647<br>648<br>649<br>650<br>651<br>652<br>653<br>654<br>655<br>656<br>657<br>658<br>659<br>660<br>661<br>662<br>663<br>664<br>665<br>666<br>667<br>668<br>669<br>670<br>671<br>672<br>673<br>674<br>675<br>676<br>677<br>678<br>679<br>680<br>681<br>682<br>683<br>684<br>685<br>686<br>687<br>688<br>689<br>690<br>691<br>692<br>693<br>694<br>695<br>696<br>697<br>698<br>699<br>700<br>701<br>702<br>703<br>704<br>705<br>706<br>707<br>708<br>709<br>710<br>711<br>712<br>713<br>714<br>715<br>716<br>717<br>718<br>719<br>720<br>721<br>722<br>723<br>724<br>725<br>726<br>727<br>728<br>729<br>730<br>731<br>732<br>733<br>734<br>735<br>736<br>737<br>738<br>739<br>740<br>741<br>742<br>743<br>744<br>745<br>746<br>747<br>748<br>749<br>750<br>751<br>752<br>753<br>754<br>755<br>756<br>757<br>758<br>759<br>760<br>761<br>762<br>763<br>764<br>765<br>766<br>767<br>768<br>769<br>770<br>771<br>772<br>773<br>774<br>775<br>776<br>777<br>778<br>779<br>780<br>781<br>782<br>783<br>784<br>785<br>786<br>787<br>788<br>789<br>790<br>791<br>792<br>793<br>794<br>795<br>796<br>797<br>798<br>799<br>800<br>801<br>802<br>803<br>804<br>805<br>806<br>807<br>808<br>809<br>810<br>811<br>812<br>813<br>814<br>815<br>816<br>817<br>818<br>819<br>820<br>821<br>822<br>823<br>824<br>825<br>826<br>827<br>828<br>829<br>830<br>``` | ```<br>class ReActAgentWorker(BaseAgentWorker):<br>    \"\"\"OpenAI Agent worker.\"\"\"<br>    def __init__(<br>        self,<br>        tools: Sequence[BaseTool],<br>        llm: LLM,<br>        max_iterations: int = 10,<br>        react_chat_formatter: Optional[ReActChatFormatter] = None,<br>        output_parser: Optional[ReActOutputParser] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        handle_reasoning_failure_fn: Optional[<br>            Callable[[CallbackManager, Exception], ToolOutput]<br>        ] = None,<br>    ) -> None:<br>        self._llm = llm<br>        self.callback_manager = callback_manager or llm.callback_manager<br>        self._max_iterations = max_iterations<br>        self._react_chat_formatter = react_chat_formatter or ReActChatFormatter()<br>        self._output_parser = output_parser or ReActOutputParser()<br>        self._verbose = verbose<br>        self._handle_reasoning_failure_fn = (<br>            handle_reasoning_failure_fn<br>            or tell_llm_about_failure_in_extract_reasoning_step<br>        )<br>        if len(tools) > 0 and tool_retriever is not None:<br>            raise ValueError(\"Cannot specify both tools and tool_retriever\")<br>        elif len(tools) > 0:<br>            self._get_tools = lambda _: tools<br>        elif tool_retriever is not None:<br>            tool_retriever_c = cast(ObjectRetriever[BaseTool], tool_retriever)<br>            self._get_tools = lambda message: tool_retriever_c.retrieve(message)<br>        else:<br>            self._get_tools = lambda _: []<br>    @classmethod<br>    def from_tools(<br>        cls,<br>        tools: Optional[Sequence[BaseTool]] = None,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        llm: Optional[LLM] = None,<br>        max_iterations: int = 10,<br>        react_chat_formatter: Optional[ReActChatFormatter] = None,<br>        output_parser: Optional[ReActOutputParser] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        handle_reasoning_failure_fn: Optional[<br>            Callable[[CallbackManager, Exception], ToolOutput]<br>        ] = None,<br>        **kwargs: Any,<br>    ) -> \"ReActAgentWorker\":<br>        \"\"\"Convenience constructor method from set of BaseTools (Optional).<br>        NOTE: kwargs should have been exhausted by this point. In other words<br>        the various upstream components such as BaseSynthesizer (response synthesizer)<br>        or BaseRetriever should have picked up off their respective kwargs in their<br>        constructions.<br>        Returns:<br>            ReActAgentWorker<br>        \"\"\"<br>        llm = llm or Settings.llm<br>        if callback_manager is not None:<br>            llm.callback_manager = callback_manager<br>        return cls(<br>            tools=tools or [],<br>            tool_retriever=tool_retriever,<br>            llm=llm,<br>            max_iterations=max_iterations,<br>            react_chat_formatter=react_chat_formatter,<br>            output_parser=output_parser,<br>            callback_manager=callback_manager,<br>            verbose=verbose,<br>            handle_reasoning_failure_fn=handle_reasoning_failure_fn,<br>        )<br>    def _get_prompts(self) -> PromptDictType:<br>        \"\"\"Get prompts.\"\"\"<br>        # TODO: the ReAct formatter does not explicitly specify PromptTemplate<br>        # objects, but wrap it in this to obey the interface<br>        sys_header = self._react_chat_formatter.system_header<br>        return {\"system_prompt\": PromptTemplate(sys_header)}<br>    def _update_prompts(self, prompts: PromptDictType) -> None:<br>        \"\"\"Update prompts.\"\"\"<br>        if \"system_prompt\" in prompts:<br>            sys_prompt = cast(PromptTemplate, prompts[\"system_prompt\"])<br>            self._react_chat_formatter.system_header = sys_prompt.template<br>    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>        \"\"\"Initialize step from task.\"\"\"<br>        sources: List[ToolOutput] = []<br>        current_reasoning: List[BaseReasoningStep] = []<br>        # temporary memory for new messages<br>        new_memory = ChatMemoryBuffer.from_defaults()<br>        # initialize task state<br>        task_state = {<br>            \"sources\": sources,<br>            \"current_reasoning\": current_reasoning,<br>            \"new_memory\": new_memory,<br>        }<br>        task.extra_state.update(task_state)<br>        return TaskStep(<br>            task_id=task.task_id,<br>            step_id=str(uuid.uuid4()),<br>            input=task.input,<br>            step_state={\"is_first\": True},<br>        )<br>    def get_tools(self, input: str) -> List[AsyncBaseTool]:<br>        \"\"\"Get tools.\"\"\"<br>        return [adapt_to_async_tool(t) for t in self._get_tools(input)]<br>    def _extract_reasoning_step(<br>        self, output: ChatResponse, is_streaming: bool = False<br>    ) -> Tuple[str, List[BaseReasoningStep], bool]:<br>        \"\"\"<br>        Extracts the reasoning step from the given output.<br>        This method parses the message content from the output,<br>        extracts the reasoning step, and determines whether the processing is<br>        complete. It also performs validation checks on the output and<br>        handles possible errors.<br>        \"\"\"<br>        if output.message.content is None:<br>            raise ValueError(\"Got empty message.\")<br>        message_content = output.message.content<br>        current_reasoning = []<br>        try:<br>            reasoning_step = self._output_parser.parse(message_content, is_streaming)<br>        except BaseException as exc:<br>            raise ValueError(f\"Could not parse output: {message_content}\") from exc<br>        if self._verbose:<br>            print_text(f\"{reasoning_step.get_content()}\\n\", color=\"pink\")<br>        current_reasoning.append(reasoning_step)<br>        if reasoning_step.is_done:<br>            return message_content, current_reasoning, True<br>        reasoning_step = cast(ActionReasoningStep, reasoning_step)<br>        if not isinstance(reasoning_step, ActionReasoningStep):<br>            raise ValueError(f\"Expected ActionReasoningStep, got {reasoning_step}\")<br>        return message_content, current_reasoning, False<br>    def _process_actions(<br>        self,<br>        task: Task,<br>        tools: Sequence[AsyncBaseTool],<br>        output: ChatResponse,<br>        is_streaming: bool = False,<br>    ) -> Tuple[List[BaseReasoningStep], bool]:<br>        tools_dict: Dict[str, AsyncBaseTool] = {<br>            tool.metadata.get_name(): tool for tool in tools<br>        }<br>        tool = None<br>        try:<br>            _, current_reasoning, is_done = self._extract_reasoning_step(<br>                output, is_streaming<br>            )<br>        except ValueError as exp:<br>            current_reasoning = []<br>            tool_output = self._handle_reasoning_failure_fn(self.callback_manager, exp)<br>        else:<br>            if is_done:<br>                return current_reasoning, True<br>            # call tool with input<br>            reasoning_step = cast(ActionReasoningStep, current_reasoning[-1])<br>            if reasoning_step.action in tools_dict:<br>                tool = tools_dict[reasoning_step.action]<br>                with self.callback_manager.event(<br>                    CBEventType.FUNCTION_CALL,<br>                    payload={<br>                        EventPayload.FUNCTION_CALL: reasoning_step.action_input,<br>                        EventPayload.TOOL: tool.metadata,<br>                    },<br>                ) as event:<br>                    try:<br>                        dispatcher.event(<br>                            AgentToolCallEvent(<br>                                arguments=json.dumps({**reasoning_step.action_input}),<br>                                tool=tool.metadata,<br>                            )<br>                        )<br>                        tool_output = tool.call(**reasoning_step.action_input)<br>                    except Exception as e:<br>                        tool_output = ToolOutput(<br>                            content=f\"Error: {e!s}\",<br>                            tool_name=tool.metadata.name,<br>                            raw_input={\"kwargs\": reasoning_step.action_input},<br>                            raw_output=e,<br>                            is_error=True,<br>                        )<br>                    event.on_end(<br>                        payload={EventPayload.FUNCTION_OUTPUT: str(tool_output)}<br>                    )<br>            else:<br>                tool_output = self._handle_nonexistent_tool_name(reasoning_step)<br>        task.extra_state[\"sources\"].append(tool_output)<br>        observation_step = ObservationReasoningStep(<br>            observation=str(tool_output),<br>            return_direct=(<br>                tool.metadata.return_direct and not tool_output.is_error<br>                if tool<br>                else False<br>            ),<br>        )<br>        current_reasoning.append(observation_step)<br>        if self._verbose:<br>            print_text(f\"{observation_step.get_content()}\\n\", color=\"blue\")<br>        return (<br>            current_reasoning,<br>            tool.metadata.return_direct and not tool_output.is_error if tool else False,<br>        )<br>    async def _aprocess_actions(<br>        self,<br>        task: Task,<br>        tools: Sequence[AsyncBaseTool],<br>        output: ChatResponse,<br>        is_streaming: bool = False,<br>    ) -> Tuple[List[BaseReasoningStep], bool]:<br>        tools_dict = {tool.metadata.name: tool for tool in tools}<br>        tool = None<br>        try:<br>            _, current_reasoning, is_done = self._extract_reasoning_step(<br>                output, is_streaming<br>            )<br>        except ValueError as exp:<br>            current_reasoning = []<br>            tool_output = self._handle_reasoning_failure_fn(self.callback_manager, exp)<br>        else:<br>            if is_done:<br>                return current_reasoning, True<br>            # call tool with input<br>            reasoning_step = cast(ActionReasoningStep, current_reasoning[-1])<br>            if reasoning_step.action in tools_dict:<br>                tool = tools_dict[reasoning_step.action]<br>                with self.callback_manager.event(<br>                    CBEventType.FUNCTION_CALL,<br>                    payload={<br>                        EventPayload.FUNCTION_CALL: reasoning_step.action_input,<br>                        EventPayload.TOOL: tool.metadata,<br>                    },<br>                ) as event:<br>                    try:<br>                        dispatcher.event(<br>                            AgentToolCallEvent(<br>                                arguments=json.dumps({**reasoning_step.action_input}),<br>                                tool=tool.metadata,<br>                            )<br>                        )<br>                        tool_output = await tool.acall(**reasoning_step.action_input)<br>                    except Exception as e:<br>                        tool_output = ToolOutput(<br>                            content=f\"Error: {e!s}\",<br>                            tool_name=tool.metadata.name,<br>                            raw_input={\"kwargs\": reasoning_step.action_input},<br>                            raw_output=e,<br>                            is_error=True,<br>                        )<br>                    event.on_end(<br>                        payload={EventPayload.FUNCTION_OUTPUT: str(tool_output)}<br>                    )<br>            else:<br>                tool_output = self._handle_nonexistent_tool_name(reasoning_step)<br>        task.extra_state[\"sources\"].append(tool_output)<br>        observation_step = ObservationReasoningStep(<br>            observation=str(tool_output),<br>            return_direct=(<br>                tool.metadata.return_direct and not tool_output.is_error<br>                if tool<br>                else False<br>            ),<br>        )<br>        current_reasoning.append(observation_step)<br>        if self._verbose:<br>            print_text(f\"{observation_step.get_content()}\\n\", color=\"blue\")<br>        return (<br>            current_reasoning,<br>            tool.metadata.return_direct and not tool_output.is_error if tool else False,<br>        )<br>    def _handle_nonexistent_tool_name(<br>        self, reasoning_step: ActionReasoningStep<br>    ) -> ToolOutput:<br>        # We still emit a `tool_output` object to the task, so that the LLM can know<br>        # it has hallucinated in the next reasoning step.<br>        with self.callback_manager.event(<br>            CBEventType.FUNCTION_CALL,<br>            payload={<br>                EventPayload.FUNCTION_CALL: reasoning_step.action_input,<br>            },<br>        ) as event:<br>            # TODO(L10N): This should be localized.<br>            content = f\"Error: No such tool named `{reasoning_step.action}`.\"<br>            tool_output = ToolOutput(<br>                content=content,<br>                tool_name=reasoning_step.action,<br>                raw_input={\"kwargs\": reasoning_step.action_input},<br>                raw_output=content,<br>                is_error=True,<br>            )<br>            event.on_end(payload={EventPayload.FUNCTION_OUTPUT: str(tool_output)})<br>        return tool_output<br>    def _get_response(<br>        self,<br>        current_reasoning: List[BaseReasoningStep],<br>        sources: List[ToolOutput],<br>    ) -> AgentChatResponse:<br>        \"\"\"Get response from reasoning steps.\"\"\"<br>        if len(current_reasoning) == 0:<br>            raise ValueError(\"No reasoning steps were taken.\")<br>        elif len(current_reasoning) == self._max_iterations:<br>            raise ValueError(\"Reached max iterations.\")<br>        if isinstance(current_reasoning[-1], ResponseReasoningStep):<br>            response_step = cast(ResponseReasoningStep, current_reasoning[-1])<br>            response_str = response_step.response<br>        elif (<br>            isinstance(current_reasoning[-1], ObservationReasoningStep)<br>            and current_reasoning[-1].return_direct<br>        ):<br>            response_str = current_reasoning[-1].observation<br>        else:<br>            response_str = current_reasoning[-1].get_content()<br>        # TODO: add sources from reasoning steps<br>        return AgentChatResponse(response=response_str, sources=sources)<br>    def _get_task_step_response(<br>        self, agent_response: AGENT_CHAT_RESPONSE_TYPE, step: TaskStep, is_done: bool<br>    ) -> TaskStepOutput:<br>        \"\"\"Get task step response.\"\"\"<br>        if is_done:<br>            new_steps = []<br>        else:<br>            new_steps = [<br>                step.get_next_step(<br>                    step_id=str(uuid.uuid4()),<br>                    # NOTE: input is unused<br>                    input=None,<br>                )<br>            ]<br>        return TaskStepOutput(<br>            output=agent_response,<br>            task_step=step,<br>            is_last=is_done,<br>            next_steps=new_steps,<br>        )<br>    def _infer_stream_chunk_is_final(<br>        self, chunk: ChatResponse, missed_chunks_storage: list<br>    ) -> bool:<br>        \"\"\"Infers if a chunk from a live stream is the start of the final<br>        reasoning step. (i.e., and should eventually become<br>        ResponseReasoningStep \u2014 not part of this function's logic tho.).<br>        Args:<br>            chunk (ChatResponse): the current chunk stream to check<br>            missed_chunks_storage (list): list to store missed chunks<br>        Returns:<br>            bool: Boolean on whether the chunk is the start of the final response<br>        \"\"\"<br>        latest_content = chunk.message.content<br>        if latest_content:<br>            # doesn't follow thought-action format<br>            # keep first chunks<br>            if len(latest_content) < len(\"Thought\"):<br>                missed_chunks_storage.append(chunk)<br>            elif not latest_content.startswith(\"Thought\"):<br>                return True<br>            elif \"Answer: \" in latest_content:<br>                missed_chunks_storage.clear()<br>                return True<br>        return False<br>    def _add_back_chunk_to_stream(<br>        self,<br>        chunks: List[ChatResponse],<br>        chat_stream: Generator[ChatResponse, None, None],<br>    ) -> Generator[ChatResponse, None, None]:<br>        \"\"\"Helper method for adding back initial chunk stream of final response<br>        back to the rest of the chat_stream.<br>        Args:<br>            chunks List[ChatResponse]: the chunks to add back to the beginning of the<br>                                    chat_stream.<br>        Return:<br>            Generator[ChatResponse, None, None]: the updated chat_stream<br>        \"\"\"<br>        def gen() -> Generator[ChatResponse, None, None]:<br>            yield from chunks<br>            yield from chat_stream<br>        return gen()<br>    async def _async_add_back_chunk_to_stream(<br>        self,<br>        chunks: List[ChatResponse],<br>        chat_stream: AsyncGenerator[ChatResponse, None],<br>    ) -> AsyncGenerator[ChatResponse, None]:<br>        \"\"\"Helper method for adding back initial chunk stream of final response<br>        back to the rest of the chat_stream.<br>        NOTE: this itself is not an async function.<br>        Args:<br>            chunks List[ChatResponse]: the chunks to add back to the beginning of the<br>                                    chat_stream.<br>        Return:<br>            AsyncGenerator[ChatResponse, None]: the updated async chat_stream<br>        \"\"\"<br>        for chunk in chunks:<br>            yield chunk<br>        async for item in chat_stream:<br>            yield item<br>    def _run_step(<br>        self,<br>        step: TaskStep,<br>        task: Task,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        if step.input is not None:<br>            add_user_step_to_reasoning(<br>                step,<br>                task.extra_state[\"new_memory\"],<br>                task.extra_state[\"current_reasoning\"],<br>                verbose=self._verbose,<br>            )<br>        # TODO: see if we want to do step-based inputs<br>        tools = self.get_tools(task.input)<br>        input_chat = self._react_chat_formatter.format(<br>            tools,<br>            chat_history=task.memory.get(input=task.input)<br>            + task.extra_state[\"new_memory\"].get_all(),<br>            current_reasoning=task.extra_state[\"current_reasoning\"],<br>        )<br>        # send prompt<br>        chat_response = self._llm.chat(input_chat)<br>        # given react prompt outputs, call tools or return response<br>        reasoning_steps, is_done = self._process_actions(<br>            task, tools, output=chat_response<br>        )<br>        task.extra_state[\"current_reasoning\"].extend(reasoning_steps)<br>        agent_response = self._get_response(<br>            task.extra_state[\"current_reasoning\"], task.extra_state[\"sources\"]<br>        )<br>        if is_done:<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(content=agent_response.response, role=MessageRole.ASSISTANT)<br>            )<br>        return self._get_task_step_response(agent_response, step, is_done)<br>    async def _arun_step(<br>        self,<br>        step: TaskStep,<br>        task: Task,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        if step.input is not None:<br>            add_user_step_to_reasoning(<br>                step,<br>                task.extra_state[\"new_memory\"],<br>                task.extra_state[\"current_reasoning\"],<br>                verbose=self._verbose,<br>            )<br>        # TODO: see if we want to do step-based inputs<br>        tools = self.get_tools(task.input)<br>        input_chat = self._react_chat_formatter.format(<br>            tools,<br>            chat_history=task.memory.get(input=task.input)<br>            + task.extra_state[\"new_memory\"].get_all(),<br>            current_reasoning=task.extra_state[\"current_reasoning\"],<br>        )<br>        # send prompt<br>        chat_response = await self._llm.achat(input_chat)<br>        # given react prompt outputs, call tools or return response<br>        reasoning_steps, is_done = await self._aprocess_actions(<br>            task, tools, output=chat_response<br>        )<br>        task.extra_state[\"current_reasoning\"].extend(reasoning_steps)<br>        agent_response = self._get_response(<br>            task.extra_state[\"current_reasoning\"], task.extra_state[\"sources\"]<br>        )<br>        if is_done:<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(content=agent_response.response, role=MessageRole.ASSISTANT)<br>            )<br>        return self._get_task_step_response(agent_response, step, is_done)<br>    def _run_step_stream(<br>        self,<br>        step: TaskStep,<br>        task: Task,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        if step.input is not None:<br>            add_user_step_to_reasoning(<br>                step,<br>                task.extra_state[\"new_memory\"],<br>                task.extra_state[\"current_reasoning\"],<br>                verbose=self._verbose,<br>            )<br>        # TODO: see if we want to do step-based inputs<br>        tools = self.get_tools(task.input)<br>        input_chat = self._react_chat_formatter.format(<br>            tools,<br>            chat_history=task.memory.get(input=task.input)<br>            + task.extra_state[\"new_memory\"].get_all(),<br>            current_reasoning=task.extra_state[\"current_reasoning\"],<br>        )<br>        chat_stream = self._llm.stream_chat(input_chat)<br>        # iterate over stream, break out if is final answer after the \"Answer: \"<br>        full_response = ChatResponse(<br>            message=ChatMessage(content=None, role=\"assistant\")<br>        )<br>        missed_chunks_storage: List[ChatResponse] = []<br>        is_done = False<br>        for latest_chunk in chat_stream:<br>            full_response = latest_chunk<br>            is_done = self._infer_stream_chunk_is_final(<br>                latest_chunk, missed_chunks_storage<br>            )<br>            if is_done:<br>                break<br>        non_streaming_agent_response = None<br>        agent_response_stream = None<br>        if not is_done:<br>            # given react prompt outputs, call tools or return response<br>            reasoning_steps, is_done = self._process_actions(<br>                task, tools=tools, output=full_response, is_streaming=True<br>            )<br>            task.extra_state[\"current_reasoning\"].extend(reasoning_steps)<br>            # use _get_response to return intermediate response<br>            non_streaming_agent_response = self._get_response(<br>                task.extra_state[\"current_reasoning\"], task.extra_state[\"sources\"]<br>            )<br>            if is_done:<br>                non_streaming_agent_response.is_dummy_stream = True<br>                task.extra_state[\"new_memory\"].put(<br>                    ChatMessage(<br>                        content=non_streaming_agent_response.response,<br>                        role=MessageRole.ASSISTANT,<br>                    )<br>                )<br>        else:<br>            # Get the response in a separate thread so we can yield the response<br>            response_stream = self._add_back_chunk_to_stream(<br>                chunks=[*missed_chunks_storage, latest_chunk], chat_stream=chat_stream<br>            )<br>            agent_response_stream = StreamingAgentChatResponse(<br>                chat_stream=response_stream,<br>                sources=task.extra_state[\"sources\"],<br>            )<br>            thread = Thread(<br>                target=agent_response_stream.write_response_to_history,<br>                args=(task.extra_state[\"new_memory\"],),<br>                kwargs={\"on_stream_end_fn\": partial(self.finalize_task, task)},<br>            )<br>            thread.start()<br>        response = agent_response_stream or non_streaming_agent_response<br>        assert response is not None<br>        return self._get_task_step_response(response, step, is_done)<br>    async def _arun_step_stream(<br>        self,<br>        step: TaskStep,<br>        task: Task,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        if step.input is not None:<br>            add_user_step_to_reasoning(<br>                step,<br>                task.extra_state[\"new_memory\"],<br>                task.extra_state[\"current_reasoning\"],<br>                verbose=self._verbose,<br>            )<br>        # TODO: see if we want to do step-based inputs<br>        tools = self.get_tools(task.input)<br>        input_chat = self._react_chat_formatter.format(<br>            tools,<br>            chat_history=task.memory.get(input=task.input)<br>            + task.extra_state[\"new_memory\"].get_all(),<br>            current_reasoning=task.extra_state[\"current_reasoning\"],<br>        )<br>        chat_stream = await self._llm.astream_chat(input_chat)<br>        # iterate over stream, break out if is final answer after the \"Answer: \"<br>        full_response = ChatResponse(<br>            message=ChatMessage(content=None, role=\"assistant\")<br>        )<br>        missed_chunks_storage: List[ChatResponse] = []<br>        is_done = False<br>        async for latest_chunk in chat_stream:<br>            full_response = latest_chunk<br>            is_done = self._infer_stream_chunk_is_final(<br>                latest_chunk, missed_chunks_storage<br>            )<br>            if is_done:<br>                break<br>        non_streaming_agent_response = None<br>        agent_response_stream = None<br>        if not is_done:<br>            # given react prompt outputs, call tools or return response<br>            reasoning_steps, is_done = await self._aprocess_actions(<br>                task, tools=tools, output=full_response, is_streaming=True<br>            )<br>            task.extra_state[\"current_reasoning\"].extend(reasoning_steps)<br>            # use _get_response to return intermediate response<br>            non_streaming_agent_response = self._get_response(<br>                task.extra_state[\"current_reasoning\"], task.extra_state[\"sources\"]<br>            )<br>            if is_done:<br>                non_streaming_agent_response.is_dummy_stream = True<br>                task.extra_state[\"new_memory\"].put(<br>                    ChatMessage(<br>                        content=non_streaming_agent_response.response,<br>                        role=MessageRole.ASSISTANT,<br>                    )<br>                )<br>        else:<br>            # Get the response in a separate thread so we can yield the response<br>            response_stream = self._async_add_back_chunk_to_stream(<br>                chunks=[*missed_chunks_storage, latest_chunk], chat_stream=chat_stream<br>            )<br>            agent_response_stream = StreamingAgentChatResponse(<br>                achat_stream=response_stream,<br>                sources=task.extra_state[\"sources\"],<br>            )<br>            # create task to write chat response to history<br>            asyncio.create_task(<br>                agent_response_stream.awrite_response_to_history(<br>                    task.extra_state[\"new_memory\"],<br>                    on_stream_end_fn=partial(self.finalize_task, task),<br>                )<br>            )<br>            # wait until response writing is done<br>            agent_response_stream._ensure_async_setup()<br>            assert agent_response_stream.is_function_false_event is not None<br>            await agent_response_stream.is_function_false_event.wait()<br>        response = agent_response_stream or non_streaming_agent_response<br>        assert response is not None<br>        return self._get_task_step_response(response, step, is_done)<br>    @trace_method(\"run_step\")<br>    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        return self._run_step(step, task)<br>    @trace_method(\"run_step\")<br>    async def arun_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        return await self._arun_step(step, task)<br>    @trace_method(\"run_step\")<br>    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        # TODO: figure out if we need a different type for TaskStepOutput<br>        return self._run_step_stream(step, task)<br>    @trace_method(\"run_step\")<br>    async def astream_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async stream).\"\"\"<br>        return await self._arun_step_stream(step, task)<br>    def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>        \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>        # add new messages to memory<br>        task.memory.set(<br>            task.memory.get_all() + task.extra_state[\"new_memory\"].get_all()<br>        )<br>        # reset new memory<br>        task.extra_state[\"new_memory\"].reset()<br>    def set_callback_manager(self, callback_manager: CallbackManager) -> None:<br>        \"\"\"Set callback manager.\"\"\"<br>        # TODO: make this abstractmethod (right now will break some agent impls)<br>        self.callback_manager = callback_manager<br>``` |\n\n### from\\_tools`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/react/\\#llama_index.core.agent.react.ReActAgentWorker.from_tools \"Permanent link\")\n\n```\nfrom_tools(tools: Optional[Sequence[BaseTool]] = None, tool_retriever: Optional[ObjectRetriever[BaseTool]] = None, llm: Optional[LLM] = None, max_iterations: int = 10, react_chat_formatter: Optional[ReActChatFormatter] = None, output_parser: Optional[ReActOutputParser] = None, callback_manager: Optional[CallbackManager] = None, verbose: bool = False, handle_reasoning_failure_fn: Optional[Callable[[CallbackManager, Exception], ToolOutput]] = None, **kwargs: Any) -> ReActAgentWorker\n\n```\n\nConvenience constructor method from set of BaseTools (Optional).\n\nNOTE: kwargs should have been exhausted by this point. In other words\nthe various upstream components such as BaseSynthesizer (response synthesizer)\nor BaseRetriever should have picked up off their respective kwargs in their\nconstructions.\n\n**Returns:**\n\n| Type | Description |\n| --- | --- |\n| `ReActAgentWorker` | ReActAgentWorker |\n\nSource code in `llama-index-core/llama_index/core/agent/react/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>``` | ```<br>@classmethod<br>def from_tools(<br>    cls,<br>    tools: Optional[Sequence[BaseTool]] = None,<br>    tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>    llm: Optional[LLM] = None,<br>    max_iterations: int = 10,<br>    react_chat_formatter: Optional[ReActChatFormatter] = None,<br>    output_parser: Optional[ReActOutputParser] = None,<br>    callback_manager: Optional[CallbackManager] = None,<br>    verbose: bool = False,<br>    handle_reasoning_failure_fn: Optional[<br>        Callable[[CallbackManager, Exception], ToolOutput]<br>    ] = None,<br>    **kwargs: Any,<br>) -> \"ReActAgentWorker\":<br>    \"\"\"Convenience constructor method from set of BaseTools (Optional).<br>    NOTE: kwargs should have been exhausted by this point. In other words<br>    the various upstream components such as BaseSynthesizer (response synthesizer)<br>    or BaseRetriever should have picked up off their respective kwargs in their<br>    constructions.<br>    Returns:<br>        ReActAgentWorker<br>    \"\"\"<br>    llm = llm or Settings.llm<br>    if callback_manager is not None:<br>        llm.callback_manager = callback_manager<br>    return cls(<br>        tools=tools or [],<br>        tool_retriever=tool_retriever,<br>        llm=llm,<br>        max_iterations=max_iterations,<br>        react_chat_formatter=react_chat_formatter,<br>        output_parser=output_parser,<br>        callback_manager=callback_manager,<br>        verbose=verbose,<br>        handle_reasoning_failure_fn=handle_reasoning_failure_fn,<br>    )<br>``` |\n\n### initialize\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/react/\\#llama_index.core.agent.react.ReActAgentWorker.initialize_step \"Permanent link\")\n\n```\ninitialize_step(task: Task, **kwargs: Any) -> TaskStep\n\n```\n\nInitialize step from task.\n\nSource code in `llama-index-core/llama_index/core/agent/react/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>``` | ```<br>def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>    \"\"\"Initialize step from task.\"\"\"<br>    sources: List[ToolOutput] = []<br>    current_reasoning: List[BaseReasoningStep] = []<br>    # temporary memory for new messages<br>    new_memory = ChatMemoryBuffer.from_defaults()<br>    # initialize task state<br>    task_state = {<br>        \"sources\": sources,<br>        \"current_reasoning\": current_reasoning,<br>        \"new_memory\": new_memory,<br>    }<br>    task.extra_state.update(task_state)<br>    return TaskStep(<br>        task_id=task.task_id,<br>        step_id=str(uuid.uuid4()),<br>        input=task.input,<br>        step_state={\"is_first\": True},<br>    )<br>``` |\n\n### get\\_tools [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/react/\\#llama_index.core.agent.react.ReActAgentWorker.get_tools \"Permanent link\")\n\n```\nget_tools(input: str) -> List[AsyncBaseTool]\n\n```\n\nGet tools.\n\nSource code in `llama-index-core/llama_index/core/agent/react/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>222<br>223<br>224<br>``` | ```<br>def get_tools(self, input: str) -> List[AsyncBaseTool]:<br>    \"\"\"Get tools.\"\"\"<br>    return [adapt_to_async_tool(t) for t in self._get_tools(input)]<br>``` |\n\n### run\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/react/\\#llama_index.core.agent.react.ReActAgentWorker.run_step \"Permanent link\")\n\n```\nrun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step.\n\nSource code in `llama-index-core/llama_index/core/agent/react/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>793<br>794<br>795<br>796<br>``` | ```<br>@trace_method(\"run_step\")<br>def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step.\"\"\"<br>    return self._run_step(step, task)<br>``` |\n\n### arun\\_step`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/react/\\#llama_index.core.agent.react.ReActAgentWorker.arun_step \"Permanent link\")\n\n```\narun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async).\n\nSource code in `llama-index-core/llama_index/core/agent/react/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>798<br>799<br>800<br>801<br>802<br>803<br>``` | ```<br>@trace_method(\"run_step\")<br>async def arun_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async).\"\"\"<br>    return await self._arun_step(step, task)<br>``` |\n\n### stream\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/react/\\#llama_index.core.agent.react.ReActAgentWorker.stream_step \"Permanent link\")\n\n```\nstream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (stream).\n\nSource code in `llama-index-core/llama_index/core/agent/react/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>805<br>806<br>807<br>808<br>809<br>``` | ```<br>@trace_method(\"run_step\")<br>def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step (stream).\"\"\"<br>    # TODO: figure out if we need a different type for TaskStepOutput<br>    return self._run_step_stream(step, task)<br>``` |\n\n### astream\\_step`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/react/\\#llama_index.core.agent.react.ReActAgentWorker.astream_step \"Permanent link\")\n\n```\nastream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async stream).\n\nSource code in `llama-index-core/llama_index/core/agent/react/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>811<br>812<br>813<br>814<br>815<br>816<br>``` | ```<br>@trace_method(\"run_step\")<br>async def astream_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async stream).\"\"\"<br>    return await self._arun_step_stream(step, task)<br>``` |\n\n### finalize\\_task [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/react/\\#llama_index.core.agent.react.ReActAgentWorker.finalize_task \"Permanent link\")\n\n```\nfinalize_task(task: Task, **kwargs: Any) -> None\n\n```\n\nFinalize task, after all the steps are completed.\n\nSource code in `llama-index-core/llama_index/core/agent/react/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>818<br>819<br>820<br>821<br>822<br>823<br>824<br>825<br>``` | ```<br>def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>    \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>    # add new messages to memory<br>    task.memory.set(<br>        task.memory.get_all() + task.extra_state[\"new_memory\"].get_all()<br>    )<br>    # reset new memory<br>    task.extra_state[\"new_memory\"].reset()<br>``` |\n\n### set\\_callback\\_manager [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/react/\\#llama_index.core.agent.react.ReActAgentWorker.set_callback_manager \"Permanent link\")\n\n```\nset_callback_manager(callback_manager: CallbackManager) -> None\n\n```\n\nSet callback manager.\n\nSource code in `llama-index-core/llama_index/core/agent/react/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>827<br>828<br>829<br>830<br>``` | ```<br>def set_callback_manager(self, callback_manager: CallbackManager) -> None:<br>    \"\"\"Set callback manager.\"\"\"<br>    # TODO: make this abstractmethod (right now will break some agent impls)<br>    self.callback_manager = callback_manager<br>``` |\n\n## ReActChatFormatter [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/react/\\#llama_index.core.agent.react.ReActChatFormatter \"Permanent link\")\n\nBases: `BaseAgentChatFormatter`\n\nReAct chat formatter.\n\nSource code in `llama-index-core/llama_index/core/agent/react/formatter.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>``` | ```<br>class ReActChatFormatter(BaseAgentChatFormatter):<br>    \"\"\"ReAct chat formatter.\"\"\"<br>    system_header: str = REACT_CHAT_SYSTEM_HEADER  # default<br>    context: str = \"\"  # not needed w/ default<br>    def format(<br>        self,<br>        tools: Sequence[BaseTool],<br>        chat_history: List[ChatMessage],<br>        current_reasoning: Optional[List[BaseReasoningStep]] = None,<br>    ) -> List[ChatMessage]:<br>        \"\"\"Format chat history into list of ChatMessage.\"\"\"<br>        current_reasoning = current_reasoning or []<br>        format_args = {<br>            \"tool_desc\": \"\\n\".join(get_react_tool_descriptions(tools)),<br>            \"tool_names\": \", \".join([tool.metadata.get_name() for tool in tools]),<br>        }<br>        if self.context:<br>            format_args[\"context\"] = self.context<br>        fmt_sys_header = self.system_header.format(**format_args)<br>        # format reasoning history as alternating user and assistant messages<br>        # where the assistant messages are thoughts and actions and the user<br>        # messages are observations<br>        reasoning_history = []<br>        for reasoning_step in current_reasoning:<br>            if isinstance(reasoning_step, ObservationReasoningStep):<br>                message = ChatMessage(<br>                    role=MessageRole.USER,<br>                    content=reasoning_step.get_content(),<br>                )<br>            else:<br>                message = ChatMessage(<br>                    role=MessageRole.ASSISTANT,<br>                    content=reasoning_step.get_content(),<br>                )<br>            reasoning_history.append(message)<br>        return [<br>            ChatMessage(role=MessageRole.SYSTEM, content=fmt_sys_header),<br>            *chat_history,<br>            *reasoning_history,<br>        ]<br>    @classmethod<br>    def from_defaults(<br>        cls,<br>        system_header: Optional[str] = None,<br>        context: Optional[str] = None,<br>    ) -> \"ReActChatFormatter\":<br>        \"\"\"Create ReActChatFormatter from defaults.\"\"\"<br>        if not system_header:<br>            system_header = (<br>                REACT_CHAT_SYSTEM_HEADER<br>                if not context<br>                else CONTEXT_REACT_CHAT_SYSTEM_HEADER<br>            )<br>        return ReActChatFormatter(<br>            system_header=system_header,<br>            context=context or \"\",<br>        )<br>    @classmethod<br>    def from_context(cls, context: str) -> \"ReActChatFormatter\":<br>        \"\"\"Create ReActChatFormatter from context.<br>        NOTE: deprecated<br>        \"\"\"<br>        logger.warning(<br>            \"ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\"<br>        )<br>        return ReActChatFormatter.from_defaults(<br>            system_header=CONTEXT_REACT_CHAT_SYSTEM_HEADER, context=context<br>        )<br>``` |\n\n### format [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/react/\\#llama_index.core.agent.react.ReActChatFormatter.format \"Permanent link\")\n\n```\nformat(tools: Sequence[BaseTool], chat_history: List[ChatMessage], current_reasoning: Optional[List[BaseReasoningStep]] = None) -> List[ChatMessage]\n\n```\n\nFormat chat history into list of ChatMessage.\n\nSource code in `llama-index-core/llama_index/core/agent/react/formatter.py`\n\n|     |     |\n| --- | --- |\n| ```<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>89<br>90<br>91<br>92<br>93<br>94<br>95<br>96<br>``` | ```<br>def format(<br>    self,<br>    tools: Sequence[BaseTool],<br>    chat_history: List[ChatMessage],<br>    current_reasoning: Optional[List[BaseReasoningStep]] = None,<br>) -> List[ChatMessage]:<br>    \"\"\"Format chat history into list of ChatMessage.\"\"\"<br>    current_reasoning = current_reasoning or []<br>    format_args = {<br>        \"tool_desc\": \"\\n\".join(get_react_tool_descriptions(tools)),<br>        \"tool_names\": \", \".join([tool.metadata.get_name() for tool in tools]),<br>    }<br>    if self.context:<br>        format_args[\"context\"] = self.context<br>    fmt_sys_header = self.system_header.format(**format_args)<br>    # format reasoning history as alternating user and assistant messages<br>    # where the assistant messages are thoughts and actions and the user<br>    # messages are observations<br>    reasoning_history = []<br>    for reasoning_step in current_reasoning:<br>        if isinstance(reasoning_step, ObservationReasoningStep):<br>            message = ChatMessage(<br>                role=MessageRole.USER,<br>                content=reasoning_step.get_content(),<br>            )<br>        else:<br>            message = ChatMessage(<br>                role=MessageRole.ASSISTANT,<br>                content=reasoning_step.get_content(),<br>            )<br>        reasoning_history.append(message)<br>    return [<br>        ChatMessage(role=MessageRole.SYSTEM, content=fmt_sys_header),<br>        *chat_history,<br>        *reasoning_history,<br>    ]<br>``` |\n\n### from\\_defaults`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/react/\\#llama_index.core.agent.react.ReActChatFormatter.from_defaults \"Permanent link\")\n\n```\nfrom_defaults(system_header: Optional[str] = None, context: Optional[str] = None) -> ReActChatFormatter\n\n```\n\nCreate ReActChatFormatter from defaults.\n\nSource code in `llama-index-core/llama_index/core/agent/react/formatter.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>``` | ```<br>@classmethod<br>def from_defaults(<br>    cls,<br>    system_header: Optional[str] = None,<br>    context: Optional[str] = None,<br>) -> \"ReActChatFormatter\":<br>    \"\"\"Create ReActChatFormatter from defaults.\"\"\"<br>    if not system_header:<br>        system_header = (<br>            REACT_CHAT_SYSTEM_HEADER<br>            if not context<br>            else CONTEXT_REACT_CHAT_SYSTEM_HEADER<br>        )<br>    return ReActChatFormatter(<br>        system_header=system_header,<br>        context=context or \"\",<br>    )<br>``` |\n\n### from\\_context`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/react/\\#llama_index.core.agent.react.ReActChatFormatter.from_context \"Permanent link\")\n\n```\nfrom_context(context: str) -> ReActChatFormatter\n\n```\n\nCreate ReActChatFormatter from context.\n\nNOTE: deprecated\n\nSource code in `llama-index-core/llama_index/core/agent/react/formatter.py`\n\n|     |     |\n| --- | --- |\n| ```<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>``` | ```<br>@classmethod<br>def from_context(cls, context: str) -> \"ReActChatFormatter\":<br>    \"\"\"Create ReActChatFormatter from context.<br>    NOTE: deprecated<br>    \"\"\"<br>    logger.warning(<br>        \"ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\"<br>    )<br>    return ReActChatFormatter.from_defaults(<br>        system_header=CONTEXT_REACT_CHAT_SYSTEM_HEADER, context=context<br>    )<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "React - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/agent/react/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/llama_debug/#llama_index.core.callbacks.llama_debug.LlamaDebugHandler)\n\n# Llama debug\n\n## LlamaDebugHandler [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/llama_debug/\\#llama_index.core.callbacks.llama_debug.LlamaDebugHandler \"Permanent link\")\n\nBases: `PythonicallyPrintingBaseHandler`\n\nCallback handler that keeps track of debug info.\n\nNOTE: this is a beta feature. The usage within our codebase, and the interface\nmay change.\n\nThis handler simply keeps track of event starts/ends, separated by event types.\nYou can use this callback handler to keep track of and debug events.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `event_starts_to_ignore` | `Optional[List[CBEventType]]` | list of event types to<br>ignore when tracking event starts. | `None` |\n| `event_ends_to_ignore` | `Optional[List[CBEventType]]` | list of event types to<br>ignore when tracking event ends. | `None` |\n\nSource code in `llama-index-core/llama_index/core/callbacks/llama_debug.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>``` | ```<br>class LlamaDebugHandler(PythonicallyPrintingBaseHandler):<br>    \"\"\"Callback handler that keeps track of debug info.<br>    NOTE: this is a beta feature. The usage within our codebase, and the interface<br>    may change.<br>    This handler simply keeps track of event starts/ends, separated by event types.<br>    You can use this callback handler to keep track of and debug events.<br>    Args:<br>        event_starts_to_ignore (Optional[List[CBEventType]]): list of event types to<br>            ignore when tracking event starts.<br>        event_ends_to_ignore (Optional[List[CBEventType]]): list of event types to<br>            ignore when tracking event ends.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        event_starts_to_ignore: Optional[List[CBEventType]] = None,<br>        event_ends_to_ignore: Optional[List[CBEventType]] = None,<br>        print_trace_on_end: bool = True,<br>        logger: Optional[logging.Logger] = None,<br>    ) -> None:<br>        \"\"\"Initialize the llama debug handler.\"\"\"<br>        self._event_pairs_by_type: Dict[CBEventType, List[CBEvent]] = defaultdict(list)<br>        self._event_pairs_by_id: Dict[str, List[CBEvent]] = defaultdict(list)<br>        self._sequential_events: List[CBEvent] = []<br>        self._cur_trace_id: Optional[str] = None<br>        self._trace_map: Dict[str, List[str]] = defaultdict(list)<br>        self.print_trace_on_end = print_trace_on_end<br>        event_starts_to_ignore = (<br>            event_starts_to_ignore if event_starts_to_ignore else []<br>        )<br>        event_ends_to_ignore = event_ends_to_ignore if event_ends_to_ignore else []<br>        super().__init__(<br>            event_starts_to_ignore=event_starts_to_ignore,<br>            event_ends_to_ignore=event_ends_to_ignore,<br>            logger=logger,<br>        )<br>    def on_event_start(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        parent_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> str:<br>        \"\"\"Store event start data by event type.<br>        Args:<br>            event_type (CBEventType): event type to store.<br>            payload (Optional[Dict[str, Any]]): payload to store.<br>            event_id (str): event id to store.<br>            parent_id (str): parent event id.<br>        \"\"\"<br>        event = CBEvent(event_type, payload=payload, id_=event_id)<br>        self._event_pairs_by_type[event.event_type].append(event)<br>        self._event_pairs_by_id[event.id_].append(event)<br>        self._sequential_events.append(event)<br>        return event.id_<br>    def on_event_end(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> None:<br>        \"\"\"Store event end data by event type.<br>        Args:<br>            event_type (CBEventType): event type to store.<br>            payload (Optional[Dict[str, Any]]): payload to store.<br>            event_id (str): event id to store.<br>        \"\"\"<br>        event = CBEvent(event_type, payload=payload, id_=event_id)<br>        self._event_pairs_by_type[event.event_type].append(event)<br>        self._event_pairs_by_id[event.id_].append(event)<br>        self._sequential_events.append(event)<br>        self._trace_map = defaultdict(list)<br>    def get_events(self, event_type: Optional[CBEventType] = None) -> List[CBEvent]:<br>        \"\"\"Get all events for a specific event type.\"\"\"<br>        if event_type is not None:<br>            return self._event_pairs_by_type[event_type]<br>        return self._sequential_events<br>    def _get_event_pairs(self, events: List[CBEvent]) -> List[List[CBEvent]]:<br>        \"\"\"Helper function to pair events according to their ID.\"\"\"<br>        event_pairs: Dict[str, List[CBEvent]] = defaultdict(list)<br>        for event in events:<br>            event_pairs[event.id_].append(event)<br>        return sorted(<br>            event_pairs.values(),<br>            key=lambda x: datetime.strptime(x[0].time, TIMESTAMP_FORMAT),<br>        )<br>    def _get_time_stats_from_event_pairs(<br>        self, event_pairs: List[List[CBEvent]]<br>    ) -> EventStats:<br>        \"\"\"Calculate time-based stats for a set of event pairs.\"\"\"<br>        total_secs = 0.0<br>        for event_pair in event_pairs:<br>            start_time = datetime.strptime(event_pair[0].time, TIMESTAMP_FORMAT)<br>            end_time = datetime.strptime(event_pair[-1].time, TIMESTAMP_FORMAT)<br>            total_secs += (end_time - start_time).total_seconds()<br>        return EventStats(<br>            total_secs=total_secs,<br>            average_secs=total_secs / len(event_pairs),<br>            total_count=len(event_pairs),<br>        )<br>    def get_event_pairs(<br>        self, event_type: Optional[CBEventType] = None<br>    ) -> List[List[CBEvent]]:<br>        \"\"\"Pair events by ID, either all events or a specific type.\"\"\"<br>        if event_type is not None:<br>            return self._get_event_pairs(self._event_pairs_by_type[event_type])<br>        return self._get_event_pairs(self._sequential_events)<br>    def get_llm_inputs_outputs(self) -> List[List[CBEvent]]:<br>        \"\"\"Get the exact LLM inputs and outputs.\"\"\"<br>        return self._get_event_pairs(self._event_pairs_by_type[CBEventType.LLM])<br>    def get_event_time_info(<br>        self, event_type: Optional[CBEventType] = None<br>    ) -> EventStats:<br>        event_pairs = self.get_event_pairs(event_type)<br>        return self._get_time_stats_from_event_pairs(event_pairs)<br>    def flush_event_logs(self) -> None:<br>        \"\"\"Clear all events from memory.\"\"\"<br>        self._event_pairs_by_type = defaultdict(list)<br>        self._event_pairs_by_id = defaultdict(list)<br>        self._sequential_events = []<br>    def start_trace(self, trace_id: Optional[str] = None) -> None:<br>        \"\"\"Launch a trace.\"\"\"<br>        self._trace_map = defaultdict(list)<br>        self._cur_trace_id = trace_id<br>    def end_trace(<br>        self,<br>        trace_id: Optional[str] = None,<br>        trace_map: Optional[Dict[str, List[str]]] = None,<br>    ) -> None:<br>        \"\"\"Shutdown the current trace.\"\"\"<br>        self._trace_map = trace_map or defaultdict(list)<br>        if self.print_trace_on_end:<br>            self.print_trace_map()<br>    def _print_trace_map(self, cur_event_id: str, level: int = 0) -> None:<br>        \"\"\"Recursively print trace map to terminal for debugging.\"\"\"<br>        event_pair = self._event_pairs_by_id[cur_event_id]<br>        if event_pair:<br>            time_stats = self._get_time_stats_from_event_pairs([event_pair])<br>            indent = \" \" * level * 2<br>            self._print(<br>                f\"{indent}|_{event_pair[0].event_type} -> {time_stats.total_secs} seconds\",<br>            )<br>        child_event_ids = self._trace_map[cur_event_id]<br>        for child_event_id in child_event_ids:<br>            self._print_trace_map(child_event_id, level=level + 1)<br>    def print_trace_map(self) -> None:<br>        \"\"\"Print simple trace map to terminal for debugging of the most recent trace.\"\"\"<br>        self._print(\"*\" * 10)<br>        self._print(f\"Trace: {self._cur_trace_id}\")<br>        self._print_trace_map(BASE_TRACE_EVENT, level=1)<br>        self._print(\"*\" * 10)<br>    @property<br>    def event_pairs_by_type(self) -> Dict[CBEventType, List[CBEvent]]:<br>        return self._event_pairs_by_type<br>    @property<br>    def events_pairs_by_id(self) -> Dict[str, List[CBEvent]]:<br>        return self._event_pairs_by_id<br>    @property<br>    def sequential_events(self) -> List[CBEvent]:<br>        return self._sequential_events<br>``` |\n\n### on\\_event\\_start [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/llama_debug/\\#llama_index.core.callbacks.llama_debug.LlamaDebugHandler.on_event_start \"Permanent link\")\n\n```\non_event_start(event_type: CBEventType, payload: Optional[Dict[str, Any]] = None, event_id: str = '', parent_id: str = '', **kwargs: Any) -> str\n\n```\n\nStore event start data by event type.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `event_type` | `CBEventType` | event type to store. | _required_ |\n| `payload` | `Optional[Dict[str, Any]]` | payload to store. | `None` |\n| `event_id` | `str` | event id to store. | `''` |\n| `parent_id` | `str` | parent event id. | `''` |\n\nSource code in `llama-index-core/llama_index/core/callbacks/llama_debug.py`\n\n|     |     |\n| --- | --- |\n| ```<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>``` | ```<br>def on_event_start(<br>    self,<br>    event_type: CBEventType,<br>    payload: Optional[Dict[str, Any]] = None,<br>    event_id: str = \"\",<br>    parent_id: str = \"\",<br>    **kwargs: Any,<br>) -> str:<br>    \"\"\"Store event start data by event type.<br>    Args:<br>        event_type (CBEventType): event type to store.<br>        payload (Optional[Dict[str, Any]]): payload to store.<br>        event_id (str): event id to store.<br>        parent_id (str): parent event id.<br>    \"\"\"<br>    event = CBEvent(event_type, payload=payload, id_=event_id)<br>    self._event_pairs_by_type[event.event_type].append(event)<br>    self._event_pairs_by_id[event.id_].append(event)<br>    self._sequential_events.append(event)<br>    return event.id_<br>``` |\n\n### on\\_event\\_end [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/llama_debug/\\#llama_index.core.callbacks.llama_debug.LlamaDebugHandler.on_event_end \"Permanent link\")\n\n```\non_event_end(event_type: CBEventType, payload: Optional[Dict[str, Any]] = None, event_id: str = '', **kwargs: Any) -> None\n\n```\n\nStore event end data by event type.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `event_type` | `CBEventType` | event type to store. | _required_ |\n| `payload` | `Optional[Dict[str, Any]]` | payload to store. | `None` |\n| `event_id` | `str` | event id to store. | `''` |\n\nSource code in `llama-index-core/llama_index/core/callbacks/llama_debug.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>``` | ```<br>def on_event_end(<br>    self,<br>    event_type: CBEventType,<br>    payload: Optional[Dict[str, Any]] = None,<br>    event_id: str = \"\",<br>    **kwargs: Any,<br>) -> None:<br>    \"\"\"Store event end data by event type.<br>    Args:<br>        event_type (CBEventType): event type to store.<br>        payload (Optional[Dict[str, Any]]): payload to store.<br>        event_id (str): event id to store.<br>    \"\"\"<br>    event = CBEvent(event_type, payload=payload, id_=event_id)<br>    self._event_pairs_by_type[event.event_type].append(event)<br>    self._event_pairs_by_id[event.id_].append(event)<br>    self._sequential_events.append(event)<br>    self._trace_map = defaultdict(list)<br>``` |\n\n### get\\_events [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/llama_debug/\\#llama_index.core.callbacks.llama_debug.LlamaDebugHandler.get_events \"Permanent link\")\n\n```\nget_events(event_type: Optional[CBEventType] = None) -> List[CBEvent]\n\n```\n\nGet all events for a specific event type.\n\nSource code in `llama-index-core/llama_index/core/callbacks/llama_debug.py`\n\n|     |     |\n| --- | --- |\n| ```<br>102<br>103<br>104<br>105<br>106<br>107<br>``` | ```<br>def get_events(self, event_type: Optional[CBEventType] = None) -> List[CBEvent]:<br>    \"\"\"Get all events for a specific event type.\"\"\"<br>    if event_type is not None:<br>        return self._event_pairs_by_type[event_type]<br>    return self._sequential_events<br>``` |\n\n### get\\_event\\_pairs [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/llama_debug/\\#llama_index.core.callbacks.llama_debug.LlamaDebugHandler.get_event_pairs \"Permanent link\")\n\n```\nget_event_pairs(event_type: Optional[CBEventType] = None) -> List[List[CBEvent]]\n\n```\n\nPair events by ID, either all events or a specific type.\n\nSource code in `llama-index-core/llama_index/core/callbacks/llama_debug.py`\n\n|     |     |\n| --- | --- |\n| ```<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>``` | ```<br>def get_event_pairs(<br>    self, event_type: Optional[CBEventType] = None<br>) -> List[List[CBEvent]]:<br>    \"\"\"Pair events by ID, either all events or a specific type.\"\"\"<br>    if event_type is not None:<br>        return self._get_event_pairs(self._event_pairs_by_type[event_type])<br>    return self._get_event_pairs(self._sequential_events)<br>``` |\n\n### get\\_llm\\_inputs\\_outputs [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/llama_debug/\\#llama_index.core.callbacks.llama_debug.LlamaDebugHandler.get_llm_inputs_outputs \"Permanent link\")\n\n```\nget_llm_inputs_outputs() -> List[List[CBEvent]]\n\n```\n\nGet the exact LLM inputs and outputs.\n\nSource code in `llama-index-core/llama_index/core/callbacks/llama_debug.py`\n\n|     |     |\n| --- | --- |\n| ```<br>145<br>146<br>147<br>``` | ```<br>def get_llm_inputs_outputs(self) -> List[List[CBEvent]]:<br>    \"\"\"Get the exact LLM inputs and outputs.\"\"\"<br>    return self._get_event_pairs(self._event_pairs_by_type[CBEventType.LLM])<br>``` |\n\n### flush\\_event\\_logs [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/llama_debug/\\#llama_index.core.callbacks.llama_debug.LlamaDebugHandler.flush_event_logs \"Permanent link\")\n\n```\nflush_event_logs() -> None\n\n```\n\nClear all events from memory.\n\nSource code in `llama-index-core/llama_index/core/callbacks/llama_debug.py`\n\n|     |     |\n| --- | --- |\n| ```<br>155<br>156<br>157<br>158<br>159<br>``` | ```<br>def flush_event_logs(self) -> None:<br>    \"\"\"Clear all events from memory.\"\"\"<br>    self._event_pairs_by_type = defaultdict(list)<br>    self._event_pairs_by_id = defaultdict(list)<br>    self._sequential_events = []<br>``` |\n\n### start\\_trace [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/llama_debug/\\#llama_index.core.callbacks.llama_debug.LlamaDebugHandler.start_trace \"Permanent link\")\n\n```\nstart_trace(trace_id: Optional[str] = None) -> None\n\n```\n\nLaunch a trace.\n\nSource code in `llama-index-core/llama_index/core/callbacks/llama_debug.py`\n\n|     |     |\n| --- | --- |\n| ```<br>161<br>162<br>163<br>164<br>``` | ```<br>def start_trace(self, trace_id: Optional[str] = None) -> None:<br>    \"\"\"Launch a trace.\"\"\"<br>    self._trace_map = defaultdict(list)<br>    self._cur_trace_id = trace_id<br>``` |\n\n### end\\_trace [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/llama_debug/\\#llama_index.core.callbacks.llama_debug.LlamaDebugHandler.end_trace \"Permanent link\")\n\n```\nend_trace(trace_id: Optional[str] = None, trace_map: Optional[Dict[str, List[str]]] = None) -> None\n\n```\n\nShutdown the current trace.\n\nSource code in `llama-index-core/llama_index/core/callbacks/llama_debug.py`\n\n|     |     |\n| --- | --- |\n| ```<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>``` | ```<br>def end_trace(<br>    self,<br>    trace_id: Optional[str] = None,<br>    trace_map: Optional[Dict[str, List[str]]] = None,<br>) -> None:<br>    \"\"\"Shutdown the current trace.\"\"\"<br>    self._trace_map = trace_map or defaultdict(list)<br>    if self.print_trace_on_end:<br>        self.print_trace_map()<br>``` |\n\n### print\\_trace\\_map [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/llama_debug/\\#llama_index.core.callbacks.llama_debug.LlamaDebugHandler.print_trace_map \"Permanent link\")\n\n```\nprint_trace_map() -> None\n\n```\n\nPrint simple trace map to terminal for debugging of the most recent trace.\n\nSource code in `llama-index-core/llama_index/core/callbacks/llama_debug.py`\n\n|     |     |\n| --- | --- |\n| ```<br>190<br>191<br>192<br>193<br>194<br>195<br>``` | ```<br>def print_trace_map(self) -> None:<br>    \"\"\"Print simple trace map to terminal for debugging of the most recent trace.\"\"\"<br>    self._print(\"*\" * 10)<br>    self._print(f\"Trace: {self._cur_trace_id}\")<br>    self._print_trace_map(BASE_TRACE_EVENT, level=1)<br>    self._print(\"*\" * 10)<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Llama debug - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/llama_debug/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/agent/dashscope/#llama_index.agent.dashscope.DashScopeAgent)\n\n# Dashscope\n\n## DashScopeAgent [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/dashscope/\\#llama_index.agent.dashscope.DashScopeAgent \"Permanent link\")\n\nBases: `BaseAgent`\n\nDashScope agent simple wrapper for Alibaba cloud bailian high-level agent api.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-dashscope/llama_index/agent/dashscope/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>``` | ```<br>class DashScopeAgent(BaseAgent):<br>    \"\"\"<br>    DashScope agent simple wrapper for Alibaba cloud bailian high-level agent api.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        app_id: str,<br>        chat_session: bool = True,<br>        workspace: str = None,<br>        api_key: str = None,<br>        verbose: bool = False,<br>    ) -> None:<br>        \"\"\"Init params.<br>        Args:<br>            app_id (str): id of Alibaba cloud bailian application<br>            chat_session (bool): When need to keep chat session, defaults to True.<br>            workspace(str, `optional`): Workspace of Alibaba cloud bailian<br>            api_key (str, optional): The api api_key, can be None,<br>                if None, will get from ENV DASHSCOPE_API_KEY.<br>            verbose: Output verbose info or not.<br>        \"\"\"<br>        self.app_id = app_id<br>        self.chat_session = chat_session<br>        self.workspace = workspace<br>        self.api_key = api_key<br>        self._verbose = verbose<br>        self._session_id = None<br>    @trace_method(\"chat\")<br>    def chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None, **kwargs<br>    ) -> AgentChatResponse:<br>        return self._chat(message=message, stream=False, **kwargs)<br>    async def achat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> AgentChatResponse:<br>        raise NotImplementedError(\"achat not implemented\")<br>    @trace_method(\"chat\")<br>    def stream_chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None, **kwargs<br>    ) -> StreamingAgentChatResponse:<br>        return self._chat(message=message, stream=True, **kwargs)<br>    async def astream_chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> StreamingAgentChatResponse:<br>        raise NotImplementedError(\"astream_chat not implemented\")<br>    def reset(self) -> None:<br>        self._session_id = None<br>    @property<br>    def chat_history(self) -> List[ChatMessage]:<br>        raise NotImplementedError(\"chat_history not implemented\")<br>    @property<br>    def get_session_id(self) -> str:<br>        return self._session_id<br>    def _chat(<br>        self,<br>        message: str,<br>        stream: bool = False,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        **kwargs,<br>    ) -> Union[AgentChatResponse, StreamingAgentChatResponse]:<br>        \"\"\"Call app completion service.<br>        Args:<br>            message (str): Message for chatting with LLM.<br>            chat_history (List[ChatMessage], `optional`): The user provided chat history. Defaults to None.<br>            **kwargs:<br>                session_id(str, `optional`): Session if for multiple rounds call.<br>                biz_params(dict, `optional`): The extra parameters for flow or plugin.<br>        Raises:<br>            ValueError: The request failed with http code and message.<br>        Returns:<br>            Union[AgentChatResponse, StreamingAgentChatResponse]<br>        \"\"\"<br>        if stream:<br>            kwargs[\"stream\"] = True<br>        if self.chat_session:<br>            kwargs[\"session_id\"] = self._session_id<br>        response = Application.call(<br>            app_id=self.app_id,<br>            prompt=message,<br>            history=None,<br>            workspace=self.workspace,<br>            api_key=self.api_key,<br>            **kwargs,<br>        )<br>        if stream:<br>            return StreamingAgentChatResponse(<br>                chat_stream=(self.from_dashscope_response(rsp) for rsp in response)<br>            )<br>        else:<br>            if response.status_code != HTTPStatus.OK:<br>                raise ValueError(<br>                    f\"Chat failed with status: {response.status_code}, request id: {response.request_id}, \"<br>                    f\"code: {response.code}, message: {response.message}\"<br>                )<br>            if self._verbose:<br>                print(\"Got chat response: %s\" % response)<br>            self._session_id = response.output.session_id<br>            return AgentChatResponse(response=response.output.text)<br>    def from_dashscope_response(self, response: ApplicationResponse) -> ChatResponse:<br>        if response.status_code != HTTPStatus.OK:<br>            raise ValueError(<br>                f\"Chat failed with status: {response.status_code}, request id: {response.request_id}, \"<br>                f\"code: {response.code}, message: {response.message}\"<br>            )<br>        if self._verbose and response.output.finish_reason == \"stop\":<br>            print(\"Got final chat response: %s\" % response)<br>        self._session_id = response.output.session_id<br>        return ChatResponse(<br>            message=ChatMessage(<br>                role=MessageRole.ASSISTANT, content=response.output.text<br>            )<br>        )<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Dashscope - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/agent/dashscope/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/literalai/#llama_index.callbacks.literalai.literalai_callback_handler)\n\n# Literalai\n\n## literalai\\_callback\\_handler [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/literalai/\\#llama_index.callbacks.literalai.literalai_callback_handler \"Permanent link\")\n\n```\nliteralai_callback_handler(batch_size: int = 5, api_key: Optional[str] = None, url: Optional[str] = None, environment: Optional[str] = None, disabled: bool = False) -> BaseCallbackHandler\n\n```\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-literalai/llama_index/callbacks/literalai/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>``` | ```<br>def literalai_callback_handler(<br>    batch_size: int = 5,<br>    api_key: Optional[str] = None,<br>    url: Optional[str] = None,<br>    environment: Optional[str] = None,<br>    disabled: bool = False,<br>) -> BaseCallbackHandler:<br>    try:<br>        from literalai import LiteralClient<br>        from literalai.my_types import Environment<br>        literalai_client = LiteralClient(<br>            batch_size=batch_size,<br>            api_key=api_key,<br>            url=url,<br>            environment=cast(Environment, environment),<br>            disabled=disabled,<br>        )<br>        literalai_client.instrument_llamaindex()<br>        class QueryEndEventHandler(BaseEventHandler):<br>            \"\"\"This handler will flush the Literal Client cache to Literal AI at the end of each query.\"\"\"<br>            @classmethod<br>            def class_name(cls) -> str:<br>                \"\"\"Class name.\"\"\"<br>                return \"QueryEndEventHandler\"<br>            def handle(self, event: BaseEvent, **kwargs) -> None:<br>                \"\"\"Flushes the Literal cache when receiving the QueryEnd event.\"\"\"<br>                try:<br>                    if isinstance(event, QueryEndEvent):<br>                        literalai_client.flush()<br>                except Exception as e:<br>                    logging.error(<br>                        \"Error in Literal AI global handler : %s\",<br>                        str(e),<br>                        exc_info=True,<br>                    )<br>        dispatcher = get_dispatcher()<br>        event_handler = QueryEndEventHandler()<br>        dispatcher.add_event_handler(event_handler)<br>    except ImportError:<br>        raise ImportError(<br>            \"Please install the Literal AI Python SDK with `pip install -U literalai`\"<br>        )<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Literalai - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/literalai/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/databricks/#llama_index.embeddings.databricks.DatabricksEmbedding)\n\n# Databricks\n\n## DatabricksEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/databricks/\\#llama_index.embeddings.databricks.DatabricksEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nDatabricks class for text embedding.\n\nDatabricks adheres to the OpenAI API, so this integration aligns closely with the existing OpenAIEmbedding class.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model` | `str` | The unique ID of the embedding model as served by the Databricks endpoint. | _required_ |\n| `endpoint` | `Optional[str]` | The url of the Databricks endpoint. Can be set as an environment variable ( `DATABRICKS_SERVING_ENDPOINT`). | `None` |\n| `api_key` | `Optional[str]` | The Databricks API key to use. Can be set as an environment variable ( `DATABRICKS_TOKEN`). | `None` |\n\n**Examples:**\n\n`pip install llama-index-embeddings-databricks`\n\n```\nimport os\nfrom llama_index.core import Settings\nfrom llama_index.embeddings.databricks import DatabricksEmbedding\n\n# Set up the DatabricksEmbedding class with the required model, API key and serving endpoint\nos.environ[\"DATABRICKS_TOKEN\"] = \"<MY TOKEN>\"\nos.environ[\"DATABRICKS_SERVING_ENDPOINT\"] = \"<MY ENDPOINT>\"\nembed_model  = DatabricksEmbedding(model=\"databricks-bge-large-en\")\nSettings.embed_model = embed_model\n\n# Embed some text\nembeddings = embed_model.get_text_embedding(\"The DatabricksEmbedding integration works great.\")\n\n```\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-databricks/llama_index/embeddings/databricks/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>``` | ````<br>class DatabricksEmbedding(BaseEmbedding):<br>    \"\"\"Databricks class for text embedding.<br>    Databricks adheres to the OpenAI API, so this integration aligns closely with the existing OpenAIEmbedding class.<br>    Args:<br>        model (str): The unique ID of the embedding model as served by the Databricks endpoint.<br>        endpoint (Optional[str]): The url of the Databricks endpoint. Can be set as an environment variable (`DATABRICKS_SERVING_ENDPOINT`).<br>        api_key (Optional[str]): The Databricks API key to use. Can be set as an environment variable (`DATABRICKS_TOKEN`).<br>    Examples:<br>        `pip install llama-index-embeddings-databricks`<br>        ```python<br>        import os<br>        from llama_index.core import Settings<br>        from llama_index.embeddings.databricks import DatabricksEmbedding<br>        # Set up the DatabricksEmbedding class with the required model, API key and serving endpoint<br>        os.environ[\"DATABRICKS_TOKEN\"] = \"<MY TOKEN>\"<br>        os.environ[\"DATABRICKS_SERVING_ENDPOINT\"] = \"<MY ENDPOINT>\"<br>        embed_model  = DatabricksEmbedding(model=\"databricks-bge-large-en\")<br>        Settings.embed_model = embed_model<br>        # Embed some text<br>        embeddings = embed_model.get_text_embedding(\"The DatabricksEmbedding integration works great.\")<br>        ```<br>    \"\"\"<br>    additional_kwargs: Dict[str, Any] = Field(<br>        default_factory=dict, description=\"Additional kwargs as for the OpenAI API.\"<br>    )<br>    model: str = Field(<br>        description=\"The ID of a model hosted on the databricks endpoint.\"<br>    )<br>    api_key: str = Field(description=\"The Databricks API key.\")<br>    endpoint: str = Field(description=\"The Databricks API endpoint.\")<br>    max_retries: int = Field(<br>        default=10, description=\"Maximum number of retries.\", gte=0<br>    )<br>    timeout: float = Field(default=60.0, description=\"Timeout for each request.\", gte=0)<br>    default_headers: Optional[Dict[str, str]] = Field(<br>        default=None, description=\"The default headers for API requests.\"<br>    )<br>    reuse_client: bool = Field(<br>        default=True,<br>        description=(<br>            \"Reuse the client between requests. When doing anything with large \"<br>            \"volumes of async API calls, setting this to false can improve stability.\"<br>        ),<br>    )<br>    _query_engine: str = PrivateAttr()<br>    _text_engine: str = PrivateAttr()<br>    _client: Optional[OpenAI] = PrivateAttr()<br>    _aclient: Optional[AsyncOpenAI] = PrivateAttr()<br>    _http_client: Optional[httpx.Client] = PrivateAttr()<br>    def __init__(<br>        self,<br>        model: str,<br>        endpoint: Optional[str] = None,<br>        embed_batch_size: int = 100,<br>        additional_kwargs: Optional[Dict[str, Any]] = None,<br>        api_key: Optional[str] = None,<br>        max_retries: int = 10,<br>        timeout: float = 60.0,<br>        reuse_client: bool = True,<br>        callback_manager: Optional[CallbackManager] = None,<br>        default_headers: Optional[Dict[str, str]] = None,<br>        http_client: Optional[httpx.Client] = None,<br>        num_workers: Optional[int] = None,<br>        **kwargs: Any,<br>    ) -> None:<br>        additional_kwargs = additional_kwargs or {}<br>        api_key = get_from_param_or_env(\"api_key\", api_key, \"DATABRICKS_TOKEN\")<br>        endpoint = get_from_param_or_env(<br>            \"endpoint\", endpoint, \"DATABRICKS_SERVING_ENDPOINT\"<br>        )<br>        super().__init__(<br>            model=model,<br>            endpoint=endpoint,<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            model_name=model,<br>            additional_kwargs=additional_kwargs,<br>            api_key=api_key,<br>            max_retries=max_retries,<br>            reuse_client=reuse_client,<br>            timeout=timeout,<br>            default_headers=default_headers,<br>            num_workers=num_workers,<br>            **kwargs,<br>        )<br>        self._client = None<br>        self._aclient = None<br>        self._http_client = http_client<br>    def _get_client(self) -> OpenAI:<br>        if not self.reuse_client:<br>            return OpenAI(**self._get_credential_kwargs())<br>        if self._client is None:<br>            self._client = OpenAI(**self._get_credential_kwargs())<br>        return self._client<br>    def _get_aclient(self) -> AsyncOpenAI:<br>        if not self.reuse_client:<br>            return AsyncOpenAI(**self._get_credential_kwargs())<br>        if self._aclient is None:<br>            self._aclient = AsyncOpenAI(**self._get_credential_kwargs())<br>        return self._aclient<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"DatabricksEmbedding\"<br>    def _get_credential_kwargs(self) -> Dict[str, Any]:<br>        return {<br>            \"api_key\": self.api_key,<br>            \"base_url\": self.endpoint,<br>            \"max_retries\": self.max_retries,<br>            \"timeout\": self.timeout,<br>            \"default_headers\": self.default_headers,<br>            \"http_client\": self._http_client,<br>        }<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        client = self._get_client()<br>        return get_embedding(<br>            client,<br>            query,<br>            engine=self.model,<br>            **self.additional_kwargs,<br>        )<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        aclient = self._get_aclient()<br>        return await aget_embedding(<br>            aclient,<br>            query,<br>            engine=self.model,<br>            **self.additional_kwargs,<br>        )<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        client = self._get_client()<br>        return get_embedding(<br>            client,<br>            text,<br>            engine=self.model,<br>            **self.additional_kwargs,<br>        )<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously get text embedding.\"\"\"<br>        aclient = self._get_aclient()<br>        return await aget_embedding(<br>            aclient,<br>            text,<br>            engine=self.model,<br>            **self.additional_kwargs,<br>        )<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.<br>        By default, this is a wrapper around _get_text_embedding.<br>        Can be overridden for batch queries.<br>        \"\"\"<br>        client = self._get_client()<br>        return get_embeddings(<br>            client,<br>            texts,<br>            engine=self.model,<br>            **self.additional_kwargs,<br>        )<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Asynchronously get text embeddings.\"\"\"<br>        aclient = self._get_aclient()<br>        return await aget_embeddings(<br>            aclient,<br>            texts,<br>            engine=self.model,<br>            **self.additional_kwargs,<br>        )<br>```` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Databricks - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/databricks/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/dashscope/#llama_index.embeddings.dashscope.DashScopeEmbedding)\n\n# Dashscope\n\n## DashScopeEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/dashscope/\\#llama_index.embeddings.dashscope.DashScopeEmbedding \"Permanent link\")\n\nBases: `MultiModalEmbedding`\n\nDashScope class for text embedding.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model_name` | `str` | Model name for embedding.<br>Defaults to DashScopeTextEmbeddingModels.TEXT\\_EMBEDDING\\_V2.<br>Options are:<br>```<br>- DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V1<br>- DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V2<br>``` | `TEXT_EMBEDDING_V2` |\n| `text_type` | `str` | The input type, \\['query', 'document'\\],<br>For asymmetric tasks such as retrieval, in order to achieve better<br>retrieval results, it is recommended to distinguish between query<br>text (query) and base text (document) types, clustering Symmetric<br>tasks such as classification and classification do not need to<br>be specially specified, and the system default<br>value \"document\" can be used. | `'document'` |\n| `api_key` | `str` | The DashScope api key. | `None` |\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-dashscope/llama_index/embeddings/dashscope/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>``` | ```<br>class DashScopeEmbedding(MultiModalEmbedding):<br>    \"\"\"DashScope class for text embedding.<br>    Args:<br>        model_name (str): Model name for embedding.<br>            Defaults to DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V2.<br>                Options are:<br>                - DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V1<br>                - DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V2<br>        text_type (str): The input type, ['query', 'document'],<br>            For asymmetric tasks such as retrieval, in order to achieve better<br>            retrieval results, it is recommended to distinguish between query<br>            text (query) and base text (document) types, clustering Symmetric<br>            tasks such as classification and classification do not need to<br>            be specially specified, and the system default<br>            value \"document\" can be used.<br>        api_key (str): The DashScope api key.<br>    \"\"\"<br>    _api_key: Optional[str] = PrivateAttr()<br>    _text_type: Optional[str] = PrivateAttr()<br>    def __init__(<br>        self,<br>        model_name: str = DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V2,<br>        text_type: str = \"document\",<br>        api_key: Optional[str] = None,<br>        embed_batch_size: int = EMBED_MAX_BATCH_SIZE,<br>        **kwargs: Any,<br>    ) -> None:<br>        super().__init__(<br>            model_name=model_name,<br>            embed_batch_size=embed_batch_size,<br>            **kwargs,<br>        )<br>        self._api_key = api_key<br>        self._text_type = text_type<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"DashScopeEmbedding\"<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        emb = get_text_embedding(<br>            self.model_name,<br>            query,<br>            api_key=self._api_key,<br>            text_type=\"query\",<br>        )<br>        if len(emb) > 0 and emb[0] is not None:<br>            return emb[0]<br>        else:<br>            return []<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        emb = get_text_embedding(<br>            self.model_name,<br>            text,<br>            api_key=self._api_key,<br>            text_type=self._text_type,<br>        )<br>        if len(emb) > 0 and emb[0] is not None:<br>            return emb[0]<br>        else:<br>            return []<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        return get_text_embedding(<br>            self.model_name,<br>            texts,<br>            api_key=self._api_key,<br>            text_type=self._text_type,<br>        )<br>    # TODO: use proper async methods<br>    async def _aget_text_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._get_text_embedding(query)<br>    # TODO: user proper async methods<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self._get_query_embedding(query)<br>    def get_batch_query_embedding(self, embedding_file_url: str) -> Optional[str]:<br>        \"\"\"Get batch query embeddings.<br>        Args:<br>            embedding_file_url (str): The url of the file to embedding which with lines of text to embedding.<br>        Returns:<br>            str: The url of the embedding result, format ref:<br>                 https://help.aliyun.com/zh/dashscope/developer-reference/text-embedding-async-api-details.<br>        \"\"\"<br>        return get_batch_text_embedding(<br>            self.model_name,<br>            embedding_file_url,<br>            api_key=self._api_key,<br>            text_type=self._text_type,<br>        )<br>    def get_batch_text_embedding(self, embedding_file_url: str) -> Optional[str]:<br>        \"\"\"Get batch text embeddings.<br>        Args:<br>            embedding_file_url (str): The url of the file to embedding which with lines of text to embedding.<br>        Returns:<br>            str: The url of the embedding result, format ref:<br>                 https://help.aliyun.com/zh/dashscope/developer-reference/text-embedding-async-api-details.<br>        \"\"\"<br>        return get_batch_text_embedding(<br>            self.model_name,<br>            embedding_file_url,<br>            api_key=self._api_key,<br>            text_type=self._text_type,<br>        )<br>    def _get_image_embedding(self, img_file_path: ImageType) -> List[float]:<br>        \"\"\"<br>        Embed the input image synchronously.<br>        \"\"\"<br>        input = [{\"image\": img_file_path}]<br>        return get_multimodal_embedding(<br>            self.model_name, input=input, api_key=self._api_key<br>        )<br>    async def _aget_image_embedding(self, img_file_path: ImageType) -> List[float]:<br>        \"\"\"<br>        Embed the input image asynchronously.<br>        \"\"\"<br>        return self._get_image_embedding(img_file_path=img_file_path)<br>    def get_multimodal_embedding(<br>        self, input: List[Dict], auto_truncation: bool = False<br>    ) -> List[float]:<br>        \"\"\"Call DashScope multimodal embedding.<br>        ref: https://help.aliyun.com/zh/dashscope/developer-reference/one-peace-multimodal-embedding-api-details.<br>        Args:<br>            input (str): The input of the multimodal embedding, eg:<br>                [{'factor': 1, 'text': '\u4f60\u597d'},<br>                {'factor': 2, 'audio': 'https://dashscope.oss-cn-beijing.aliyuncs.com/audios/cow.flac'},<br>                {'factor': 3, 'image': 'https://dashscope.oss-cn-beijing.aliyuncs.com/images/256_1.png'}]<br>        Raises:<br>            ImportError: Need install dashscope package.<br>        Returns:<br>            List[float]: The embedding result<br>        \"\"\"<br>        return get_multimodal_embedding(<br>            self.model_name,<br>            input=input,<br>            api_key=self._api_key,<br>            auto_truncation=auto_truncation,<br>        )<br>``` |\n\n### get\\_batch\\_query\\_embedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/dashscope/\\#llama_index.embeddings.dashscope.DashScopeEmbedding.get_batch_query_embedding \"Permanent link\")\n\n```\nget_batch_query_embedding(embedding_file_url: str) -> Optional[str]\n\n```\n\nGet batch query embeddings.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `embedding_file_url` | `str` | The url of the file to embedding which with lines of text to embedding. | _required_ |\n\n**Returns:**\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `str` | `Optional[str]` | The url of the embedding result, format ref:<br>https://help.aliyun.com/zh/dashscope/developer-reference/text-embedding-async-api-details. |\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-dashscope/llama_index/embeddings/dashscope/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>``` | ```<br>def get_batch_query_embedding(self, embedding_file_url: str) -> Optional[str]:<br>    \"\"\"Get batch query embeddings.<br>    Args:<br>        embedding_file_url (str): The url of the file to embedding which with lines of text to embedding.<br>    Returns:<br>        str: The url of the embedding result, format ref:<br>             https://help.aliyun.com/zh/dashscope/developer-reference/text-embedding-async-api-details.<br>    \"\"\"<br>    return get_batch_text_embedding(<br>        self.model_name,<br>        embedding_file_url,<br>        api_key=self._api_key,<br>        text_type=self._text_type,<br>    )<br>``` |\n\n### get\\_batch\\_text\\_embedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/dashscope/\\#llama_index.embeddings.dashscope.DashScopeEmbedding.get_batch_text_embedding \"Permanent link\")\n\n```\nget_batch_text_embedding(embedding_file_url: str) -> Optional[str]\n\n```\n\nGet batch text embeddings.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `embedding_file_url` | `str` | The url of the file to embedding which with lines of text to embedding. | _required_ |\n\n**Returns:**\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `str` | `Optional[str]` | The url of the embedding result, format ref:<br>https://help.aliyun.com/zh/dashscope/developer-reference/text-embedding-async-api-details. |\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-dashscope/llama_index/embeddings/dashscope/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>``` | ```<br>def get_batch_text_embedding(self, embedding_file_url: str) -> Optional[str]:<br>    \"\"\"Get batch text embeddings.<br>    Args:<br>        embedding_file_url (str): The url of the file to embedding which with lines of text to embedding.<br>    Returns:<br>        str: The url of the embedding result, format ref:<br>             https://help.aliyun.com/zh/dashscope/developer-reference/text-embedding-async-api-details.<br>    \"\"\"<br>    return get_batch_text_embedding(<br>        self.model_name,<br>        embedding_file_url,<br>        api_key=self._api_key,<br>        text_type=self._text_type,<br>    )<br>``` |\n\n### get\\_multimodal\\_embedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/dashscope/\\#llama_index.embeddings.dashscope.DashScopeEmbedding.get_multimodal_embedding \"Permanent link\")\n\n```\nget_multimodal_embedding(input: List[Dict], auto_truncation: bool = False) -> List[float]\n\n```\n\nCall DashScope multimodal embedding.\nref: https://help.aliyun.com/zh/dashscope/developer-reference/one-peace-multimodal-embedding-api-details.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `input` | `str` | The input of the multimodal embedding, eg:<br>\\[{'factor': 1, 'text': '\u4f60\u597d'},<br>{'factor': 2, 'audio': 'https://dashscope.oss-cn-beijing.aliyuncs.com/audios/cow.flac'},<br>{'factor': 3, 'image': 'https://dashscope.oss-cn-beijing.aliyuncs.com/images/256\\_1.png'}\\] | _required_ |\n\n**Raises:**\n\n| Type | Description |\n| --- | --- |\n| `ImportError` | Need install dashscope package. |\n\n**Returns:**\n\n| Type | Description |\n| --- | --- |\n| `List[float]` | List\\[float\\]: The embedding result |\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-dashscope/llama_index/embeddings/dashscope/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>``` | ```<br>def get_multimodal_embedding(<br>    self, input: List[Dict], auto_truncation: bool = False<br>) -> List[float]:<br>    \"\"\"Call DashScope multimodal embedding.<br>    ref: https://help.aliyun.com/zh/dashscope/developer-reference/one-peace-multimodal-embedding-api-details.<br>    Args:<br>        input (str): The input of the multimodal embedding, eg:<br>            [{'factor': 1, 'text': '\u4f60\u597d'},<br>            {'factor': 2, 'audio': 'https://dashscope.oss-cn-beijing.aliyuncs.com/audios/cow.flac'},<br>            {'factor': 3, 'image': 'https://dashscope.oss-cn-beijing.aliyuncs.com/images/256_1.png'}]<br>    Raises:<br>        ImportError: Need install dashscope package.<br>    Returns:<br>        List[float]: The embedding result<br>    \"\"\"<br>    return get_multimodal_embedding(<br>        self.model_name,<br>        input=input,<br>        api_key=self._api_key,<br>        auto_truncation=auto_truncation,<br>    )<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Dashscope - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/dashscope/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/#contributing-to-llamaindex)\n\n# Contributing to LlamaIndex [\\#](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/\\#contributing-to-llamaindex \"Permanent link\")\n\nInterested in contributing to LlamaIndex? Here's how to get started!\n\n## QuickStart [\\#](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/\\#quickstart \"Permanent link\")\n\nFor python users who just want to dive in and start contributing, here's a quick guide on the env setup (if any of this doesn't make sense, read on to the [full guide](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/#development-guidelines)):\n\n1. Fork the repo and clone your fork\n2. `cd llama_index`\n3. Setup a new venv with `poetry shell`\n4. Install dev (and/or docs) dependencies with `poetry install --only dev,docs`\n5. Install the packages you intend to edit (i.e. `pip install -e  llama-index-core` or `pip install -e llama-index-integrations/llms/llama-index-llms-openai`)\n\n## Contribution Guideline [\\#](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/\\#contribution-guideline \"Permanent link\")\n\nThe best part of LlamaIndex is our community of users and contributors.\n\n### What should I work on? [\\#](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/\\#what-should-i-work-on \"Permanent link\")\n\n1. \ud83c\udd95 Extend core modules by contributing an integration\n2. \ud83d\udce6 Contribute a Tool, Reader, Pack, or Dataset (formerly from llama-hub)\n3. \ud83e\udde0 Add new capabilities to core\n4. \ud83d\udc1b Fix bugs\n5. \ud83c\udf89 Add usage examples\n6. \ud83e\uddea Add experimental features\n7. \ud83d\udcc4 Improve code quality & documentation\n\nAlso, join our Discord for ideas and discussions: [https://discord.gg/dGcwcsnxhU](https://discord.gg/dGcwcsnxhU).\n\n### 1\\. \ud83c\udd95 Extend Core Modules [\\#](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/\\#1-extend-core-modules \"Permanent link\")\n\nThe most impactful way to contribute to LlamaIndex is by extending our core modules:\n![LlamaIndex modules](https://github.com/run-llama/llama_index/raw/main/docs/docs/_static/contribution/contrib.png)\n\nWe welcome contributions in _all_ modules shown above.\nSo far, we have implemented a core set of functionalities for each, all of\nwhich are encapsulated in the LlamaIndex core package. As a contributor,\nyou can help each module unlock its full potential. Provided below are\nbrief description of these modules. You can also refer to their respective\nfolders within this Github repository for some example integrations.\n\nContributing an integration involves submitting the source code for a new Python\npackage. For now, these integrations will live in the LlamaIndex Github repository\nand the team will be responsible for publishing the package to PyPi. (Having\nthese packages live outside of this repository and maintained by our community\nmembers is in consideration.)\n\n#### Creating A New Integration Package [\\#](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/\\#creating-a-new-integration-package \"Permanent link\")\n\nBoth `llama-index` and `llama-index-core` come equipped\nwith a command-line tool that can be used to initialize a new integration package.\n\n```\ncd ./llama-index-integrations/llms\nllamaindex-cli new-package --kind \"llms\" --name \"gemini\"\n\n```\n\nExecuting the above commands will create a new folder called `llama-index-llms-gemini`\nwithin the `llama-index-integrations/llms` directory.\n\nPlease ensure to add a detailed README for your new package as it will appear in\nboth [llamahub.ai](https://llamahub.ai) as well as the PyPi.org website.\nIn addition to preparing your source code and supplying a detailed README, we\nalso ask that you fill in some\nmetadata for your package to appear in [llamahub.ai](https://llamahub.ai) with the\ncorrect information. You do so by adding the required metadata under the `[tool.llamahub]`\nsection with your new package's `pyproject.toml`.\n\nBelow is the example of the metadata required for all of our integration packages. Please\nreplace the default author \"llama-index\" with your own Github user name.\n\n```\n[tool.llamahub]\ncontains_example = false\nimport_path = \"llama_index.llms.anthropic\"\n\n[tool.llamahub.class_authors]\nAnthropic = \"llama-index\"\n\n```\n\n( [source](https://github.com/run-llama/llama_index/blob/main/llama-index-integrations/llms/llama-index-llms-anthropic/pyproject.toml))\n\n#### Module Details [\\#](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/\\#module-details \"Permanent link\")\n\nBelow, we will describe what each module does, give a high-level idea of the interface, show existing implementations, and give some ideas for contribution.\n\n* * *\n\n#### Data Loaders [\\#](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/\\#data-loaders \"Permanent link\")\n\nA data loader ingests data of any format from anywhere into `Document` objects, which can then be parsed and indexed.\n\n**Interface**:\n\n- `load_data` takes arbitrary arguments as input (e.g. path to data), and outputs a sequence of `Document` objects.\n- `lazy_load_data` takes arbitrary arguments as input (e.g. path to data), and outputs an iterable object of `Document` objects. This is a lazy version of `load_data`, which is useful for large datasets.\n\n> **Note**: If only `lazy_load_data` is implemented, `load_data` will be delegated to it.\n\n**Examples**:\n\n- [Database Reader](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/readers/llama-index-readers-database)\n- [Jira Reader](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/readers/llama-index-readers-jira)\n- [MongoDB Reader](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/readers/llama-index-readers-mongodb)\n\nContributing a data loader is easy and super impactful for the community.\n\n**Ideas**\n\n- Want to load something but there's no LlamaHub data loader for it yet? Make a PR!\n\n* * *\n\n#### Node Parser [\\#](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/\\#node-parser \"Permanent link\")\n\nA node parser parses `Document` objects into `Node` objects (atomic units of data that LlamaIndex operates over, e.g., chunk of text, image, or table).\nIt is responsible for splitting text (via text splitters) and explicitly modeling the relationship between units of data (e.g. A is the source of B, C is a chunk after D).\n\n**Interface**: `get_nodes_from_documents` takes a sequence of `Document` objects as input, and outputs a sequence of `Node` objects.\n\n**Examples**:\n\n- [Hierarchical Node Parser](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/node_parser/relational/hierarchical.py)\n\nSee [the API reference](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/) for full details.\n\n**Ideas**:\n\n- Add new `Node` relationships to model hierarchical documents (e.g. play-act-scene, chapter-section-heading).\n\n* * *\n\n#### Text Splitters [\\#](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/\\#text-splitters \"Permanent link\")\n\nText splitter splits a long text `str` into smaller text `str` chunks with desired size and splitting \"strategy\" since LLMs have a limited context window size, and the quality of text chunk used as context impacts the quality of query results.\n\n**Interface**: `split_text` takes a `str` as input, and outputs a sequence of `str`\n\n**Examples**:\n\n- [Token Text Splitter](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/node_parser/text/token.py)\n- [Sentence Splitter](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/node_parser/text/sentence.py)\n- [Code Splitter](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/node_parser/text/code.py)\n\n* * *\n\n#### Document/Index/KV Stores [\\#](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/\\#documentindexkv-stores \"Permanent link\")\n\nUnder the hood, LlamaIndex also supports a swappable **storage layer** that allows you to customize Document Stores (where ingested documents (i.e., `Node` objects) are stored), and Index Stores (where index metadata are stored)\n\nWe have an underlying key-value abstraction backing the document/index stores.\nCurrently we support in-memory and MongoDB storage for these stores. Open to contributions!\n\nSee [the API reference](https://docs.llamaindex.ai/en/stable/api_reference/storage/kvstore/) for details.\n\n* * *\n\n#### Managed Index [\\#](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/\\#managed-index \"Permanent link\")\n\nA managed index is used to represent an index that's managed via an API, exposing API calls to index documents and query documents.\n\nFor example, we support the [VectaraIndex](https://github.com/run-llama/llama_index/tree/ca09272af000307762d301c99da46ddc70d3bfd2/llama_index/indices/managed/vectara).\nOpen to contributions!\n\nSee [Managed Index docs](https://docs.llamaindex.ai/en/stable/community/integrations/managed_indices/) for details.\n\n* * *\n\n#### Vector Stores [\\#](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/\\#vector-stores \"Permanent link\")\n\nOur vector store classes store embeddings and support lookup via similarity search.\nThese serve as the main data store and retrieval engine for our vector index.\n\n**Interface**:\n\n- `add` takes in a sequence of `NodeWithEmbeddings` and inserts the embeddings (and possibly the node contents & metadata) into the vector store.\n- `delete` removes entries given document IDs.\n- `query` retrieves top-k most similar entries given a query embedding.\n- `get_nodes` get nodes by ID or filters\n- `delete_nodes` delete nodes by ID or filters\n- `clear` clears an entire db of data\n\n**Examples**:\n\n- [Chroma](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/vector_stores/llama-index-vector-stores-chroma)\n- [Qdrant](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/vector_stores/llama-index-vector-stores-qdrant)\n- [Pinecone](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/vector_stores/llama-index-vector-stores-pinecone)\n- [Faiss](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/vector_stores/llama-index-vector-stores-faiss)\n\n**Ideas**:\n\n- See a vector database out there that we don't support yet? A vector store missing methods like `get_nodes` and `delete_nodes`? Make a PR!\n\nSee [reference](https://docs.llamaindex.ai/en/stable/api_reference/storage/vector_store/) for full details.\n\n* * *\n\n#### Retrievers [\\#](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/\\#retrievers \"Permanent link\")\n\nOur retriever classes are lightweight classes that implement a `retrieve` method.\nThey may take in an index class as input - by default, each of our indices\n(list, vector, keyword) has an associated retriever. The output is a set of\n`NodeWithScore` objects (a `Node` object with an extra `score` field).\n\nYou may also choose to implement your own retriever classes on top of your own\ndata if you wish.\n\n**Interface**:\n\n- `retrieve` takes in a `str` or `QueryBundle` as input, and outputs a list of `NodeWithScore` objects\n\n**Examples**:\n\n- [Vector Index Retriever](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/indices/vector_store/retrievers/retriever.py)\n- [Property Graph Index Retriever](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/indices/property_graph/retriever.py)\n- [Router Retriever](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/retrievers/router_retriever.py)\n\n**Ideas**:\n\n- Besides the \"default\" retrievers built on top of each index, what about fancier retrievers? E.g. retrievers that take in other retrievers as input? Or other types of data?\n\n* * *\n\n#### Query Engines [\\#](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/\\#query-engines \"Permanent link\")\n\nOur query engine classes are lightweight classes that implement a `query` method; the query returns a response type.\nFor instance, they may take in a retriever class as input; our `RetrieverQueryEngine`\ntakes in a `retriever` as input as well as a `BaseSynthesizer` class for response synthesis, and\nthe `query` method performs retrieval and synthesis before returning the final result.\nThey may take in other query engine classes as input too.\n\n**Interface**:\n\n- `query` takes in a `str` or `QueryBundle` as input, and outputs a `Response` object.\n\n**Examples**:\n\n- [Retriever Query Engine](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/query_engine/retriever_query_engine.py)\n- [Citation Query Engine](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/query_engine/citation_query_engine.py)\n\n* * *\n\n#### Query Transforms [\\#](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/\\#query-transforms \"Permanent link\")\n\nA query transform augments a raw query string with associated transformations to improve index querying.\nThis can interpreted as a pre-processing stage, before the core index query logic is executed.\n\n**Interface**: `run` takes in a `str` or `Querybundle` as input, and outputs a transformed `QueryBundle`.\n\n**Examples**:\n\n- [Hypothetical Document Embeddings](https://github.com/run-llama/llama_index/blob/e490158e1562c903d99a7fb4a3cb4407b192d63a/llama-index-core/llama_index/core/indices/query/query_transform/base.py#L109)\n- [Query Decompose](https://github.com/run-llama/llama_index/blob/e490158e1562c903d99a7fb4a3cb4407b192d63a/llama-index-core/llama_index/core/indices/query/query_transform/base.py#L165)\n\nSee [guide](https://docs.llamaindex.ai/en/stable/examples/query_transformations/query_transform_cookbook/?h=query+transform) for more information.\n\n* * *\n\n#### Node Postprocessors [\\#](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/\\#node-postprocessors \"Permanent link\")\n\nA node postprocessor refines a list of retrieved nodes given configuration and context.\n\n**Interface**: `postprocess_nodes` takes a list of `Nodes` and extra metadata (e.g. similarity and query), and outputs a refined list of `Nodes`.\n\n**Examples**:\n\n- [Keyword Postprocessor](https://github.com/run-llama/llama_index/blob/e490158e1562c903d99a7fb4a3cb4407b192d63a/llama-index-core/llama_index/core/postprocessor/node.py#L20): filters nodes based on keyword match\n- [Colbert Rerank Postprocessor](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/postprocessor/llama-index-postprocessor-colbert-rerank/llama_index/postprocessor/colbert_rerank): reranks retrieved nodes.\n- [Presidio Postprocessor](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/postprocessor/llama-index-postprocessor-presidio): provides some data privacy on retrieved nodes by omitting personal information.\n\n* * *\n\n#### Output Parsers [\\#](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/\\#output-parsers \"Permanent link\")\n\nAn output parser enables us to extract structured output from the plain text output generated by the LLM.\n\n**Interface**:\n\n- `format`: formats a query `str` with structured output formatting instructions, and outputs the formatted `str`\n- `parse`: takes a `str` (from LLM response) as input, and gives a parsed structured output (optionally also validated, error-corrected).\n\n**Examples**:\n\n- [Guardrails Output Parser](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/output_parsers/llama-index-output-parsers-guardrails)\n- [Langchain Output Parser](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/output_parsers/llama-index-output-parsers-langchain)\n\nSee [guide](https://docs.llamaindex.ai/en/stable/module_guides/querying/structured_outputs/) for more information.\n\n* * *\n\n### 2\\. \ud83d\udce6 Contribute a Pack, Reader, Tool, or Dataset (formerly from llama-hub) [\\#](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/\\#2-contribute-a-pack-reader-tool-or-dataset-formerly-from-llama-hub \"Permanent link\")\n\nContributing a new Reader or Tool involves submitting a new package within\nthe [llama-index-integrations/readers](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/readers) and [llama-index-integrations/tools](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/tools),\nfolders respectively.\n\nThe LlamaIndex command-line tool can be used to initialize new Packs and Integrations. (NOTE: `llama-index-cli` comes installed with `llama-index`.)\n\n```\ncd ./llama-index-packs\nllamaindex-cli new-package --kind \"packs\" --name \"my new pack\"\n\ncd ./llama-index-integrations/readers\nllamaindex-cli new-package --kind \"readers\" --name \"new reader\"\n\n```\n\nExecuting the first set of shell commands will create a new folder called `llama-index-packs-my-new-pack`\nwithin the `llama-index-packs` directory. While the second set will create a new\npackage directory called `llama-index-readers-new-reader` within the `llama-index-integrations/readers` directory.\n\nPlease ensure to add a detailed README for your new package as it will appear in\nboth [llamahub.ai](https://llamahub.ai) as well as the PyPi.org website.\nIn addition to preparing your source code and supplying a detailed README, we\nalso ask that you fill in some\nmetadata for your package to appear in [llamahub.ai](https://llamahub.ai) with the\ncorrect information. You do so by adding the required metadata under the `[tool.llamahub]`\nsection with your new package's `pyproject.toml`.\n\nBelow is the example of the metadata required for packs, readers and tools:\n\n```\n[tool.llamahub]\ncontains_example = true\nimport_path = \"llama_index.packs.agent_search_retriever\"\n\n[tool.llamahub.class_authors]\nAgentSearchRetrieverPack = \"logan-markewich\"\n\n```\n\n( [source](https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-agent-search-retriever/pyproject.toml))\n\n### 3\\. \ud83e\udde0 Add new capabilities to core [\\#](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/\\#3-add-new-capabilities-to-core \"Permanent link\")\n\nWe would greatly appreciate any and all contributions to our core abstractions\nthat represent enhancements from the current set of capabilities.\nGeneral improvements that make these core abstractions more robust and thus\neasier to build on are also welcome!\n\n### 4\\. \ud83d\udc1b Fix Bugs [\\#](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/\\#4-fix-bugs \"Permanent link\")\n\nMost bugs are reported and tracked in the [Github Issues Page](https://github.com/run-llama/llama_index/issues).\nWe try our best in triaging and tagging these issues:\n\n- Issues tagged as `bug` are confirmed bugs.\n- New contributors may want to start with issues tagged with `good first issue`.\n\nPlease feel free to open an issue and/or assign an issue to yourself.\n\n### 5\\. \ud83c\udf89 Add Usage Examples [\\#](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/\\#5-add-usage-examples \"Permanent link\")\n\nIf you have applied LlamaIndex to a unique use-case (e.g. interesting dataset, customized index structure, complex query), we would love your contribution in the form of:\n\n1. a guide: e.g. [Guide to LlamIndex + Structured Data](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/structured_data/)\n2. an example notebook: e.g. [Email Info Extraction](https://docs.llamaindex.ai/en/stable/examples/usecases/email_data_extraction/)\n\n### 6\\. \ud83e\uddea Add Experimental Features [\\#](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/\\#6-add-experimental-features \"Permanent link\")\n\nIf you have a crazy idea, make a PR for it!\nWhether if it's the latest research, or what you thought of in the shower, we'd love to see creative ways to improve LlamaIndex.\n\n### 7\\. \ud83d\udcc4 Improve Code Quality & Documentation [\\#](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/\\#7-improve-code-quality-documentation \"Permanent link\")\n\nWe would love your help in making the project cleaner, more robust, and more understandable. If you find something confusing, it most likely is for other people as well. Help us be better!\n\n## Development Guidelines [\\#](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/\\#development-guidelines \"Permanent link\")\n\n### Repo Structure [\\#](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/\\#repo-structure \"Permanent link\")\n\nThe `llama_index` repo is structured as a mono-repo of many packages. For example, `llama-index-core/`, `llama-index-integrations/llms/llama-index-llms-openai`, and `llama-index-integrations/embeddings/llama-index-embeddings-openai` are all separate python packages. This organization should hopefully direct you to where you want to make a change or add a new modules.\n\n### Setting up environment [\\#](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/\\#setting-up-environment \"Permanent link\")\n\nLlamaIndex is a Python package. We've tested primarily with Python versions >= 3.8. Here's a quick\nand dirty guide to setting up your environment for local development.\n\n1. Fork [LlamaIndex Github repo](https://github.com/run-llama/llama_index/)\\\\* and clone it locally. (New to GitHub / git? Here's [how](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/fork-a-repo).)\n2. In a terminal, `cd` into the directory of your local clone of your forked repo.\n3. Install [pre-commit hooks](https://pre-commit.com/)\\\\* by running `pre-commit install`. These hooks are small house-keeping scripts executed every time you make a git commit, which automates away a lot of chores.\n4. Prepare a [virtual environment](https://python-poetry.org/docs/managing-environments/).\n5. [Install Poetry](https://python-poetry.org/docs/#installation)\\*. This will help you manage package dependencies.\n6. Execute `poetry shell`. This command will create a [virtual environment](https://python-poetry.org/docs/managing-environments/) specific for this package, which keeps installed packages contained to this project. (New to Poetry, the dependency & packaging manager for Python? Read about its basic usage [here](https://python-poetry.org/docs/basic-usage/).)\n7. Execute `poetry install --only dev,docs`\\*. This will install all dependencies needed for local development. To see what will be installed, read the `pyproject.toml` under that directory.\n8. `cd` into the specific package you want to work on. For example, if I want to work on the core package, I execute `cd llama-index-core/`. (New to terminal / command line? Here's a [getting started guide](https://www.freecodecamp.org/news/command-line-for-beginners/).)\n9. Install that specific integration with `pip install -e .` (or alternatively, `pip install -e <path to package>`). This will install the package in editable mode, which means any changes you make to that package will show up when you run your code again. **NOTE:** If working in a notebook, you will need to restart it for changes to packages to show up.\n\nSteps marked with an asterisk ( `*`) are one-time tasks. You don't have to repeat them when you attempt to contribute on something else next time.\n\nNow you should be set!\n\n### Validating your Change [\\#](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/\\#validating-your-change \"Permanent link\")\n\nLet's make sure to `format/lint` our change. For bigger changes,\nlet's also make sure to `test` it and perhaps create an `example notebook`.\n\n#### Formatting/Linting [\\#](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/\\#formattinglinting \"Permanent link\")\n\nWe run an assortment of linters: `black`, `ruff`, `mypy`.\n\nIf you have installed pre-commit hooks in this repo, they should have taken care of the formatting and linting automatically.\n\nIf -- for whatever reason -- you would like to do it manually, you can format and lint your changes with the following commands in the root directory:\n\n```\nmake format; make lint\n\n```\n\nUnder the hood, we still install pre-commit hooks for you, so that you don't have to do this manually next time.\n\n#### Testing [\\#](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/\\#testing \"Permanent link\")\n\nIf you modified or added code logic, **create test(s)**, because they help preventing other maintainers from accidentally breaking the nice things you added / re-introducing the bugs you fixed.\n\n- In almost all cases, add **unit tests**.\n- If your change involves adding a new integration, also add **integration tests**. When doing so, please [mock away](https://pytest-mock.readthedocs.io/en/latest/) the remote system that you're integrating LlamaIndex with, so that when the remote system changes, LlamaIndex developers won't see test failures.\n\nReciprocally, you should **run existing tests** (from every package that you touched) before making a git commit, so that you can be sure you didn't break someone else's good work.\n\n(By the way, when a test is run with the goal of detecting whether something broke in a new version of the codebase, it's referred to as a \" [regression test](https://www.browserstack.com/guide/regression-testing)\". You'll also hear people say \"the test _regressed_\" as a more diplomatic way of saying \"the test _failed_\".)\n\nOur tests are stored in the `tests` folders under each package directory. We use the testing framework [pytest](https://docs.pytest.org/), so you can **just run `pytest` in each package you touched** to run all its tests.\n\nRegardless of whether you have run them locally, a [CI system](https://www.atlassian.com/continuous-delivery/continuous-integration) will run all affected tests on your PR when you submit one anyway. There, tests are orchestrated with [Pants](https://www.pantsbuild.org/), the build system of our choice. There is a slight chance that tests broke on CI didn't break on your local machine or the other way around. When that happens, please take our CI as the source of truth. This is because our release pipeline (which builds the packages users are going to download from PyPI) are run in the CI, not on your machine (even if you volunteer), so it's the CI that is the golden standard.\n\n### Creating an Example Notebook [\\#](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/\\#creating-an-example-notebook \"Permanent link\")\n\nFor changes that involve entirely new features, it may be worth adding an example Jupyter notebook to showcase\nthis feature.\n\nExample notebooks can be found in [this folder](https://github.com/run-llama/llama_index/tree/main/docs/docs/examples).\n\n### Creating a pull request [\\#](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/\\#creating-a-pull-request \"Permanent link\")\n\nSee [these instructions](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request-from-a-fork)\nto open a pull request against the main LlamaIndex repo.\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Code - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/CONTRIBUTING/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/#welcome-to-llamaindex)\n\n# Welcome to LlamaIndex \ud83e\udd99 ! [\\#](https://docs.llamaindex.ai/en/stable/\\#welcome-to-llamaindex \"Permanent link\")\n\nLlamaIndex is a framework for building context-augmented generative AI applications with [LLMs](https://en.wikipedia.org/wiki/Large_language_model) including [agents](https://docs.llamaindex.ai/en/stable/understanding/agent/basic_agent/) and [workflows](https://docs.llamaindex.ai/en/stable/understanding/workflows/).\n\n- [Introduction](https://docs.llamaindex.ai/en/stable/#introduction)\n\nWhat is context augmentation? What are agents and workflows? How does LlamaIndex help build them?\n\n- [Use cases](https://docs.llamaindex.ai/en/stable/#use-cases)\n\nWhat kind of apps can you build with LlamaIndex? Who should use it?\n\n- [Getting started](https://docs.llamaindex.ai/en/stable/#getting-started)\n\nGet started in Python or TypeScript in just 5 lines of code!\n\n- [LlamaCloud](https://docs.llamaindex.ai/en/stable/#llamacloud)\n\nManaged services for LlamaIndex including [LlamaParse](https://docs.cloud.llamaindex.ai/llamaparse/getting_started), the world's best document parser.\n\n- [Community](https://docs.llamaindex.ai/en/stable/#community)\n\nGet help and meet collaborators on Discord, Twitter, LinkedIn, and learn how to contribute to the project.\n\n- [Related projects](https://docs.llamaindex.ai/en/stable/#related-projects)\n\nCheck out our library of connectors, readers, and other integrations at [LlamaHub](https://llamahub.ai) as well as demos and starter apps like [create-llama](https://www.npmjs.com/package/create-llama).\n\n\n## Introduction [\\#](https://docs.llamaindex.ai/en/stable/\\#introduction \"Permanent link\")\n\n### What is context augmentation? [\\#](https://docs.llamaindex.ai/en/stable/\\#what-is-context-augmentation \"Permanent link\")\n\nLLMs offer a natural language interface between humans and data. LLMs come pre-trained on huge amounts of publicly available data, but they are not trained on **your** data. Your data may be private or specific to the problem you're trying to solve. It's behind APIs, in SQL databases, or trapped in PDFs and slide decks.\n\nContext augmentation makes your data available to the LLM to solve the problem at hand. LlamaIndex provides the tools to build any of context-augmentation use case, from prototype to production. Our tools allow you to ingest, parse, index and process your data and quickly implement complex query workflows combining data access with LLM prompting.\n\nThe most popular example of context-augmentation is [Retrieval-Augmented Generation or RAG](https://docs.llamaindex.ai/en/stable/getting_started/concepts/), which combines context with LLMs at inference time.\n\n### What are agents? [\\#](https://docs.llamaindex.ai/en/stable/\\#what-are-agents \"Permanent link\")\n\n[Agents](https://docs.llamaindex.ai/en/stable/understanding/agent/basic_agent/) are LLM-powered knowledge assistants that use tools to perform tasks like research, data extraction, and more. Agents range from simple question-answering to being able to sense, decide and take actions in order to complete tasks.\n\nLlamaIndex provides a framework for building agents including the ability to use RAG pipelines as one of many tools to complete a task.\n\n### What are workflows? [\\#](https://docs.llamaindex.ai/en/stable/\\#what-are-workflows \"Permanent link\")\n\n[Workflows](https://docs.llamaindex.ai/en/stable/understanding/workflows/) are multi-step processes that combine one or more agents, data connectors, and other tools to complete a task. They are event-driven software that allows you to combine RAG data sources and multiple agents to create a complex application that can perform a wide variety of tasks with reflection, error-correction, and other hallmarks of advanced LLM applications.\n\n### LlamaIndex is the framework for Context-Augmented LLM Applications [\\#](https://docs.llamaindex.ai/en/stable/\\#llamaindex-is-the-framework-for-context-augmented-llm-applications \"Permanent link\")\n\nLlamaIndex imposes no restriction on how you use LLMs. You can use LLMs as auto-complete, chatbots, agents, and more. It just makes using them easier. We provide tools like:\n\n- **Data connectors** ingest your existing data from their native source and format. These could be APIs, PDFs, SQL, and (much) more.\n- **Data indexes** structure your data in intermediate representations that are easy and performant for LLMs to consume.\n- **Engines** provide natural language access to your data. For example:\n  - Query engines are powerful interfaces for question-answering (e.g. a RAG flow).\n  - Chat engines are conversational interfaces for multi-message, \"back and forth\" interactions with your data.\n- **Agents** are LLM-powered knowledge workers augmented by tools, from simple helper functions to API integrations and more.\n- **Observability/Evaluation** integrations that enable you to rigorously experiment, evaluate, and monitor your app in a virtuous cycle.\n- **Workflows** allow you to combine all of the above into an event-driven system for flexible than other, graph-based approaches.\n\n## Use cases [\\#](https://docs.llamaindex.ai/en/stable/\\#use-cases \"Permanent link\")\n\nSome popular use cases for LlamaIndex and context augmentation in general include:\n\n- [Question-Answering](https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/) (Retrieval-Augmented Generation aka RAG)\n- [Chatbots](https://docs.llamaindex.ai/en/stable/use_cases/chatbots/)\n- [Document Understanding and Data Extraction](https://docs.llamaindex.ai/en/stable/use_cases/extraction/)\n- [Autonomous Agents](https://docs.llamaindex.ai/en/stable/use_cases/agents/) that can perform research and take actions\n- [Multi-modal applications](https://docs.llamaindex.ai/en/stable/use_cases/multimodal/) that combine text, images, and other data types\n- [Fine-tuning](https://docs.llamaindex.ai/en/stable/use_cases/fine_tuning/) models on data to improve performance\n\nCheck out our [use cases](https://docs.llamaindex.ai/en/stable/use_cases/) documentation for more examples and links to tutorials.\n\n### \ud83d\udc68\u200d\ud83d\udc69\u200d\ud83d\udc67\u200d\ud83d\udc66 Who is LlamaIndex for? [\\#](https://docs.llamaindex.ai/en/stable/\\#who-is-llamaindex-for \"Permanent link\")\n\nLlamaIndex provides tools for beginners, advanced users, and everyone in between.\n\nOur high-level API allows beginner users to use LlamaIndex to ingest and query their data in 5 lines of code.\n\nFor more complex applications, our lower-level APIs allow advanced users to customize and extend any module -- data connectors, indices, retrievers, query engines, and reranking modules -- to fit their needs.\n\n## Getting Started [\\#](https://docs.llamaindex.ai/en/stable/\\#getting-started \"Permanent link\")\n\nLlamaIndex is available in Python (these docs) and [Typescript](https://ts.llamaindex.ai/). If you're not sure where to start, we recommend reading [how to read these docs](https://docs.llamaindex.ai/en/stable/getting_started/reading/) which will point you to the right place based on your experience level.\n\n### 30 second quickstart [\\#](https://docs.llamaindex.ai/en/stable/\\#30-second-quickstart \"Permanent link\")\n\nSet an environment variable called `OPENAI_API_KEY` with an [OpenAI API key](https://platform.openai.com/api-keys). Install the Python library:\n\n```\npip install llama-index\n\n```\n\nPut some documents in a folder called `data`, then ask questions about them with our famous 5-line starter:\n\n```\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"data\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"Some question about the data should go here\")\nprint(response)\n\n```\n\nIf any part of this trips you up, don't worry! Check out our more comprehensive starter tutorials using [remote APIs like OpenAI](https://docs.llamaindex.ai/en/stable/getting_started/starter_example/) or [any model that runs on your laptop](https://docs.llamaindex.ai/en/stable/getting_started/starter_example_local/).\n\n## LlamaCloud [\\#](https://docs.llamaindex.ai/en/stable/\\#llamacloud \"Permanent link\")\n\nIf you're an enterprise developer, check out [**LlamaCloud**](https://llamaindex.ai/enterprise). It is an end-to-end managed service for data parsing, ingestion, indexing, and retrieval, allowing you to get production-quality data for your production LLM application. It's available both hosted on our servers or as a self-hosted solution.\n\n### LlamaParse [\\#](https://docs.llamaindex.ai/en/stable/\\#llamaparse \"Permanent link\")\n\nLlamaParse is our state-of-the-art document parsing solution. It's available as part of LlamaCloud and also available as a self-serve API. You can [sign up](https://cloud.llamaindex.ai/) and parse up to 1000 pages/day for free, or enter a credit card for unlimited parsing. [Learn more](https://llamaindex.ai/enterprise).\n\n## Community [\\#](https://docs.llamaindex.ai/en/stable/\\#community \"Permanent link\")\n\nNeed help? Have a feature suggestion? Join the LlamaIndex community:\n\n- [Twitter](https://twitter.com/llama_index)\n- [Discord](https://discord.gg/dGcwcsnxhU)\n- [LinkedIn](https://www.linkedin.com/company/llamaindex/)\n\n### Getting the library [\\#](https://docs.llamaindex.ai/en/stable/\\#getting-the-library \"Permanent link\")\n\n- LlamaIndex Python\n  - [LlamaIndex Python Github](https://github.com/run-llama/llama_index)\n  - [Python Docs](https://docs.llamaindex.ai/) (what you're reading now)\n  - [LlamaIndex on PyPi](https://pypi.org/project/llama-index/)\n- LlamaIndex.TS (Typescript/Javascript package):\n  - [LlamaIndex.TS Github](https://github.com/run-llama/LlamaIndexTS)\n  - [TypeScript Docs](https://ts.llamaindex.ai/)\n  - [LlamaIndex.TS on npm](https://www.npmjs.com/package/llamaindex)\n\n### Contributing [\\#](https://docs.llamaindex.ai/en/stable/\\#contributing \"Permanent link\")\n\nWe are open-source and always welcome contributions to the project! Check out our [contributing guide](https://docs.llamaindex.ai/en/stable/CONTRIBUTING/) for full details on how to extend the core library or add an integration to a third party like an LLM, a vector store, an agent tool and more.\n\n## Related projects [\\#](https://docs.llamaindex.ai/en/stable/\\#related-projects \"Permanent link\")\n\nThere's more to the LlamaIndex universe! Check out some of our other projects:\n\n- [LlamaHub](https://llamahub.ai) \\| A large (and growing!) collection of custom data connectors\n- [SEC Insights](https://secinsights.ai) \\| A LlamaIndex-powered application for financial research\n- [create-llama](https://www.npmjs.com/package/create-llama) \\| A CLI tool to quickly scaffold LlamaIndex projects\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "LlamaIndex - LlamaIndex",
        "sitemap": {
          "lastmod": "2024-09-07",
          "changefreq": "daily"
        },
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/DOCS_README/#documentation-guide)\n\n# Documentation Guide [\\#](https://docs.llamaindex.ai/en/stable/DOCS_README/\\#documentation-guide \"Permanent link\")\n\n## A guide for docs contributors [\\#](https://docs.llamaindex.ai/en/stable/DOCS_README/\\#a-guide-for-docs-contributors \"Permanent link\")\n\nThe `docs` directory contains the sphinx source text for LlamaIndex docs, visit\nhttps://docs.llamaindex.ai/en/stable/ to read the full documentation.\n\nThis guide is made for anyone who's interested in running LlamaIndex documentation locally,\nmaking changes to it and making contributions. LlamaIndex is made by the thriving community\nbehind it, and you're always welcome to make contributions to the project and the\ndocumentation.\n\n## Build Docs [\\#](https://docs.llamaindex.ai/en/stable/DOCS_README/\\#build-docs \"Permanent link\")\n\nIf you haven't already, clone the LlamaIndex Github repo to a local directory:\n\n```\ngit clone https://github.com/run-llama/llama_index.git && cd llama_index\n\n```\n\nInstall all dependencies required for building docs (mainly `mkdocs` and its extension):\n\n- [Install poetry](https://python-poetry.org/docs/#installation) \\- this will help you manage package dependencies\n- `poetry shell` \\- this command creates a virtual environment, which keeps installed packages contained to this project\n- `poetry install --only docs` \\- this will install all dependencies needed for building docs\n\nBuild with mkdocs:\n\n```\ncd docs\nmkdocs serve --dirty\n\n```\n\n**NOTE:** The `--dirty` option will mean that only changed files will be re-built, decreasing the time it takes to iterate on a page.\n\nAnd open your browser at http://localhost:8000/ to view the generated docs.\n\nThis hosted version will re-build and update as changes are made to the docs.\n\n## Config [\\#](https://docs.llamaindex.ai/en/stable/DOCS_README/\\#config \"Permanent link\")\n\nAll config for mkdocs is in the `mkdocs.yml` file.\n\nRunning the command `python docs/prepare_for_build.py` from the root of the llama-index repo will update the mkdocs.yml API Reference and examples nav with the latest changes, as well as writing new api reference files.\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Docs - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/DOCS_README/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/honeyhive/#llama_index.callbacks.honeyhive.honeyhive_callback_handler)\n\n# Honeyhive\n\n## honeyhive\\_callback\\_handler [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/honeyhive/\\#llama_index.callbacks.honeyhive.honeyhive_callback_handler \"Permanent link\")\n\n```\nhoneyhive_callback_handler(**kwargs: Any) -> BaseCallbackHandler\n\n```\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-honeyhive/llama_index/callbacks/honeyhive/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>8<br>9<br>``` | ```<br>def honeyhive_callback_handler(**kwargs: Any) -> BaseCallbackHandler:<br>    return HoneyHiveLlamaIndexTracer(**kwargs)<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Honeyhive - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/honeyhive/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/deepeval/#llama_index.callbacks.deepeval.deepeval_callback_handler)\n\n# Deepeval\n\n## deepeval\\_callback\\_handler [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/deepeval/\\#llama_index.callbacks.deepeval.deepeval_callback_handler \"Permanent link\")\n\n```\ndeepeval_callback_handler(**eval_params: Any) -> BaseCallbackHandler\n\n```\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-deepeval/llama_index/callbacks/deepeval/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>8<br>9<br>``` | ```<br>def deepeval_callback_handler(**eval_params: Any) -> BaseCallbackHandler:<br>    return LlamaIndexCallbackHandler(**eval_params)<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Deepeval - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/deepeval/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/azure_inference/#llama_index.embeddings.azure_inference.AzureAIEmbeddingsModel)\n\n# Azure inference\n\n## AzureAIEmbeddingsModel [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/azure_inference/\\#llama_index.embeddings.azure_inference.AzureAIEmbeddingsModel \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nAzure AI model inference for embeddings.\n\n**Examples:**\n\n```\nfrom llama_index.core import Settings\nfrom llama_index.embeddings.azure_inference import AzureAIEmbeddingsModel\n\nllm = AzureAIEmbeddingsModel(\n    endpoint=\"https://[your-endpoint].inference.ai.azure.com\",\n    credential=\"your-api-key\",\n)\n\n# # If using Microsoft Entra ID authentication, you can create the\n# # client as follows\n#\n# from azure.identity import DefaultAzureCredential\n#\n# embed_model = AzureAIEmbeddingsModel(\n#     endpoint=\"https://[your-endpoint].inference.ai.azure.com\",\n#     credential=DefaultAzureCredential()\n# )\n#\n# # If you plan to use asynchronous calling, make sure to use the async\n# # credentials as follows\n#\n# from azure.identity.aio import DefaultAzureCredential as DefaultAzureCredentialAsync\n#\n# embed_model = AzureAIEmbeddingsModel(\n#     endpoint=\"https://[your-endpoint].inference.ai.azure.com\",\n#     credential=DefaultAzureCredentialAsync()\n# )\n\n# Once the client is instantiated, you can set the context to use the model\nSettings.embed_model = embed_model\n\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\n\n```\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-azure-inference/llama_index/embeddings/azure_inference/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>``` | ````<br>class AzureAIEmbeddingsModel(BaseEmbedding):<br>    \"\"\"Azure AI model inference for embeddings.<br>    Examples:<br>        ```python<br>        from llama_index.core import Settings<br>        from llama_index.embeddings.azure_inference import AzureAIEmbeddingsModel<br>        llm = AzureAIEmbeddingsModel(<br>            endpoint=\"https://[your-endpoint].inference.ai.azure.com\",<br>            credential=\"your-api-key\",<br>        )<br>        # # If using Microsoft Entra ID authentication, you can create the<br>        # # client as follows<br>        #<br>        # from azure.identity import DefaultAzureCredential<br>        #<br>        # embed_model = AzureAIEmbeddingsModel(<br>        #     endpoint=\"https://[your-endpoint].inference.ai.azure.com\",<br>        #     credential=DefaultAzureCredential()<br>        # )<br>        #<br>        # # If you plan to use asynchronous calling, make sure to use the async<br>        # # credentials as follows<br>        #<br>        # from azure.identity.aio import DefaultAzureCredential as DefaultAzureCredentialAsync<br>        #<br>        # embed_model = AzureAIEmbeddingsModel(<br>        #     endpoint=\"https://[your-endpoint].inference.ai.azure.com\",<br>        #     credential=DefaultAzureCredentialAsync()<br>        # )<br>        # Once the client is instantiated, you can set the context to use the model<br>        Settings.embed_model = embed_model<br>        documents = SimpleDirectoryReader(\"./data\").load_data()<br>        index = VectorStoreIndex.from_documents(documents)<br>        ```<br>    \"\"\"<br>    model_kwargs: Dict[str, Any] = Field(<br>        default_factory=dict, description=\"Additional kwargs model parameters.\"<br>    )<br>    _client: EmbeddingsClient = PrivateAttr()<br>    _async_client: EmbeddingsClientAsync = PrivateAttr()<br>    def __init__(<br>        self,<br>        endpoint: str = None,<br>        credential: Union[str, AzureKeyCredential, \"TokenCredential\"] = None,<br>        model_name: str = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>        num_workers: Optional[int] = None,<br>        client_kwargs: Optional[Dict[str, Any]] = None,<br>        **kwargs: Any<br>    ):<br>        client_kwargs = client_kwargs or {}<br>        endpoint = get_from_param_or_env(<br>            \"endpoint\", endpoint, \"AZURE_INFERENCE_ENDPOINT\", None<br>        )<br>        credential = get_from_param_or_env(<br>            \"credential\", credential, \"AZURE_INFERENCE_CREDENTIAL\", None<br>        )<br>        credential = (<br>            AzureKeyCredential(credential)<br>            if isinstance(credential, str)<br>            else credential<br>        )<br>        if not endpoint:<br>            raise ValueError(<br>                \"You must provide an endpoint to use the Azure AI model inference LLM.\"<br>                \"Pass the endpoint as a parameter or set the AZURE_INFERENCE_ENDPOINT\"<br>                \"environment variable.\"<br>            )<br>        if not credential:<br>            raise ValueError(<br>                \"You must provide an credential to use the Azure AI model inference LLM.\"<br>                \"Pass the credential as a parameter or set the AZURE_INFERENCE_CREDENTIAL\"<br>            )<br>        super().__init__(<br>            model_name=model_name or \"unknown\",<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            num_workers=num_workers,<br>            **kwargs,<br>        )<br>        self._client = EmbeddingsClient(<br>            endpoint=endpoint,<br>            credential=credential,<br>            user_agent=\"llamaindex\",<br>            **client_kwargs,<br>        )<br>        self._async_client = EmbeddingsClientAsync(<br>            endpoint=endpoint,<br>            credential=credential,<br>            user_agent=\"llamaindex\",<br>            **client_kwargs,<br>        )<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"AzureAIEmbeddingsModel\"<br>    @property<br>    def _model_kwargs(self) -> Dict[str, Any]:<br>        additional_kwargs = {}<br>        if self.model_name and self.model_name != \"unknown\":<br>            additional_kwargs[\"model\"] = self.model_name<br>        if self.model_kwargs:<br>            # pass any extra model parameters<br>            additional_kwargs.update(self.model_kwargs)<br>        return additional_kwargs<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self._client.embed(input=[query], **self._model_kwargs).data[0].embedding<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        return (<br>            (await self._async_client.embed(input=[query], **self._model_kwargs))<br>            .data[0]<br>            .embedding<br>        )<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._client.embed(input=[text], **self._model_kwargs).data[0].embedding<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously get text embedding.\"\"\"<br>        return (<br>            (await self._async_client.embed(input=[text], **self._model_kwargs))<br>            .data[0]<br>            .embedding<br>        )<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        embedding_response = self._client.embed(input=texts, **self._model_kwargs).data<br>        return [embed.embedding for embed in embedding_response]<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Asynchronously get text embeddings.\"\"\"<br>        embedding_response = await self._async_client.embed(<br>            input=texts, **self._model_kwargs<br>        )<br>        return [embed.embedding for embed in embedding_response.data]<br>```` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Azure inference - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/azure_inference/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/uptrain/#llama_index.callbacks.uptrain.UpTrainCallbackHandler)\n\n# Uptrain\n\n## UpTrainCallbackHandler [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/uptrain/\\#llama_index.callbacks.uptrain.UpTrainCallbackHandler \"Permanent link\")\n\nBases: `BaseCallbackHandler`\n\nUpTrain callback handler.\n\nThis class is responsible for handling the UpTrain API and logging events to UpTrain.\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-uptrain/llama_index/callbacks/uptrain/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>``` | ```<br>class UpTrainCallbackHandler(BaseCallbackHandler):<br>    \"\"\"<br>    UpTrain callback handler.<br>    This class is responsible for handling the UpTrain API and logging events to UpTrain.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        api_key: str,<br>        key_type: Literal[\"uptrain\", \"openai\"],<br>        project_name: str = \"uptrain_llamaindex\",<br>    ) -> None:<br>        \"\"\"Initialize the UpTrain callback handler.\"\"\"<br>        try:<br>            from uptrain import APIClient, EvalLLM, Settings<br>        except ImportError:<br>            raise ImportError(<br>                \"UpTrainCallbackHandler requires the 'uptrain' package. \"<br>                \"Please install it using 'pip install uptrain'.\"<br>            )<br>        nest_asyncio.apply()<br>        super().__init__(<br>            event_starts_to_ignore=[],<br>            event_ends_to_ignore=[],<br>        )<br>        self.schema = UpTrainDataSchema(project_name=project_name)<br>        self._event_pairs_by_id: Dict[str, List[CBEvent]] = defaultdict(list)<br>        self._trace_map: Dict[str, List[str]] = defaultdict(list)<br>        # Based on whether the user enters an UpTrain API key or an OpenAI API key, the client is initialized<br>        # If both are entered, the UpTrain API key is used<br>        if key_type == \"uptrain\":<br>            settings = Settings(uptrain_access_token=api_key)<br>            self.uptrain_client = APIClient(settings=settings)<br>        elif key_type == \"openai\":<br>            settings = Settings(openai_api_key=api_key)<br>            self.uptrain_client = EvalLLM(settings=settings)<br>        else:<br>            raise ValueError(\"Invalid key type: Must be 'uptrain' or 'openai'\")<br>    def uptrain_evaluate(<br>        self,<br>        evaluation_name: str,<br>        data: List[Dict[str, str]],<br>        checks: List[str],<br>    ) -> None:<br>        \"\"\"Run an evaluation on the UpTrain server using UpTrain client.\"\"\"<br>        if self.uptrain_client.__class__.__name__ == \"APIClient\":<br>            uptrain_result = self.uptrain_client.log_and_evaluate(<br>                project_name=self.schema.project_name,<br>                evaluation_name=evaluation_name,<br>                data=data,<br>                checks=checks,<br>            )<br>        else:<br>            uptrain_result = self.uptrain_client.evaluate(<br>                project_name=self.schema.project_name,<br>                evaluation_name=evaluation_name,<br>                data=data,<br>                checks=checks,<br>            )<br>        self.schema.uptrain_results[self.schema.project_name].append(uptrain_result)<br>        score_name_map = {<br>            \"score_context_relevance\": \"Context Relevance Score\",<br>            \"score_factual_accuracy\": \"Factual Accuracy Score\",<br>            \"score_response_completeness\": \"Response Completeness Score\",<br>            \"score_sub_query_completeness\": \"Sub Query Completeness Score\",<br>            \"score_context_reranking\": \"Context Reranking Score\",<br>            \"score_context_conciseness\": \"Context Conciseness Score\",<br>        }<br>        # Print the results<br>        for row in uptrain_result:<br>            columns = list(row.keys())<br>            for column in columns:<br>                if column == \"question\":<br>                    print(f\"\\nQuestion: {row[column]}\")<br>                elif column == \"response\":<br>                    print(f\"Response: {row[column]}\\n\")<br>                elif column.startswith(\"score\"):<br>                    if column in score_name_map:<br>                        print(f\"{score_name_map[column]}: {row[column]}\")<br>                    else:<br>                        print(f\"{column}: {row[column]}\")<br>            print()<br>    def on_event_start(<br>        self,<br>        event_type: CBEventType,<br>        payload: Any = None,<br>        event_id: str = \"\",<br>        parent_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> str:<br>        \"\"\"Run when an event starts and return id of event.\"\"\"<br>        event = CBEvent(event_type, payload=payload, id_=event_id)<br>        self._event_pairs_by_id[event.id_].append(event)<br>        if event_type is CBEventType.QUERY:<br>            self.schema.question = payload[\"query_str\"]<br>        if event_type is CBEventType.TEMPLATING and \"template_vars\" in payload:<br>            template_vars = payload[\"template_vars\"]<br>            self.schema.context = template_vars.get(\"context_str\", \"\")<br>        elif event_type is CBEventType.RERANKING and \"nodes\" in payload:<br>            self.schema.eval_types.add(\"reranking\")<br>            # Store old context data<br>            self.schema.old_context = [node.text for node in payload[\"nodes\"]]<br>        elif event_type is CBEventType.SUB_QUESTION:<br>            # For the first sub question, store parent question and parent id<br>            if \"sub_question\" not in self.schema.eval_types:<br>                self.schema.parent_question = self.schema.question<br>                self.schema.eval_types.add(\"sub_question\")<br>            # Store sub question data - question and parent id<br>            self.schema.sub_question_parent_id = parent_id<br>        return event_id<br>    def on_event_end(<br>        self,<br>        event_type: CBEventType,<br>        payload: Any = None,<br>        event_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> None:<br>        \"\"\"Run when an event ends.\"\"\"<br>        try:<br>            from uptrain import Evals<br>        except ImportError:<br>            raise ImportError(<br>                \"UpTrainCallbackHandler requires the 'uptrain' package. \"<br>                \"Please install it using 'pip install uptrain'.\"<br>            )<br>        event = CBEvent(event_type, payload=payload, id_=event_id)<br>        self._event_pairs_by_id[event.id_].append(event)<br>        self._trace_map = defaultdict(list)<br>        if event_id == self.schema.sub_question_parent_id:<br>            # Perform individual evaluations for sub questions (but send all sub questions at once)<br>            self.uptrain_evaluate(<br>                evaluation_name=\"sub_question_answering\",<br>                data=list(self.schema.sub_question_map.values()),<br>                checks=[<br>                    Evals.CONTEXT_RELEVANCE,<br>                    Evals.FACTUAL_ACCURACY,<br>                    Evals.RESPONSE_COMPLETENESS,<br>                ],<br>            )<br>            # Perform evaluation for question and all sub questions (as a whole)<br>            sub_questions = [<br>                sub_question[\"question\"]<br>                for sub_question in self.schema.sub_question_map.values()<br>            ]<br>            sub_questions_formatted = \"\\n\".join(<br>                [<br>                    f\"{index}. {string}\"<br>                    for index, string in enumerate(sub_questions, start=1)<br>                ]<br>            )<br>            self.uptrain_evaluate(<br>                evaluation_name=\"sub_query_completeness\",<br>                data=[<br>                    {<br>                        \"question\": self.schema.parent_question,<br>                        \"sub_questions\": sub_questions_formatted,<br>                    }<br>                ],<br>                checks=[Evals.SUB_QUERY_COMPLETENESS],<br>            )<br>            self.schema.eval_types.remove(\"sub_question\")<br>        # Should not be called for sub questions<br>        if (<br>            event_type is CBEventType.SYNTHESIZE<br>            and \"sub_question\" not in self.schema.eval_types<br>        ):<br>            self.schema.response = payload[\"response\"].response<br>            # Perform evaluation for synthesization<br>            if \"reranking\" in self.schema.eval_types:<br>                if self.schema.reranking_type == \"rerank\":<br>                    evaluation_name = \"question_answering_rerank\"<br>                else:<br>                    evaluation_name = \"question_answering_resize\"<br>                self.schema.eval_types.remove(\"reranking\")<br>            else:<br>                evaluation_name = \"question_answering\"<br>            self.uptrain_evaluate(<br>                evaluation_name=evaluation_name,<br>                data=[<br>                    {<br>                        \"question\": self.schema.question,<br>                        \"context\": self.schema.context,<br>                        \"response\": self.schema.response,<br>                    }<br>                ],<br>                checks=[<br>                    Evals.CONTEXT_RELEVANCE,<br>                    Evals.FACTUAL_ACCURACY,<br>                    Evals.RESPONSE_COMPLETENESS,<br>                ],<br>            )<br>        elif event_type is CBEventType.RERANKING:<br>            # Store new context data<br>            self.schema.new_context = [node.text for node in payload[\"nodes\"]]<br>            if len(self.schema.old_context) == len(self.schema.new_context):<br>                self.schema.reranking_type = \"rerank\"<br>                context = \"\\n\".join(<br>                    [<br>                        f\"{index}. {string}\"<br>                        for index, string in enumerate(self.schema.old_context, start=1)<br>                    ]<br>                )<br>                reranked_context = \"\\n\".join(<br>                    [<br>                        f\"{index}. {string}\"<br>                        for index, string in enumerate(self.schema.new_context, start=1)<br>                    ]<br>                )<br>                # Perform evaluation for reranking<br>                self.uptrain_evaluate(<br>                    evaluation_name=\"context_reranking\",<br>                    data=[<br>                        {<br>                            \"question\": self.schema.question,<br>                            \"context\": context,<br>                            \"reranked_context\": reranked_context,<br>                        }<br>                    ],<br>                    checks=[<br>                        Evals.CONTEXT_RERANKING,<br>                    ],<br>                )<br>            else:<br>                self.schema.reranking_type = \"resize\"<br>                context = \"\\n\".join(self.schema.old_context)<br>                concise_context = \"\\n\".join(self.schema.new_context)<br>                # Perform evaluation for resizing<br>                self.uptrain_evaluate(<br>                    evaluation_name=\"context_conciseness\",<br>                    data=[<br>                        {<br>                            \"question\": self.schema.question,<br>                            \"context\": context,<br>                            \"concise_context\": concise_context,<br>                        }<br>                    ],<br>                    checks=[<br>                        Evals.CONTEXT_CONCISENESS,<br>                    ],<br>                )<br>        elif event_type is CBEventType.SUB_QUESTION:<br>            # Store sub question data<br>            self.schema.sub_question_map[event_id][\"question\"] = payload[<br>                \"sub_question\"<br>            ].sub_q.sub_question<br>            self.schema.sub_question_map[event_id][\"context\"] = (<br>                payload[\"sub_question\"].sources[0].node.text<br>            )<br>            self.schema.sub_question_map[event_id][\"response\"] = payload[<br>                \"sub_question\"<br>            ].answer<br>    def start_trace(self, trace_id: Optional[str] = None) -> None:<br>        self._trace_map = defaultdict(list)<br>        return super().start_trace(trace_id)<br>    def end_trace(<br>        self,<br>        trace_id: Optional[str] = None,<br>        trace_map: Optional[Dict[str, List[str]]] = None,<br>    ) -> None:<br>        self._trace_map = trace_map or defaultdict(list)<br>        return super().end_trace(trace_id, trace_map)<br>    def build_trace_map(<br>        self,<br>        cur_event_id: str,<br>        trace_map: Any,<br>    ) -> Dict[str, Any]:<br>        event_pair = self._event_pairs_by_id[cur_event_id]<br>        if event_pair:<br>            event_data = {<br>                \"event_type\": event_pair[0].event_type,<br>                \"event_id\": event_pair[0].id_,<br>                \"children\": {},<br>            }<br>            trace_map[cur_event_id] = event_data<br>        child_event_ids = self._trace_map[cur_event_id]<br>        for child_event_id in child_event_ids:<br>            self.build_trace_map(child_event_id, event_data[\"children\"])<br>        return trace_map<br>``` |\n\n### uptrain\\_evaluate [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/uptrain/\\#llama_index.callbacks.uptrain.UpTrainCallbackHandler.uptrain_evaluate \"Permanent link\")\n\n```\nuptrain_evaluate(evaluation_name: str, data: List[Dict[str, str]], checks: List[str]) -> None\n\n```\n\nRun an evaluation on the UpTrain server using UpTrain client.\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-uptrain/llama_index/callbacks/uptrain/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>``` | ```<br>def uptrain_evaluate(<br>    self,<br>    evaluation_name: str,<br>    data: List[Dict[str, str]],<br>    checks: List[str],<br>) -> None:<br>    \"\"\"Run an evaluation on the UpTrain server using UpTrain client.\"\"\"<br>    if self.uptrain_client.__class__.__name__ == \"APIClient\":<br>        uptrain_result = self.uptrain_client.log_and_evaluate(<br>            project_name=self.schema.project_name,<br>            evaluation_name=evaluation_name,<br>            data=data,<br>            checks=checks,<br>        )<br>    else:<br>        uptrain_result = self.uptrain_client.evaluate(<br>            project_name=self.schema.project_name,<br>            evaluation_name=evaluation_name,<br>            data=data,<br>            checks=checks,<br>        )<br>    self.schema.uptrain_results[self.schema.project_name].append(uptrain_result)<br>    score_name_map = {<br>        \"score_context_relevance\": \"Context Relevance Score\",<br>        \"score_factual_accuracy\": \"Factual Accuracy Score\",<br>        \"score_response_completeness\": \"Response Completeness Score\",<br>        \"score_sub_query_completeness\": \"Sub Query Completeness Score\",<br>        \"score_context_reranking\": \"Context Reranking Score\",<br>        \"score_context_conciseness\": \"Context Conciseness Score\",<br>    }<br>    # Print the results<br>    for row in uptrain_result:<br>        columns = list(row.keys())<br>        for column in columns:<br>            if column == \"question\":<br>                print(f\"\\nQuestion: {row[column]}\")<br>            elif column == \"response\":<br>                print(f\"Response: {row[column]}\\n\")<br>            elif column.startswith(\"score\"):<br>                if column in score_name_map:<br>                    print(f\"{score_name_map[column]}: {row[column]}\")<br>                else:<br>                    print(f\"{column}: {row[column]}\")<br>        print()<br>``` |\n\n### on\\_event\\_start [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/uptrain/\\#llama_index.callbacks.uptrain.UpTrainCallbackHandler.on_event_start \"Permanent link\")\n\n```\non_event_start(event_type: CBEventType, payload: Any = None, event_id: str = '', parent_id: str = '', **kwargs: Any) -> str\n\n```\n\nRun when an event starts and return id of event.\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-uptrain/llama_index/callbacks/uptrain/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>``` | ```<br>def on_event_start(<br>    self,<br>    event_type: CBEventType,<br>    payload: Any = None,<br>    event_id: str = \"\",<br>    parent_id: str = \"\",<br>    **kwargs: Any,<br>) -> str:<br>    \"\"\"Run when an event starts and return id of event.\"\"\"<br>    event = CBEvent(event_type, payload=payload, id_=event_id)<br>    self._event_pairs_by_id[event.id_].append(event)<br>    if event_type is CBEventType.QUERY:<br>        self.schema.question = payload[\"query_str\"]<br>    if event_type is CBEventType.TEMPLATING and \"template_vars\" in payload:<br>        template_vars = payload[\"template_vars\"]<br>        self.schema.context = template_vars.get(\"context_str\", \"\")<br>    elif event_type is CBEventType.RERANKING and \"nodes\" in payload:<br>        self.schema.eval_types.add(\"reranking\")<br>        # Store old context data<br>        self.schema.old_context = [node.text for node in payload[\"nodes\"]]<br>    elif event_type is CBEventType.SUB_QUESTION:<br>        # For the first sub question, store parent question and parent id<br>        if \"sub_question\" not in self.schema.eval_types:<br>            self.schema.parent_question = self.schema.question<br>            self.schema.eval_types.add(\"sub_question\")<br>        # Store sub question data - question and parent id<br>        self.schema.sub_question_parent_id = parent_id<br>    return event_id<br>``` |\n\n### on\\_event\\_end [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/uptrain/\\#llama_index.callbacks.uptrain.UpTrainCallbackHandler.on_event_end \"Permanent link\")\n\n```\non_event_end(event_type: CBEventType, payload: Any = None, event_id: str = '', **kwargs: Any) -> None\n\n```\n\nRun when an event ends.\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-uptrain/llama_index/callbacks/uptrain/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>``` | ```<br>def on_event_end(<br>    self,<br>    event_type: CBEventType,<br>    payload: Any = None,<br>    event_id: str = \"\",<br>    **kwargs: Any,<br>) -> None:<br>    \"\"\"Run when an event ends.\"\"\"<br>    try:<br>        from uptrain import Evals<br>    except ImportError:<br>        raise ImportError(<br>            \"UpTrainCallbackHandler requires the 'uptrain' package. \"<br>            \"Please install it using 'pip install uptrain'.\"<br>        )<br>    event = CBEvent(event_type, payload=payload, id_=event_id)<br>    self._event_pairs_by_id[event.id_].append(event)<br>    self._trace_map = defaultdict(list)<br>    if event_id == self.schema.sub_question_parent_id:<br>        # Perform individual evaluations for sub questions (but send all sub questions at once)<br>        self.uptrain_evaluate(<br>            evaluation_name=\"sub_question_answering\",<br>            data=list(self.schema.sub_question_map.values()),<br>            checks=[<br>                Evals.CONTEXT_RELEVANCE,<br>                Evals.FACTUAL_ACCURACY,<br>                Evals.RESPONSE_COMPLETENESS,<br>            ],<br>        )<br>        # Perform evaluation for question and all sub questions (as a whole)<br>        sub_questions = [<br>            sub_question[\"question\"]<br>            for sub_question in self.schema.sub_question_map.values()<br>        ]<br>        sub_questions_formatted = \"\\n\".join(<br>            [<br>                f\"{index}. {string}\"<br>                for index, string in enumerate(sub_questions, start=1)<br>            ]<br>        )<br>        self.uptrain_evaluate(<br>            evaluation_name=\"sub_query_completeness\",<br>            data=[<br>                {<br>                    \"question\": self.schema.parent_question,<br>                    \"sub_questions\": sub_questions_formatted,<br>                }<br>            ],<br>            checks=[Evals.SUB_QUERY_COMPLETENESS],<br>        )<br>        self.schema.eval_types.remove(\"sub_question\")<br>    # Should not be called for sub questions<br>    if (<br>        event_type is CBEventType.SYNTHESIZE<br>        and \"sub_question\" not in self.schema.eval_types<br>    ):<br>        self.schema.response = payload[\"response\"].response<br>        # Perform evaluation for synthesization<br>        if \"reranking\" in self.schema.eval_types:<br>            if self.schema.reranking_type == \"rerank\":<br>                evaluation_name = \"question_answering_rerank\"<br>            else:<br>                evaluation_name = \"question_answering_resize\"<br>            self.schema.eval_types.remove(\"reranking\")<br>        else:<br>            evaluation_name = \"question_answering\"<br>        self.uptrain_evaluate(<br>            evaluation_name=evaluation_name,<br>            data=[<br>                {<br>                    \"question\": self.schema.question,<br>                    \"context\": self.schema.context,<br>                    \"response\": self.schema.response,<br>                }<br>            ],<br>            checks=[<br>                Evals.CONTEXT_RELEVANCE,<br>                Evals.FACTUAL_ACCURACY,<br>                Evals.RESPONSE_COMPLETENESS,<br>            ],<br>        )<br>    elif event_type is CBEventType.RERANKING:<br>        # Store new context data<br>        self.schema.new_context = [node.text for node in payload[\"nodes\"]]<br>        if len(self.schema.old_context) == len(self.schema.new_context):<br>            self.schema.reranking_type = \"rerank\"<br>            context = \"\\n\".join(<br>                [<br>                    f\"{index}. {string}\"<br>                    for index, string in enumerate(self.schema.old_context, start=1)<br>                ]<br>            )<br>            reranked_context = \"\\n\".join(<br>                [<br>                    f\"{index}. {string}\"<br>                    for index, string in enumerate(self.schema.new_context, start=1)<br>                ]<br>            )<br>            # Perform evaluation for reranking<br>            self.uptrain_evaluate(<br>                evaluation_name=\"context_reranking\",<br>                data=[<br>                    {<br>                        \"question\": self.schema.question,<br>                        \"context\": context,<br>                        \"reranked_context\": reranked_context,<br>                    }<br>                ],<br>                checks=[<br>                    Evals.CONTEXT_RERANKING,<br>                ],<br>            )<br>        else:<br>            self.schema.reranking_type = \"resize\"<br>            context = \"\\n\".join(self.schema.old_context)<br>            concise_context = \"\\n\".join(self.schema.new_context)<br>            # Perform evaluation for resizing<br>            self.uptrain_evaluate(<br>                evaluation_name=\"context_conciseness\",<br>                data=[<br>                    {<br>                        \"question\": self.schema.question,<br>                        \"context\": context,<br>                        \"concise_context\": concise_context,<br>                    }<br>                ],<br>                checks=[<br>                    Evals.CONTEXT_CONCISENESS,<br>                ],<br>            )<br>    elif event_type is CBEventType.SUB_QUESTION:<br>        # Store sub question data<br>        self.schema.sub_question_map[event_id][\"question\"] = payload[<br>            \"sub_question\"<br>        ].sub_q.sub_question<br>        self.schema.sub_question_map[event_id][\"context\"] = (<br>            payload[\"sub_question\"].sources[0].node.text<br>        )<br>        self.schema.sub_question_map[event_id][\"response\"] = payload[<br>            \"sub_question\"<br>        ].answer<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Uptrain - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/uptrain/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/azure_openai/#llama_index.embeddings.azure_openai.AzureOpenAIEmbedding)\n\n# Azure openai\n\n## AzureOpenAIEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/azure_openai/\\#llama_index.embeddings.azure_openai.AzureOpenAIEmbedding \"Permanent link\")\n\nBases: `OpenAIEmbedding`\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-azure-openai/llama_index/embeddings/azure_openai/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>``` | ```<br>class AzureOpenAIEmbedding(OpenAIEmbedding):<br>    azure_endpoint: Optional[str] = Field(<br>        default=None, description=\"The Azure endpoint to use.\", validate_default=True<br>    )<br>    azure_deployment: Optional[str] = Field(<br>        default=None, description=\"The Azure deployment to use.\", validate_default=True<br>    )<br>    api_base: str = Field(<br>        default=\"\",<br>        description=\"The base URL for Azure deployment.\",<br>        validate_default=True,<br>    )<br>    api_version: str = Field(<br>        default=\"\",<br>        description=\"The version for Azure OpenAI API.\",<br>        validate_default=True,<br>    )<br>    azure_ad_token_provider: Optional[AnnotatedProvider] = Field(<br>        default=None,<br>        description=\"Callback function to provide Azure AD token.\",<br>        exclude=True,<br>    )<br>    use_azure_ad: bool = Field(<br>        description=\"Indicates if Microsoft Entra ID (former Azure AD) is used for token authentication\"<br>    )<br>    _azure_ad_token: Any = PrivateAttr(default=None)<br>    _client: AzureOpenAI = PrivateAttr()<br>    _aclient: AsyncAzureOpenAI = PrivateAttr()<br>    def __init__(<br>        self,<br>        mode: str = OpenAIEmbeddingMode.TEXT_SEARCH_MODE,<br>        model: str = OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        additional_kwargs: Optional[Dict[str, Any]] = None,<br>        api_key: Optional[str] = None,<br>        api_version: Optional[str] = None,<br>        # azure specific<br>        azure_endpoint: Optional[str] = None,<br>        azure_deployment: Optional[str] = None,<br>        azure_ad_token_provider: Optional[AzureADTokenProvider] = None,<br>        use_azure_ad: bool = False,<br>        deployment_name: Optional[str] = None,<br>        max_retries: int = 10,<br>        reuse_client: bool = True,<br>        callback_manager: Optional[CallbackManager] = None,<br>        num_workers: Optional[int] = None,<br>        # custom httpx client<br>        http_client: Optional[httpx.Client] = None,<br>        async_http_client: Optional[httpx.AsyncClient] = None,<br>        **kwargs: Any,<br>    ):<br>        azure_endpoint = get_from_param_or_env(<br>            \"azure_endpoint\", azure_endpoint, \"AZURE_OPENAI_ENDPOINT\", \"\"<br>        )<br>        azure_deployment = resolve_from_aliases(<br>            azure_deployment,<br>            deployment_name,<br>        )<br>        super().__init__(<br>            mode=mode,<br>            model=model,<br>            embed_batch_size=embed_batch_size,<br>            additional_kwargs=additional_kwargs,<br>            api_key=api_key,<br>            api_version=api_version,<br>            azure_endpoint=azure_endpoint,<br>            azure_deployment=azure_deployment,<br>            azure_ad_token_provider=azure_ad_token_provider,<br>            use_azure_ad=use_azure_ad,<br>            max_retries=max_retries,<br>            reuse_client=reuse_client,<br>            callback_manager=callback_manager,<br>            http_client=http_client,<br>            async_http_client=async_http_client,<br>            num_workers=num_workers,<br>            **kwargs,<br>        )<br>    @model_validator(mode=\"before\")<br>    @classmethod<br>    def validate_env(cls, values: Dict[str, Any]) -> Dict[str, Any]:<br>        \"\"\"Validate necessary credentials are set.\"\"\"<br>        if (<br>            values.get(\"api_base\") == \"https://api.openai.com/v1\"<br>            and values.get(\"azure_endpoint\") is None<br>        ):<br>            raise ValueError(<br>                \"You must set OPENAI_API_BASE to your Azure endpoint. \"<br>                \"It should look like https://YOUR_RESOURCE_NAME.openai.azure.com/\"<br>            )<br>        if values.get(\"api_version\") is None:<br>            raise ValueError(\"You must set OPENAI_API_VERSION for Azure OpenAI.\")<br>        return values<br>    def _get_client(self) -> AzureOpenAI:<br>        if not self.reuse_client:<br>            return AzureOpenAI(**self._get_credential_kwargs())<br>        if self._client is None:<br>            self._client = AzureOpenAI(**self._get_credential_kwargs())<br>        return self._client<br>    def _get_aclient(self) -> AsyncAzureOpenAI:<br>        if not self.reuse_client:<br>            return AsyncAzureOpenAI(**self._get_credential_kwargs(is_async=True))<br>        if self._aclient is None:<br>            self._aclient = AsyncAzureOpenAI(<br>                **self._get_credential_kwargs(is_async=True)<br>            )<br>        return self._aclient<br>    def _get_credential_kwargs(self, is_async: bool = False) -> Dict[str, Any]:<br>        if self.use_azure_ad:<br>            self._azure_ad_token = refresh_openai_azuread_token(self._azure_ad_token)<br>            self.api_key = self._azure_ad_token.token<br>        else:<br>            self.api_key = get_from_param_or_env(<br>                \"api_key\", self.api_key, \"AZURE_OPENAI_API_KEY\"<br>            )<br>        return {<br>            \"api_key\": self.api_key,<br>            \"azure_ad_token_provider\": self.azure_ad_token_provider,<br>            \"azure_endpoint\": self.azure_endpoint,<br>            \"azure_deployment\": self.azure_deployment,<br>            \"api_version\": self.api_version,<br>            \"default_headers\": self.default_headers,<br>            \"http_client\": self._async_http_client if is_async else self._http_client,<br>        }<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"AzureOpenAIEmbedding\"<br>``` |\n\n### validate\\_env`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/azure_openai/\\#llama_index.embeddings.azure_openai.AzureOpenAIEmbedding.validate_env \"Permanent link\")\n\n```\nvalidate_env(values: Dict[str, Any]) -> Dict[str, Any]\n\n```\n\nValidate necessary credentials are set.\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-azure-openai/llama_index/embeddings/azure_openai/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>``` | ```<br>@model_validator(mode=\"before\")<br>@classmethod<br>def validate_env(cls, values: Dict[str, Any]) -> Dict[str, Any]:<br>    \"\"\"Validate necessary credentials are set.\"\"\"<br>    if (<br>        values.get(\"api_base\") == \"https://api.openai.com/v1\"<br>        and values.get(\"azure_endpoint\") is None<br>    ):<br>        raise ValueError(<br>            \"You must set OPENAI_API_BASE to your Azure endpoint. \"<br>            \"It should look like https://YOUR_RESOURCE_NAME.openai.azure.com/\"<br>        )<br>    if values.get(\"api_version\") is None:<br>        raise ValueError(\"You must set OPENAI_API_VERSION for Azure OpenAI.\")<br>    return values<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Azure openai - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/azure_openai/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/arize_phoenix/#llama_index.callbacks.arize_phoenix.arize_phoenix_callback_handler)\n\n# Arize phoenix\n\n## arize\\_phoenix\\_callback\\_handler [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/arize_phoenix/\\#llama_index.callbacks.arize_phoenix.arize_phoenix_callback_handler \"Permanent link\")\n\n```\narize_phoenix_callback_handler(**kwargs: Any) -> BaseCallbackHandler\n\n```\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-arize-phoenix/llama_index/callbacks/arize_phoenix/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>``` | ```<br>def arize_phoenix_callback_handler(**kwargs: Any) -> BaseCallbackHandler:<br>    # newer versions of arize, v2.x<br>    try:<br>        from openinference.instrumentation.llama_index import LlamaIndexInstrumentor<br>        from opentelemetry.exporter.otlp.proto.http.trace_exporter import (<br>            OTLPSpanExporter,<br>        )<br>        from opentelemetry.sdk import trace as trace_sdk<br>        from opentelemetry.sdk.trace.export import SimpleSpanProcessor<br>        endpoint = kwargs.get(\"endpoint\", \"http://127.0.0.1:6006/v1/traces\")<br>        tracer_provider = trace_sdk.TracerProvider()<br>        tracer_provider.add_span_processor(<br>            SimpleSpanProcessor(OTLPSpanExporter(endpoint))<br>        )<br>        return LlamaIndexInstrumentor().instrument(<br>            tracer_provider=kwargs.get(\"tracer_provider\", tracer_provider)<br>        )<br>    except ImportError:<br>        # using an older version of arize<br>        pass<br>    # older versions of arize, v1.x<br>    try:<br>        from phoenix.trace.llama_index import OpenInferenceTraceCallbackHandler<br>    except ImportError:<br>        raise ImportError(<br>            \"Please install Arize Phoenix with `pip install -q arize-phoenix`\"<br>        )<br>    return OpenInferenceTraceCallbackHandler(**kwargs)<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Arize phoenix - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/arize_phoenix/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    }
  ],
  "unique_links": [
    "https://docs.llamaindex.ai/en/stable/",
    "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/uptrain/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/azure_inference/",
    "https://docs.llamaindex.ai/en/stable/CHANGELOG/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/deepinfra/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/azure_openai/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/fastembed/",
    "https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/condense_question/",
    "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/arize_phoenix/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/cloudflare_workersai/",
    "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/argilla/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/elasticsearch/",
    "https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/",
    "https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/context/",
    "https://docs.llamaindex.ai/en/stable/api_reference/agent/introspective/",
    "https://docs.llamaindex.ai/en/stable/api_reference/agent/",
    "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/llama_debug/",
    "https://docs.llamaindex.ai/en/stable/DOCS_README/",
    "https://docs.llamaindex.ai/en/stable/api_reference/",
    "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/",
    "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/langfuse/",
    "https://docs.llamaindex.ai/en/stable/CONTRIBUTING/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/alephalpha/",
    "https://docs.llamaindex.ai/en/stable/api_reference/agent/react/",
    "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/deepeval/",
    "https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/",
    "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/agentops/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/anyscale/",
    "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/honeyhive/",
    "https://docs.llamaindex.ai/en/stable/api_reference/agent/llm_compiler/",
    "https://docs.llamaindex.ai/en/stable/api_reference/agent/openai_legacy/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/cohere/",
    "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/aim/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/bedrock/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/databricks/",
    "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/wandb/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/",
    "https://docs.llamaindex.ai/en/stable/api_reference/agent/lats/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/dashscope/",
    "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/openinference/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/clarifai/",
    "https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/condense_plus_context/",
    "https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/simple/",
    "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/literalai/",
    "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/token_counter/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/clip/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/adapter/",
    "https://docs.llamaindex.ai/en/stable/api_reference/agent/coa/",
    "https://docs.llamaindex.ai/en/stable/api_reference/agent/dashscope/",
    "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/promptlayer/"
  ]
}