{
  "input_url": "https://supabase.com/docs/guides/ai",
  "data": [
    {
      "markdown": "AI & Vectors\n\n# Semantic Text Deduplication\n\n## Finding duplicate movie reviews with Supabase Vecs.\n\n* * *\n\nThis guide will walk you through a [\"Semantic Text Deduplication\"](https://github.com/supabase/supabase/blob/master/examples/ai/semantic_text_deduplication.ipynb) example using Colab and Supabase Vecs. You'll learn how to find similar movie reviews using embeddings, and remove any that seem like duplicates. You will:\n\n1. Launch a Postgres database that uses pgvector to store embeddings\n2. Launch a notebook that connects to your database\n3. Load the IMDB dataset\n4. Use the `sentence-transformers/all-MiniLM-L6-v2` model to create an embedding representing the semantic meaning of each review.\n5. Search for all duplicates.\n\n## Project setup [\\#](\\#project-setup)\n\nLet's create a new Postgres database. This is as simple as starting a new Project in Supabase:\n\n1. [Create a new project](https://database.new/) in the Supabase dashboard.\n2. Enter your project details. Remember to store your password somewhere safe.\n\nYour database will be available in less than a minute.\n\n**Finding your credentials:**\n\nYou can find your project credentials inside the project [settings](https://supabase.com/dashboard/project/_/settings/), including:\n\n- [Database credentials](https://supabase.com/dashboard/project/_/settings/database): connection strings and connection pooler details.\n- [API credentials](https://supabase.com/dashboard/project/_/settings/database): your serverless API URL and `anon` / `service_role` keys.\n\n## Launching a notebook [\\#](\\#launching-a-notebook)\n\nLaunch our [`semantic_text_deduplication`](https://github.com/supabase/supabase/blob/master/examples/ai/semantic_text_deduplication.ipynb) notebook in Colab:\n\n[![](https://supabase.com/docs/img/ai/colab-badge.svg)](https://colab.research.google.com/github/supabase/supabase/blob/master/examples/ai/semantic_text_deduplication.ipynb)\n\nAt the top of the notebook, you'll see a button `Copy to Drive`. Click this button to copy the notebook to your Google Drive.\n\n## Connecting to your database [\\#](\\#connecting-to-your-database)\n\nInside the Notebook, find the cell which specifies the `DB_CONNECTION`. It will contain some code like this:\n\n`\n_10\nimport vecs\n_10\n_10\nDB_CONNECTION = \"postgresql://<user>:<password>@<host>:<port>/<db_name>\"\n_10\n_10\n# create vector store client\n_10\nvx = vecs.create_client(DB_CONNECTION)\n`\n\nReplace the `DB_CONNECTION` with your own connection string for your database. You can find the Postgres connection string in the [Database Settings](https://supabase.com/dashboard/project/_/settings/database) of your Supabase project.\n\nSQLAlchemy requires the connection string to start with `postgresql://` (instead of `postgres://`). Don't forget to rename this after copying the string from the dashboard.\n\nYou must use the \"connection pooling\" string (domain ending in `*.pooler.supabase.com`) with Google Colab since Colab does not support IPv6.\n\n## Stepping through the notebook [\\#](\\#stepping-through-the-notebook)\n\nNow all that's left is to step through the notebook. You can do this by clicking the \"execute\" button ( `ctrl+enter`) at the top left of each code cell. The notebook guides you through the process of creating a collection, adding data to it, and querying it.\n\nYou can view the inserted items in the [Table Editor](https://supabase.com/dashboard/project/_/editor/), by selecting the `vecs` schema from the schema dropdown.\n\n![Colab documents](https://supabase.com/docs/img/ai/google-colab/colab-documents.png)\n\n## Deployment [\\#](\\#deployment)\n\nIf you have your own infrastructure for deploying Python apps, you can continue to use `vecs` as described in this guide.\n\nAlternatively if you would like to quickly deploy using Supabase, check out our guide on using the [Hugging Face Inference API](/docs/guides/ai/hugging-face) in Edge Functions using TypeScript.\n\n## Next steps [\\#](\\#next-steps)\n\nYou can now start building your own applications with Vecs. Check our [examples](/docs/guides/ai#examples) for ideas.",
      "metadata": {
        "ogUrl": "https://supabase.com/docs/guides/ai/quickstarts/text-deduplication",
        "title": "Semantic Text Deduplication | Supabase Docs",
        "robots": "index, follow",
        "ogImage": "https://obuldanrptloktxcffvn.supabase.co/functions/v1/og-images?site=docs&type=ai&title=Semantic%20Text%20Deduplication&description=undefined",
        "ogTitle": "Semantic Text Deduplication | Supabase Docs",
        "language": "en",
        "sourceURL": "https://supabase.com/docs/guides/ai/quickstarts/text-deduplication",
        "description": "Finding duplicate movie reviews with Supabase Vecs.",
        "modifiedTime": "2024-09-06T19:20:40.301Z",
        "ogDescription": "Finding duplicate movie reviews with Supabase Vecs.",
        "publishedTime": "2024-09-06T19:20:40.300Z",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "AI & Vectors\n\n# Semantic Image Search with Amazon Titan\n\n## Implement semantic image search with Amazon Titan and Supabase Vector in Python.\n\n* * *\n\n[Amazon Bedrock](https://aws.amazon.com/bedrock) is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon. Each model is accessible through a common API which implements a broad set of features to help build generative AI applications with security, privacy, and responsible AI in mind.\n\n[Amazon Titan](https://aws.amazon.com/bedrock/titan/) is a family of foundation models (FMs) for text and image generation, summarization, classification, open-ended Q&A, information extraction, and text or image search.\n\nIn this guide we'll look at how we can get started with Amazon Bedrock and Supabase Vector in Python using the Amazon Titan multimodal model and the [vecs client](/docs/guides/ai/vecs-python-client).\n\nYou can find the full application code as a Python Poetry project on [GitHub](https://github.com/supabase/supabase/tree/master/examples/ai/aws_bedrock_image_search).\n\n## Create a new Python project with Poetry [\\#](\\#create-a-new-python-project-with-poetry)\n\n[Poetry](https://python-poetry.org/) provides packaging and dependency management for Python. If you haven't already, install poetry via pip:\n\n`\n_10\npip install poetry\n`\n\nThen initialize a new project:\n\n`\n_10\npoetry new aws_bedrock_image_search\n`\n\n## Spin up a Postgres Database with pgvector [\\#](\\#spin-up-a-postgres-database-with-pgvector)\n\nIf you haven't already, head over to [database.new](https://database.new) and create a new project. Every Supabase project comes with a full Postgres database and the [pgvector extension](/docs/guides/database/extensions/pgvector) preconfigured.\n\nWhen creating your project, make sure to note down your database password as you will need it to construct the `DB_URL` in the next step.\n\nYou can find the database connection string in your Supabase Dashboard [database settings](https://supabase.com/dashboard/project/_/settings/database). Select \"Use connection pooling\" with `Mode: Session` for a direct connection to your Postgres database. It will look something like this:\n\n`\n_10\npostgresql://postgres.[PROJECT-REF]:[YOUR-PASSWORD]@aws-0-[REGION].pooler.supabase.com:5432/postgres\n`\n\n## Install the dependencies [\\#](\\#install-the-dependencies)\n\nWe will need to add the following dependencies to our project:\n\n- [`vecs`](https://github.com/supabase/vecs#vecs): Supabase Vector Python Client.\n- [`boto3`](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html): AWS SDK for Python.\n- [`matplotlib`](https://matplotlib.org/): for displaying our image result.\n\n`\n_10\npoetry add vecs boto3 matplotlib\n`\n\n## Import the necessary dependencies [\\#](\\#import-the-necessary-dependencies)\n\nAt the top of your main python script, import the dependencies and store your `DB URL` from above in a variable:\n\n`\n_10\nimport sys\n_10\nimport boto3\n_10\nimport vecs\n_10\nimport json\n_10\nimport base64\n_10\nfrom matplotlib import pyplot as plt\n_10\nfrom matplotlib import image as mpimg\n_10\nfrom typing import Optional\n_10\n_10\nDB_CONNECTION = \"postgresql://postgres.[PROJECT-REF]:[YOUR-PASSWORD]@aws-0-[REGION].pooler.supabase.com:5432/postgres\"\n`\n\nNext, get the [credentials to your AWS account](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html) and instantiate the `boto3` client:\n\n`\n_10\nbedrock_client = boto3.client(\n_10\n    'bedrock-runtime',\n_10\n    region_name='us-west-2',\n_10\n    # Credentials from your AWS account\n_10\n    aws_access_key_id='<replace_your_own_credentials>',\n_10\n    aws_secret_access_key='<replace_your_own_credentials>',\n_10\n    aws_session_token='<replace_your_own_credentials>',\n_10\n)\n`\n\n## Create embeddings for your images [\\#](\\#create-embeddings-for-your-images)\n\nIn the root of your project, create a new folder called `images` and add some images. You can use the images from the example project on [GitHub](https://github.com/supabase/supabase/tree/master/examples/ai/aws_bedrock_image_search/images) or you can find license free images on [unsplash](https://unsplash.com).\n\nTo send images to the Amazon Bedrock API we need to need to encode them as `base64` strings. Create the following helper methods:\n\n`\n_44\ndef readFileAsBase64(file_path):\n_44\n    \"\"\"Encode image as base64 string.\"\"\"\n_44\n    try:\n_44\n        with open(file_path, \"rb\") as image_file:\n_44\n            input_image = base64.b64encode(image_file.read()).decode(\"utf8\")\n_44\n        return input_image\n_44\n    except:\n_44\n        print(\"bad file name\")\n_44\n        sys.exit(0)\n_44\n_44\n_44\ndef construct_bedrock_image_body(base64_string):\n_44\n    \"\"\"Construct the request body.\n_44\n_44\n    https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-embed-mm.html\n_44\n    \"\"\"\n_44\n    return json.dumps(\n_44\n        {\n_44\n            \"inputImage\": base64_string,\n_44\n            \"embeddingConfig\": {\"outputEmbeddingLength\": 1024},\n_44\n        }\n_44\n    )\n_44\n_44\n_44\ndef get_embedding_from_titan_multimodal(body):\n_44\n    \"\"\"Invoke the Amazon Titan Model via API request.\"\"\"\n_44\n    response = bedrock_client.invoke_model(\n_44\n        body=body,\n_44\n        modelId=\"amazon.titan-embed-image-v1\",\n_44\n        accept=\"application/json\",\n_44\n        contentType=\"application/json\",\n_44\n    )\n_44\n_44\n    response_body = json.loads(response.get(\"body\").read())\n_44\n    print(response_body)\n_44\n    return response_body[\"embedding\"]\n_44\n_44\n_44\ndef encode_image(file_path):\n_44\n    \"\"\"Generate embedding for the image at file_path.\"\"\"\n_44\n    base64_string = readFileAsBase64(file_path)\n_44\n    body = construct_bedrock_image_body(base64_string)\n_44\n    emb = get_embedding_from_titan_multimodal(body)\n_44\n    return emb\n`\n\nNext, create a `seed` method, which will create a new Supabase Vector Collection, generate embeddings for your images, and upsert the embeddings into your database:\n\n`\n_40\ndef seed():\n_40\n    # create vector store client\n_40\n    vx = vecs.create_client(DB_CONNECTION)\n_40\n_40\n    # get or create a collection of vectors with 1024 dimensions\n_40\n    images = vx.get_or_create_collection(name=\"image_vectors\", dimension=1024)\n_40\n_40\n    # Generate image embeddings with Amazon Titan Model\n_40\n    img_emb1 = encode_image('./images/one.jpg')\n_40\n    img_emb2 = encode_image('./images/two.jpg')\n_40\n    img_emb3 = encode_image('./images/three.jpg')\n_40\n    img_emb4 = encode_image('./images/four.jpg')\n_40\n_40\n    # add records to the *images* collection\n_40\n    images.upsert(\n_40\n        records=[\\\n_40\\\n            (\\\n_40\\\n                \"one.jpg\",       # the vector's identifier\\\n_40\\\n                img_emb1,        # the vector. list or np.array\\\n_40\\\n                {\"type\": \"jpg\"}  # associated  metadata\\\n_40\\\n            ), (\\\n_40\\\n                \"two.jpg\",\\\n_40\\\n                img_emb2,\\\n_40\\\n                {\"type\": \"jpg\"}\\\n_40\\\n            ), (\\\n_40\\\n                \"three.jpg\",\\\n_40\\\n                img_emb3,\\\n_40\\\n                {\"type\": \"jpg\"}\\\n_40\\\n            ), (\\\n_40\\\n                \"four.jpg\",\\\n_40\\\n                img_emb4,\\\n_40\\\n                {\"type\": \"jpg\"}\\\n_40\\\n            )\\\n_40\\\n        ]\n_40\n    )\n_40\n    print(\"Inserted images\")\n_40\n_40\n    # index the collection for fast search performance\n_40\n    images.create_index()\n_40\n    print(\"Created index\")\n`\n\nAdd this method as a script in your `pyproject.toml` file:\n\n`\n_10\n[tool.poetry.scripts]\n_10\nseed = \"image_search.main:seed\"\n_10\nsearch = \"image_search.main:search\"\n`\n\nAfter activating the virtual environtment with `poetry shell` you can now run your seed script via `poetry run seed`. You can inspect the generated embeddings in your Supabase Dashboard by visiting the [Table Editor](https://supabase.com/dashboard/project/_/editor), selecting the `vecs` schema, and the `image_vectors` table.\n\n## Perform an image search from a text query [\\#](\\#perform-an-image-search-from-a-text-query)\n\nWith Supabase Vector we can easily query our embeddings. We can use either an image as the search input or alternatively we can generate an embedding from a string input and use that as the query input:\n\n`\n_28\ndef search(query_term: Optional[str] = None):\n_28\n    if query_term is None:\n_28\n        query_term = sys.argv[1]\n_28\n_28\n    # create vector store client\n_28\n    vx = vecs.create_client(DB_CONNECTION)\n_28\n    images = vx.get_or_create_collection(name=\"image_vectors\", dimension=1024)\n_28\n_28\n    # Encode text query\n_28\n    text_emb = get_embedding_from_titan_multimodal(json.dumps(\n_28\n        {\n_28\n            \"inputText\": query_term,\n_28\n            \"embeddingConfig\": {\"outputEmbeddingLength\": 1024},\n_28\n        }\n_28\n    ))\n_28\n_28\n    # query the collection filtering metadata for \"type\" = \"jpg\"\n_28\n    results = images.query(\n_28\n        data=text_emb,                      # required\n_28\n        limit=1,                            # number of records to return\n_28\n        filters={\"type\": {\"$eq\": \"jpg\"}},   # metadata filters\n_28\n    )\n_28\n    result = results[0]\n_28\n    print(result)\n_28\n    plt.title(result)\n_28\n    image = mpimg.imread('./images/' + result)\n_28\n    plt.imshow(image)\n_28\n    plt.show()\n`\n\nBy limiting the query to one result, we can show the most relevant image to the user. Finally we use `matplotlib` to show the image result to the user.\n\nThat's it, go ahead and test it out by running `poetry run search` and you will be presented with an image of a \"bike in front of a red brick wall\".\n\n## Conclusion [\\#](\\#conclusion)\n\nWith just a couple of lines of Python you are able to implement image search as well as reverse image search using the Amazon Titan multimodal model and Supabase Vector.",
      "metadata": {
        "ogUrl": "https://supabase.com/docs/guides/ai/examples/semantic-image-search-amazon-titan",
        "title": "Semantic Image Search with Amazon Titan | Supabase Docs",
        "robots": "index, follow",
        "ogImage": "https://obuldanrptloktxcffvn.supabase.co/functions/v1/og-images?site=docs&type=ai&title=Semantic%20Image%20Search%20with%20Amazon%20Titan&description=Implement%20semantic%20image%20search%20with%20Amazon%20Titan%20and%20Supabase%20Vector%20in%20Python.",
        "ogTitle": "Semantic Image Search with Amazon Titan | Supabase Docs",
        "language": "en",
        "sourceURL": "https://supabase.com/docs/guides/ai/examples/semantic-image-search-amazon-titan",
        "description": "Implement semantic image search with Amazon Titan and Supabase Vector in Python.",
        "modifiedTime": "2024-09-06T19:20:41.221Z",
        "ogDescription": "Implement semantic image search with Amazon Titan and Supabase Vector in Python.",
        "publishedTime": "2024-09-06T19:20:41.221Z",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "AI & Vectors\n\n# Face similarity search\n\n## Identify the celebrities who look most similar to you using Supabase Vecs.\n\n* * *\n\nThis guide will walk you through a [\"Face Similarity Search\"](https://github.com/supabase/supabase/blob/master/examples/ai/face_similarity.ipynb) example using Colab and Supabase Vecs. You will be able to identify the celebrities who look most similar to you (or any other person). You will:\n\n1. Launch a Postgres database that uses pgvector to store embeddings\n2. Launch a notebook that connects to your database\n3. Load the \" `ashraq/tmdb-people-image`\" celebrity dataset\n4. Use the `face_recognition` model to create an embedding for every celebrity photo.\n5. Search for similar faces inside the dataset.\n\n## Project setup [\\#](\\#project-setup)\n\nLet's create a new Postgres database. This is as simple as starting a new Project in Supabase:\n\n1. [Create a new project](https://database.new/) in the Supabase dashboard.\n2. Enter your project details. Remember to store your password somewhere safe.\n\nYour database will be available in less than a minute.\n\n**Finding your credentials:**\n\nYou can find your project credentials inside the project [settings](https://supabase.com/dashboard/project/_/settings/), including:\n\n- [Database credentials](https://supabase.com/dashboard/project/_/settings/database): connection strings and connection pooler details.\n- [API credentials](https://supabase.com/dashboard/project/_/settings/database): your serverless API URL and `anon` / `service_role` keys.\n\n## Launching a notebook [\\#](\\#launching-a-notebook)\n\nLaunch our [`semantic_text_deduplication`](https://github.com/supabase/supabase/blob/master/examples/ai/face_similarity.ipynb) notebook in Colab:\n\n[![](https://supabase.com/docs/img/ai/colab-badge.svg)](https://colab.research.google.com/github/supabase/supabase/blob/master/examples/ai/face_similarity.ipynb)\n\nAt the top of the notebook, you'll see a button `Copy to Drive`. Click this button to copy the notebook to your Google Drive.\n\n## Connecting to your database [\\#](\\#connecting-to-your-database)\n\nInside the Notebook, find the cell which specifies the `DB_CONNECTION`. It will contain some code like this:\n\n`\n_10\nimport vecs\n_10\n_10\nDB_CONNECTION = \"postgresql://<user>:<password>@<host>:<port>/<db_name>\"\n_10\n_10\n# create vector store client\n_10\nvx = vecs.create_client(DB_CONNECTION)\n`\n\nReplace the `DB_CONNECTION` with your own connection string for your database. You can find the Postgres connection string in the [Database Settings](https://supabase.com/dashboard/project/_/settings/database) of your Supabase project.\n\nSQLAlchemy requires the connection string to start with `postgresql://` (instead of `postgres://`). Don't forget to rename this after copying the string from the dashboard.\n\nYou must use the \"connection pooling\" string (domain ending in `*.pooler.supabase.com`) with Google Colab since Colab does not support IPv6.\n\n## Stepping through the notebook [\\#](\\#stepping-through-the-notebook)\n\nNow all that's left is to step through the notebook. You can do this by clicking the \"execute\" button ( `ctrl+enter`) at the top left of each code cell. The notebook guides you through the process of creating a collection, adding data to it, and querying it.\n\nYou can view the inserted items in the [Table Editor](https://supabase.com/dashboard/project/_/editor/), by selecting the `vecs` schema from the schema dropdown.\n\n![Colab documents](https://supabase.com/docs/img/ai/google-colab/colab-documents.png)\n\n## Next steps [\\#](\\#next-steps)\n\nYou can now start building your own applications with Vecs. Check our [examples](/docs/guides/ai#examples) for ideas.",
      "metadata": {
        "ogUrl": "https://supabase.com/docs/guides/ai/quickstarts/face-similarity",
        "title": "Face similarity search | Supabase Docs",
        "robots": "index, follow",
        "ogImage": "https://obuldanrptloktxcffvn.supabase.co/functions/v1/og-images?site=docs&type=ai&title=Face%20similarity%20search&description=undefined",
        "ogTitle": "Face similarity search | Supabase Docs",
        "language": "en",
        "sourceURL": "https://supabase.com/docs/guides/ai/quickstarts/face-similarity",
        "description": "Identify the celebrities who look most similar to you using Supabase Vecs.",
        "modifiedTime": "2024-09-06T19:20:40.301Z",
        "ogDescription": "Identify the celebrities who look most similar to you using Supabase Vecs.",
        "publishedTime": "2024-09-06T19:20:40.300Z",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "AI & Vectors\n\n# Roboflow\n\n## Learn how to integrate Supabase with Roboflow, a tool for running fine-tuned and foundation vision models.\n\n* * *\n\nIn this guide, we will walk through two examples of using [Roboflow Inference](https://inference.roboflow.com) to run fine-tuned and foundation models. We will run inference and save predictions using an object detection model and [CLIP](https://github.com/openai/CLIP).\n\n## Project setup [\\#](\\#project-setup)\n\nLet's create a new Postgres database. This is as simple as starting a new Project in Supabase:\n\n1. [Create a new project](https://database.new/) in the Supabase dashboard.\n2. Enter your project details. Remember to store your password somewhere safe.\n\nYour database will be available in less than a minute.\n\n**Finding your credentials:**\n\nYou can find your project credentials inside the project [settings](https://supabase.com/dashboard/project/_/settings/), including:\n\n- [Database credentials](https://supabase.com/dashboard/project/_/settings/database): connection strings and connection pooler details.\n- [API credentials](https://supabase.com/dashboard/project/_/settings/database): your serverless API URL and `anon` / `service_role` keys.\n\n## Save computer vision predictions [\\#](\\#save-computer-vision-predictions)\n\nOnce you have a trained vision model, you need to create business logic for your application. In many cases, you want to save inference results to a file.\n\nThe steps below show you how to run a vision model locally and save predictions to Supabase.\n\n### Preparation: Set up a model [\\#](\\#preparation-set-up-a-model)\n\nBefore you begin, you will need an object detection model trained on your data.\n\nYou can [train a model on Roboflow](https://blog.roboflow.com/getting-started-with-roboflow/), leveraging end-to-end tools from data management and annotation to deployment, or [upload custom model weights](https://docs.roboflow.com/deploy/upload-custom-weights) for deployment.\n\nAll models have an infinitely scalable API through which you can query your model, and can be run locally.\n\nFor this guide, we will use a demo [rock, paper, scissors](https://universe.roboflow.com/roboflow-58fyf/rock-paper-scissors-sxsw) model.\n\n### Step 1: Install and start Roboflow Inference [\\#](\\#step-1-install-and-start-roboflow-inference)\n\nYou will deploy our model locally using Roboflow Inference, a computer vision inference server.\n\nTo install and start Roboflow Inference, first install Docker on your machine.\n\nThen, run:\n\n`\n_10\npip install inference inference-cli inference-sdk && inference server start\n`\n\nAn inference server will be available at `http://localhost:9001`.\n\n### Step 2: Run inference on an image [\\#](\\#step-2-run-inference-on-an-image)\n\nYou can run inference on images and videos. Let's run inference on an image.\n\nCreate a new Python file and add the following code:\n\n`\n_13\nfrom inference_sdk import InferenceHTTPClient\n_13\n_13\nimage = \"example.jpg\"\n_13\nMODEL_ID = \"rock-paper-scissors-sxsw/11\"\n_13\n_13\nclient = InferenceHTTPClient(\n_13\n    api_url=\"http://localhost:9001\",\n_13\n    api_key=\"ROBOFLOW_API_KEY\"\n_13\n)\n_13\nwith client.use_model(MODEL_ID):\n_13\n    predictions = client.infer(image)\n_13\n_13\nprint(predictions)\n`\n\nAbove, replace:\n\n1. The image URL with the name of the image on which you want to run inference.\n2. `ROBOFLOW_API_KEY` with your Roboflow API key. [Learn how to retrieve your Roboflow API key](https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key).\n3. `MODEL_ID` with your Roboflow model ID. [Learn how to retrieve your model ID](https://docs.roboflow.com/api-reference/workspace-and-project-ids).\n\nWhen you run the code above, a list of predictions will be printed to the console:\n\n`\n_10\n{'time': 0.05402109300121083, 'image': {'width': 640, 'height': 480}, 'predictions': [{'x': 312.5, 'y': 392.0, 'width': 255.0, 'height': 110.0, 'confidence': 0.8620790839195251, 'class': 'Paper', 'class_id': 0}]}\n`\n\n### Step 3: Save results in Supabase [\\#](\\#step-3-save-results-in-supabase)\n\nTo save results in Supabase, add the following code to your script:\n\n`\n_10\nimport os\n_10\nfrom supabase import create_client, Client\n_10\n_10\nurl: str = os.environ.get(\"SUPABASE_URL\")\n_10\nkey: str = os.environ.get(\"SUPABASE_KEY\")\n_10\nsupabase: Client = create_client(url, key)\n_10\n_10\nresult = supabase.table('predictions') \\\n_10\n    .insert({\"filename\": image, \"predictions\": predictions}) \\\n_10\n    .execute()\n`\n\nYou can then query your predictions using the following code:\n\n`\n_10\nresult = supabase.table('predictions') \\\n_10\n    .select(\"predictions\") \\\n_10\n    .filter(\"filename\", \"eq\", image) \\\n_10\n    .execute()\n_10\n_10\nprint(result)\n`\n\nHere is an example result:\n\n`\n_10\ndata=[{'predictions': {'time': 0.08492901099998562, 'image': {'width': 640, 'height': 480}, 'predictions': [{'x': 312.5, 'y': 392.0, 'width': 255.0, 'height': 110.0, 'confidence': 0.8620790839195251, 'class': 'Paper', 'class_id': 0}]}}, {'predictions': {'time': 0.08818970100037404, 'image': {'width': 640, 'height': 480}, 'predictions': [{'x': 312.5, 'y': 392.0, 'width': 255.0, 'height': 110.0, 'confidence': 0.8620790839195251, 'class': 'Paper', 'class_id': 0}]}}] count=None\n`\n\n## Calculate and save CLIP embeddings [\\#](\\#calculate-and-save-clip-embeddings)\n\nYou can use the Supabase vector database functionality to store and query CLIP embeddings.\n\nRoboflow Inference provides a HTTP interface through which you can calculate image and text embeddings using CLIP.\n\n### Step 1: Install and start Roboflow Inference [\\#](\\#step-1-install-and-start-roboflow-inference)\n\nSee [Step #1: Install and Start Roboflow Inference](#step-1-install-and-start-roboflow-inference) above to install and start Roboflow Inference.\n\n### Step 2: Run CLIP on an image [\\#](\\#step-2-run-clip-on-an-image)\n\nCreate a new Python file and add the following code:\n\n`\n_32\nimport cv2\n_32\nimport supervision as sv\n_32\nimport requests\n_32\nimport base64\n_32\nimport os\n_32\n_32\nIMAGE_DIR = \"images/train/images/\"\n_32\nAPI_KEY = \"\"\n_32\nSERVER_URL = \"http://localhost:9001\"\n_32\n_32\nresults = []\n_32\n_32\nfor i, image in enumerate(os.listdir(IMAGE_DIR)):\n_32\n    print(f\"Processing image {image}\")\n_32\n    infer_clip_payload = {\n_32\n        \"image\": {\n_32\n            \"type\": \"base64\",\n_32\n            \"value\": base64.b64encode(open(IMAGE_DIR + image, \"rb\").read()).decode(\"utf-8\"),\n_32\n        },\n_32\n    }\n_32\n_32\n    res = requests.post(\n_32\n        f\"{SERVER_URL}/clip/embed_image?api_key={API_KEY}\",\n_32\n        json=infer_clip_payload,\n_32\n    )\n_32\n_32\n    embeddings = res.json()['embeddings']\n_32\n_32\n    results.append({\n_32\n        \"filename\": image,\n_32\n        \"embeddings\": embeddings\n_32\n    })\n`\n\nThis code will calculate CLIP embeddings for each image in the directory and print the results to the console.\n\nAbove, replace:\n\n1. `IMAGE_DIR` with the directory containing the images on which you want to run inference.\n2. `ROBOFLOW_API_KEY` with your Roboflow API key. [Learn how to retrieve your Roboflow API key](https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key).\n\nYou can also calculate CLIP embeddings in the cloud by setting `SERVER_URL` to `https://infer.roboflow.com`.\n\n### Step 3: Save embeddings in Supabase [\\#](\\#step-3-save-embeddings-in-supabase)\n\nYou can store your image embeddings in Supabase using the Supabase `vecs` Python package:\n\nFirst, install `vecs`:\n\n`\n_10\npip install vecs\n`\n\nNext, add the following code to your script to create an index:\n\n`\n_26\n_26\nimport vecs\n_26\n_26\nDB_CONNECTION = \"postgresql://postgres:[password]@[host]:[port]/[database]\"\n_26\n_26\nvx = vecs.create_client(DB_CONNECTION)\n_26\n_26\n# create a collection of vectors with 3 dimensions\n_26\nimages = vx.get_or_create_collection(name=\"image_vectors\", dimension=512)\n_26\n_26\nfor result in results:\n_26\n    image = result[\"filename\"]\n_26\n    embeddings = result[\"embeddings\"][0]\n_26\n_26\n    # insert a vector into the collection\n_26\n    images.upsert(\n_26\n        records=[\\\n_26\\\n            (\\\n_26\\\n                image,\\\n_26\\\n                embeddings,\\\n_26\\\n                {} # metadata\\\n_26\\\n            )\\\n_26\\\n        ]\n_26\n    )\n_26\n_26\nimages.create_index()\n`\n\nReplace `DB_CONNECTION` with the authentication information for your database. You can retrieve this from the Supabase dashboard in `Project Settings > Database Settings`.\n\nYou can then query your embeddings using the following code:\n\n`\n_17\ninfer_clip_payload = {\n_17\n    \"text\": \"cat\",\n_17\n}\n_17\n_17\nres = requests.post(\n_17\n    f\"{SERVER_URL}/clip/embed_text?api_key={API_KEY}\",\n_17\n    json=infer_clip_payload,\n_17\n)\n_17\n_17\nembeddings = res.json()['embeddings']\n_17\n_17\nresult = images.query(\n_17\n    data=embeddings[0],\n_17\n    limit=1\n_17\n)\n_17\n_17\nprint(result[0])\n`\n\n## Resources [\\#](\\#resources)\n\n- [Roboflow Inference documentation](https://inference.roboflow.com)\n- [Roboflow Getting Started guide](https://blog.roboflow.com/getting-started-with-roboflow/)\n- [How to Build a Semantic Image Search Engine with Supabase and OpenAI CLIP](https://blog.roboflow.com/how-to-use-semantic-search-supabase-openai-clip/)",
      "metadata": {
        "ogUrl": "https://supabase.com/docs/guides/ai/integrations/roboflow",
        "title": "Roboflow | Supabase Docs",
        "robots": "index, follow",
        "ogImage": "https://obuldanrptloktxcffvn.supabase.co/functions/v1/og-images?site=docs&type=ai&title=Roboflow&description=undefined",
        "ogTitle": "Roboflow | Supabase Docs",
        "language": "en",
        "sourceURL": "https://supabase.com/docs/guides/ai/integrations/roboflow",
        "description": "Learn how to integrate Supabase with Roboflow, a tool for running fine-tuned and foundation vision models.",
        "modifiedTime": "2024-09-06T19:20:40.691Z",
        "ogDescription": "Learn how to integrate Supabase with Roboflow, a tool for running fine-tuned and foundation vision models.",
        "publishedTime": "2024-09-06T19:20:40.691Z",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "AI & Vectors\n\n# Learn how to integrate Supabase with LlamaIndex, a data framework for your LLM applications.\n\n## Learn how to integrate Supabase with LlamaIndex, a data framework for your LLM applications.\n\n* * *\n\nThis guide will walk you through a basic example using the LlamaIndex [SupabaseVectorStore](https://github.com/supabase/supabase/blob/master/examples/ai/llamaindex/llamaindex.ipynb).\n\n## Project setup [\\#](\\#project-setup)\n\nLet's create a new Postgres database. This is as simple as starting a new Project in Supabase:\n\n1. [Create a new project](https://database.new/) in the Supabase dashboard.\n2. Enter your project details. Remember to store your password somewhere safe.\n\nYour database will be available in less than a minute.\n\n**Finding your credentials:**\n\nYou can find your project credentials inside the project [settings](https://supabase.com/dashboard/project/_/settings/), including:\n\n- [Database credentials](https://supabase.com/dashboard/project/_/settings/database): connection strings and connection pooler details.\n- [API credentials](https://supabase.com/dashboard/project/_/settings/database): your serverless API URL and `anon` / `service_role` keys.\n\n## Launching a notebook [\\#](\\#launching-a-notebook)\n\nLaunch our [LlamaIndex](https://github.com/supabase/supabase/blob/master/examples/ai/llamaindex/llamaindex.ipynb) notebook in Colab:\n\n[![](https://supabase.com/docs/img/ai/colab-badge.svg)](https://colab.research.google.com/github/supabase/supabase/blob/master/examples/ai/llamaindex/llamaindex.ipynb)\n\nAt the top of the notebook, you'll see a button `Copy to Drive`. Click this button to copy the notebook to your Google Drive.\n\n## Fill in your OpenAI credentials [\\#](\\#fill-in-your-openai-credentials)\n\nInside the Notebook, add your `OPENAI_API_KEY` key. Find the cell which contains this code:\n\n`\n_10\nimport os\n_10\nos.environ['OPENAI_API_KEY'] = \"[your_openai_api_key]\"\n`\n\n## Connecting to your database [\\#](\\#connecting-to-your-database)\n\nInside the Notebook, find the cell which specifies the `DB_CONNECTION`. It will contain some code like this:\n\n`\n_10\nDB_CONNECTION = \"postgresql://<user>:<password>@<host>:<port>/<db_name>\"\n_10\n_10\n# create vector store client\n_10\nvx = vecs.create_client(DB_CONNECTION)\n`\n\nReplace the `DB_CONNECTION` with your own connection string for your database. You can find the Postgres connection string in the [Database Settings](https://supabase.com/dashboard/project/_/settings/database) of your Supabase project.\n\nSQLAlchemy requires the connection string to start with `postgresql://` (instead of `postgres://`). Don't forget to rename this after copying the string from the dashboard.\n\nYou must use the \"connection pooling\" string (domain ending in `*.pooler.supabase.com`) with Google Colab since Colab does not support IPv6.\n\n## Stepping through the notebook [\\#](\\#stepping-through-the-notebook)\n\nNow all that's left is to step through the notebook. You can do this by clicking the \"execute\" button ( `ctrl+enter`) at the top left of each code cell. The notebook guides you through the process of creating a collection, adding data to it, and querying it.\n\nYou can view the inserted items in the [Table Editor](https://supabase.com/dashboard/project/_/editor/), by selecting the `vecs` schema from the schema dropdown.\n\n![Colab documents](https://supabase.com/docs/img/ai/google-colab/colab-documents.png)\n\n## Resources [\\#](\\#resources)\n\n- Visit the LlamaIndex + SupabaseVectorStore [docs](https://gpt-index.readthedocs.io/en/latest/examples/vector_stores/SupabaseVectorIndexDemo.html)\n- Visit the official LlamaIndex [repo](https://github.com/jerryjliu/llama_index/)",
      "metadata": {
        "ogUrl": "https://supabase.com/docs/guides/ai/integrations/llamaindex",
        "title": "Learn how to integrate Supabase with LlamaIndex, a data framework for your LLM applications. | Supabase Docs",
        "robots": "index, follow",
        "ogImage": "https://obuldanrptloktxcffvn.supabase.co/functions/v1/og-images?site=docs&type=ai&title=Learn%20how%20to%20integrate%20Supabase%20with%20LlamaIndex%2C%20a%20data%20framework%20for%20your%20LLM%20applications.&description=undefined",
        "ogTitle": "Learn how to integrate Supabase with LlamaIndex, a data framework for your LLM applications. | Supabase Docs",
        "language": "en",
        "sourceURL": "https://supabase.com/docs/guides/ai/integrations/llamaindex",
        "description": "Learn how to integrate Supabase with LlamaIndex, a data framework for your LLM applications.",
        "modifiedTime": "2024-09-06T19:20:41.221Z",
        "ogDescription": "Learn how to integrate Supabase with LlamaIndex, a data framework for your LLM applications.",
        "publishedTime": "2024-09-06T19:20:41.221Z",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "AI & Vectors\n\n# Vector search with Next.js and OpenAI\n\n## Learn how to build a ChatGPT-style doc search powered by Next.js, OpenAI, and Supabase.\n\n* * *\n\nWhile our [Headless Vector search](/docs/guides/ai/examples/headless-vector-search) provides a toolkit for generative Q&A, in this tutorial we'll go more in-depth, build a custom ChatGPT-like search experience from the ground-up using Next.js. You will:\n\n1. Convert your markdown into embeddings using OpenAI.\n2. Store you embeddings in Postgres using pgvector.\n3. Deploy a function for answering your users' questions.\n\nYou can read our [Supabase Clippy](https://supabase.com/blog/chatgpt-supabase-docs) blog post for a full example.\n\nWe assume that you have a Next.js project with a collection of `.mdx` files nested inside your `pages` directory. We will start developing locally with the Supabase CLI and then push our local database changes to our hosted Supabase project. You can find the [full Next.js example on GitHub](https://github.com/supabase-community/nextjs-openai-doc-search).\n\n## Create a project [\\#](\\#create-a-project)\n\n1. [Create a new project](https://supabase.com/dashboard) in the Supabase Dashboard.\n2. Enter your project details.\n3. Wait for the new database to launch.\n\n## Prepare the database [\\#](\\#prepare-the-database)\n\nLet's prepare the database schema. We can use the \"OpenAI Vector Search\" quickstart in the [SQL Editor](https://supabase.com/dashboard/project/_/sql), or you can copy/paste the SQL below and run it yourself.\n\nDashboardSQL\n\n1. Go to the [SQL Editor](https://supabase.com/dashboard/project/_/sql) page in the Dashboard.\n2. Click **OpenAI Vector Search**.\n3. Click **Run**.\n\n## Pre-process the knowledge base at build time [\\#](\\#pre-process-the-knowledge-base-at-build-time)\n\nWith our database set up, we need to process and store all `.mdx` files in the `pages` directory. You can find the full script [here](https://github.com/supabase-community/nextjs-openai-doc-search/blob/main/lib/generate-embeddings.ts), or follow the steps below:\n\n1\n\n### Generate Embeddings\n\nCreate a new file `lib/generate-embeddings.ts` and copy the code over from [GitHub](https://github.com/supabase-community/nextjs-openai-doc-search/blob/main/lib/generate-embeddings.ts).\n\n`\n1\ncurl \\\n2\nhttps://raw.githubusercontent.com/supabase-community/nextjs-openai-doc-search/main/lib/generate-embeddings.ts \\\n3\n-o \"lib/generate-embeddings.ts\"\n`\n\n2\n\n### Set up environment variables\n\nWe need some environment variables to run the script. Add them to your `.env` file and make sure your `.env` file is not committed to source control!\nYou can get your local Supabase credentials by running `supabase status`.\n\n`\n1\nNEXT_PUBLIC_SUPABASE_URL=\n2\nNEXT_PUBLIC_SUPABASE_ANON_KEY=\n3\nSUPABASE_SERVICE_ROLE_KEY=\n4\n5\n# Get your key at https://platform.openai.com/account/api-keys\n6\nOPENAI_API_KEY=\n`\n\n3\n\n### Run script at build time\n\nInclude the script in your `package.json` script commands to enable Vercel to automaticall run it at build time.\n\n`\n1\n\"scripts\": {\n2\n\"dev\": \"next dev\",\n3\n\"build\": \"pnpm run embeddings && next build\",\n4\n\"start\": \"next start\",\n5\n\"embeddings\": \"tsx lib/generate-embeddings.ts\"\n6\n},\n`\n\n## Create text completion with OpenAI API [\\#](\\#create-text-completion-with-openai-api)\n\nAnytime a user asks a question, we need to create an embedding for their question, perform a similarity search, and then send a text completion request to the OpenAI API with the query and then context content merged together into a prompt.\n\nAll of this is glued together in a [Vercel Edge Function](https://vercel.com/docs/concepts/functions/edge-functions), the code for which can be found on [GitHub](https://github.com/supabase-community/nextjs-openai-doc-search/blob/main/pages/api/vector-search.ts).\n\n1\n\n### Create Embedding for Question\n\nIn order to perform similarity search we need to turn the question into an embedding.\n\n``\n1\nconst embeddingResponse = await fetch('https://api.openai.com/v1/embeddings', {\n2\nmethod: 'POST',\n3\nheaders: {\n4\n    Authorization: `Bearer ${openAiKey}`,\n5\n    'Content-Type': 'application/json',\n6\n},\n7\nbody: JSON.stringify({\n8\n    model: 'text-embedding-ada-002',\n9\n    input: sanitizedQuery.replaceAll('\\n', ' '),\n10\n}),\n11\n})\n12\n13\nif (embeddingResponse.status !== 200) {\n14\nthrow new ApplicationError('Failed to create embedding for question', embeddingResponse)\n15\n}\n16\n17\nconst {\n18\ndata: [{ embedding }],\n19\n} = await embeddingResponse.json()\n``\n\n2\n\n### Perform similarity search\n\nUsing the `embeddingResponse` we can now perform similarity search by performing an remote procedure call (RPC) to the database function we created earlier.\n\n`\n1\nconst { error: matchError, data: pageSections } = await supabaseClient.rpc(\n2\n'match_page_sections',\n3\n{\n4\n    embedding,\n5\n    match_threshold: 0.78,\n6\n    match_count: 10,\n7\n    min_content_length: 50,\n8\n}\n9\n)\n`\n\n3\n\n### Perform text completion request\n\nWith the relevant content for the user's question identified, we can now build the prompt and make a text completion request via the OpenAI API.\n\nIf successful, the OpenAI API will respond with a `text/event-stream` response that we can simply forward to the client where we'll process the event stream to smoothly print the answer to the user.\n\n``\n1\nconst prompt = codeBlock`\n2\n${oneLine`\n3\n    You are a very enthusiastic Supabase representative who loves\n4\n    to help people! Given the following sections from the Supabase\n5\n    documentation, answer the question using only that information,\n6\n    outputted in markdown format. If you are unsure and the answer\n7\n    is not explicitly written in the documentation, say\n8\n    \"Sorry, I don't know how to help with that.\"\n9\n`}\n10\n11\nContext sections:\n12\n${contextText}\n13\n14\nQuestion: \"\"\"\n15\n${sanitizedQuery}\n16\n\"\"\"\n17\n18\nAnswer as markdown (including related code snippets if available):\n19\n`\n20\n21\nconst completionOptions: CreateCompletionRequest = {\n22\nmodel: 'gpt-3.5-turbo-instruct',\n23\nprompt,\n24\nmax_tokens: 512,\n25\ntemperature: 0,\n26\nstream: true,\n27\n}\n28\n29\nconst response = await fetch('https://api.openai.com/v1/completions', {\n30\nmethod: 'POST',\n31\nheaders: {\n32\n    Authorization: `Bearer ${openAiKey}`,\n33\n    'Content-Type': 'application/json',\n34\n},\n35\nbody: JSON.stringify(completionOptions),\n36\n})\n37\n38\nif (!response.ok) {\n39\nconst error = await response.json()\n40\nthrow new ApplicationError('Failed to generate completion', error)\n41\n}\n42\n43\n// Proxy the streamed SSE response from OpenAI\n44\nreturn new Response(response.body, {\n45\nheaders: {\n46\n    'Content-Type': 'text/event-stream',\n47\n},\n48\n})\n``\n\n## Display the answer on the frontend [\\#](\\#display-the-answer-on-the-frontend)\n\nIn a last step, we need to process the event stream from the OpenAI API and print the answer to the user. The full code for this can be found on [GitHub](https://github.com/supabase-community/nextjs-openai-doc-search/blob/main/components/SearchDialog.tsx).\n\n``\n1\nconst handleConfirm = React.useCallback(\n2\nasync (query: string) => {\n3\n    setAnswer(undefined)\n4\n    setQuestion(query)\n5\n    setSearch('')\n6\n    dispatchPromptData({ index: promptIndex, answer: undefined, query })\n7\n    setHasError(false)\n8\n    setIsLoading(true)\n9\n10\n    const eventSource = new SSE(`api/vector-search`, {\n11\n      headers: {\n12\n        apikey: process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY ?? '',\n13\n        Authorization: `Bearer ${process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY}`,\n14\n        'Content-Type': 'application/json',\n15\n      },\n16\n      payload: JSON.stringify({ query }),\n17\n    })\n18\n19\n    function handleError<T>(err: T) {\n20\n      setIsLoading(false)\n21\n      setHasError(true)\n22\n      console.error(err)\n23\n    }\n24\n25\n    eventSource.addEventListener('error', handleError)\n26\n    eventSource.addEventListener('message', (e: any) => {\n27\n      try {\n28\n        setIsLoading(false)\n29\n30\n        if (e.data === '[DONE]') {\n31\n          setPromptIndex((x) => {\n32\n            return x + 1\n33\n          })\n34\n          return\n35\n        }\n36\n37\n        const completionResponse: CreateCompletionResponse = JSON.parse(e.data)\n38\n        const text = completionResponse.choices[0].text\n39\n40\n        setAnswer((answer) => {\n41\n          const currentAnswer = answer ?? ''\n42\n43\n          dispatchPromptData({\n44\n            index: promptIndex,\n45\n            answer: currentAnswer + text,\n46\n          })\n47\n48\n          return (answer ?? '') + text\n49\n        })\n50\n      } catch (err) {\n51\n        handleError(err)\n52\n      }\n53\n    })\n54\n55\n    eventSource.stream()\n56\n57\n    eventSourceRef.current = eventSource\n58\n59\n    setIsLoading(true)\n60\n},\n61\n[promptIndex, promptData]\n62\n)\n``\n\n## Learn more [\\#](\\#learn-more)\n\nWant to learn more about the awesome tech that is powering this?\n\n- Read about how we built [ChatGPT for the Supabase Docs](https://supabase.com/blog/chatgpt-supabase-docs).\n- Read the pgvector Docs for [Embeddings and vector similarity](https://supabase.com/docs/guides/database/extensions/pgvector)\n- Watch Greg's video for a full breakdown:\n\nWatch video guide\n\n![Video guide preview](https://supabase.com/docs/_next/image?url=http%3A%2F%2Fimg.youtube.com%2Fvi%2FxmfNUCjszh4%2F0.jpg&w=3840&q=75&dpl=dpl_GiCDf4oknfdUcgmXNidH7itZWLva)\n\n### Is this helpful?\n\nYesNo\n\nThanks for your feedback!\n\nOn this page\n\n- [Create a project](#create-a-project)\n- [Prepare the database](#prepare-the-database)\n- [Pre-process the knowledge base at build time](#pre-process-the-knowledge-base-at-build-time)\n- [Create text completion with OpenAI API](#create-text-completion-with-openai-api)\n- [Display the answer on the frontend](#display-the-answer-on-the-frontend)\n- [Learn more](#learn-more)\n\n1. We only collect analytics essential to ensuring smooth operation of our services. [Learn more](https://supabase.com/privacy)\n\n\n\n\n\n   AcceptOpt out[Learn more](https://supabase.com/privacy)",
      "metadata": {
        "ogUrl": "https://supabase.com/docs/guides/ai/examples/nextjs-vector-search",
        "title": "Vector search with Next.js and OpenAI | Supabase Docs",
        "robots": "index, follow",
        "ogImage": "https://obuldanrptloktxcffvn.supabase.co/functions/v1/og-images?site=docs&type=ai&title=Vector%20search%20with%20Next.js%20and%20OpenAI&description=undefined",
        "ogTitle": "Vector search with Next.js and OpenAI | Supabase Docs",
        "language": "en",
        "sourceURL": "https://supabase.com/docs/guides/ai/examples/nextjs-vector-search",
        "description": "Learn how to build a ChatGPT-style doc search powered by Next.js, OpenAI, and Supabase.",
        "modifiedTime": "2024-09-06T19:20:40.691Z",
        "ogDescription": "Learn how to build a ChatGPT-style doc search powered by Next.js, OpenAI, and Supabase.",
        "publishedTime": "2024-09-06T19:20:40.691Z",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "AI & Vectors\n\n# Python client\n\n## Manage unstructured vector stores in PostgreSQL.\n\n* * *\n\nSupabase provides a Python client called [`vecs`](https://github.com/supabase/vecs) for managing unstructured vector stores. This client provides a set of useful tools for creating and querying collections in PostgreSQL using the [pgvector](/docs/guides/database/extensions/pgvector) extension.\n\n## Quick start [\\#](\\#quick-start)\n\nLet's see how Vecs works using a local database. Make sure you have the Supabase CLI [installed](/docs/guides/cli#installation) on your machine.\n\n### Initialize your project [\\#](\\#initialize-your-project)\n\nStart a local Postgres instance in any folder using the `init` and `start` commands. Make sure you have Docker running!\n\n`\n_10\n# Initialize your project\n_10\nsupabase init\n_10\n_10\n# Start Postgres\n_10\nsupabase start\n`\n\n### Create a collection [\\#](\\#create-a-collection)\n\nInside a Python shell, run the following commands to create a new collection called \"docs\", with 3 dimensions.\n\n`\n_10\nimport vecs\n_10\n_10\n# create vector store client\n_10\nvx = vecs.create_client(\"postgresql://postgres:postgres@localhost:54322/postgres\")\n_10\n_10\n# create a collection of vectors with 3 dimensions\n_10\ndocs = vx.get_or_create_collection(name=\"docs\", dimension=3)\n`\n\n### Add embeddings [\\#](\\#add-embeddings)\n\nNow we can insert some embeddings into our \"docs\" collection using the `upsert()` command:\n\n`\n_13\nimport vecs\n_13\n_13\n# create vector store client\n_13\ndocs = vecs.get_or_create_collection(name=\"docs\", dimension=3)\n_13\n_13\n# a collection of vectors with 3 dimensions\n_13\nvectors=[\\\n_13\\\n(\"vec0\", [0.1, 0.2, 0.3], {\"year\": 1973}),\\\n_13\\\n(\"vec1\", [0.7, 0.8, 0.9], {\"year\": 2012})\\\n_13\\\n]\n_13\n_13\n# insert our vectors\n_13\ndocs.upsert(vectors=vectors)\n`\n\n### Query the collection [\\#](\\#query-the-collection)\n\nYou can now query the collection to retrieve a relevant match:\n\n`\n_10\nimport vecs\n_10\n_10\ndocs = vecs.get_or_create_collection(name=\"docs\", dimension=3)\n_10\n_10\n# query the collection filtering metadata for \"year\" = 2012\n_10\ndocs.query(\n_10\n    data=[0.4,0.5,0.6],      # required\n_10\n    limit=1,                         # number of records to return\n_10\n    filters={\"year\": {\"$eq\": 2012}}, # metadata filters\n_10\n)\n`\n\n## Deep dive [\\#](\\#deep-dive)\n\nFor a more in-depth guide on `vecs` collections, see [API](/docs/guides/ai/python/api).\n\n## Resources [\\#](\\#resources)\n\n- Official Vecs Documentation: [https://supabase.github.io/vecs/api](https://supabase.github.io/vecs/api)\n- Source Code: [https://github.com/supabase/vecs](https://github.com/supabase/vecs)",
      "metadata": {
        "ogUrl": "https://supabase.com/docs/guides/ai/vecs-python-client",
        "title": "Python client | Supabase Docs",
        "robots": "index, follow",
        "ogImage": "https://obuldanrptloktxcffvn.supabase.co/functions/v1/og-images?site=docs&type=ai&title=Python%20client&description=undefined",
        "ogTitle": "Python client | Supabase Docs",
        "language": "en",
        "sourceURL": "https://supabase.com/docs/guides/ai/vecs-python-client",
        "description": "Manage unstructured vector stores in PostgreSQL.",
        "modifiedTime": "2024-09-06T19:20:40.691Z",
        "ogDescription": "Manage unstructured vector stores in PostgreSQL.",
        "publishedTime": "2024-09-06T19:20:40.691Z",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "AI & Vectors\n\n# AI & Vectors\n\n## The best vector database is the database you already have.\n\n* * *\n\nSupabase provides an open source toolkit for developing AI applications using Postgres and pgvector. Use the Supabase client libraries to store, index, and query your vector embeddings at scale.\n\nThe toolkit includes:\n\n- A [vector store](/docs/guides/ai/vector-columns) and embeddings support using Postgres and pgvector.\n- A [Python client](/docs/guides/ai/vecs-python-client) for managing unstructured embeddings.\n- An [embedding generation](/docs/guides/ai/quickstarts/generate-text-embeddings) process using open source models directly in Edge Functions.\n- [Database migrations](/docs/guides/ai/examples/headless-vector-search#prepare-your-database) for managing structured embeddings.\n- Integrations with all popular AI providers, such as [OpenAI](/docs/guides/ai/examples/openai), [Hugging Face](/docs/guides/ai/hugging-face), [LangChain](/docs/guides/ai/langchain), and more.\n\nYou can use Supabase to build different types of search features for your app, including:\n\n- [Semantic search](/docs/guides/ai/semantic-search): search by meaning rather than exact keywords\n- [Keyword search](/docs/guides/ai/keyword-search): search by words or phrases\n- [Hybrid search](/docs/guides/ai/hybrid-search): combine semantic search with keyword search\n\n## Examples [\\#](\\#examples)\n\nCheck out all of the AI [templates and examples](https://github.com/supabase/supabase/tree/master/examples/ai) in our GitHub repository.\n\n[![Headless Vector Search](https://supabase.com/docs/img/icons/github-icon-light.svg)\\\\\n\\\\\nHeadless Vector Search\\\\\n\\\\\nA toolkit to perform vector similarity search on your knowledge base embeddings.](/docs/guides/ai/examples/headless-vector-search)\n\n[![Image Search with OpenAI CLIP](https://supabase.com/docs/img/icons/github-icon-light.svg)\\\\\n\\\\\nImage Search with OpenAI CLIP\\\\\n\\\\\nImplement image search with the OpenAI CLIP Model and Supabase Vector.](/docs/guides/ai/examples/image-search-openai-clip)\n\n[![Hugging Face inference](https://supabase.com/docs/img/icons/github-icon-light.svg)\\\\\n\\\\\nHugging Face inference\\\\\n\\\\\nGenerate image captions using Hugging Face.](/docs/guides/ai/examples/huggingface-image-captioning)\n\n[![OpenAI completions](https://supabase.com/docs/img/icons/github-icon-light.svg)\\\\\n\\\\\nOpenAI completions\\\\\n\\\\\nGenerate GPT text completions using OpenAI in Edge Functions.](/docs/guides/ai/examples/openai)\n\n[![Building ChatGPT Plugins](https://supabase.com/docs/img/icons/github-icon-light.svg)\\\\\n\\\\\nBuilding ChatGPT Plugins\\\\\n\\\\\nUse Supabase as a Retrieval Store for your ChatGPT plugin.](/docs/guides/ai/examples/building-chatgpt-plugins)\n\n[![Vector search with Next.js and OpenAI](https://supabase.com/docs/img/icons/github-icon-light.svg)\\\\\n\\\\\nVector search with Next.js and OpenAI\\\\\n\\\\\nLearn how to build a ChatGPT-style doc search powered by Next.js, OpenAI, and Supabase.](/docs/guides/ai/examples/nextjs-vector-search)\n\n## Integrations [\\#](\\#integrations)\n\n[OpenAI\\\\\n\\\\\nOpenAI is an AI research and deployment company. Supabase provides a simple way to use OpenAI in your applications.](/docs/guides/ai/examples/building-chatgpt-plugins)\n\n[Amazon Bedrock\\\\\n\\\\\nA fully managed service that offers a choice of high-performing foundation models from leading AI companies.](/docs/guides/ai/integrations/amazon-bedrock)\n\n[Hugging Face\\\\\n\\\\\nHugging Face is an open-source provider of NLP technologies. Supabase provides a simple way to use Hugging Face's models in your applications.](/docs/guides/ai/hugging-face)\n\n[LangChain\\\\\n\\\\\nLangChain is a language-agnostic, open-source, and self-hosted API for text translation, summarization, and sentiment analysis.](/docs/guides/ai/langchain)\n\n[LlamaIndex\\\\\n\\\\\nLlamaIndex is a data framework for your LLM applications.](/docs/guides/ai/integrations/llamaindex)\n\n## Case studies [\\#](\\#case-studies)\n\n[Berri AI Boosts Productivity by Migrating from AWS RDS to Supabase with pgvector\\\\\n\\\\\nLearn how Berri AI overcame challenges with self-hosting their vector database on AWS RDS and successfully migrated to Supabase.](https://supabase.com/customers/berriai)\n\n[Mendable switches from Pinecone to Supabase for PostgreSQL vector embeddings\\\\\n\\\\\nHow Mendable boosts efficiency and accuracy of chat powered search for documentation using Supabase with pgvector](https://supabase.com/customers/mendableai)\n\n[Markprompt: GDPR-Compliant AI Chatbots for Docs and Websites\\\\\n\\\\\nAI-powered chatbot platform, Markprompt, empowers developers to deliver efficient and GDPR-compliant prompt experiences on top of their content, by leveraging Supabase's secure and privacy-focused database and authentication solutions](https://supabase.com/customers/markprompt)",
      "metadata": {
        "ogUrl": "https://supabase.com/docs/guides/ai",
        "title": "AI & Vectors | Supabase Docs",
        "robots": "index, follow",
        "ogImage": "https://obuldanrptloktxcffvn.supabase.co/functions/v1/og-images?site=docs&type=ai&title=AI%20%26%20Vectors&description=The%20best%20vector%20database%20is%20the%20database%20you%20already%20have.",
        "ogTitle": "AI & Vectors | Supabase Docs",
        "language": "en",
        "sourceURL": "https://supabase.com/docs/guides/ai",
        "description": "The best vector database is the database you already have.",
        "modifiedTime": "2024-09-06T19:20:40.301Z",
        "ogDescription": "The best vector database is the database you already have.",
        "publishedTime": "2024-09-06T19:20:40.300Z",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "AI & Vectors\n\n# Structured and Unstructured\n\n## Supabase is flexible enough to associate structured and unstructured metadata with embeddings.\n\n* * *\n\nMost vector stores treat metadata associated with embeddings like NoSQL, unstructured data. Supabase is flexible enough to store unstructured and structured metadata.\n\n## Structured [\\#](\\#structured)\n\n`\n_11\ncreate table docs (\n_11\nid uuid primary key,\n_11\nembedding vector(3),\n_11\ncontent text,\n_11\nurl string\n_11\n);\n_11\n_11\ninsert into docs\n_11\n(id, embedding, content, url)\n_11\nvalues\n_11\n('79409372-7556-4ccc-ab8f-5786a6cfa4f7', array[0.1, 0.2, 0.3], 'Hello world', '/hello-world');\n`\n\nNotice that we've associated two pieces of metadata, `content` and `url`, with the embedding. Those fields can be filtered, constrained, indexed, and generally operated on using the full power of SQL. Structured metadata fits naturally with a traditional Supabase application, and can be managed via database [migrations](/docs/guides/getting-started/local-development#database-migrations).\n\n## Unstructured [\\#](\\#unstructured)\n\n`\n_14\ncreate table docs (\n_14\nid uuid primary key,\n_14\nembedding vector(3),\n_14\nmeta jsonb\n_14\n);\n_14\n_14\ninsert into docs\n_14\n(id, embedding, meta)\n_14\nvalues\n_14\n(\n_14\n    '79409372-7556-4ccc-ab8f-5786a6cfa4f7',\n_14\n    array[0.1, 0.2, 0.3],\n_14\n    '{\"content\": \"Hello world\", \"url\": \"/hello-world\"}'\n_14\n);\n`\n\nAn unstructured approach does not specify the metadata fields that are expected. It stores all metadata in a flexible `json`/ `jsonb` column. The tradeoff is that the querying/filtering capabilities of a schemaless data type are less flexible than when each field has a dedicated column. It also pushes the burden of metadata data integrity onto application code, which is more error prone than enforcing constraints in the database.\n\nThe unstructured approach is recommended:\n\n- for ephemeral/interactive workloads e.g. data science or scientific research\n- when metadata fields are user-defined or unknown\n- during rapid prototyping\n\nClient libraries like python's [vecs](https://github.com/supabase/vecs) use this structure. For example, running:\n\n`\n_10\n#!/usr/bin/env python3\n_10\nimport vecs\n_10\n_10\ndocs = vx.get_or_create_collection(name=\"docs\", dimension=1536)\n_10\n_10\ndocs.upsert(vectors=[\\\n_10\\\n('79409372-7556-4ccc-ab8f-5786a6cfa4f7', [100, 200, 300], { url: '/hello-world' })\\\n_10\\\n])\n`\n\nautomatically creates the unstructured SQL table during the call to `get_or_create_collection`.\n\nNote that when working with client libraries that emit SQL DDL, like `create table ...`, you should add that SQL to your migrations when moving to production to maintain a single source of truth for your database's schema.\n\n## Hybrid [\\#](\\#hybrid)\n\nThe structured metadata style is recommended when the fields being tracked are known in advance. If you have a combination of known and unknown metadata fields, you can accommodate the unknown fields by adding a `json`/ `jsonb` column to the table. In that situation, known fields should continue to use dedicated columns for best query performance and throughput.\n\n`\n_18\ncreate table docs (\n_18\nid uuid primary key,\n_18\nembedding vector(3),\n_18\ncontent text,\n_18\nurl string,\n_18\nmeta jsonb\n_18\n);\n_18\n_18\ninsert into docs\n_18\n(id, embedding, content, url, meta)\n_18\nvalues\n_18\n(\n_18\n    '79409372-7556-4ccc-ab8f-5786a6cfa4f7',\n_18\n    array[0.1, 0.2, 0.3],\n_18\n    'Hello world',\n_18\n    '/hello-world',\n_18\n    '{\"key\": \"value\"}'\n_18\n);\n`\n\n## Choosing the right model [\\#](\\#choosing-the-right-model)\n\nBoth approaches create a table where you can store your embeddings and some metadata. You should choose the best approach for your use-case. In summary:\n\n- Structured metadata is best when fields are known in advance or query patterns are predictable e.g. a production Supabase application\n- Unstructured metadata is best when fields are unknown/user-defined or when working with data interactively e.g. exploratory research\n\nBoth approaches are valid, and the one you should choose depends on your use-case.",
      "metadata": {
        "ogUrl": "https://supabase.com/docs/guides/ai/structured-unstructured",
        "title": "Structured and Unstructured | Supabase Docs",
        "robots": "index, follow",
        "ogImage": "https://obuldanrptloktxcffvn.supabase.co/functions/v1/og-images?site=docs&type=ai&title=Structured%20and%20Unstructured&description=Supabase%20is%20flexible%20enough%20to%20associate%20structured%20and%20unstructured%20metadata%20with%20embeddings.",
        "ogTitle": "Structured and Unstructured | Supabase Docs",
        "language": "en",
        "sourceURL": "https://supabase.com/docs/guides/ai/structured-unstructured",
        "description": "Supabase is flexible enough to associate structured and unstructured metadata with embeddings.",
        "modifiedTime": "2024-09-06T19:20:41.221Z",
        "ogDescription": "Supabase is flexible enough to associate structured and unstructured metadata with embeddings.",
        "publishedTime": "2024-09-06T19:20:41.221Z",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "AI & Vectors\n\n# Generating OpenAI GPT3 completions\n\n## Generate GPT text completions using OpenAI and Supabase Edge Functions.\n\n* * *\n\nOpenAI provides a [completions API](https://platform.openai.com/docs/api-reference/completions) that allows you to use their generative GPT models in your own applications.\n\nOpenAI's API is intended to be used from the server-side. Supabase offers Edge Functions to make it easy to interact with third party APIs like OpenAI.\n\n## Setup Supabase project [\\#](\\#setup-supabase-project)\n\nIf you haven't already, [install the Supabase CLI](/docs/guides/cli) and initialize your project:\n\n`\n1\nsupabase init\n`\n\n## Create edge function [\\#](\\#create-edge-function)\n\nScaffold a new edge function called `openai` by running:\n\n`\n1\nsupabase functions new openai\n`\n\nA new edge function will now exist under `./supabase/functions/openai/index.ts`.\n\nWe'll design the function to take your user's query (via POST request) and forward it to OpenAI's API.\n\nindex.ts\n\n`\n1\nimport OpenAI from 'https://deno.land/x/openai@v4.24.0/mod.ts'\n2\n3\nDeno.serve(async (req) => {\n4\nconst { query } = await req.json()\n5\nconst apiKey = Deno.env.get('OPENAI_API_KEY')\n6\nconst openai = new OpenAI({\n7\n    apiKey: apiKey,\n8\n})\n9\n10\n// Documentation here: https://github.com/openai/openai-node\n11\nconst chatCompletion = await openai.chat.completions.create({\n12\n    messages: [{ role: 'user', content: query }],\n13\n    // Choose model from here: https://platform.openai.com/docs/models\n14\n    model: 'gpt-3.5-turbo',\n15\n    stream: false,\n16\n})\n17\n18\nconst reply = chatCompletion.choices[0].message.content\n19\n20\nreturn new Response(reply, {\n21\n    headers: { 'Content-Type': 'text/plain' },\n22\n})\n23\n})\n`\n\nNote that we are setting `stream` to `false` which will wait until the entire response is complete before returning. If you wish to stream GPT's response word-by-word back to your client, set `stream` to `true`.\n\n## Create OpenAI key [\\#](\\#create-openai-key)\n\nYou may have noticed we were passing `OPENAI_API_KEY` in the Authorization header to OpenAI. To generate this key, go to [https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys) and create a new secret key.\n\nAfter getting the key, copy it into a new file called `.env.local` in your `./supabase` folder:\n\n`\n1\nOPENAI_API_KEY=your-key-here\n`\n\n## Run locally [\\#](\\#run-locally)\n\nServe the edge function locally by running:\n\n`\n1\nsupabase functions serve --env-file ./supabase/.env.local --no-verify-jwt\n`\n\nNotice how we are passing in the `.env.local` file.\n\nUse cURL or Postman to make a POST request to [http://localhost:54321/functions/v1/openai](http://localhost:54321/functions/v1/openai).\n\n`\n1\ncurl -i --location --request POST http://localhost:54321/functions/v1/openai \\\n2\n  --header 'Content-Type: application/json' \\\n3\n  --data '{\"query\":\"What is Supabase?\"}'\n`\n\nYou should see a GPT response come back from OpenAI!\n\n## Deploy [\\#](\\#deploy)\n\nDeploy your function to the cloud by runnning:\n\n`\n1\nsupabase functions deploy --no-verify-jwt openai\n2\nsupabase secrets set --env-file ./supabase/.env.local\n`\n\n## Go deeper [\\#](\\#go-deeper)\n\nIf you're interesting in learning how to use this to build your own ChatGPT, read [the blog post](/blog/chatgpt-supabase-docs) and check out the video:\n\nWatch video guide\n\n![Video guide preview](https://supabase.com/docs/_next/image?url=http%3A%2F%2Fimg.youtube.com%2Fvi%2F29p8kIqyU_Y%2F0.jpg&w=3840&q=75&dpl=dpl_GiCDf4oknfdUcgmXNidH7itZWLva)\n\n### Is this helpful?\n\nYesNo\n\nThanks for your feedback!\n\nOn this page\n\n- [Setup Supabase project](#setup-supabase-project)\n- [Create edge function](#create-edge-function)\n- [Create OpenAI key](#create-openai-key)\n- [Run locally](#run-locally)\n- [Deploy](#deploy)\n- [Go deeper](#go-deeper)\n\n1. We only collect analytics essential to ensuring smooth operation of our services. [Learn more](https://supabase.com/privacy)\n\n\n\n\n\n   AcceptOpt out[Learn more](https://supabase.com/privacy)",
      "metadata": {
        "ogUrl": "https://supabase.com/docs/guides/ai/examples/openai",
        "title": "Generating OpenAI GPT3 completions | Supabase Docs",
        "robots": "index, follow",
        "ogImage": "https://obuldanrptloktxcffvn.supabase.co/functions/v1/og-images?site=docs&type=ai&title=Generating%20OpenAI%20GPT3%20completions&description=Generate%20GPT%20text%20completions%20using%20OpenAI%20and%20Supabase%20Edge%20Functions.",
        "ogTitle": "Generating OpenAI GPT3 completions | Supabase Docs",
        "language": "en",
        "sourceURL": "https://supabase.com/docs/guides/ai/examples/openai",
        "description": "Generate GPT text completions using OpenAI and Supabase Edge Functions.",
        "modifiedTime": "2024-09-06T19:20:40.301Z",
        "ogDescription": "Generate GPT text completions using OpenAI and Supabase Edge Functions.",
        "publishedTime": "2024-09-06T19:20:40.300Z",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "AI & Vectors\n\n# Building ChatGPT plugins\n\n## Use Supabase as a Retrieval Store for your ChatGPT plugin.\n\n* * *\n\nChatGPT recently released [Plugins](https://openai.com/blog/chatgpt-plugins) which help ChatGPT access up-to-date information, run computations, or use third-party services.\nIf you're building a plugin for ChatGPT, you'll probably want to answer questions from a specific source. We can solve this with \u201cretrieval plugins\u201d, which allow ChatGPT to access information from a database.\n\n## What is ChatGPT Retrieval Plugin? [\\#](\\#what-is-chatgpt-retrieval-plugin)\n\nA [Retrieval Plugin](https://github.com/openai/chatgpt-retrieval-plugin) is a Python project designed to inject external data into a ChatGPT conversation. It does a few things:\n\n1. Turn documents into smaller chunks.\n2. Converts chunks into embeddings using OpenAI's `text-embedding-ada-002` model.\n3. Stores the embeddings into a vector database.\n4. Queries the vector database for relevant documents when a question is asked.\n\nIt allows ChatGPT to dynamically pull relevant information into conversations from your data sources. This could be PDF documents, Confluence, or Notion knowledge bases.\n\n## Example: Chat with Postgres docs [\\#](\\#example-chat-with-postgres-docs)\n\nLet\u2019s build an example where we can \u201cask ChatGPT questions\u201d about the Postgres documentation. Although ChatGPT already knows about the Postgres documentation because it is publicly available, this is a simple example which demonstrates how to work with PDF files.\n\nThis plugin requires several steps:\n\n1. Download all the [Postgres docs as a PDF](https://www.postgresql.org/files/documentation/pdf/15/postgresql-15-US.pdf)\n2. Convert the docs into chunks of embedded text and store them in Supabase\n3. Run our plugin locally so that we can ask questions about the Postgres docs.\n\nWe'll be saving the Postgres documentation in Postgres, and ChatGPT will be retrieving the documentation whenever a user asks a question:\n\n### Step 1: Fork the ChatGPT Retrieval Plugin repository [\\#](\\#step-1-fork-the-chatgpt-retrieval-plugin-repository)\n\nFork the ChatGPT Retrieval Plugin repository to your GitHub account and clone it to your local machine. Read through the `README.md` file to understand the project structure.\n\n### Step 2: Install dependencies [\\#](\\#step-2-install-dependencies)\n\nChoose your desired datastore provider and remove unused dependencies from `pyproject.toml`. For this example, we'll use Supabase. And install dependencies with Poetry:\n\n`\n_10\npoetry install\n`\n\n### Step 3: Create a Supabase project [\\#](\\#step-3-create-a-supabase-project)\n\nCreate a [Supabase project](https://supabase.com/dashboard) and database by following the instructions [here](https://supabase.com/docs/guides/platform). Export the environment variables required for the retrieval plugin to work:\n\n`\n_10\nexport OPENAI_API_KEY=<open_ai_api_key>\n_10\nexport DATASTORE=supabase\n_10\nexport SUPABASE_URL=<supabase_url>\n_10\nexport SUPABASE_SERVICE_ROLE_KEY=<supabase_key>\n`\n\nFor Postgres datastore, you'll need to export these environment variables instead:\n\n`\n_10\nexport OPENAI_API_KEY=<open_ai_api_key>\n_10\nexport DATASTORE=postgres\n_10\nexport PG_HOST=<postgres_host_url>\n_10\nexport PG_PASSWORD=<postgres_password>\n`\n\n### Step 4: Run Postgres locally [\\#](\\#step-4-run-postgres-locally)\n\nTo start quicker you may use Supabase CLI to spin everything up locally as it already includes pgvector from the start. Install `supabase-cli`, go to the `examples/providers` folder in the repo and run:\n\n`\n_10\nsupabase start\n`\n\nThis will pull all docker images and run supabase stack in docker on your local machine. It will also apply all the necessary migrations to set the whole thing up. You can then use your local setup the same way, just export the environment variables and follow to the next steps.\n\nUsing `supabase-cli` is not required and you can use any other docker image or hosted version of PostgresDB that includes `pgvector`. Just make sure you run migrations from `examples/providers/supabase/migrations/20230414142107_init_pg_vector.sql`.\n\n### Step 5: Obtain OpenAI API key [\\#](\\#step-5-obtain-openai-api-key)\n\nTo create embeddings Plugin uses OpenAI API and `text-embedding-ada-002` model. Each time we add some data to our datastore, or try to query relevant information from it, embedding will be created either for inserted data chunk, or for the query itself. To make it work we need to export `OPENAI_API_KEY`. If you already have an account in OpenAI, you just need to go to [User Settings - API keys](https://platform.openai.com/account/api-keys) and Create new secret key.\n\n![OpenAI Secret Keys](https://supabase.com/docs/img/ai/chatgpt-plugins/openai-secret-keys.png)\n\n### Step 6: Run the plugin [\\#](\\#step-6-run-the-plugin)\n\nExecute the following command to run the plugin:\n\n`\n_10\npoetry run dev\n_10\n# output\n_10\nINFO:     Will watch for changes in these directories: ['./chatgpt-retrieval-plugin']\n_10\nINFO:     Uvicorn running on http://localhost:3333 (Press CTRL+C to quit)\n_10\nINFO:     Started reloader process [87843] using WatchFiles\n_10\nINFO:     Started server process [87849]\n_10\nINFO:     Waiting for application startup.\n_10\nINFO:     Application startup complete.\n`\n\nThe plugin will start on your localhost - port `:3333` by default.\n\n### Step 6: Populating data in the datastore [\\#](\\#step-6-populating-data-in-the-datastore)\n\nFor this example, we'll upload Postgres documentation to the datastore. Download the [Postgres documentation](https://www.postgresql.org/files/documentation/pdf/15/postgresql-15-US.pdf) and use the `/upsert-file` endpoint to upload it:\n\n`\n_10\ncurl -X POST -F \\\\\"file=@./postgresql-15-US.pdf\\\\\" <http://localhost:3333/upsert-file>\n`\n\nThe plugin will split your data and documents into smaller chunks automatically. You can view the chunks using the Supabase dashboard or any other SQL client you prefer. For the whole Postgres Documentation I got 7,904 records in my documents table, which is not a lot, but we can try to add index for `embedding` column to speed things up by a little. To do so, you should run the following SQL command:\n\n`\n_10\ncreate index on documents\n_10\nusing hnsw (embedding vector_ip_ops)\n_10\nwith (lists = 10);\n`\n\nThis will create an index for the inner product distance function. Important to note that it is an approximate index. It will change the logic from performing the exact nearest neighbor search to the approximate nearest neighbor search.\n\nWe are using `lists = 10`, because as a general guideline, you should start looking for optimal lists constant value with the formula: `rows / 1000` when you have less than 1 million records in your table.\n\n### Step 7: Using our plugin within ChatGPT [\\#](\\#step-7-using-our-plugin-within-chatgpt)\n\nTo integrate our plugin with ChatGPT, register it in the ChatGPT dashboard. Assuming you have access to ChatGPT Plugins and plugin development, select the Plugins model in a new chat, then choose \"Plugin store\" and \"Develop your own plugin.\" Enter `localhost:3333` into the domain input, and your plugin is now part of ChatGPT.\n\n![ChatGPT Plugin Store](https://supabase.com/docs/img/ai/chatgpt-plugins/chatgpt-plugin-store.png)\n\n![ChatGPT Local Plugin](https://supabase.com/docs/img/ai/chatgpt-plugins/chatgpt-local-plugin.png)\n\nYou can now ask questions about Postgres and receive answers derived from the documentation.\n\nLet's try it out: ask ChatGPT to find out when to use `check` and when to use `using`. You will be able to see what queries were sent to our plugin and what it responded to.\n\n![Ask ChatGPT](https://supabase.com/docs/img/ai/chatgpt-plugins/ask-chatgpt.png)\n\nAnd after ChatGPT receives a response from the plugin it will answer your question with the data from the documentation.\n\n![ChatGPT Reply](https://supabase.com/docs/img/ai/chatgpt-plugins/chatgpt-reply.png)\n\n## Resources [\\#](\\#resources)\n\n- ChatGPT Retrieval Plugin: [github.com/openai/chatgpt-retrieval-plugin](https://github.com/openai/chatgpt-retrieval-plugin)\n- ChatGPT Plugins: [official documentation](https://platform.openai.com/docs/plugins/introduction)",
      "metadata": {
        "ogUrl": "https://supabase.com/docs/guides/ai/examples/building-chatgpt-plugins",
        "title": "Building ChatGPT plugins | Supabase Docs",
        "robots": "index, follow",
        "ogImage": "https://obuldanrptloktxcffvn.supabase.co/functions/v1/og-images?site=docs&type=ai&title=Building%20ChatGPT%20plugins&description=undefined",
        "ogTitle": "Building ChatGPT plugins | Supabase Docs",
        "language": "en",
        "sourceURL": "https://supabase.com/docs/guides/ai/examples/building-chatgpt-plugins",
        "description": "Use Supabase as a Retrieval Store for your ChatGPT plugin.",
        "modifiedTime": "2024-09-06T19:20:41.221Z",
        "ogDescription": "Use Supabase as a Retrieval Store for your ChatGPT plugin.",
        "publishedTime": "2024-09-06T19:20:41.221Z",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "AI & Vectors\n\n# Amazon Bedrock\n\n* * *\n\n[Amazon Bedrock](https://aws.amazon.com/bedrock) is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon. Each model is accessible through a common API which implements a broad set of features to help build generative AI applications with security, privacy, and responsible AI in mind.\n\nThis guide will walk you through an example using Amazon Bedrock SDK with `vecs`. We will create embeddings using the Amazon Titan Embeddings G1 \u2013 Text v1.2 (amazon.titan-embed-text-v1) model, insert these embeddings into a PostgreSQL database using vecs, and then query the collection to find the most similar sentences to a given query sentence.\n\n## Create an Environment [\\#](\\#create-an-environment)\n\nFirst, you need to set up your environment. You will need Python 3.7+ with the `vecs` and `boto3` libraries installed.\n\nYou can install the necessary Python libraries using pip:\n\n`\n_10\npip install vecs boto3\n`\n\nYou'll also need:\n\n- [Credentials to your AWS account](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html)\n- [A Postgres Database with the pgvector extension](hosting.md)\n\n## Create Embeddings [\\#](\\#create-embeddings)\n\nNext, we will use Amazon\u2019s Titan Embedding G1 - Text v1.2 model to create embeddings for a set of sentences.\n\n`\n_34\nimport boto3\n_34\nimport vecs\n_34\nimport json\n_34\n_34\nclient = boto3.client(\n_34\n    'bedrock-runtime',\n_34\n    region_name='us-east-1',\n_34\n\t# Credentials from your AWS account\n_34\n    aws_access_key_id='<replace_your_own_credentials>',\n_34\n    aws_secret_access_key='<replace_your_own_credentials>',\n_34\n    aws_session_token='<replace_your_own_credentials>',\n_34\n)\n_34\n_34\ndataset = [\\\n_34\\\n    \"The cat sat on the mat.\",\\\n_34\\\n    \"The quick brown fox jumps over the lazy dog.\",\\\n_34\\\n    \"Friends, Romans, countrymen, lend me your ears\",\\\n_34\\\n    \"To be or not to be, that is the question.\",\\\n_34\\\n]\n_34\n_34\nembeddings = []\n_34\n_34\nfor sentence in dataset:\n_34\n    # invoke the embeddings model for each sentence\n_34\n    response = client.invoke_model(\n_34\n        body= json.dumps({\"inputText\": sentence}),\n_34\n        modelId= \"amazon.titan-embed-text-v1\",\n_34\n        accept = \"application/json\",\n_34\n        contentType = \"application/json\"\n_34\n    )\n_34\n    # collect the embedding from the response\n_34\n    response_body = json.loads(response[\"body\"].read())\n_34\n    # add the embedding to the embedding list\n_34\n    embeddings.append((sentence, response_body.get(\"embedding\"), {}))\n`\n\n### Store the Embeddings with vecs [\\#](\\#store-the-embeddings-with-vecs)\n\nNow that we have our embeddings, we can insert them into a PostgreSQL database using vecs.\n\n`\n_16\nimport vecs\n_16\n_16\nDB_CONNECTION = \"postgresql://<user>:<password>@<host>:<port>/<db_name>\"\n_16\n_16\n# create vector store client\n_16\nvx = vecs.Client(DB_CONNECTION)\n_16\n_16\n# create a collection named 'sentences' with 1536 dimensional vectors\n_16\n# to match the default dimension of the Titan Embeddings G1 - Text model\n_16\nsentences = vx.get_or_create_collection(name=\"sentences\", dimension=1536)\n_16\n_16\n# upsert the embeddings into the 'sentences' collection\n_16\nsentences.upsert(records=embeddings)\n_16\n_16\n# create an index for the 'sentences' collection\n_16\nsentences.create_index()\n`\n\n### Querying for Most Similar Sentences [\\#](\\#querying-for-most-similar-sentences)\n\nNow, we query the `sentences` collection to find the most similar sentences to a sample query sentence. First need to create an embedding for the query sentence. Next, we query the collection we created earlier to find the most similar sentences.\n\n`\n_27\nquery_sentence = \"A quick animal jumps over a lazy one.\"\n_27\n_27\n# create vector store client\n_27\nvx = vecs.Client(DB_CONNECTION)\n_27\n_27\n# create an embedding for the query sentence\n_27\nresponse = client.invoke_model(\n_27\n        body= json.dumps({\"inputText\": query_sentence}),\n_27\n        modelId= \"amazon.titan-embed-text-v1\",\n_27\n        accept = \"application/json\",\n_27\n        contentType = \"application/json\"\n_27\n    )\n_27\n_27\nresponse_body = json.loads(response[\"body\"].read())\n_27\n_27\nquery_embedding = response_body.get(\"embedding\")\n_27\n_27\n# query the 'sentences' collection for the most similar sentences\n_27\nresults = sentences.query(\n_27\n    data=query_embedding,\n_27\n    limit=3,\n_27\n    include_value = True\n_27\n)\n_27\n_27\n# print the results\n_27\nfor result in results:\n_27\n    print(result)\n`\n\nThis returns the most similar 3 records and their distance to the query vector.\n\n`\n_10\n('The quick brown fox jumps over the lazy dog.', 0.27600620558852)\n_10\n('The cat sat on the mat.', 0.609986272479202)\n_10\n('To be or not to be, that is the question.', 0.744849503688346)\n`\n\n## Resources [\\#](\\#resources)\n\n- [Amazon Bedrock](https://aws.amazon.com/bedrock)\n- [Amazon Titan](https://aws.amazon.com/bedrock/titan)\n- [Semantic Image Search with Amazon Titan](/docs/guides/ai/examples/semantic-image-search-amazon-titan)",
      "metadata": {
        "ogUrl": "https://supabase.com/docs/guides/ai/integrations/amazon-bedrock",
        "title": "Amazon Bedrock | Supabase Docs",
        "robots": "index, follow",
        "ogImage": "https://obuldanrptloktxcffvn.supabase.co/functions/v1/og-images?site=docs&type=ai&title=Amazon%20Bedrock&description=Learn%20how%20to%20integrate%20Supabase%20with%20Amazon%20Bedrock%2C%20a%20fully%20managed%20service%20of%20high-performing%20foundation%20models.",
        "ogTitle": "Amazon Bedrock | Supabase Docs",
        "language": "en",
        "sourceURL": "https://supabase.com/docs/guides/ai/integrations/amazon-bedrock",
        "description": "Learn how to integrate Supabase with Amazon Bedrock, a fully managed service of high-performing foundation models.",
        "modifiedTime": "2024-09-06T19:20:40.301Z",
        "ogDescription": "Learn how to integrate Supabase with Amazon Bedrock, a fully managed service of high-performing foundation models.",
        "publishedTime": "2024-09-06T19:20:40.300Z",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "AI & Vectors\n\n# Generate Embeddings\n\n## Generate text embeddings using Edge Functions.\n\n* * *\n\nThis guide will walk you through how to generate high quality text embeddings in [Edge Functions](/docs/guides/functions) using its built-in AI inference API, so no external API is required.\n\n## Build the Edge Function [\\#](\\#build-the-edge-function)\n\nLet's build an Edge Function that will accept an input string and generate an embedding for it. Edge Functions are server-side TypeScript HTTP endpoints that run on-demand closest to your users.\n\n1\n\n### Set up Supabase locally\n\nMake sure you have the latest version of the [Supabase CLI installed](/docs/guides/cli/getting-started).\n\nInitialize Supabase in the root directory of your app and start your local stack.\n\n`\n_10\nsupabase init\n_10\nsupabase start\n`\n\n2\n\n### Create Edge Function\n\nCreate an Edge Function that we will use to generate embeddings. We'll call this `embed` (you can name this anything you like).\n\nThis will create a new TypeScript file called `index.ts` under `./supabase/functions/embed`.\n\n`\n_10\nsupabase functions new embed\n`\n\n3\n\n### Setup Inference Session\n\nLet's create a new inference session to be used in the lifetime of this function. Multiple requests can use the same inference session.\n\nCurrently, only the `gte-small` ( [https://huggingface.co/Supabase/gte-small](https://huggingface.co/Supabase/gte-small)) text embedding model is supported in Supabase's Edge Runtime.\n\n./supabase/functions/embed/index.ts\n\n`\n_10\nconst session = new Supabase.ai.Session('gte-small');\n`\n\n4\n\n### Implement request handler\n\nModify our request handler to accept an `input` string from the POST request JSON body.\n\nThen generate the embedding by calling `session.run(input)`.\n\n./supabase/functions/embed/index.ts\n\n`\n_16\nDeno.serve(async (req) => {\n_16\n// Extract input string from JSON body\n_16\nconst { input } = await req.json();\n_16\n_16\n// Generate the embedding from the user input\n_16\nconst embedding = await session.run(input, {\n_16\n    mean_pool: true,\n_16\n    normalize: true,\n_16\n});\n_16\n_16\n// Return the embedding\n_16\nreturn new Response(\n_16\n    JSON.stringify({ embedding }),\n_16\n    { headers: { 'Content-Type': 'application/json' } }\n_16\n);\n_16\n});\n`\n\nNote the two options we pass to `session.run()`:\n\n- `mean_pool`: The first option sets `pooling` to `mean`. Pooling referes to how token-level embedding representations are compressed into a single sentence embedding that reflects the meaning of the entire sentence. Average pooling is the most common type of pooling for sentence embeddings.\n- `normalize`: The second option tells to normalize the embedding vector so that it can be used with distance measures like dot product. A normalized vector means its length (magnitude) is 1 - also referred to as a unit vector. A vector is normalized by dividing each element by the vector's length (magnitude), which maintains its direction but changes its length to 1.\n\n5\n\n### Test it!\n\nTo test the Edge Function, first start a local functions server.\n\n`\n_10\nsupabase functions serve\n`\n\nThen in a new shell, create an HTTP request using cURL and pass in your input in the JSON body.\n\n`\n_10\ncurl --request POST 'http://localhost:54321/functions/v1/embed' \\\n_10\n  --header 'Authorization: Bearer ANON_KEY' \\\n_10\n  --header 'Content-Type: application/json' \\\n_10\n  --data '{ \"input\": \"hello world\" }'\n`\n\nBe sure to replace `ANON_KEY` with your project's anonymous key. You can get this key by running `supabase status`.\n\n## Next steps [\\#](\\#next-steps)\n\n- Learn more about [embedding concepts](/docs/guides/ai/concepts)\n- [Store your embeddings](/docs/guides/ai/vector-columns) in a database",
      "metadata": {
        "ogUrl": "https://supabase.com/docs/guides/ai/quickstarts/generate-text-embeddings",
        "title": "Generate Embeddings | Supabase Docs",
        "robots": "index, follow",
        "ogImage": "https://obuldanrptloktxcffvn.supabase.co/functions/v1/og-images?site=docs&type=ai&title=Generate%20Embeddings&description=undefined",
        "ogTitle": "Generate Embeddings | Supabase Docs",
        "language": "en",
        "sourceURL": "https://supabase.com/docs/guides/ai/quickstarts/generate-text-embeddings",
        "description": "Generate text embeddings using Edge Functions.",
        "modifiedTime": "2024-09-06T19:20:40.691Z",
        "ogDescription": "Generate text embeddings using Edge Functions.",
        "publishedTime": "2024-09-06T19:20:40.691Z",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "AI & Vectors\n\n# Semantic search\n\n## Learn how to search by meaning rather than exact keywords.\n\n* * *\n\nSemantic search interprets the meaning behind user queries rather than exact [keywords](/docs/guides/ai/keyword-search). It uses machine learning to capture the intent and context behind the query, handling language nuances like synonyms, phrasing variations, and word relationships.\n\n## When to use semantic search [\\#](\\#when-to-use-semantic-search)\n\nSemantic search is useful in applications where the depth of understanding and context is important for delivering relevant results. A good example is in customer support or knowledge base search engines. Users often phrase their problems or questions in various ways, and a traditional keyword-based search might not always retrieve the most helpful documents. With semantic search, the system can understand the meaning behind the queries and match them with relevant solutions or articles, even if the exact wording differs.\n\nFor instance, a user searching for \"increase text size on display\" might miss articles titled \"How to adjust font size in settings\" in a keyword-based search system. However, a semantic search engine would understand the intent behind the query and correctly match it to relevant articles, regardless of the specific terminology used.\n\nIt's also possible to combine semantic search with keyword search to get the best of both worlds. See [Hybrid search](/docs/guides/ai/hybrid-search) for more details.\n\n## How semantic search works [\\#](\\#how-semantic-search-works)\n\nSemantic search uses an intermediate representation called an \u201cembedding vector\u201d to link database records with search queries. A vector, in the context of semantic search, is a list of numerical values. They represent various features of the text and allow for the semantic comparison between different pieces of text.\n\nThe best way to think of embeddings is by plotting them on a graph, where each embedding is a single point whose coordinates are the numerical values within its vector. Importantly, embeddings are plotted such that similar concepts are positioned close together while dissimilar concepts are far apart. For more details, see [What are embeddings?](/docs/guides/ai/concepts#what-are-embeddings)\n\nEmbeddings are generated using a language model, and embeddings are compared to each other using a similarity metric. The language model is trained to understand the semantics of language, including syntax, context, and the relationships between words. It generates embeddings for both the content in the database and the search queries. Then the similarity metric, often a function like cosine similarity or dot product, is used to compare the query embeddings with the document embeddings (in other words, to measure how close they are to each other on the graph). The documents with embeddings most similar to the query's are deemed the most relevant and are returned as search results.\n\n## Embedding models [\\#](\\#embedding-models)\n\nThere are many embedding models available today. Supabase Edge Functions has [built in support](/docs/guides/functions/examples/semantic-search) for the `gte-small` model. Others can be accessed through third-party APIs like [OpenAI](https://platform.openai.com/docs/guides/embeddings), where you send your text in the request and receive an embedding vector in the response. Others can run locally on your own compute, such as through Transformers.js for JavaScript implementations. For more information on local implementation, see [Generate embeddings](/docs/guides/ai/quickstarts/generate-text-embeddings).\n\nIt's crucial to remember that when using embedding models with semantic search, you must use the same model for all embedding comparisons. Comparing embeddings created by different models will yield meaningless results.\n\n## Semantic search in Postgres [\\#](\\#semantic-search-in-postgres)\n\nTo implement semantic search in Postgres we use `pgvector` \\- an extension that allows for efficient storage and retrieval of high-dimensional vectors. These vectors are numerical representations of text (or other types of data) generated by embedding models.\n\n1. Enable the `pgvector` extension by running:\n\n\n\n`\n_10\ncreate extension vector\n_10\nwith\n_10\nschema extensions;\n`\n\n2. Create a table to store the embeddings:\n\n\n\n`\n_10\ncreate table documents (\n_10\nid bigint primary key generated always as identity,\n_10\ncontent text,\n_10\nembedding vector(512)\n_10\n);\n`\n\n\n\nOr if you have an existing table, you can add a vector column like so:\n\n\n\n`\n_10\nalter table documents\n_10\nadd column embedding vector(512);\n`\n\n\n\nIn this example, we create a column named `embedding` which uses the newly enabled `vector` data type. The size of the vector (as indicated in parentheses) represents the number of dimensions in the embedding. Here we use 512, but adjust this to match the number of dimensions produced by your embedding model.\n\n\nFor more details on vector columns, including how to generate embeddings and store them, see [Vector columns](/docs/guides/ai/vector-columns).\n\n### Similarity metric [\\#](\\#similarity-metric)\n\n`pgvector` support 3 operators for computing distance between embeddings:\n\n| **Operator** | **Description** |\n| --- | --- |\n| `<->` | Euclidean distance |\n| `<#>` | negative inner product |\n| `<=>` | cosine distance |\n\nThese operators are used directly in your SQL query to retrieve records that are most similar to the user's search query. Choosing the right operator depends on your needs. Inner product (also known as dot product) tends to be the fastest if your vectors are normalized.\n\nThe easiest way to perform semantic search in Postgres in by creating a function:\n\n`\n_15\n-- Match documents using cosine distance (<=>)\n_15\ncreate or replace function match_documents (\n_15\nquery_embedding vector(512),\n_15\nmatch_threshold float,\n_15\nmatch_count int\n_15\n)\n_15\nreturns setof documents\n_15\nlanguage sql\n_15\nas $$\n_15\nselect *\n_15\nfrom documents\n_15\nwhere documents.embedding <=> query_embedding < 1 - match_threshold\n_15\norder by documents.embedding <=> query_embedding asc\n_15\nlimit least(match_count, 200);\n_15\n$$;\n`\n\nHere we create a function `match_documents` that accepts three parameters:\n\n1. `query_embedding`: a one-time embedding generated for the user's search query. Here we set the size to 512, but adjust this to match the number of dimensions produced by your embedding model.\n2. `match_threshold`: the minimum similarity between embeddings. This is a value between 1 and -1, where 1 is most similar and -1 is most dissimilar.\n3. `match_count`: the maximum number of results to return. Note the query may return less than this number if `match_threshold` resulted in a small shortlist. Limited to 200 records to avoid unintentionally overloading your database.\n\nIn this example, we return a `setof documents` and refer to `documents` throughout the query. Adjust this to use the relevant tables in your application.\n\nYou'll notice we are using the cosine distance ( `<=>`) operator in our query. Cosine distance is a safe default when you don't know whether or not your embeddings are normalized. If you know for a fact that they are normalized (for example, your embedding is returned from OpenAI), you can use negative inner product ( `<#>`) for better performance:\n\n`\n_15\n-- Match documents using negative inner product (<#>)\n_15\ncreate or replace function match_documents (\n_15\nquery_embedding vector(512),\n_15\nmatch_threshold float,\n_15\nmatch_count int\n_15\n)\n_15\nreturns setof documents\n_15\nlanguage sql\n_15\nas $$\n_15\nselect *\n_15\nfrom documents\n_15\nwhere documents.embedding <#> query_embedding < -match_threshold\n_15\norder by documents.embedding <#> query_embedding asc\n_15\nlimit least(match_count, 200);\n_15\n$$;\n`\n\nNote that since `<#>` is negative, we negate `match_threshold` accordingly in the `where` clause. For more information on the different operators, see the [pgvector docs](https://github.com/pgvector/pgvector?tab=readme-ov-file#vector-operators).\n\n### Calling from your application [\\#](\\#calling-from-your-application)\n\nFinally you can execute this function from your application. If you are using a Supabase client library such as [`supabase-js`](https://github.com/supabase/supabase-js), you can invoke it using the `rpc()` method:\n\n`\n_10\nconst { data: documents } = await supabase.rpc('match_documents', {\n_10\nquery_embedding: embedding, // pass the query embedding\n_10\nmatch_threshold: 0.78, // choose an appropriate threshold for your data\n_10\nmatch_count: 10, // choose the number of matches\n_10\n})\n`\n\nYou can also call this method directly from SQL:\n\n`\n_10\nselect *\n_10\nfrom match_documents(\n_10\n'[...]'::vector(512), -- pass the query embedding\n_10\n0.78, -- chose an appropriate threshold for your data\n_10\n10 -- choose the number of matches\n_10\n);\n`\n\nIn this scenario, you'll likely use a Postgres client library to establish a direct connection from your application to the database. It's best practice to parameterize your arguments before executing the query.\n\n## Next steps [\\#](\\#next-steps)\n\nAs your database scales, you will need an index on your vector columns to maintain fast query speeds. See [Vector indexes](/docs/guides/ai/vector-indexes) for an in-depth guide on the different types of indexes and how they work.\n\n## See also [\\#](\\#see-also)\n\n- [Embedding concepts](/docs/guides/ai/concepts)\n- [Vector columns](/docs/guides/ai/vector-columns)\n- [Vector indexes](/docs/guides/ai/vector-indexes)\n- [Hybrid search](/docs/guides/ai/hybrid-search)\n- [Keyword search](/docs/guides/ai/keyword-search)",
      "metadata": {
        "ogUrl": "https://supabase.com/docs/guides/ai/semantic-search",
        "title": "Semantic search | Supabase Docs",
        "robots": "index, follow",
        "ogImage": "https://obuldanrptloktxcffvn.supabase.co/functions/v1/og-images?site=docs&type=ai&title=Semantic%20search&description=Learn%20how%20to%20search%20by%20meaning%20rather%20than%20exact%20keywords.",
        "ogTitle": "Semantic search | Supabase Docs",
        "language": "en",
        "sourceURL": "https://supabase.com/docs/guides/ai/semantic-search",
        "description": "Learn how to search by meaning rather than exact keywords.",
        "modifiedTime": "2024-09-06T19:20:40.301Z",
        "ogDescription": "Learn how to search by meaning rather than exact keywords.",
        "publishedTime": "2024-09-06T19:20:40.300Z",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "AI & Vectors\n\n# Hybrid search\n\n## Combine keyword search with semantic search.\n\n* * *\n\nHybrid search combines [full text search](/docs/guides/ai/keyword-search) (searching by keyword) with [semantic search](/docs/guides/ai/semantic-search) (searching by meaning) to identify results that are both directly and contextually relevant to the user's query.\n\n## Why would I want to use hybrid search? [\\#](\\#why-would-i-want-to-use-hybrid-search)\n\nSometimes a single search method doesn't quite capture what a user is really looking for. For example, if a user searches for \"Italian recipes with tomato sauce\" on a cooking app, a keyword search would pull up recipes that specifically mention \"Italian,\" \"recipes,\" and \"tomato sauce\" in the text. However, it might miss out on dishes that are quintessentially Italian and use tomato sauce but don't explicitly label themselves with these words, or use variations like \"pasta sauce\" or \"marinara.\" On the other hand, a semantic search might understand the culinary context and find recipes that match the intent, such as a traditional \"Spaghetti Marinara,\" even if they don't match the exact keyword phrase. However, it could also suggest recipes that are contextually related but not what the user is looking for, like a \"Mexican salsa\" recipe, because it understands the context to be broadly about tomato-based sauces.\n\nHybrid search combines the strengths of both these methods. It would ensure that recipes explicitly mentioning the keywords are prioritized, thus capturing direct hits that satisfy the keyword criteria. At the same time, it would include recipes identified through semantic understanding as being related in meaning or context, like different Italian dishes that traditionally use tomato sauce but might not have been tagged explicitly with the user's search terms. It identifies results that are both directly and contextually relevant to the user's query while ideally minimizing misses and irrelevant suggestions.\n\n## When would I want to use hybrid search? [\\#](\\#when-would-i-want-to-use-hybrid-search)\n\nThe decision to use hybrid search depends on what your users are looking for in your app. For a code repository where developers need to find exact lines of code or error messages, keyword search is likely ideal because it matches specific terms. In a mental health forum where users search for advice or experiences related to their feelings, semantic search may be better because it finds results based on the meaning of a query, not just specific words. For a shopping app where customers might search for specific product names yet also be open to related suggestions, hybrid search combines the best of both worlds - finding exact matches while also uncovering similar products based on the shopping context.\n\n## How to combine search methods [\\#](\\#how-to-combine-search-methods)\n\nHybrid search merges keyword search and semantic search, but how does this process work?\n\nFirst, each search method is executed separately. Keyword search, which involves searching by specific words or phrases present in the content, will yield its own set of results. Similarly, semantic search, which involves understanding the context or meaning behind the search query rather than the specific words used, will generate its own unique results.\n\nNow with these separate result lists available, the next step is to combine them into a single, unified list. This is achieved through a process known as \u201cfusion\u201d. Fusion takes the results from both search methods and merges them together based on a certain ranking or scoring system. This system may prioritize certain results based on factors like their relevance to the search query, their ranking in the individual lists, or other criteria. The result is a final list that integrates the strengths of both keyword and semantic search methods.\n\n## Reciprocal Ranked Fusion (RRF) [\\#](\\#reciprocal-ranked-fusion-rrf)\n\nOne of the most common fusion methods is Reciprocal Ranked Fusion (RRF). The key idea behind RRF is to give more weight to the top-ranked items in each individual result list when building the final combined list.\n\nIn RRF, we iterate over each record and assign a score (noting that each record could exist in one or both lists). The score is calculated as 1 divided by that record's rank in each list, summed together between both lists. For example, if a record with an ID of `123` was ranked third in the keyword search and ninth in semantic search, it would receive a score of 13+19=0.444\\\\dfrac{1}{3} + \\\\dfrac{1}{9} = 0.44431\u200b+91\u200b=0.444. If the record was found in only one list and not the other, it would receive a score of 0 for the other list. The records are then sorted by this score to create the final list. The items with the highest scores are ranked first, and lowest scores ranked last.\n\nThis method ensures that items that are ranked high in multiple lists are given a high rank in the final list. It also ensures that items that are ranked high in only a few lists but low in others are not given a high rank in the final list. Placing the rank in the denominator when calculating score helps penalize the low ranking records.\n\n### Smoothing constant `k` [\\#](\\#smoothing-constant-k)\n\nTo prevent extremely high scores for items that are ranked first (since we're dividing by the rank), a `k` constant is often added to the denominator to smooth the score:\n\n1k+rank\\\\dfrac{1}{k+rank}k+rank1\u200b\n\nThis constant can be any positive number, but is typically small. A constant of 1 would mean that a record ranked first would have a score of 11+1=0.5\\\\dfrac{1}{1+1} = 0.51+11\u200b=0.5 instead of 111. This adjustment can help balance the influence of items that are ranked very high in individual lists when creating the final combined list.\n\n## Hybrid search in Postgres [\\#](\\#hybrid-search-in-postgres)\n\nLet's implement hybrid search in Postgres using `tsvector` (keyword search) and `pgvector` (semantic search).\n\nFirst we'll create a `documents` table to store the documents that we will search over. This is just an example - adjust this to match the structure of your application.\n\n`\n_10\ncreate table documents (\n_10\nid bigint primary key generated always as identity,\n_10\ncontent text,\n_10\nfts tsvector generated always as (to_tsvector('english', content)) stored,\n_10\nembedding vector(512)\n_10\n);\n`\n\nThe table contains 4 columns:\n\n- `id` is an auto-generated unique ID for the record. We'll use this later to match records when performing RRF.\n- `content` contains the actual text we will be searching over.\n- `fts` is an auto-generated `tsvector` column that is generated using the text in `content`. We will use this for [full text search](/docs/guides/database/full-text-search) (search by keyword).\n- `embedding` is a [vector column](/docs/guides/ai/vector-columns) that stores the vector generated from our embedding model. We will use this for [semantic search](/docs/guides/ai/semantic-search) (search by meaning). We chose 512 dimensions for this example, but adjust this to match the size of the embedding vectors generated from your preferred model.\n\nNext we'll create indexes on the `fts` and `embedding` columns so that their individual queries will remain fast at scale:\n\n`\n_10\n-- Create an index for the full-text search\n_10\ncreate index on documents using gin(fts);\n_10\n_10\n-- Create an index for the semantic vector search\n_10\ncreate index on documents using hnsw (embedding vector_ip_ops);\n`\n\nFor full text search we use a [generalized inverted (GIN) index](https://www.postgresql.org/docs/current/gin-intro.html) which is designed for handling composite values like those stored in a `tsvector`.\n\nFor semantic vector search we use an [HNSW index](/docs/guides/ai/vector-indexes/hnsw-indexes), which is a high performing approximate nearest neighbor (ANN) search algorithm. Note that we are using the `vector_ip_ops` (inner product) operator with this index because we plan on using the inner product ( `<#>`) operator later in our query. If you plan to use a different operator like cosine distance ( `<=>`), be sure to update the index accordingly. For more information, see [distance operators](/docs/guides/ai/vector-indexes#distance-operators).\n\nFinally we'll create our `hybrid_search` function:\n\n`\n_48\ncreate or replace function hybrid_search(\n_48\nquery_text text,\n_48\nquery_embedding vector(512),\n_48\nmatch_count int,\n_48\nfull_text_weight float = 1,\n_48\nsemantic_weight float = 1,\n_48\nrrf_k int = 50\n_48\n)\n_48\nreturns setof documents\n_48\nlanguage sql\n_48\nas $$\n_48\nwith full_text as (\n_48\nselect\n_48\n    id,\n_48\n    -- Note: ts_rank_cd is not indexable but will only rank matches of the where clause\n_48\n    -- which shouldn't be too big\n_48\n    row_number() over(order by ts_rank_cd(fts, websearch_to_tsquery(query_text)) desc) as rank_ix\n_48\nfrom\n_48\n    documents\n_48\nwhere\n_48\n    fts @@ websearch_to_tsquery(query_text)\n_48\norder by rank_ix\n_48\nlimit least(match_count, 30) * 2\n_48\n),\n_48\nsemantic as (\n_48\nselect\n_48\n    id,\n_48\n    row_number() over (order by embedding <#> query_embedding) as rank_ix\n_48\nfrom\n_48\n    documents\n_48\norder by rank_ix\n_48\nlimit least(match_count, 30) * 2\n_48\n)\n_48\nselect\n_48\ndocuments.*\n_48\nfrom\n_48\nfull_text\n_48\nfull outer join semantic\n_48\n    on full_text.id = semantic.id\n_48\njoin documents\n_48\n    on coalesce(full_text.id, semantic.id) = documents.id\n_48\norder by\n_48\ncoalesce(1.0 / (rrf_k + full_text.rank_ix), 0.0) * full_text_weight +\n_48\ncoalesce(1.0 / (rrf_k + semantic.rank_ix), 0.0) * semantic_weight\n_48\ndesc\n_48\nlimit\n_48\nleast(match_count, 30)\n_48\n$$;\n`\n\nLet's break this down:\n\n- **Parameters:** The function accepts quite a few parameters, but the main (required) ones are `query_text`, `query_embedding`, and `match_count`.\n\n\n  - `query_text` is the user's query text (more on this shortly)\n  - `query_embedding` is the vector representation of the user's query produced by the embedding model. We chose 512 dimensions for this example, but adjust this to match the size of the embedding vectors generated from your preferred model. This must match the size of the `embedding` vector on the `documents` table (and use the same model).\n  - `match_count` is the number of records returned in the `limit` clause.\n\nThe other parameters are optional, but give more control over the fusion process.\n  - `full_text_weight` and `semantic_weight` decide how much weight each search method gets in the final score. These are both 1 by default which means they both equally contribute towards the final rank. A `full_text_weight` of 2 and `semantic_weight` of 1 would give full-text search twice as much weight as semantic search.\n  - `rrf_k` is the `k` [smoothing constant](#smoothing-constant-k) added to the reciprocal rank. The default is 50.\n- **Return type:** The function returns a set of records from our `documents` table.\n\n- **CTE:** We create two [common table expressions (CTE)](https://www.postgresql.org/docs/current/queries-with.html), one for full-text search and one for semantic search. These perform each query individually prior to joining them.\n\n- **RRF:** The final query combines the results from the two CTEs using [reciprocal rank fusion (RRF)](#reciprocal-ranked-fusion-rrf).\n\n\n## Running hybrid search [\\#](\\#running-hybrid-search)\n\nTo use this function in SQL, we can run:\n\n`\n_10\nselect\n_10\n*\n_10\nfrom\n_10\nhybrid_search(\n_10\n    'Italian recipes with tomato sauce', -- user query\n_10\n    '[...]'::vector(512), -- embedding generated from user query\n_10\n    10\n_10\n);\n`\n\nIn practice, you will likely be calling this from the [Supabase client](/docs/reference/javascript/introduction) or through a custom backend layer. Here is a quick example of how you might call this from an [Edge Function](/docs/guides/functions) using JavaScript:\n\n`\n_38\nimport { createClient } from 'jsr:@supabase/supabase-js@2'\n_38\nimport OpenAI from 'npm:openai'\n_38\n_38\nconst supabaseUrl = Deno.env.get('SUPABASE_URL')!\n_38\nconst supabaseServiceRoleKey = Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!\n_38\nconst openaiApiKey = Deno.env.get('OPENAI_API_KEY')!\n_38\n_38\nDeno.serve(async (req) => {\n_38\n// Grab the user's query from the JSON payload\n_38\nconst { query } = await req.json()\n_38\n_38\n// Instantiate OpenAI client\n_38\nconst openai = new OpenAI({ apiKey: openaiApiKey })\n_38\n_38\n// Generate a one-time embedding for the user's query\n_38\nconst embeddingResponse = await openai.embeddings.create({\n_38\n    model: 'text-embedding-3-large',\n_38\n    input: query,\n_38\n    dimensions: 512,\n_38\n})\n_38\n_38\nconst [{ embedding }] = embeddingResponse.data\n_38\n_38\n// Instantiate the Supabase client\n_38\n// (replace service role key with user's JWT if using Supabase auth and RLS)\n_38\nconst supabase = createClient(supabaseUrl, supabaseServiceRoleKey)\n_38\n_38\n// Call hybrid_search Postgres function via RPC\n_38\nconst { data: documents } = await supabase.rpc('hybrid_search', {\n_38\n    query_text: query,\n_38\n    query_embedding: embedding,\n_38\n    match_count: 10,\n_38\n})\n_38\n_38\nreturn new Response(JSON.stringify(documents), {\n_38\n    headers: { 'Content-Type': 'application/json' },\n_38\n})\n_38\n})\n`\n\nThis uses OpenAI's `text-embedding-3-large` model to generate embeddings (shortened to 512 dimensions for faster retrieval). Swap in your preferred embedding model (and dimension size) accordingly.\n\nTo test this, make a `POST` request to the function's endpoint while passing in a JSON payload containing the user's query. Here is an example `POST` request using cURL:\n\n`\n_10\ncurl -i --location --request POST \\\n_10\n'http://127.0.0.1:54321/functions/v1/hybrid-search' \\\n_10\n  --header 'Authorization: Bearer <anonymous key>' \\\n_10\n  --header 'Content-Type: application/json' \\\n_10\n  --data '{\"query\":\"Italian recipes with tomato sauce\"}'\n`\n\nFor more information on how to create, test, and deploy edge functions, see [Getting started](/docs/guides/functions/quickstart).\n\n## See also [\\#](\\#see-also)\n\n- [Embedding concepts](/docs/guides/ai/concepts)\n- [Vector columns](/docs/guides/ai/vector-columns)\n- [Vector indexes](/docs/guides/ai/vector-indexes)\n- [Semantic search](/docs/guides/ai/semantic-search)\n- [Full text (keyword) search](/docs/guides/database/full-text-search)",
      "metadata": {
        "ogUrl": "https://supabase.com/docs/guides/ai/hybrid-search",
        "title": "Hybrid search | Supabase Docs",
        "robots": "index, follow",
        "ogImage": "https://obuldanrptloktxcffvn.supabase.co/functions/v1/og-images?site=docs&type=ai&title=Hybrid%20search&description=Combine%20keyword%20search%20with%20semantic%20search%20to%20get%20both%20direct%20and%20contextual%20results.",
        "ogTitle": "Hybrid search | Supabase Docs",
        "language": "en",
        "sourceURL": "https://supabase.com/docs/guides/ai/hybrid-search",
        "description": "Combine keyword search with semantic search to get both direct and contextual results.",
        "modifiedTime": "2024-09-06T19:20:40.691Z",
        "ogDescription": "Combine keyword search with semantic search to get both direct and contextual results.",
        "publishedTime": "2024-09-06T19:20:40.691Z",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "AI & Vectors\n\n# IVFFlat indexes\n\n* * *\n\nIVFFlat is a type of vector index for approximate nearest neighbor search. It is a frequently used index type that can improve performance when querying highly-dimensional vectors, like those representing embeddings.\n\n## Choosing an index [\\#](\\#choosing-an-index)\n\nToday `pgvector` supports two types of indexes:\n\n- [HNSW](/docs/guides/ai/vector-indexes/hnsw-indexes)\n- [IVFFlat](/docs/guides/ai/vector-indexes/ivf-indexes)\n\nIn general we recommend using [HNSW](/docs/guides/ai/vector-indexes/hnsw-indexes) because of its [performance](https://supabase.com/blog/increase-performance-pgvector-hnsw#hnsw-performance-1536-dimensions) and [robustness against changing data](/docs/guides/ai/vector-indexes/hnsw-indexes#when-should-you-create-hnsw-indexes). If you have a special use case that requires IVFFlat instead, keep reading.\n\n## Usage [\\#](\\#usage)\n\nThe way you create an IVFFlat index depends on the distance operator you are using. `pgvector` includes 3 distance operators:\n\n| Operator | Description | [**Operator class**](https://www.postgresql.org/docs/current/sql-createopclass.html) |\n| --- | --- | --- |\n| `<->` | Euclidean distance | `vector_l2_ops` |\n| `<#>` | negative inner product | `vector_ip_ops` |\n| `<=>` | cosine distance | `vector_cosine_ops` |\n\nUse the following SQL commands to create an IVFFlat index for the operator(s) used in your queries.\n\n### Euclidean L2 distance ( `vector_l2_ops`) [\\#](\\#euclidean-l2-distance--vectorl2ops-)\n\n`\n_10\ncreate index on items using ivfflat (column_name vector_l2_ops) with (lists = 100);\n`\n\n### Inner product ( `vector_ip_ops`) [\\#](\\#inner-product--vectoripops-)\n\n`\n_10\ncreate index on items using ivfflat (column_name vector_ip_ops) with (lists = 100);\n`\n\n### Cosine distance ( `vector_cosine_ops`) [\\#](\\#cosine-distance--vectorcosineops-)\n\n`\n_10\ncreate index on items using ivfflat (column_name vector_cosine_ops) with (lists = 100);\n`\n\nCurrently vectors with up to 2,000 dimensions can be indexed.\n\n## How does IVFFlat work? [\\#](\\#how-does-ivfflat-work)\n\nIVF stands for 'inverted file indexes'. It works by clustering your vectors in order to reduce the similarity search scope. Rather than comparing a vector to every other vector, the vector is only compared against vectors within the same cell cluster (or nearby clusters, depending on your configuration).\n\n### Inverted lists (cell clusters) [\\#](\\#inverted-lists-cell-clusters)\n\nWhen you create the index, you choose the number of inverted lists (cell clusters). Increase this number to speed up queries, but at the expense of recall.\n\nFor example, to create an index with 100 lists on a column that uses the cosine operator:\n\n`\n_10\ncreate index on items using ivfflat (column_name vector_cosine_ops) with (lists = 100);\n`\n\nFor more info on the different operators, see [Distance operations](#distance-operators).\n\nFor every query, you can set the number of probes (1 by default). The number of probes corresponds to the number of nearby cells to probe for a match. Increase this for better recall at the expense of speed.\n\nTo set the number of probes for the duration of the session run:\n\n`\n_10\nset ivfflat.probes = 10;\n`\n\nTo set the number of probes only for the current transaction run:\n\n`\n_10\nbegin;\n_10\nset local ivfflat.probes = 10;\n_10\nselect ...\n_10\ncommit;\n`\n\nIf the number of probes is the same as the number of lists, exact nearest neighbor search will be performed and the planner won't use the index.\n\n### Approximate nearest neighbor [\\#](\\#approximate-nearest-neighbor)\n\nOne important note with IVF indexes is that nearest neighbor search is approximate, since exact search on high dimensional data can't be indexed efficiently. This means that similarity results will change (slightly) after you add an index (trading recall for speed).\n\n## When should you create IVFFlat indexes? [\\#](\\#when-should-you-create-ivfflat-indexes)\n\n`pgvector` recommends building IVFFlat indexes only after the table has sufficient data, so that the internal IVFFlat cell clusters are based on your data's distribution. Anytime the distribution changes significantly, consider rebuilding indexes.\n\n## Resources [\\#](\\#resources)\n\nRead more about indexing on `pgvector`'s [GitHub page](https://github.com/pgvector/pgvector#indexing).",
      "metadata": {
        "ogUrl": "https://supabase.com/docs/guides/ai/vector-indexes/ivf-indexes",
        "title": "IVFFlat indexes | Supabase Docs",
        "robots": "index, follow",
        "ogImage": "https://obuldanrptloktxcffvn.supabase.co/functions/v1/og-images?site=docs&type=ai&title=IVFFlat%20indexes&description=Understanding%20IVFFlat%20indexes%20in%20pgvector",
        "ogTitle": "IVFFlat indexes | Supabase Docs",
        "language": "en",
        "sourceURL": "https://supabase.com/docs/guides/ai/vector-indexes/ivf-indexes",
        "description": "Understanding IVFFlat indexes in pgvector",
        "modifiedTime": "2024-09-06T19:20:40.691Z",
        "ogDescription": "Understanding IVFFlat indexes in pgvector",
        "publishedTime": "2024-09-06T19:20:40.691Z",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "AI & Vectors\n\n# Image Search with OpenAI CLIP\n\n## Implement image search with the OpenAI CLIP Model and Supabase Vector.\n\n* * *\n\nThe [OpenAI CLIP Model](https://github.com/openai/CLIP) was trained on a variety of (image, text)-pairs. You can use the CLIP model for:\n\n- Text-to-Image / Image-To-Text / Image-to-Image / Text-to-Text Search\n- You can fine-tune it on your own image and text data with the regular SentenceTransformers training code.\n\n[SentenceTransformers](https://www.sbert.net/examples/applications/image-search/README.html) provides models that allow you to embed images and text into the same vector space. You can use this to find similar images as well as to implement image search.\n\nYou can find the full application code as a Python Poetry project on [GitHub](https://github.com/supabase/supabase/tree/master/examples/ai/image_search#image-search-with-supabase-vector).\n\n## Create a new Python project with Poetry [\\#](\\#create-a-new-python-project-with-poetry)\n\n[Poetry](https://python-poetry.org/) provides packaging and dependency management for Python. If you haven't already, install poetry via pip:\n\n`\n_10\npip install poetry\n`\n\nThen initialize a new project:\n\n`\n_10\npoetry new image-search\n`\n\n## Setup Supabase project [\\#](\\#setup-supabase-project)\n\nIf you haven't already, [install the Supabase CLI](/docs/guides/cli), then initialize Supabase in the root of your newly created poetry project:\n\n`\n_10\nsupabase init\n`\n\nNext, start your local Supabase stack:\n\n`\n_10\nsupabase start\n`\n\nThis will start up the Supabase stack locally and print out a bunch of environment details, including your local `DB URL`. Make a note of that for later user.\n\n## Install the dependencies [\\#](\\#install-the-dependencies)\n\nWe will need to add the following dependencies to our project:\n\n- [`vecs`](https://github.com/supabase/vecs#vecs): Supabase Vector Python Client.\n- [`sentence-transformers`](https://huggingface.co/sentence-transformers/clip-ViT-B-32): a framework for sentence, text and image embeddings (used with OpenAI CLIP model)\n- [`matplotlib`](https://matplotlib.org/): for displaying our image result\n\n`\n_10\npoetry add vecs sentence-transformers matplotlib\n`\n\n## Import the necessary dependencies [\\#](\\#import-the-necessary-dependencies)\n\nAt the top of your main python script, import the dependencies and store your `DB URL` from above in a variable:\n\n`\n_10\nfrom PIL import Image\n_10\nfrom sentence_transformers import SentenceTransformer\n_10\nimport vecs\n_10\nfrom matplotlib import pyplot as plt\n_10\nfrom matplotlib import image as mpimg\n_10\n_10\nDB_CONNECTION = \"postgresql://postgres:postgres@localhost:54322/postgres\"\n`\n\n## Create embeddings for your images [\\#](\\#create-embeddings-for-your-images)\n\nIn the root of your project, create a new folder called `images` and add some images. You can use the images from the example project on [GitHub](https://github.com/supabase/supabase/tree/master/examples/ai/image_search/images) or you can find license free images on [unsplash](https://unsplash.com).\n\nNext, create a `seed` method, which will create a new Supabase Vector Collection, generate embeddings for your images, and upsert the embeddings into your database:\n\n`\n_43\ndef seed():\n_43\n    # create vector store client\n_43\n    vx = vecs.create_client(DB_CONNECTION)\n_43\n_43\n    # create a collection of vectors with 3 dimensions\n_43\n    images = vx.get_or_create_collection(name=\"image_vectors\", dimension=512)\n_43\n_43\n    # Load CLIP model\n_43\n    model = SentenceTransformer('clip-ViT-B-32')\n_43\n_43\n    # Encode an image:\n_43\n    img_emb1 = model.encode(Image.open('./images/one.jpg'))\n_43\n    img_emb2 = model.encode(Image.open('./images/two.jpg'))\n_43\n    img_emb3 = model.encode(Image.open('./images/three.jpg'))\n_43\n    img_emb4 = model.encode(Image.open('./images/four.jpg'))\n_43\n_43\n    # add records to the *images* collection\n_43\n    images.upsert(\n_43\n        records=[\\\n_43\\\n            (\\\n_43\\\n                \"one.jpg\",        # the vector's identifier\\\n_43\\\n                img_emb1,          # the vector. list or np.array\\\n_43\\\n                {\"type\": \"jpg\"}   # associated  metadata\\\n_43\\\n            ), (\\\n_43\\\n                \"two.jpg\",\\\n_43\\\n                img_emb2,\\\n_43\\\n                {\"type\": \"jpg\"}\\\n_43\\\n            ), (\\\n_43\\\n                \"three.jpg\",\\\n_43\\\n                img_emb3,\\\n_43\\\n                {\"type\": \"jpg\"}\\\n_43\\\n            ), (\\\n_43\\\n                \"four.jpg\",\\\n_43\\\n                img_emb4,\\\n_43\\\n                {\"type\": \"jpg\"}\\\n_43\\\n            )\\\n_43\\\n        ]\n_43\n    )\n_43\n    print(\"Inserted images\")\n_43\n_43\n    # index the collection for fast search performance\n_43\n    images.create_index()\n_43\n    print(\"Created index\")\n`\n\nAdd this method as a script in your `pyproject.toml` file:\n\n`\n_10\n[tool.poetry.scripts]\n_10\nseed = \"image_search.main:seed\"\n_10\nsearch = \"image_search.main:search\"\n`\n\nAfter activating the virtual environtment with `poetry shell` you can now run your seed script via `poetry run seed`. You can inspect the generated embeddings in your local database by visiting the local Supabase dashboard at [localhost:54323](http://localhost:54323/project/default/editor), selecting the `vecs` schema, and the `image_vectors` database.\n\n## Perform an image search from a text query [\\#](\\#perform-an-image-search-from-a-text-query)\n\nWith Supabase Vector we can easily query our embeddings. We can use either an image as search input or alternative we can generate an embedding from a string input and use that as the query input:\n\n`\n_23\ndef search():\n_23\n    # create vector store client\n_23\n    vx = vecs.create_client(DB_CONNECTION)\n_23\n    images = vx.get_or_create_collection(name=\"image_vectors\", dimension=512)\n_23\n_23\n    # Load CLIP model\n_23\n    model = SentenceTransformer('clip-ViT-B-32')\n_23\n    # Encode text query\n_23\n    query_string = \"a bike in front of a red brick wall\"\n_23\n    text_emb = model.encode(query_string)\n_23\n_23\n    # query the collection filtering metadata for \"type\" = \"jpg\"\n_23\n    results = images.query(\n_23\n        data=text_emb,                      # required\n_23\n        limit=1,                            # number of records to return\n_23\n        filters={\"type\": {\"$eq\": \"jpg\"}},   # metadata filters\n_23\n    )\n_23\n    result = results[0]\n_23\n    print(result)\n_23\n    plt.title(result)\n_23\n    image = mpimg.imread('./images/' + result)\n_23\n    plt.imshow(image)\n_23\n    plt.show()\n`\n\nBy limiting the query to one result, we can show the most relevant image to the user. Finally we use `matplotlib` to show the image result to the user.\n\nThat's it, go ahead and test it out by running `poetry run search` and you will be presented with an image of a \"bike in front of a red brick wall\".\n\n## Conclusion [\\#](\\#conclusion)\n\nWith just a couple of lines of Python you are able to implement image search as well as reverse image search using OpenAI's CLIP model and Supabase Vector.",
      "metadata": {
        "ogUrl": "https://supabase.com/docs/guides/ai/examples/image-search-openai-clip",
        "title": "Image Search with OpenAI CLIP | Supabase Docs",
        "robots": "index, follow",
        "ogImage": "https://obuldanrptloktxcffvn.supabase.co/functions/v1/og-images?site=docs&type=ai&title=Image%20Search%20with%20OpenAI%20CLIP&description=Implement%20image%20search%20with%20the%20OpenAI%20CLIP%20Model%20and%20Supabase%20Vector.",
        "ogTitle": "Image Search with OpenAI CLIP | Supabase Docs",
        "language": "en",
        "sourceURL": "https://supabase.com/docs/guides/ai/examples/image-search-openai-clip",
        "description": "Implement image search with the OpenAI CLIP Model and Supabase Vector.",
        "modifiedTime": "2024-09-06T19:20:40.301Z",
        "ogDescription": "Implement image search with the OpenAI CLIP Model and Supabase Vector.",
        "publishedTime": "2024-09-06T19:20:40.300Z",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "AI & Vectors\n\n# Choosing your Compute Add-on\n\n## Choosing the right Compute Add-on for your vector workload.\n\n* * *\n\nYou have two options for scaling your vector workload:\n\n1. Increase the size of your database. This guide will help you choose the right size for your workload.\n2. Spread your workload across multiple databases. You can find more details about this approach in [Engineering for Scale](engineering-for-scale).\n\n## Dimensionality [\\#](\\#dimensionality)\n\nThe number of dimensions in your embeddings is the most important factor in choosing the right Compute Add-on. In general, the lower the dimensionality the better the performance. We've provided guidance for some of the more common embedding dimensions below. For each benchmark, we used [Vecs](https://github.com/supabase/vecs) to create a collection, upload the embeddings to a single table, and create both the `IVFFlat` and `HNSW` indexes for `inner-product` distance measure for the embedding column. We then ran a series of queries to measure the performance of different compute add-ons:\n\n## HNSW [\\#](\\#hnsw)\n\n### 384 dimensions [\\#](\\#hnsw-384-dimensions)\n\nThis benchmark uses the dbpedia-entities-openai-1M dataset containing 1,000,000 embeddings of text, regenerated for 384 dimension embeddings. Each embedding is generated using [gte-small](https://huggingface.co/Supabase/gte-small).\n\ngte-small-384\n\n| Compute Size | Vectors | m | ef\\_construction | ef\\_search | QPS | Latency Mean | Latency p95 | RAM Usage | RAM |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| Micro | 100,000 | 16 | 64 | 60 | 580 | 0.017 sec | 0.024 sec | 1.2 (Swap) | 1 GB |\n| Small | 250,000 | 24 | 64 | 60 | 440 | 0.022 sec | 0.033 sec | 2 GB | 2 GB |\n| Medium | 500,000 | 24 | 64 | 80 | 350 | 0.028 sec | 0.045 sec | 4 GB | 4 GB |\n| Large | 1,000,000 | 32 | 80 | 100 | 270 | 0.073 sec | 0.108 sec | 7 GB | 8 GB |\n| XL | 1,000,000 | 32 | 80 | 100 | 525 | 0.038 sec | 0.059 sec | 9 GB | 16 GB |\n| 2XL | 1,000,000 | 32 | 80 | 100 | 790 | 0.025 sec | 0.037 sec | 9 GB | 32 GB |\n| 4XL | 1,000,000 | 32 | 80 | 100 | 1650 | 0.015 sec | 0.018 sec | 11 GB | 64 GB |\n| 8XL | 1,000,000 | 32 | 80 | 100 | 2690 | 0.015 sec | 0.016 sec | 13 GB | 128 GB |\n| 12XL | 1,000,000 | 32 | 80 | 100 | 3900 | 0.014 sec | 0.016 sec | 13 GB | 192 GB |\n| 16XL | 1,000,000 | 32 | 80 | 100 | 4200 | 0.014 sec | 0.016 sec | 20 GB | 256 GB |\n\nAccuracy was 0.99 for benchmarks.\n\n### 960 dimensions [\\#](\\#hnsw-960-dimensions)\n\nThis benchmark uses the [gist-960](http://corpus-texmex.irisa.fr/) dataset, which contains 1,000,000 embeddings of images. Each embedding is 960 dimensions.\n\ngist-960\n\n| Compute Size | Vectors | m | ef\\_construction | ef\\_search | QPS | Latency Mean | Latency p95 | RAM Usage | RAM |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| Micro | 30,000 | 16 | 64 | 65 | 430 | 0.024 sec | 0.034 sec | 1.2 GB (Swap) | 1 GB |\n| Small | 100,000 | 32 | 80 | 60 | 260 | 0.040 sec | 0.054 sec | 2.2 GB (Swap) | 2 GB |\n| Medium | 250,000 | 32 | 80 | 90 | 120 | 0.083 sec | 0.106 sec | 4 GB | 4 GB |\n| Large | 500,000 | 32 | 80 | 120 | 160 | 0.063 sec | 0.087 sec | 7 GB | 8 GB |\n| XL | 1,000,000 | 32 | 80 | 200 | 200 | 0.049 sec | 0.072 sec | 13 GB | 16 GB |\n| 2XL | 1,000,000 | 32 | 80 | 200 | 340 | 0.025 sec | 0.029 sec | 17 GB | 32 GB |\n| 4XL | 1,000,000 | 32 | 80 | 200 | 630 | 0.031 sec | 0.050 sec | 18 GB | 64 GB |\n| 8XL | 1,000,000 | 32 | 80 | 200 | 1100 | 0.034 sec | 0.048 sec | 19 GB | 128 GB |\n| 12XL | 1,000,000 | 32 | 80 | 200 | 1420 | 0.041 sec | 0.095 sec | 21 GB | 192 GB |\n| 16XL | 1,000,000 | 32 | 80 | 200 | 1650 | 0.037 sec | 0.081 sec | 23 GB | 256 GB |\n\nAccuracy was 0.99 for benchmarks.\n\nQPS can also be improved by increasing [`m` and `ef_construction`](/docs/guides/ai/going-to-prod#hnsw-understanding-efconstruction--efsearch--and-m). This will allow you to use a smaller value for `ef_search` and increase QPS.\n\n### 1536 dimensions [\\#](\\#hnsw-1536-dimensions)\n\nThis benchmark uses the [dbpedia-entities-openai-1M](https://huggingface.co/datasets/KShivendu/dbpedia-entities-openai-1M) dataset, which contains 1,000,000 embeddings of text. And 224,482 embeddings from [Wikipedia articles](https://huggingface.co/datasets/Supabase/wikipedia-en-embeddings) for compute add-ons `large` and below. Each embedding is 1536 dimensions created with the [OpenAI Embeddings API](https://platform.openai.com/docs/guides/embeddings).\n\nOpenAI-1536\n\n| Compute Size | Vectors | m | ef\\_construction | ef\\_search | QPS | Latency Mean | Latency p95 | RAM Usage | RAM |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| Micro | 15,000 | 16 | 40 | 40 | 480 | 0.011 sec | 0.016 sec | 1.2 GB (Swap) | 1 GB |\n| Small | 50,000 | 32 | 64 | 100 | 175 | 0.031 sec | 0.051 sec | 2.2 GB (Swap) | 2 GB |\n| Medium | 100,000 | 32 | 64 | 100 | 240 | 0.083 sec | 0.126 sec | 4 GB | 4 GB |\n| Large | 224,482 | 32 | 64 | 100 | 280 | 0.017 sec | 0.028 sec | 8 GB | 8 GB |\n| XL | 500,000 | 24 | 56 | 100 | 360 | 0.055 sec | 0.135 sec | 13 GB | 16 GB |\n| 2XL | 1,000,000 | 24 | 56 | 250 | 560 | 0.036 sec | 0.058 sec | 32 GB | 32 GB |\n| 4XL | 1,000,000 | 24 | 56 | 250 | 950 | 0.021 sec | 0.033 sec | 39 GB | 64 GB |\n| 8XL | 1,000,000 | 24 | 56 | 250 | 1650 | 0.016 sec | 0.023 sec | 40 GB | 128 GB |\n| 12XL | 1,000,000 | 24 | 56 | 250 | 1900 | 0.015 sec | 0.021 sec | 38 GB | 192 GB |\n| 16XL | 1,000,000 | 24 | 56 | 250 | 2200 | 0.015 sec | 0.020 sec | 40 GB | 256 GB |\n\nAccuracy was 0.99 for benchmarks.\n\nQPS can also be improved by increasing [`m` and `ef_construction`](/docs/guides/ai/going-to-prod#hnsw-understanding-efconstruction--efsearch--and-m). This will allow you to use a smaller value for `ef_search` and increase QPS. For example, increasing `m` to 32 and `ef_construction` to 80 for 4XL will increase QPS to 1280.\n\nIt is possible to upload more vectors to a single table if Memory allows it (for example, 4XL plan and higher for OpenAI embeddings). But it will affect the performance of the queries: QPS will be lower, and latency will be higher. Scaling should be almost linear, but it is recommended to benchmark your workload to find the optimal number of vectors per table and per database instance.\n\n## IVFFlat [\\#](\\#ivfflat)\n\n### 384 dimensions [\\#](\\#ivfflat-384-dimensions)\n\nThis benchmark uses the dbpedia-entities-openai-1M dataset containing 1,000,000 embeddings of text, regenerated for 384 dimension embeddings. Each embedding is generated using [gte-small](https://huggingface.co/Supabase/gte-small).\n\ngte-small-384, accuracy=.98gte-small-384, accuracy=.99\n\n| Compute Size | Vectors | Lists | Probes | QPS | Latency Mean | Latency p95 | RAM Usage | RAM |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| Micro | 100,000 | 500 | 50 | 205 | 0.048 sec | 0.066 sec | 1.2 GB (Swap) | 1 GB |\n| Small | 250,000 | 1000 | 60 | 160 | 0.062 sec | 0.079 sec | 2 GB | 2 GB |\n| Medium | 500,000 | 2000 | 80 | 120 | 0.082 sec | 0.104 sec | 3.2 GB | 4 GB |\n| Large | 1,000,000 | 5000 | 150 | 75 | 0.269 sec | 0.375 sec | 6.5 GB | 8 GB |\n| XL | 1,000,000 | 5000 | 150 | 150 | 0.131 sec | 0.178 sec | 9 GB | 16 GB |\n| 2XL | 1,000,000 | 5000 | 150 | 300 | 0.066 sec | 0.099 sec | 10 GB | 32 GB |\n| 4XL | 1,000,000 | 5000 | 150 | 570 | 0.035 sec | 0.046 sec | 10 GB | 64 GB |\n| 8XL | 1,000,000 | 5000 | 150 | 1400 | 0.023 sec | 0.028 sec | 12 GB | 128 GB |\n| 12XL | 1,000,000 | 5000 | 150 | 1550 | 0.030 sec | 0.039 sec | 12 GB | 192 GB |\n| 16XL | 1,000,000 | 5000 | 150 | 1800 | 0.030 sec | 0.039 sec | 16 GB | 256 GB |\n\n### 960 dimensions [\\#](\\#ivfflat-960-dimensions)\n\nThis benchmark uses the [gist-960](http://corpus-texmex.irisa.fr/) dataset, which contains 1,000,000 embeddings of images. Each embedding is 960 dimensions.\n\ngist-960, probes = 10\n\n| Compute Size | Vectors | Lists | QPS | Latency Mean | Latency p95 | RAM Usage | RAM |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| Micro | 30,000 | 30 | 75 | 0.065 sec | 0.088 sec | 1.1 GB (Swap) | 1 GB |\n| Small | 100,000 | 100 | 78 | 0.064 sec | 0.092 sec | 1.8 GB | 2 GB |\n| Medium | 250,000 | 250 | 58 | 0.085 sec | 0.129 sec | 3.2 GB | 4 GB |\n| Large | 500,000 | 500 | 55 | 0.088 sec | 0.140 sec | 5 GB | 8 GB |\n| XL | 1,000,000 | 1000 | 110 | 0.046 sec | 0.070 sec | 14 GB | 16 GB |\n| 2XL | 1,000,000 | 1000 | 235 | 0.083 sec | 0.136 sec | 10 GB | 32 GB |\n| 4XL | 1,000,000 | 1000 | 420 | 0.071 sec | 0.106 sec | 11 GB | 64 GB |\n| 8XL | 1,000,000 | 1000 | 815 | 0.072 sec | 0.106 sec | 13 GB | 128 GB |\n| 12XL | 1,000,000 | 1000 | 1150 | 0.052 sec | 0.078 sec | 15.5 GB | 192 GB |\n| 16XL | 1,000,000 | 1000 | 1345 | 0.072 sec | 0.106 sec | 17.5 GB | 256 GB |\n\n### 1536 dimensions [\\#](\\#ivfflat-1536-dimensions)\n\nThis benchmark uses the [dbpedia-entities-openai-1M](https://huggingface.co/datasets/KShivendu/dbpedia-entities-openai-1M) dataset, which contains 1,000,000 embeddings of text. Each embedding is 1536 dimensions created with the [OpenAI Embeddings API](https://platform.openai.com/docs/guides/embeddings).\n\nOpenAI-1536, probes = 10OpenAI-1536, probes = 40\n\n| Compute Size | Vectors | Lists | QPS | Latency Mean | Latency p95 | RAM Usage | RAM |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| Micro | 20,000 | 40 | 135 | 0.372 sec | 0.412 sec | 1.2 GB (Swap) | 1 GB |\n| Small | 50,000 | 100 | 140 | 0.357 sec | 0.398 sec | 1.8 GB | 2 GB |\n| Medium | 100,000 | 200 | 130 | 0.383 sec | 0.446 sec | 3.7 GB | 4 GB |\n| Large | 250,000 | 500 | 130 | 0.378 sec | 0.434 sec | 7 GB | 8 GB |\n| XL | 500,000 | 1000 | 235 | 0.213 sec | 0.271 sec | 13.5 GB | 16 GB |\n| 2XL | 1,000,000 | 2000 | 380 | 0.133 sec | 0.236 sec | 30 GB | 32 GB |\n| 4XL | 1,000,000 | 2000 | 720 | 0.068 sec | 0.120 sec | 35 GB | 64 GB |\n| 8XL | 1,000,000 | 2000 | 1250 | 0.039 sec | 0.066 sec | 38 GB | 128 GB |\n| 12XL | 1,000,000 | 2000 | 1600 | 0.030 sec | 0.052 sec | 41 GB | 192 GB |\n| 16XL | 1,000,000 | 2000 | 1790 | 0.029 sec | 0.051 sec | 45 GB | 256 GB |\n\nFor 1,000,000 vectors 10 probes results to accuracy of 0.91. And for 500,000 vectors and below 10 probes results to accuracy in the range of 0.95 - 0.99. To increase accuracy, you need to increase the number of probes.\n\nIt is possible to upload more vectors to a single table if Memory allows it (for example, 4XL plan and higher for OpenAI embeddings). But it will affect the performance of the queries: QPS will be lower, and latency will be higher. Scaling should be almost linear, but it is recommended to benchmark your workload to find the optimal number of vectors per table and per database instance.\n\n## Performance tips [\\#](\\#performance-tips)\n\nThere are various ways to improve your pgvector performance. Here are some tips:\n\n### Pre-warming your database [\\#](\\#pre-warming-your-database)\n\nIt's useful to execute a few thousand \u201cwarm-up\u201d queries before going into production. This helps help with RAM utilization. This can also help to determine that you've selected the right compute size for your workload.\n\n### Finetune index parameters [\\#](\\#finetune-index-parameters)\n\nYou can increase the Requests per Second by increasing `m` and `ef_construction` or `lists`. This also has an important caveat: building the index takes longer with higher values for these parameters.\n\nHNSWIVFFlat\n\nCheck out more tips and the complete step-by-step guide in [Going to Production for AI applications](going-to-prod).\n\n## Benchmark methodology [\\#](\\#benchmark-methodology)\n\nWe follow techniques outlined in the [ANN Benchmarks](https://github.com/erikbern/ann-benchmarks) methodology. A Python test runner is responsible for uploading the data, creating the index, and running the queries. The pgvector engine is implemented using [vecs](https://github.com/supabase/vecs), a Python client for pgvector.\n\nEach test is run for a minimum of 30-40 minutes. They include a series of experiments executed at different concurrency levels to measure the engine's performance under different load types. The results are then averaged.\n\nAs a general recommendation, we suggest using a concurrency level of 5 or more for most workloads and 30 or more for high-load workloads.",
      "metadata": {
        "ogUrl": "https://supabase.com/docs/guides/ai/choosing-compute-addon",
        "title": "Choosing your Compute Add-on | Supabase Docs",
        "robots": "index, follow",
        "ogImage": "https://obuldanrptloktxcffvn.supabase.co/functions/v1/og-images?site=docs&type=ai&title=Choosing%20your%20Compute%20Add-on&description=Choosing%20the%20right%20Compute%20Add-on%20for%20your%20vector%20workload.",
        "ogTitle": "Choosing your Compute Add-on | Supabase Docs",
        "language": "en",
        "sourceURL": "https://supabase.com/docs/guides/ai/choosing-compute-addon",
        "description": "Choosing the right Compute Add-on for your vector workload.",
        "modifiedTime": "2024-09-06T19:20:40.301Z",
        "ogDescription": "Choosing the right Compute Add-on for your vector workload.",
        "publishedTime": "2024-09-06T19:20:40.300Z",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "AI & Vectors\n\n# RAG with Permissions\n\n## Fine-grain access control with Retrieval Augmented Generation.\n\n* * *\n\nSince pgvector is built on top of Postgres, you can implement fine-grain access control on your vector database using [Row Level Security (RLS)](/docs/guides/database/postgres/row-level-security). This means you can restrict which documents are returned during a vector similarity search to users that have access to them. Supabase also supports [Foreign Data Wrappers (FDW)](/docs/guides/database/extensions/wrappers/overview) which means you can use an external database or data source to determine these permissions if your user data doesn't exist in Supabase.\n\nUse this guide to learn how to restrict access to documents when performing retrieval augmented generation (RAG).\n\n## Example [\\#](\\#example)\n\nIn a typical RAG setup, your documents are chunked into small subsections and similarity is performed over those sections:\n\n`\n_16\n-- Track documents/pages/files/etc\n_16\ncreate table documents (\n_16\nid bigint primary key generated always as identity,\n_16\nname text not null,\n_16\nowner_id uuid not null references auth.users (id) default auth.uid(),\n_16\ncreated_at timestamp with time zone not null default now()\n_16\n);\n_16\n_16\n-- Store the content and embedding vector for each section in the document\n_16\n-- with a reference to original document (one-to-many)\n_16\ncreate table document_sections (\n_16\nid bigint primary key generated always as identity,\n_16\ndocument_id bigint not null references documents (id),\n_16\ncontent text not null,\n_16\nembedding vector (384)\n_16\n);\n`\n\nNotice how we record the `owner_id` on each document. Let's create an RLS policy that restricts access to `document_sections` based on whether or not they own the linked document:\n\n`\n_12\n-- enable row level security\n_12\nalter table document_sections enable row level security;\n_12\n_12\n-- setup RLS for select operations\n_12\ncreate policy \"Users can query their own document sections\"\n_12\non document_sections for select to authenticated using (\n_12\ndocument_id in (\n_12\n    select id\n_12\n    from documents\n_12\n    where (owner_id = (select auth.uid()))\n_12\n)\n_12\n);\n`\n\nIn this example, the current user is determined using the built-in `auth.uid()` function when the query is executed through your project's auto-generated [REST API](/docs/guides/api). If you are connecting to your Supabase database through a direct Postgres connection, see [Direct Postgres Connection](#direct-postgres-connection) below for directions on how to achieve the same access control.\n\nNow every `select` query executed on `document_sections` will implicitly filter the returned sections based on whether or not the current user has access to them.\n\nFor example, executing:\n\n`\n_10\nselect * from document_sections;\n`\n\nas an authenticated user will only return rows that they are the owner of (as determined by the linked document). More importantly, semantic search over these sections (or any additional filtering for that matter) will continue to respect these RLS policies:\n\n`\n_10\n-- Perform inner product similarity based on a match_threshold\n_10\nselect *\n_10\nfrom document_sections\n_10\nwhere document_sections.embedding <#> embedding < -match_threshold\n_10\norder by document_sections.embedding <#> embedding;\n`\n\nThe above example only configures `select` access to users. If you wanted, you could create more RLS policies for inserts, updates, and deletes in order to apply the same permission logic for those other operations. See [Row Level Security](/docs/guides/database/postgres/row-level-security) for a more in-depth guide on RLS policies.\n\n## Alternative scenarios [\\#](\\#alternative-scenarios)\n\nEvery app has its own unique requirements and may differ from the above example. Here are some alternative scenarios we often see and how they are implemented in Supabase.\n\n### Documents owned by multiple people [\\#](\\#documents-owned-by-multiple-people)\n\nInstead of a one-to-many relationship between `users` and `documents`, you may require a many-to-many relationship so that multiple people can access the same document. Let's reimplement this using a join table:\n\n`\n_10\ncreate table document_owners (\n_10\nid bigint primary key generated always as identity,\n_10\nowner_id uuid not null references auth.users (id) default auth.uid(),\n_10\ndocument_id bigint not null references documents (id)\n_10\n);\n`\n\nThen your RLS policy would change to:\n\n`\n_10\ncreate policy \"Users can query their own document sections\"\n_10\non document_sections for select to authenticated using (\n_10\ndocument_id in (\n_10\n    select document_id\n_10\n    from document_owners\n_10\n    where (owner_id = (select auth.uid()))\n_10\n)\n_10\n);\n`\n\nInstead of directly querying the `documents` table, we query the join table.\n\n### User and document data live outside of Supabase [\\#](\\#user-and-document-data-live-outside-of-supabase)\n\nYou may have an existing system that stores users, documents, and their permissions in a separate database. Let's explore the scenario where this data exists in another Postgres database. We'll use a foreign data wrapper (FDW) to connect to the external DB from within your Supabase DB:\n\nRLS is latency-sensitive, so extra caution should be taken before implementing this method. Use the [query plan analyzer](https://supabase.com/docs/guides/platform/performance#optimizing-poor-performing-queries) to measure execution times for your queries to ensure they are within expected ranges. For enterprise applications, contact [enterprise@supabase.io](mailto:enterprise@supabase.io).\n\nFor data sources other than Postgres, see [Foreign Data Wrappers](/docs/guides/database/extensions/wrappers/overview) for a list of external sources supported today. If your data lives in a source not provided in the list, please contact [support](https://supabase.com/dashboard/support/new) and we'll be happy to discuss your use case.\n\nLet's assume your external DB contains a `users` and `documents` table like this:\n\n`\n_12\ncreate table public.users (\n_12\nid bigint primary key generated always as identity,\n_12\nemail text not null,\n_12\ncreated_at timestamp with time zone not null default now()\n_12\n);\n_12\n_12\ncreate table public.documents (\n_12\nid bigint primary key generated always as identity,\n_12\nname text not null,\n_12\nowner_id bigint not null references public.users (id),\n_12\ncreated_at timestamp with time zone not null default now()\n_12\n);\n`\n\nIn your Supabase DB, let's create foreign tables that link to the above tables:\n\n`\n_16\ncreate schema external;\n_16\ncreate extension postgres_fdw with schema extensions;\n_16\n_16\n-- Setup the foreign server\n_16\ncreate server foreign_server\n_16\nforeign data wrapper postgres_fdw\n_16\noptions (host '<db-host>', port '<db-port>', dbname '<db-name>');\n_16\n_16\n-- Map local 'authenticated' role to external 'postgres' user\n_16\ncreate user mapping for authenticated\n_16\nserver foreign_server\n_16\noptions (user 'postgres', password '<user-password>');\n_16\n_16\n-- Import foreign 'users' and 'documents' tables into 'external' schema\n_16\nimport foreign schema public limit to (users, documents)\n_16\nfrom server foreign_server into external;\n`\n\nThis example maps the `authenticated` role in Supabase to the `postgres` user in the external DB. In production, it's best to create a custom user on the external DB that has the minimum permissions necessary to access the information you need.\n\nOn the Supabase DB, we use the built-in `authenticated` role which is automatically used when end users make authenticated requests over your auto-generated REST API. If you plan to connect to your Supabase DB over a direct Postgres connection instead of the REST API, you can change this to any user you like. See [Direct Postgres Connection](#direct-postgres-connection) for more info.\n\nWe'll store `document_sections` and their embeddings in Supabase so that we can perform similarity search over them via pgvector.\n\n`\n_10\ncreate table document_sections (\n_10\nid bigint primary key generated always as identity,\n_10\ndocument_id bigint not null,\n_10\ncontent text not null,\n_10\nembedding vector (384)\n_10\n);\n`\n\nWe maintain a reference to the foreign document via `document_id`, but without a foreign key reference since foreign keys can only be added to local tables. Be sure to use the same ID data type that you use on your external documents table.\n\nSince we're managing users and authentication outside of Supabase, we have two options:\n\n1. Make a direct Postgres connection to the Supabase DB and set the current user every request\n2. Issue a custom JWT from your system and use it to authenticate with the REST API\n\n#### Direct Postgres connection [\\#](\\#direct-postgres-connection)\n\nYou can directly connect to your Supabase Postgres DB using the [connection info](/dashboard/project/_/settings/database) on your project's database settings page. To use RLS with this method, we use a custom session variable that contains the current user's ID:\n\n`\n_12\n-- enable row level security\n_12\nalter table document_sections enable row level security;\n_12\n_12\n-- setup RLS for select operations\n_12\ncreate policy \"Users can query their own document sections\"\n_12\non document_sections for select to authenticated using (\n_12\ndocument_id in (\n_12\n    select id\n_12\n    from external.documents\n_12\n    where owner_id = current_setting('app.current_user_id')::bigint\n_12\n)\n_12\n);\n`\n\nThe session variable is accessed through the `current_setting()` function. We name the variable `app.current_user_id` here, but you can modify this to any name you like. We also cast it to a `bigint` since that was the data type of the `user.id` column. Change this to whatever data type you use for your ID.\n\nNow for every request, we set the user's ID at the beginning of the session:\n\n`\n_10\nset app.current_user_id = '<current-user-id>';\n`\n\nThen all subsequent queries will inherit the permission of that user:\n\n`\n_10\n-- Only document sections owned by the user are returned\n_10\nselect *\n_10\nfrom document_sections\n_10\nwhere document_sections.embedding <#> embedding < -match_threshold\n_10\norder by document_sections.embedding <#> embedding;\n`\n\nYou might be tempted to discard RLS completely and simply filter by user within the `where` clause. Though this will work, we recommend RLS as a general best practice since RLS is always applied even as new queries and application logic is introduced in the future.\n\n#### Custom JWT with REST API [\\#](\\#custom-jwt-with-rest-api)\n\nIf you would like to use the auto-generated REST API to query your Supabase database using JWTs from an external auth provider, you can get your auth provider to issue a custom JWT for Supabase.\n\nSee the [Clerk Supabase docs](https://clerk.com/docs/integrations/databases/supabase) for an example of how this can be done. Modify the instructions to work with your own auth provider as needed.\n\nNow we can simply use the same RLS policy from our first example:\n\n`\n_12\n-- enable row level security\n_12\nalter table document_sections enable row level security;\n_12\n_12\n-- setup RLS for select operations\n_12\ncreate policy \"Users can query their own document sections\"\n_12\non document_sections for select to authenticated using (\n_12\ndocument_id in (\n_12\n    select id\n_12\n    from documents\n_12\n    where (owner_id = (select auth.uid()))\n_12\n)\n_12\n);\n`\n\nUnder the hood, `auth.uid()` references `current_setting('request.jwt.claim.sub')` which corresponds to the JWT's `sub` (subject) claim. This setting is automatically set at the beginning of each request to the REST API.\n\nAll subsequent queries will inherit the permission of that user:\n\n`\n_10\n-- Only document sections owned by the user are returned\n_10\nselect *\n_10\nfrom document_sections\n_10\nwhere document_sections.embedding <#> embedding < -match_threshold\n_10\norder by document_sections.embedding <#> embedding;\n`\n\n### Other scenarios [\\#](\\#other-scenarios)\n\nThere are endless approaches to this problem based on the complexities of each system. Luckily Postgres comes with all the primitives needed to provide access control in the way that works best for your project.\n\nIf the examples above didn't fit your use case or you need to adjust them slightly to better fit your existing system, feel free to reach out to [support](https://supabase.com/dashboard/support/new) and we'll be happy to assist you.",
      "metadata": {
        "ogUrl": "https://supabase.com/docs/guides/ai/rag-with-permissions",
        "title": "RAG with Permissions | Supabase Docs",
        "robots": "index, follow",
        "ogImage": "https://obuldanrptloktxcffvn.supabase.co/functions/v1/og-images?site=docs&type=ai&title=RAG%20with%20Permissions&description=Implement%20fine-grain%20access%20control%20with%20retrieval%20augmented%20generation",
        "ogTitle": "RAG with Permissions | Supabase Docs",
        "language": "en",
        "sourceURL": "https://supabase.com/docs/guides/ai/rag-with-permissions",
        "description": "Implement fine-grain access control with retrieval augmented generation",
        "modifiedTime": "2024-09-06T19:20:40.691Z",
        "ogDescription": "Implement fine-grain access control with retrieval augmented generation",
        "publishedTime": "2024-09-06T19:20:40.691Z",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "AI & Vectors\n\n# LangChain\n\n* * *\n\n[LangChain](https://langchain.com/) is a popular framework for working with AI, Vectors, and embeddings. LangChain supports using Supabase as a [vector store](https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/supabase), using the `pgvector` extension.\n\n## Initializing your database [\\#](\\#initializing-your-database)\n\nPrepare you database with the relevant tables:\n\nDashboardSQL\n\n1. Go to the [SQL Editor](https://supabase.com/dashboard/project/_/sql) page in the Dashboard.\n2. Click **LangChain** in the Quick start section.\n3. Click **Run**.\n\n## Usage [\\#](\\#usage)\n\nYou can now search your documents using any Node.js application. This is intended to be run on a secure server route.\n\n``\n_28\nimport { SupabaseVectorStore } from 'langchain/vectorstores/supabase'\n_28\nimport { OpenAIEmbeddings } from 'langchain/embeddings/openai'\n_28\nimport { createClient } from '@supabase/supabase-js'\n_28\n_28\nconst supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY\n_28\nif (!supabaseKey) throw new Error(`Expected SUPABASE_SERVICE_ROLE_KEY`)\n_28\n_28\nconst url = process.env.SUPABASE_URL\n_28\nif (!url) throw new Error(`Expected env var SUPABASE_URL`)\n_28\n_28\nexport const run = async () => {\n_28\nconst client = createClient(url, supabaseKey)\n_28\n_28\nconst vectorStore = await SupabaseVectorStore.fromTexts(\n_28\n    ['Hello world', 'Bye bye', \"What's this?\"],\n_28\n    [{ id: 2 }, { id: 1 }, { id: 3 }],\n_28\n    new OpenAIEmbeddings(),\n_28\n    {\n_28\n      client,\n_28\n      tableName: 'documents',\n_28\n      queryName: 'match_documents',\n_28\n    }\n_28\n)\n_28\n_28\nconst resultOne = await vectorStore.similaritySearch('Hello world', 1)\n_28\n_28\nconsole.log(resultOne)\n_28\n}\n``\n\n### Simple metadata filtering [\\#](\\#simple-metadata-filtering)\n\nGiven the above `match_documents` Postgres function, you can also pass a filter parameter to only return documents with a specific metadata field value. This filter parameter is a JSON object, and the `match_documents` function will use the Postgres JSONB Containment operator `@>` to filter documents by the metadata field values you specify. See details on the [Postgres JSONB Containment operator](https://www.postgresql.org/docs/current/datatype-json.html#JSON-CONTAINMENT) for more information.\n\n``\n_32\nimport { SupabaseVectorStore } from 'langchain/vectorstores/supabase'\n_32\nimport { OpenAIEmbeddings } from 'langchain/embeddings/openai'\n_32\nimport { createClient } from '@supabase/supabase-js'\n_32\n_32\n// First, follow set-up instructions above\n_32\n_32\nconst privateKey = process.env.SUPABASE_SERVICE_ROLE_KEY\n_32\nif (!privateKey) throw new Error(`Expected env var SUPABASE_SERVICE_ROLE_KEY`)\n_32\n_32\nconst url = process.env.SUPABASE_URL\n_32\nif (!url) throw new Error(`Expected env var SUPABASE_URL`)\n_32\n_32\nexport const run = async () => {\n_32\nconst client = createClient(url, privateKey)\n_32\n_32\nconst vectorStore = await SupabaseVectorStore.fromTexts(\n_32\n    ['Hello world', 'Hello world', 'Hello world'],\n_32\n    [{ user_id: 2 }, { user_id: 1 }, { user_id: 3 }],\n_32\n    new OpenAIEmbeddings(),\n_32\n    {\n_32\n      client,\n_32\n      tableName: 'documents',\n_32\n      queryName: 'match_documents',\n_32\n    }\n_32\n)\n_32\n_32\nconst result = await vectorStore.similaritySearch('Hello world', 1, {\n_32\n    user_id: 3,\n_32\n})\n_32\n_32\nconsole.log(result)\n_32\n}\n``\n\n### Advanced metadata filtering [\\#](\\#advanced-metadata-filtering)\n\nYou can also use query builder-style filtering ( [similar to how the Supabase JavaScript library works](https://supabase.com/docs/reference/javascript/using-filters)) instead of passing an object. Note that since the filter properties will be in the metadata column, you need to use arrow operators ( `->` for integer or `->>` for text) as defined in [Postgrest API documentation](https://postgrest.org/en/stable/references/api/tables_views.html?highlight=operators#json-columns) and specify the data type of the property (e.g. the column should look something like `metadata->some_int_value::int`).\n\n``\n_62\nimport { SupabaseFilterRPCCall, SupabaseVectorStore } from 'langchain/vectorstores/supabase'\n_62\nimport { OpenAIEmbeddings } from 'langchain/embeddings/openai'\n_62\nimport { createClient } from '@supabase/supabase-js'\n_62\n_62\n// First, follow set-up instructions above\n_62\n_62\nconst privateKey = process.env.SUPABASE_SERVICE_ROLE_KEY\n_62\nif (!privateKey) throw new Error(`Expected env var SUPABASE_SERVICE_ROLE_KEY`)\n_62\n_62\nconst url = process.env.SUPABASE_URL\n_62\nif (!url) throw new Error(`Expected env var SUPABASE_URL`)\n_62\n_62\nexport const run = async () => {\n_62\nconst client = createClient(url, privateKey)\n_62\n_62\nconst embeddings = new OpenAIEmbeddings()\n_62\n_62\nconst store = new SupabaseVectorStore(embeddings, {\n_62\n    client,\n_62\n    tableName: 'documents',\n_62\n})\n_62\n_62\nconst docs = [\\\n_62\\\n    {\\\n_62\\\n      pageContent:\\\n_62\\\n        'This is a long text, but it actually means something because vector database does not understand Lorem Ipsum. So I would need to expand upon the notion of quantum fluff, a theoretical concept where subatomic particles coalesce to form transient multidimensional spaces. Yet, this abstraction holds no real-world application or comprehensible meaning, reflecting a cosmic puzzle.',\\\n_62\\\n      metadata: { b: 1, c: 10, stuff: 'right' },\\\n_62\\\n    },\\\n_62\\\n    {\\\n_62\\\n      pageContent:\\\n_62\\\n        'This is a long text, but it actually means something because vector database does not understand Lorem Ipsum. So I would need to proceed by discussing the echo of virtual tweets in the binary corridors of the digital universe. Each tweet, like a pixelated canary, hums in an unseen frequency, a fascinatingly perplexing phenomenon that, while conjuring vivid imagery, lacks any concrete implication or real-world relevance, portraying a paradox of multidimensional spaces in the age of cyber folklore.',\\\n_62\\\n      metadata: { b: 2, c: 9, stuff: 'right' },\\\n_62\\\n    },\\\n_62\\\n    { pageContent: 'hello', metadata: { b: 1, c: 9, stuff: 'right' } },\\\n_62\\\n    { pageContent: 'hello', metadata: { b: 1, c: 9, stuff: 'wrong' } },\\\n_62\\\n    { pageContent: 'hi', metadata: { b: 2, c: 8, stuff: 'right' } },\\\n_62\\\n    { pageContent: 'bye', metadata: { b: 3, c: 7, stuff: 'right' } },\\\n_62\\\n    { pageContent: \"what's this\", metadata: { b: 4, c: 6, stuff: 'right' } },\\\n_62\\\n]\n_62\n_62\nawait store.addDocuments(docs)\n_62\n_62\nconst funcFilterA: SupabaseFilterRPCCall = (rpc) =>\n_62\n    rpc\n_62\n      .filter('metadata->b::int', 'lt', 3)\n_62\n      .filter('metadata->c::int', 'gt', 7)\n_62\n      .textSearch('content', `'multidimensional' & 'spaces'`, {\n_62\n        config: 'english',\n_62\n      })\n_62\n_62\nconst resultA = await store.similaritySearch('quantum', 4, funcFilterA)\n_62\n_62\nconst funcFilterB: SupabaseFilterRPCCall = (rpc) =>\n_62\n    rpc\n_62\n      .filter('metadata->b::int', 'lt', 3)\n_62\n      .filter('metadata->c::int', 'gt', 7)\n_62\n      .filter('metadata->>stuff', 'eq', 'right')\n_62\n_62\nconst resultB = await store.similaritySearch('hello', 2, funcFilterB)\n_62\n_62\nconsole.log(resultA, resultB)\n_62\n}\n``\n\n## Hybrid search [\\#](\\#hybrid-search)\n\nLangChain supports the concept of a hybrid search, which combines Similarity Search with Full Text Search. Read the official docs to get started: [Supabase Hybrid Search](https://js.langchain.com/docs/modules/indexes/retrievers/supabase-hybrid).\n\nYou can install the LangChain Hybrid Search function though our [database.dev package manager](https://database.dev/langchain/hybrid_search).\n\n## Resources [\\#](\\#resources)\n\n- Official [LangChain site](https://langchain.com/).\n- Official [LangChain docs](https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/supabase).\n- Supabase [Hybrid Search](https://js.langchain.com/docs/modules/indexes/retrievers/supabase-hybrid).",
      "metadata": {
        "ogUrl": "https://supabase.com/docs/guides/ai/langchain",
        "title": "LangChain | Supabase Docs",
        "robots": "index, follow",
        "ogImage": "https://obuldanrptloktxcffvn.supabase.co/functions/v1/og-images?site=docs&type=ai&title=LangChain&description=Learn%20how%20to%20integrate%20Supabase%20with%20LangChain%2C%20a%20popular%20framework%20for%20composing%20AI%2C%20Vectors%2C%20and%20embeddings",
        "ogTitle": "LangChain | Supabase Docs",
        "language": "en",
        "sourceURL": "https://supabase.com/docs/guides/ai/langchain",
        "description": "Learn how to integrate Supabase with LangChain, a popular framework for composing AI, Vectors, and embeddings",
        "modifiedTime": "2024-09-06T19:20:40.301Z",
        "ogDescription": "Learn how to integrate Supabase with LangChain, a popular framework for composing AI, Vectors, and embeddings",
        "publishedTime": "2024-09-06T19:20:40.300Z",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "AI & Vectors\n\n# Vector columns\n\n* * *\n\nSupabase offers a number of different ways to store and query vectors within Postgres. The SQL included in this guide is applicable for clients in all programming languages. If you are a Python user see your [Python client options](/docs/guides/ai/python-clients) after reading the `Learn` section.\n\nVectors in Supabase are enabled via [pgvector](https://github.com/pgvector/pgvector/), a PostgreSQL extension for storing and querying vectors in Postgres. It can be used to store [embeddings](/docs/guides/ai/concepts#what-are-embeddings).\n\n## Usage [\\#](\\#usage)\n\n### Enable the extension [\\#](\\#enable-the-extension)\n\nDashboardSQL\n\n1. Go to the [Database](https://supabase.com/dashboard/project/_/database/tables) page in the Dashboard.\n2. Click on **Extensions** in the sidebar.\n3. Search for \"vector\" and enable the extension.\n\n### Create a table to store vectors [\\#](\\#create-a-table-to-store-vectors)\n\nAfter enabling the `vector` extension, you will get access to a new data type called `vector`. The size of the vector (indicated in parenthesis) represents the number of dimensions stored in that vector.\n\n`\n_10\ncreate table documents (\n_10\nid serial primary key,\n_10\ntitle text not null,\n_10\nbody text not null,\n_10\nembedding vector(384)\n_10\n);\n`\n\nIn the above SQL snippet, we create a `documents` table with a column called `embedding` (note this is just a regular Postgres column - you can name it whatever you like). We give the `embedding` column a `vector` data type with 384 dimensions. Change this to the number of dimensions produced by your embedding model. For example, if you are [generating embeddings](/docs/guides/ai/quickstarts/generate-text-embeddings) using the open source [`gte-small`](https://huggingface.co/Supabase/gte-small) model, you would set this number to 384 since that model produces 384 dimensions.\n\nIn general, embeddings with fewer dimensions perform best. See our [analysis on fewer dimensions in pgvector](https://supabase.com/blog/fewer-dimensions-are-better-pgvector).\n\n### Storing a vector / embedding [\\#](\\#storing-a-vector--embedding)\n\nIn this example we'll generate a vector using Transformers.js, then store it in the database using the Supabase JavaScript client.\n\n`\n_21\nimport { pipeline } from '@xenova/transformers'\n_21\nconst generateEmbedding = await pipeline('feature-extraction', 'Supabase/gte-small')\n_21\n_21\nconst title = 'First post!'\n_21\nconst body = 'Hello world!'\n_21\n_21\n// Generate a vector using Transformers.js\n_21\nconst output = await generateEmbedding(body, {\n_21\npooling: 'mean',\n_21\nnormalize: true,\n_21\n})\n_21\n_21\n// Extract the embedding output\n_21\nconst embedding = Array.from(output.data)\n_21\n_21\n// Store the vector in Postgres\n_21\nconst { data, error } = await supabase.from('documents').insert({\n_21\ntitle,\n_21\nbody,\n_21\nembedding,\n_21\n})\n`\n\nThis example uses the JavaScript Supabase client, but you can modify it to work with any [supported language library](/docs#client-libraries).\n\n### Querying a vector / embedding [\\#](\\#querying-a-vector--embedding)\n\nSimilarity search is the most common use case for vectors. `pgvector` support 3 new operators for computing distance:\n\n| Operator | Description |\n| --- | --- |\n| `<->` | Euclidean distance |\n| `<#>` | negative inner product |\n| `<=>` | cosine distance |\n\nChoosing the right operator depends on your needs. Dot product tends to be the fastest if your vectors are normalized. For more information on how embeddings work and how they relate to each other, see [What are Embeddings?](/docs/guides/ai/concepts#what-are-embeddings).\n\nSupabase client libraries like `supabase-js` connect to your Postgres instance via [PostgREST](/docs/guides/getting-started/architecture#postgrest-api). PostgREST does not currently support `pgvector` similarity operators, so we'll need to wrap our query in a Postgres function and call it via the `rpc()` method:\n\n`\n_23\ncreate or replace function match_documents (\n_23\nquery_embedding vector(384),\n_23\nmatch_threshold float,\n_23\nmatch_count int\n_23\n)\n_23\nreturns table (\n_23\nid bigint,\n_23\ntitle text,\n_23\nbody text,\n_23\nsimilarity float\n_23\n)\n_23\nlanguage sql stable\n_23\nas $$\n_23\nselect\n_23\n    documents.id,\n_23\n    documents.title,\n_23\n    documents.body,\n_23\n    1 - (documents.embedding <=> query_embedding) as similarity\n_23\nfrom documents\n_23\nwhere 1 - (documents.embedding <=> query_embedding) > match_threshold\n_23\norder by (documents.embedding <=> query_embedding) asc\n_23\nlimit match_count;\n_23\n$$;\n`\n\nThis function takes a `query_embedding` argument and compares it to all other embeddings in the `documents` table. Each comparison returns a similarity score. If the similarity is greater than the `match_threshold` argument, it is returned. The number of rows returned is limited by the `match_count` argument.\n\nFeel free to modify this method to fit the needs of your application. The `match_threshold` ensures that only documents that have a minimum similarity to the `query_embedding` are returned. Without this, you may end up returning documents that subjectively don't match. This value will vary for each application - you will need to perform your own testing to determine the threshold that makes sense for your app.\n\nIf you index your vector column, ensure that the `order by` sorts by the distance function directly (rather than sorting by the calculated `similarity` column, which may lead to the index being ignored and poor performance).\n\nTo execute the function from your client library, call `rpc()` with the name of your Postgres function:\n\n`\n_10\nconst { data: documents } = await supabaseClient.rpc('match_documents', {\n_10\nquery_embedding: embedding, // Pass the embedding you want to compare\n_10\nmatch_threshold: 0.78, // Choose an appropriate threshold for your data\n_10\nmatch_count: 10, // Choose the number of matches\n_10\n})\n`\n\nIn this example `embedding` would be another embedding you wish to compare against your table of pre-generated embedding documents. For example if you were building a search engine, every time the user submits their query you would first generate an embedding on the search query itself, then pass it into the above `rpc()` function to match.\n\nBe sure to use embeddings produced from the same embedding model when calculating distance. Comparing embeddings from two different models will produce no meaningful result.\n\nVectors and embeddings can be used for much more than search. Learn more about embeddings at [What are Embeddings?](/docs/guides/ai/concepts#what-are-embeddings).\n\n### Indexes [\\#](\\#indexes)\n\nOnce your vector table starts to grow, you will likely want to add an index to speed up queries. See [Vector indexes](/docs/guides/ai/vector-indexes) to learn how vector indexes work and how to create them.",
      "metadata": {
        "ogUrl": "https://supabase.com/docs/guides/ai/vector-columns",
        "title": "Vector columns | Supabase Docs",
        "robots": "index, follow",
        "ogImage": "https://obuldanrptloktxcffvn.supabase.co/functions/v1/og-images?site=docs&type=ai&title=Vector%20columns&description=Learn%20how%20to%20use%20vectors%20within%20your%20own%20Postgres%20tables",
        "ogTitle": "Vector columns | Supabase Docs",
        "language": "en",
        "sourceURL": "https://supabase.com/docs/guides/ai/vector-columns",
        "description": "Learn how to use vectors within your own Postgres tables",
        "modifiedTime": "2024-09-06T19:20:41.221Z",
        "ogDescription": "Learn how to use vectors within your own Postgres tables",
        "publishedTime": "2024-09-06T19:20:41.221Z",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "AI & Vectors\n\n# Hugging Face Inference API\n\n* * *\n\n[Hugging Face](https://huggingface.co) is an open source hub for AI/ML models and tools. With over 100,000 machine learning models available, Hugging Face provides a great way to integrate specialized AI & ML tasks into your application.\n\nThere are 3 ways to use Hugging Face models in your application:\n\n1. Use the [Transformers](https://huggingface.co/docs/transformers/index) Python library to perform inference in a Python backend.\n2. [Generate embeddings](/docs/guides/ai/quickstarts/generate-text-embeddings) directly in Edge Functions using Transformers.js.\n3. Use Hugging Face's hosted [Inference API](https://huggingface.co/inference-api) to execute AI tasks remotely on Hugging Face servers. This guide will walk you through this approach.\n\n## AI tasks [\\#](\\#ai-tasks)\n\nBelow are some of the types of tasks you can perform with Hugging Face:\n\n### Natural language [\\#](\\#natural-language)\n\n- [Summarization](https://huggingface.co/tasks/summarization)\n- [Text classification](https://huggingface.co/tasks/text-classification)\n- [Text generation](https://huggingface.co/tasks/text-generation)\n- [Translation](https://huggingface.co/tasks/translation)\n- [Fill in the blank](https://huggingface.co/tasks/fill-mask)\n\n### Computer vision [\\#](\\#computer-vision)\n\n- [Image to text](https://huggingface.co/tasks/image-to-text)\n- [Text to image](https://huggingface.co/tasks/text-to-image)\n- [Image classification](https://huggingface.co/tasks/image-classification)\n- [Video classification](https://huggingface.co/tasks/video-classification)\n- [Object detection](https://huggingface.co/tasks/object-detection)\n- [Image segmentation](https://huggingface.co/tasks/image-segmentation)\n\n### Audio [\\#](\\#audio)\n\n- [Text to speech](https://huggingface.co/tasks/text-to-speech)\n- [Speech to text](https://huggingface.co/tasks/automatic-speech-recognition)\n- [Audio classification](https://huggingface.co/tasks/audio-classification)\n\nSee a [full list of tasks](https://huggingface.co/tasks).\n\n## Access token [\\#](\\#access-token)\n\nFirst generate a Hugging Face access token for your app:\n\n[https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n\nName your token based on the app its being used for and the environment. For example, if you are building an image generation app you might create 2 tokens:\n\n- \"My Image Generator (Dev)\"\n- \"My Image Generator (Prod)\"\n\nSince we will be using this token for the inference API, choose the `read` role.\n\nThough it is possible to use the Hugging Face inference API today without an access token, [you may be rate limited](https://huggingface.co/docs/huggingface.js/inference/README#usage).\n\nTo ensure you don't experience any unexpected downtime or errors, we recommend creating an access token.\n\n## Edge Functions [\\#](\\#edge-functions)\n\nEdge Functions are server-side TypeScript functions that run on-demand. Since Edge Functions run on a server, you can safely give them access to your Hugging Face access token.\n\nYou will need the `supabase` CLI [installed](/docs/guides/cli) for the following commands to work.\n\nTo create a new Edge Function, navigate to your local project and initialize Supabase if you haven't already:\n\n`\n_10\nsupabase init\n`\n\nThen create an Edge Function:\n\n`\n_10\nsupabase functions new text-to-image\n`\n\nCreate a file called `.env.local` to store your Hugging Face access token:\n\n`\n_10\nHUGGING_FACE_ACCESS_TOKEN=<your-token-here>\n`\n\nLet's modify the Edge Function to import Hugging Face's inference client and perform a `text-to-image` request:\n\n`\n_20\nimport { serve } from 'https://deno.land/std@0.168.0/http/server.ts'\n_20\nimport { HfInference } from 'https://esm.sh/@huggingface/inference@2.3.2'\n_20\n_20\nconst hf = new HfInference(Deno.env.get('HUGGING_FACE_ACCESS_TOKEN'))\n_20\n_20\nserve(async (req) => {\n_20\nconst { prompt } = await req.json()\n_20\n_20\nconst image = await hf.textToImage(\n_20\n    {\n_20\n      inputs: prompt,\n_20\n      model: 'stabilityai/stable-diffusion-2',\n_20\n    },\n_20\n    {\n_20\n      use_cache: false,\n_20\n    }\n_20\n)\n_20\n_20\nreturn new Response(image)\n_20\n})\n`\n\n1. This function creates a new instance of `HfInference` using the `HUGGING_FACE_ACCESS_TOKEN` environment variable.\n\n2. It expects a POST request that includes a JSON request body. The JSON body should include a parameter called `prompt` that represents the text-to-image prompt that we will pass to Hugging Face's inference API.\n\n3. Next we call `textToImage()`, passing in the user's prompt along with the model that we would like to use for the image generation. Today Hugging Face recommends `stabilityai/stable-diffusion-2`, but you can change this to any other text-to-image model. You can see a list of which models are supported for each task by navigating to their [models page](https://huggingface.co/models?pipeline_tag=text-to-image) and filtering by task.\n\n4. We set `use_cache` to `false` so that repeat queries with the same prompt will produce new images. If the task and model you are using is deterministic (will always produce the same result based on the same input), consider setting `use_cache` to `true` for faster responses.\n\n5. The `image` result returned from the API will be a `Blob`. We can pass the `Blob` directly into a `new Response()` which will automatically set the content type and body of the response from the `image`.\n\n\nFinally let's serve the Edge Function locally to test it:\n\n`\n_10\nsupabase functions serve --env-file .env.local --no-verify-jwt\n`\n\nRemember to pass in the `.env.local` file using the `--env-file` parameter so that the Edge Function can access the `HUGGING_FACE_ACCESS_TOKEN`.\n\nFor demo purposes we set `--no-verify-jwt` to make it easy to test the Edge Function without passing in a JWT token. In a real application you will need to pass the JWT as a `Bearer` token in the `Authorization` header.\n\nAt this point, you can make an API request to your Edge Function using your preferred frontend framework (Next.js, React, Expo, etc). We can also test from the terminal using `curl`:\n\n`\n_10\ncurl --output result.jpg --location --request POST 'http://localhost:54321/functions/v1/text-to-image' \\\n_10\n  --header 'Content-Type: application/json' \\\n_10\n  --data '{\"prompt\":\"Llama wearing sunglasses\"}'\n`\n\nIn this example, your generated image will save to `result.jpg`:\n\n![Llama wearing sunglasses example](https://supabase.com/docs/img/ai/hugging-face/llama-sunglasses-example.png)\n\n## Next steps [\\#](\\#next-steps)\n\nYou can now create an Edge Function that invokes a Hugging Face task using your model of choice.\n\nTry running some other [AI tasks](#ai-tasks).\n\n## Resources [\\#](\\#resources)\n\n- Official [Hugging Face site](https://huggingface.co/).\n- Official [Hugging Face JS docs](https://huggingface.co/docs/huggingface.js).\n- [Generate image captions](/docs/guides/ai/examples/huggingface-image-captioning) using Hugging Face.",
      "metadata": {
        "ogUrl": "https://supabase.com/docs/guides/ai/hugging-face",
        "title": "Hugging Face Inference API | Supabase Docs",
        "robots": "index, follow",
        "ogImage": "https://obuldanrptloktxcffvn.supabase.co/functions/v1/og-images?site=docs&type=ai&title=Hugging%20Face%20Inference%20API&description=Learn%20how%20to%20integrate%20hugging%20face%20models%20with%20Supabase",
        "ogTitle": "Hugging Face Inference API | Supabase Docs",
        "language": "en",
        "sourceURL": "https://supabase.com/docs/guides/ai/hugging-face",
        "description": "Learn how to integrate hugging face models with Supabase",
        "modifiedTime": "2024-09-06T19:20:40.301Z",
        "ogDescription": "Learn how to integrate hugging face models with Supabase",
        "publishedTime": "2024-09-06T19:20:40.300Z",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "AI & Vectors\n\n# Video Search with Mixpeek Multimodal Embeddings\n\n## Implement video search with the Mixpeek Multimodal Embed API and Supabase Vector.\n\n* * *\n\nThe [Mixpeek Embed API](https://docs.mixpeek.com/api-documentation/inference/embed) allows you to generate embeddings for various types of content, including videos and text. You can use these embeddings for:\n\n- Text-to-Video / Video-To-Text / Video-to-Video / Text-to-Text Search\n- Fine-tuning on your own video and text data\n\nThis guide demonstrates how to implement video search using Mixpeek Embed for video processing and embedding, and Supabase Vector for storing and querying embeddings.\n\nYou can find the full application code as a Python Poetry project on [GitHub](https://github.com/yourusername/your-repo-name).\n\n## Create a new Python project with Poetry [\\#](\\#create-a-new-python-project-with-poetry)\n\n[Poetry](https://python-poetry.org/) provides packaging and dependency management for Python. If you haven't already, install poetry via pip:\n\n`\n_10\npip install poetry\n`\n\nThen initialize a new project:\n\n`\n_10\npoetry new video-search\n`\n\n## Setup Supabase project [\\#](\\#setup-supabase-project)\n\nIf you haven't already, [install the Supabase CLI](https://supabase.com/docs/guides/cli), then initialize Supabase in the root of your newly created poetry project:\n\n`\n_10\nsupabase init\n`\n\nNext, start your local Supabase stack:\n\n`\n_10\nsupabase start\n`\n\nThis will start up the Supabase stack locally and print out a bunch of environment details, including your local `DB URL`. Make a note of that for later use.\n\n## Install the dependencies [\\#](\\#install-the-dependencies)\n\nAdd the following dependencies to your project:\n\n- [`supabase`](https://github.com/supabase-community/supabase-py): Supabase Python Client\n- [`mixpeek`](https://github.com/mixpeek/python-client): Mixpeek Python Client for embedding generation\n\n`\n_10\npoetry add supabase mixpeek\n`\n\n## Import the necessary dependencies [\\#](\\#import-the-necessary-dependencies)\n\nAt the top of your main Python script, import the dependencies and store your environment variables:\n\n`\n_10\nfrom supabase import create_client, Client\n_10\nfrom mixpeek import Mixpeek\n_10\nimport os\n_10\n_10\nSUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n_10\nSUPABASE_KEY = os.getenv(\"SUPABASE_API_KEY\")\n_10\nMIXPEEK_API_KEY = os.getenv(\"MIXPEEK_API_KEY\")\n`\n\n## Create embeddings for your videos [\\#](\\#create-embeddings-for-your-videos)\n\nNext, create a `seed` method, which will create a new Supabase table, generate embeddings for your video chunks, and insert the embeddings into your database:\n\n`\n_46\ndef seed():\n_46\n    # Initialize Supabase and Mixpeek clients\n_46\n    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n_46\n    mixpeek = Mixpeek(MIXPEEK_API_KEY)\n_46\n_46\n    # Create a table for storing video chunk embeddings\n_46\n    supabase.table(\"video_chunks\").create({\n_46\n        \"id\": \"text\",\n_46\n        \"start_time\": \"float8\",\n_46\n        \"end_time\": \"float8\",\n_46\n        \"embedding\": \"vector(768)\",\n_46\n        \"metadata\": \"jsonb\"\n_46\n    })\n_46\n_46\n    # Process and embed video\n_46\n    video_url = \"https://example.com/your_video.mp4\"\n_46\n    processed_chunks = mixpeek.tools.video.process(\n_46\n        video_source=video_url,\n_46\n        chunk_interval=1,  # 1 second intervals\n_46\n        resolution=[720, 1280]\n_46\n    )\n_46\n_46\n    for chunk in processed_chunks:\n_46\n        print(f\"Processing video chunk: {chunk['start_time']}\")\n_46\n_46\n        # Generate embedding using Mixpeek\n_46\n        embed_response = mixpeek.embed.video(\n_46\n            model_id=\"vuse-generic-v1\",\n_46\n            input=chunk['base64_chunk'],\n_46\n            input_type=\"base64\"\n_46\n        )\n_46\n_46\n        # Insert into Supabase\n_46\n        supabase.table(\"video_chunks\").insert({\n_46\n            \"id\": f\"chunk_{chunk['start_time']}\",\n_46\n            \"start_time\": chunk[\"start_time\"],\n_46\n            \"end_time\": chunk[\"end_time\"],\n_46\n            \"embedding\": embed_response['embedding'],\n_46\n            \"metadata\": {\"video_url\": video_url}\n_46\n        }).execute()\n_46\n_46\n    print(\"Video processed and embeddings inserted\")\n_46\n_46\n    # Create index for fast search performance\n_46\n    supabase.query(\"CREATE INDEX ON video_chunks USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100)\").execute()\n_46\n    print(\"Created index\")\n`\n\nAdd this method as a script in your `pyproject.toml` file:\n\n`\n_10\n[tool.poetry.scripts]\n_10\nseed = \"video_search.main:seed\"\n_10\nsearch = \"video_search.main:search\"\n`\n\nAfter activating the virtual environment with `poetry shell`, you can now run your seed script via `poetry run seed`. You can inspect the generated embeddings in your local database by visiting the local Supabase dashboard at [localhost:54323](http://localhost:54323/project/default/editor).\n\n## Perform a video search from a text query [\\#](\\#perform-a-video-search-from-a-text-query)\n\nWith Supabase Vector, you can query your embeddings. You can use either a video clip as search input or alternatively, you can generate an embedding from a string input and use that as the query input:\n\n`\n_32\ndef search():\n_32\n    # Initialize Supabase and Mixpeek clients\n_32\n    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n_32\n    mixpeek = Mixpeek(MIXPEEK_API_KEY)\n_32\n_32\n    # Generate embedding for text query\n_32\n    query_string = \"a car chase scene\"\n_32\n    text_emb = mixpeek.embed.video(\n_32\n        model_id=\"vuse-generic-v1\",\n_32\n        input=query_string,\n_32\n        input_type=\"text\"\n_32\n    )\n_32\n_32\n    # Query the collection\n_32\n    results = supabase.rpc(\n_32\n        'match_video_chunks',\n_32\n        {\n_32\n            'query_embedding': text_emb['embedding'],\n_32\n            'match_threshold': 0.8,\n_32\n            'match_count': 5\n_32\n        }\n_32\n    ).execute()\n_32\n_32\n    # Display the results\n_32\n    if results.data:\n_32\n        for result in results.data:\n_32\n            print(f\"Matched chunk from {result['start_time']} to {result['end_time']} seconds\")\n_32\n            print(f\"Video URL: {result['metadata']['video_url']}\")\n_32\n            print(f\"Similarity: {result['similarity']}\")\n_32\n            print(\"---\")\n_32\n    else:\n_32\n        print(\"No matching video chunks found\")\n`\n\nThis query will return the top 5 most similar video chunks from your database.\n\nYou can now test it out by running `poetry run search`, and you will be presented with the most relevant video chunks to the query \"a car chase scene\".\n\n## Conclusion [\\#](\\#conclusion)\n\nWith just a couple of Python scripts, you are able to implement video search as well as reverse video search using Mixpeek Embed and Supabase Vector. This approach allows for powerful semantic search capabilities that can be integrated into various applications, enabling you to search through video content using both text and video queries.",
      "metadata": {
        "ogUrl": "https://supabase.com/docs/guides/ai/examples/mixpeek-video-search",
        "title": "Video Search with Mixpeek Multimodal Embeddings | Supabase Docs",
        "robots": "index, follow",
        "ogImage": "https://obuldanrptloktxcffvn.supabase.co/functions/v1/og-images?site=docs&type=ai&title=Video%20Search%20with%20Mixpeek%20Multimodal%20Embeddings&description=Implement%20video%20search%20with%20the%20Mixpeek%20Multimodal%20Embed%20API%20and%20Supabase%20Vector.",
        "ogTitle": "Video Search with Mixpeek Multimodal Embeddings | Supabase Docs",
        "language": "en",
        "sourceURL": "https://supabase.com/docs/guides/ai/examples/mixpeek-video-search",
        "description": "Implement video search with the Mixpeek Multimodal Embed API and Supabase Vector.",
        "modifiedTime": "2024-09-06T19:20:40.691Z",
        "ogDescription": "Implement video search with the Mixpeek Multimodal Embed API and Supabase Vector.",
        "publishedTime": "2024-09-06T19:20:40.691Z",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "AI & Vectors\n\n# HNSW indexes\n\n* * *\n\nHNSW is an algorithm for approximate nearest neighbor search. It is a frequently used index type that can improve performance when querying highly-dimensional vectors, like those representing embeddings.\n\n## Usage [\\#](\\#usage)\n\nThe way you create an HNSW index depends on the distance operator you are using. `pgvector` includes 3 distance operators:\n\n| Operator | Description | [**Operator class**](https://www.postgresql.org/docs/current/sql-createopclass.html) |\n| --- | --- | --- |\n| `<->` | Euclidean distance | `vector_l2_ops` |\n| `<#>` | negative inner product | `vector_ip_ops` |\n| `<=>` | cosine distance | `vector_cosine_ops` |\n\nUse the following SQL commands to create an HNSW index for the operator(s) used in your queries.\n\n### Euclidean L2 distance ( `vector_l2_ops`) [\\#](\\#euclidean-l2-distance--vectorl2ops-)\n\n`\n_10\ncreate index on items using hnsw (column_name vector_l2_ops);\n`\n\n### Inner product ( `vector_ip_ops`) [\\#](\\#inner-product--vectoripops-)\n\n`\n_10\ncreate index on items using hnsw (column_name vector_ip_ops);\n`\n\n### Cosine distance ( `vector_cosine_ops`) [\\#](\\#cosine-distance--vectorcosineops-)\n\n`\n_10\ncreate index on items using hnsw (column_name vector_cosine_ops);\n`\n\nCurrently vectors with up to 2,000 dimensions can be indexed.\n\n## How does HNSW work? [\\#](\\#how-does-hnsw-work)\n\nHNSW uses proximity graphs (graphs connecting nodes based on distance between them) to approximate nearest-neighbor search. To understand HNSW, we can break it down into 2 parts:\n\n- **Hierarchical (H):** The algorithm operates over multiple layers\n- **Navigable Small World (NSW):** Each vector is a node within a graph and is connected to several other nodes\n\n### Hierarchical [\\#](\\#hierarchical)\n\nThe hierarchical aspect of HNSW builds off of the idea of skip lists.\n\nSkip lists are multi-layer linked lists. The bottom layer is a regular linked list connecting an ordered sequence of elements. Each new layer above removes some elements from the underlying layer (based on a fixed probability), producing a sparser subsequence that \u201cskips\u201d over elements.\n\nWhen searching for an element, the algorithm begins at the top layer and traverses its linked list horizontally. If the target element is found, the algorithm stops and returns it. Otherwise if the next element in the list is greater than the target (or `NULL`), the algorithm drops down to the next layer below. Since each layer below is less sparse than the layer above (with the bottom layer connecting all elements), the target will eventually be found. Skip lists offer O(log n) average complexity for both search and insertion/deletion.\n\n### Navigable Small World [\\#](\\#navigable-small-world)\n\nA navigable small world (NSW) is a special type of proximity graph that also includes long-range connections between nodes. These long-range connections support the \u201csmall world\u201d property of the graph, meaning almost every node can be reached from any other node within a few hops. Without these additional long-range connections, many hops would be required to reach a far-away node.\n\nThe \u201cnavigable\u201d part of NSW specifically refers to the ability to logarithmically scale the greedy search algorithm on the graph, an algorithm that attempts to make only the locally optimal choice at each hop. Without this property, the graph may still be considered a small world with short paths between far-away nodes, but the greedy algorithm tends to miss them. Greedy search is ideal for NSW because it is quick to navigate and has low computational costs.\n\n### **Hierarchical +** Navigable Small World [\\#](\\#hierarchical--navigable-small-world)\n\nHNSW combines these two concepts. From the hierarchical perspective, the bottom layer consists of a NSW made up of short links between nodes. Each layer above \u201cskips\u201d elements and creates longer links between nodes further away from each other.\n\nJust like skip lists, search starts at the top layer and works its way down until it finds the target element. However, instead of comparing a scalar value at each layer to determine whether or not to descend to the layer below, a multi-dimensional distance measure (such as Euclidean distance) is used.\n\n## When should you create HNSW indexes? [\\#](\\#when-should-you-create-hnsw-indexes)\n\nHNSW should be your default choice when creating a vector index. Add the index when you don't need 100% accuracy and are willing to trade a small amount of accuracy for a lot of throughput.\n\nUnlike IVFFlat indexes, you are safe to build an HNSW index immediately after the table is created. HNSW indexes are based on graphs which inherently are not affected by the same limitations as IVFFlat. As new data is added to the table, the index will be filled automatically and the index structure will remain optimal.\n\n## Resources [\\#](\\#resources)\n\nRead more about indexing on `pgvector`'s [GitHub page](https://github.com/pgvector/pgvector#indexing).",
      "metadata": {
        "ogUrl": "https://supabase.com/docs/guides/ai/vector-indexes/hnsw-indexes",
        "title": "HNSW indexes | Supabase Docs",
        "robots": "index, follow",
        "ogImage": "https://obuldanrptloktxcffvn.supabase.co/functions/v1/og-images?site=docs&type=ai&title=HNSW%20indexes&description=Understanding%20HNSW%20indexes%20in%20pgvector",
        "ogTitle": "HNSW indexes | Supabase Docs",
        "language": "en",
        "sourceURL": "https://supabase.com/docs/guides/ai/vector-indexes/hnsw-indexes",
        "description": "Understanding HNSW indexes in pgvector",
        "modifiedTime": "2024-09-06T19:20:40.301Z",
        "ogDescription": "Understanding HNSW indexes in pgvector",
        "publishedTime": "2024-09-06T19:20:40.300Z",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "AI & Vectors\n\n# Choosing a Client\n\n* * *\n\nAs described in [Structured & Unstructured Embeddings](/docs/guides/ai/structured-unstructured), AI workloads come in many forms.\n\nFor data science or ephemeral workloads, the [Supabase Vecs](https://supabase.github.io/vecs/) client gets you started quickly. All you need is a connection string and vecs handles setting up your database to store and query vectors with associated metadata.\n\nYou can get your connection string from the [**Database Settings**](https://supabase.com/dashboard/project/_/settings/database) page in your dashboard. Make sure to check **Use connection pooling**, then copy the URI. Also, change the URI scheme from `postgres` to `postgresql`. `vecs` uses SQLAlchemy under the hood, which only supports `postgresql` as a dialect.\n\nFor production python applications with version controlled migrations, we recommend adding first class vector support to your toolchain by [registering the vector type with your ORM](https://github.com/pgvector/pgvector-python). pgvector provides bindings for the most commonly used SQL drivers/libraries including Django, SQLAlchemy, SQLModel, psycopg, asyncpg and Peewee.",
      "metadata": {
        "ogUrl": "https://supabase.com/docs/guides/ai/python-clients",
        "title": "Choosing a Client | Supabase Docs",
        "robots": "index, follow",
        "ogImage": "https://obuldanrptloktxcffvn.supabase.co/functions/v1/og-images?site=docs&type=ai&title=Choosing%20a%20Client&description=Learn%20how%20to%20manage%20vectors%20using%20Python",
        "ogTitle": "Choosing a Client | Supabase Docs",
        "language": "en",
        "sourceURL": "https://supabase.com/docs/guides/ai/python-clients",
        "description": "Learn how to manage vectors using Python",
        "modifiedTime": "2024-09-06T19:20:41.221Z",
        "ogDescription": "Learn how to manage vectors using Python",
        "publishedTime": "2024-09-06T19:20:41.221Z",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "AI & Vectors\n\n# Generate image captions using Hugging Face\n\n## Use the Hugging Face Inference API to make calls to 100,000+ Machine Learning models from Supabase Edge Functions.\n\n* * *\n\nWe can combine Hugging Face with [Supabase Storage](https://supabase.com/storage) and [Database Webhooks](https://supabase.com/docs/guides/database/webhooks) to automatically caption for any image we upload to a storage bucket.\n\n## About Hugging Face [\\#](\\#about-hugging-face)\n\n[Hugging Face](https://huggingface.co/) is the collaboration platform for the machine learning community.\n\n[Huggingface.js](https://huggingface.co/docs/huggingface.js/index) provides a convenient way to make calls to 100,000+ Machine Learning models, making it easy to incorporate AI functionality into your [Supabase Edge Functions](https://supabase.com/edge-functions).\n\n## Setup [\\#](\\#setup)\n\n- Open your Supabase project dashboard or [create a new project](https://supabase.com/dashboard/projects).\n- [Create a new bucket](https://supabase.com/dashboard/project/_/storage/buckets) called `images`.\n- Generate TypeScript types from remote Database.\n- Create a new Database table called `image_caption`.\n  - Create `id` column of type `uuid` which references `storage.objects.id`.\n  - Create a `caption` column of type `text`.\n- Regenerate TypeScript types to include new `image_caption` table.\n- Deploy the function to Supabase: `supabase functions deploy huggingface-image-captioning`.\n- Create the Database Webhook in the [Supabase Dashboard](https://supabase.com/dashboard/project/_/database/hooks) to trigger the `huggingface-image-captioning` function anytime a record is added to the `storage.objects` table.\n\n## Generate TypeScript types [\\#](\\#generate-typescript-types)\n\nTo generate the types.ts file for the storage and public schemas, run the following command in the terminal:\n\n`\n_10\nsupabase gen types typescript --project-id=your-project-ref --schema=storage,public > supabase/functions/huggingface-image-captioning/types.ts\n`\n\n## Code [\\#](\\#code)\n\nFind the complete code on [GitHub](https://github.com/supabase/supabase/tree/master/examples/edge-functions/supabase/functions/huggingface-image-captioning).\n\n``\n_49\nimport { serve } from 'https://deno.land/std@0.168.0/http/server.ts'\n_49\nimport { HfInference } from 'https://esm.sh/@huggingface/inference@2.3.2'\n_49\nimport { createClient } from 'jsr:@supabase/supabase-js@2'\n_49\nimport { Database } from './types.ts'\n_49\n_49\nconsole.log('Hello from `huggingface-image-captioning` function!')\n_49\n_49\nconst hf = new HfInference(Deno.env.get('HUGGINGFACE_ACCESS_TOKEN'))\n_49\n_49\ntype SoRecord = Database['storage']['Tables']['objects']['Row']\n_49\ninterface WebhookPayload {\n_49\ntype: 'INSERT' | 'UPDATE' | 'DELETE'\n_49\ntable: string\n_49\nrecord: SoRecord\n_49\nschema: 'public'\n_49\nold_record: null | SoRecord\n_49\n}\n_49\n_49\nserve(async (req) => {\n_49\nconst payload: WebhookPayload = await req.json()\n_49\nconst soRecord = payload.record\n_49\nconst supabaseAdminClient = createClient<Database>(\n_49\n    // Supabase API URL - env var exported by default when deployed.\n_49\n    Deno.env.get('SUPABASE_URL') ?? '',\n_49\n    // Supabase API SERVICE ROLE KEY - env var exported by default when deployed.\n_49\n    Deno.env.get('SUPABASE_SERVICE_ROLE_KEY') ?? ''\n_49\n)\n_49\n_49\n// Construct image url from storage\n_49\nconst { data, error } = await supabaseAdminClient.storage\n_49\n    .from(soRecord.bucket_id!)\n_49\n    .createSignedUrl(soRecord.path_tokens!.join('/'), 60)\n_49\nif (error) throw error\n_49\nconst { signedUrl } = data\n_49\n_49\n// Run image captioning with Huggingface\n_49\nconst imgDesc = await hf.imageToText({\n_49\n    data: await (await fetch(signedUrl)).blob(),\n_49\n    model: 'nlpconnect/vit-gpt2-image-captioning',\n_49\n})\n_49\n_49\n// Store image caption in Database table\n_49\nawait supabaseAdminClient\n_49\n    .from('image_caption')\n_49\n    .insert({ id: soRecord.id!, caption: imgDesc.generated_text })\n_49\n    .throwOnError()\n_49\n_49\nreturn new Response('ok')\n_49\n})\n``",
      "metadata": {
        "ogUrl": "https://supabase.com/docs/guides/ai/examples/huggingface-image-captioning",
        "title": "Generate image captions using Hugging Face | Supabase Docs",
        "robots": "index, follow",
        "ogImage": "https://obuldanrptloktxcffvn.supabase.co/functions/v1/og-images?site=docs&type=ai&title=Generate%20image%20captions%20using%20Hugging%20Face&description=Use%20the%20Hugging%20Face%20Inference%20API%20to%20make%20calls%20to%20100%2C000%2B%20Machine%20Learning%20models%20from%20Supabase%20Edge%20Functions.",
        "ogTitle": "Generate image captions using Hugging Face | Supabase Docs",
        "language": "en",
        "sourceURL": "https://supabase.com/docs/guides/ai/examples/huggingface-image-captioning",
        "description": "Use the Hugging Face Inference API to make calls to 100,000+ Machine Learning models from Supabase Edge Functions.",
        "modifiedTime": "2024-09-06T19:20:41.221Z",
        "ogDescription": "Use the Hugging Face Inference API to make calls to 100,000+ Machine Learning models from Supabase Edge Functions.",
        "publishedTime": "2024-09-06T19:20:41.221Z",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "AI & Vectors\n\n# Google Colab\n\n## Use Google Colab to manage your Supabase Vector store.\n\n* * *\n\n[![](https://supabase.com/docs/img/ai/colab-badge.svg)](https://colab.research.google.com/github/supabase/supabase/blob/master/examples/ai/vector_hello_world.ipynb)\n\nGoogle Colab is a hosted Jupyter Notebook service. It provides free access to computing resources, including GPUs and TPUs, and is well-suited to machine learning, data science, and education. We can use Colab to manage collections using [Supabase Vecs](/docs/guides/ai/vecs-python-client).\n\nIn this tutorial we'll connect to a database running on the Supabase [platform](https://supabase.com/dashboard/). If you don't already have a database, you can create one here: [database.new](https://database.new).\n\n## Create a new notebook [\\#](\\#create-a-new-notebook)\n\nStart by visiting [colab.research.google.com](https://colab.research.google.com/). There you can create a new notebook.\n\n![Google Colab new notebook](https://supabase.com/docs/img/ai/google-colab/colab-new.png)\n\n## Install Vecs [\\#](\\#install-vecs)\n\nWe'll use the Supabase Vector client, [Vecs](/docs/guides/ai/vecs-python-client), to manage our collections.\n\nAt the top of the notebook add the notebook paste the following code and hit the \"execute\" button ( `ctrl+enter`):\n\n`\n_10\npip install vecs\n`\n\n![Install vecs](https://supabase.com/docs/img/ai/google-colab/install-vecs.png)\n\n## Connect to your database [\\#](\\#connect-to-your-database)\n\nFind the Postgres pooler connection string for your Supabase project in the [database settings](https://supabase.com/dashboard/project/_/settings/database) of the dashboard. Copy the \"URI\" format, which should look something like `postgres://postgres.xxxx:password@xxxx.pooler.supabase.com:6543/postgres`\n\nCreate a new code block below the install block ( `ctrl+m b`) and add the following code using the Postgres URI you copied above:\n\n`\n_10\nimport vecs\n_10\n_10\nDB_CONNECTION = \"postgres://postgres.xxxx:password@xxxx.pooler.supabase.com:6543/postgres\"\n_10\n_10\n# create vector store client\n_10\nvx = vecs.create_client(DB_CONNECTION)\n`\n\nExecute the code block ( `ctrl+enter`). If no errors were returned then your connection was successful.\n\n## Create a collection [\\#](\\#create-a-collection)\n\nNow we're going to create a new collection and insert some documents.\n\nCreate a new code block below the install block ( `ctrl+m b`). Add the following code to the code block and execute it ( `ctrl+enter`):\n\n`\n_16\ncollection = vx.get_or_create_collection(name=\"colab_collection\", dimension=3)\n_16\n_16\ncollection.upsert(\n_16\n    vectors=[\\\n_16\\\n        (\\\n_16\\\n         \"vec0\",           # the vector's identifier\\\n_16\\\n         [0.1, 0.2, 0.3],  # the vector. list or np.array\\\n_16\\\n         {\"year\": 1973}    # associated  metadata\\\n_16\\\n        ),\\\n_16\\\n        (\\\n_16\\\n         \"vec1\",\\\n_16\\\n         [0.7, 0.8, 0.9],\\\n_16\\\n         {\"year\": 2012}\\\n_16\\\n        )\\\n_16\\\n    ]\n_16\n)\n`\n\nThis will create a table inside your database within the `vecs` schema, called `colab_collection`. You can view the inserted items in the [Table Editor](https://supabase.com/dashboard/project/_/editor/), by selecting the `vecs` schema from the schema dropdown.\n\n![Colab documents](https://supabase.com/docs/img/ai/google-colab/colab-documents.png)\n\n## Query your documents [\\#](\\#query-your-documents)\n\nNow we can search for documents based on their similarity. Create a new code block and execute the following code:\n\n`\n_10\ncollection.query(\n_10\n    query_vector=[0.4,0.5,0.6],  # required\n_10\n    limit=5,                     # number of records to return\n_10\n    filters={},                  # metadata filters\n_10\n    measure=\"cosine_distance\",   # distance measure to use\n_10\n    include_value=False,         # should distance measure values be returned?\n_10\n    include_metadata=False,      # should record metadata be returned?\n_10\n)\n`\n\nYou will see that this returns two documents in an array `['vec1', 'vec0']`:\n\n![Colab results](https://supabase.com/docs/img/ai/google-colab/colab-results.png)\n\nIt also returns a warning:\n\n`\n_10\nQuery does not have a covering index for cosine_distance.\n`\n\nYou can lean more about creating indexes in the [Vecs documentation](https://supabase.github.io/vecs/api/#create-an-index).\n\n## Resources [\\#](\\#resources)\n\n- Vecs API: [supabase.github.io/vecs/api](https://supabase.github.io/vecs/api)",
      "metadata": {
        "ogUrl": "https://supabase.com/docs/guides/ai/google-colab",
        "title": "Google Colab | Supabase Docs",
        "robots": "index, follow",
        "ogImage": "https://obuldanrptloktxcffvn.supabase.co/functions/v1/og-images?site=docs&type=ai&title=Google%20Colab&description=Use%20Google%20Colab%20to%20manage%20your%20Supabase%20Vector%20store.",
        "ogTitle": "Google Colab | Supabase Docs",
        "language": "en",
        "sourceURL": "https://supabase.com/docs/guides/ai/google-colab",
        "description": "Use Google Colab to manage your Supabase Vector store.",
        "modifiedTime": "2024-09-06T19:20:41.221Z",
        "ogDescription": "Use Google Colab to manage your Supabase Vector store.",
        "publishedTime": "2024-09-06T19:20:41.221Z",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "AI & Vectors\n\n# Adding generative Q&A for your documentation\n\n## Learn how to build a ChatGPT-style doc search powered using our headless search toolkit.\n\n* * *\n\nSupabase provides a [Headless Search Toolkit](https://github.com/supabase/headless-vector-search) for adding \"Generative Q&A\" to your documentation. The toolkit is \"headless\", so that you can integrate it into your existing website and style it to match your website theme.\n\nYou can see how this works with the Supabase docs. Just hit `cmd+k` and \"ask\" for something like \"what are the features of supabase?\". You will see that the response is streamed back, using the information provided in the docs:\n\n![headless search](https://supabase.com/docs/img/ai/headless-search/headless.png)\n\n## Tech stack [\\#](\\#tech-stack)\n\n- Supabase: Database & Edge Functions.\n- OpenAI: Embeddings and completions.\n- GitHub Actions: for ingesting your markdown docs.\n\n## Toolkit [\\#](\\#toolkit)\n\nThis toolkit consists of 2 parts:\n\n- The [Headless Vector Search](https://github.com/supabase/headless-vector-search) template which you can deploy in your own organization.\n- A [GitHub Action](https://github.com/supabase/embeddings-generator) which will ingest your markdown files, convert them to embeddings, and store them in your database.\n\n## Usage [\\#](\\#usage)\n\nThere are 3 steps to build similarity search inside your documentation:\n\n1. Prepare your database.\n2. Ingest your documentation.\n3. Add a search interface.\n\n### Prepare your database [\\#](\\#prepare-your-database)\n\nTo prepare, create a [new Supabase project](https://database.new) and store the database and API credentials, which you can find in the project [settings](https://supabase.com/dashboard/project/_/settings).\n\nNow we can use the [Headless Vector Search](https://github.com/supabase/headless-vector-search#set-up) instructions to set up the database:\n\n1. Clone the repo to your local machine: `git clone git@github.com:supabase/headless-vector-search.git`\n2. Link the repo to your remote project: `supabase link --project-ref XXX`\n3. Apply the database migrations: `supabase db push`\n4. Set your OpenAI key as a secret: `supabase secrets set OPENAI_API_KEY=sk-xxx`\n5. Deploy the Edge Functions: `supabase functions deploy --no-verify-jwt`\n6. Expose `docs` schema via API in Supabase Dashboard [settings](https://supabase.com/dashboard/project/_/settings/api) \\> `API Settings` \\> `Exposed schemas`\n\n### Ingest your documentation [\\#](\\#ingest-your-documentation)\n\nNow we need to push your documentation into the database as embeddings. You can do this manually, but to make it easier we've created a [GitHub Action](https://github.com/marketplace/actions/supabase-embeddings-generator) which can update your database every time there is a Pull Request.\n\nIn your knowledge base repository, create a new action called `.github/workflows/generate_embeddings.yml` with the following content:\n\n`\n_17\nname: 'generate_embeddings'\n_17\non: # run on main branch changes\n_17\npush:\n_17\n    branches:\n_17\n      - main\n_17\n_17\njobs:\n_17\ngenerate:\n_17\n    runs-on: ubuntu-latest\n_17\n    steps:\n_17\n      - uses: actions/checkout@v3\n_17\n      - uses: supabase/embeddings-generator@v0.0.x # Update this to the latest version.\n_17\n        with:\n_17\n          supabase-url: 'https://your-project-ref.supabase.co' # Update this to your project URL.\n_17\n          supabase-service-role-key: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}\n_17\n          openai-key: ${{ secrets.OPENAI_API_KEY }}\n_17\n          docs-root-path: 'docs' # the path to the root of your md(x) files\n`\n\nMake sure to choose the latest version, and set your `SUPABASE_SERVICE_ROLE_KEY` and `OPENAI_API_KEY` as repository secrets in your repo settings (settings > secrets > actions).\n\n### Add a search interface [\\#](\\#add-a-search-interface)\n\nNow inside your docs, you need to create a search interface. Because this is a headless interface, you can use it with any language. The only requirement is that you send the user query to the `query` Edge Function, which will stream an answer back from OpenAI. It might look something like this:\n\n``\n_31\nconst onSubmit = (e: Event) => {\n_31\ne.preventDefault()\n_31\nanswer.value = \"\"\n_31\nisLoading.value = true\n_31\n_31\nconst query = new URLSearchParams({ query: inputRef.current!.value })\n_31\nconst projectUrl = `https://your-project-ref.supabase.co/functions/v1`\n_31\nconst queryURL = `${projectURL}/${query}`\n_31\nconst eventSource = new EventSource(queryURL)\n_31\n_31\neventSource.addEventListener(\"error\", (err) => {\n_31\n    isLoading.value = false\n_31\n    console.error(err)\n_31\n})\n_31\n_31\neventSource.addEventListener(\"message\", (e: MessageEvent) => {\n_31\n    isLoading.value = false\n_31\n_31\n    if (e.data === \"[DONE]\") {\n_31\n      eventSource.close()\n_31\n      return\n_31\n    }\n_31\n_31\n    const completionResponse: CreateCompletionResponse = JSON.parse(e.data)\n_31\n    const text = completionResponse.choices[0].text\n_31\n_31\n    answer.value += text\n_31\n});\n_31\n_31\nisLoading.value = true\n_31\n}\n``\n\n## Resources [\\#](\\#resources)\n\n- Read about how we built [ChatGPT for the Supabase Docs](https://supabase.com/blog/chatgpt-supabase-docs).\n- Read the pgvector Docs for [Embeddings and vector similarity](/docs/guides/database/extensions/pgvector)\n- See how to build something like this from scratch [using Next.js](/docs/guides/ai/examples/nextjs-vector-search).",
      "metadata": {
        "ogUrl": "https://supabase.com/docs/guides/ai/examples/headless-vector-search",
        "title": "Adding generative Q&A for your documentation | Supabase Docs",
        "robots": "index, follow",
        "ogImage": "https://obuldanrptloktxcffvn.supabase.co/functions/v1/og-images?site=docs&type=ai&title=Adding%20generative%20Q%26A%20for%20your%20documentation&description=undefined",
        "ogTitle": "Adding generative Q&A for your documentation | Supabase Docs",
        "language": "en",
        "sourceURL": "https://supabase.com/docs/guides/ai/examples/headless-vector-search",
        "description": "Learn how to build a ChatGPT-style doc search powered using our headless search toolkit.",
        "modifiedTime": "2024-09-06T19:20:40.301Z",
        "ogDescription": "Learn how to build a ChatGPT-style doc search powered using our headless search toolkit.",
        "publishedTime": "2024-09-06T19:20:40.300Z",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "AI & Vectors\n\n# Engineering for Scale\n\n## Building an enterprise-grade vector architecture.\n\n* * *\n\nContent sources for vectors can be extremely large. As you grow you should run your Vector workloads across several secondary databases (sometimes called \"pods\"), which allows each collection to scale independently.\n\n## Simple workloads [\\#](\\#simple-workloads)\n\nFor small workloads, it's typical to store your data in a single database.\n\nIf you've used [Vecs](/docs/guides/ai/vecs-python-client) to create 3 different collections, you can expose collections to your web or mobile application using [views](/docs/guides/database/tables#views):\n\nFor example, with 3 collections, called `docs`, `posts`, and `images`, we could expose the \"docs\" inside the public schema like this:\n\n`\n_10\ncreate view public.docs as\n_10\nselect\n_10\nid,\n_10\nembedding,\n_10\nmetadata, # Expose the metadata as JSON\n_10\n(metadata->>'url')::text as url # Extract the URL as a string\n_10\nfrom vector\n`\n\nYou can then use any of the client libraries to access your collections within your applications:\n\n`\n_10\nconst { data, error } = await supabase\n_10\n.from('docs')\n_10\n.select('id, embedding, metadata')\n_10\n.eq('url', '/hello-world')\n`\n\n## Enterprise workloads [\\#](\\#enterprise-workloads)\n\nAs you move into production, we recommend splitting your collections into separate projects. This is because it allows your vector stores to scale independently of your production data. Vectors typically grow faster than operational data, and they have different resource requirements. Running them on separate databases removes the single-point-of-failure.\n\nYou can use as many secondary databases as you need to manage your collections. With this architecture, you have 2 options for accessing collections within your application:\n\n1. Query the collections directly using Vecs.\n2. Access the collections from your Primary database through a Wrapper.\n\nYou can use both of these in tandem to suit your use-case. We recommend option `1` wherever possible, as it offers the most scalability.\n\n### Query collections using Vecs [\\#](\\#query-collections-using-vecs)\n\nVecs provides methods for querying collections, either using a [cosine similarity function](https://supabase.github.io/vecs/api/#basic) or with [metadata filtering](https://supabase.github.io/vecs/api/#metadata-filtering).\n\n`\n_10\n# cosine similarity\n_10\ndocs.query(query_vector=[0.4,0.5,0.6], limit=5)\n_10\n_10\n# metadata filtering\n_10\ndocs.query(\n_10\n    query_vector=[0.4,0.5,0.6],\n_10\n    limit=5,\n_10\n    filters={\"year\": {\"$eq\": 2012}}, # metadata filters\n_10\n)\n`\n\n### Accessing external collections using Wrappers [\\#](\\#accessing-external-collections-using-wrappers)\n\nSupabase supports [Foreign Data Wrappers](/blog/postgres-foreign-data-wrappers-rust). Wrappers allow you to connect two databases together so that you can query them over the network.\n\nThis involves 2 steps: connecting to your remote database from the primary and creating a Foreign Table.\n\n#### Connecting your remote database [\\#](\\#connecting-your-remote-database)\n\nInside your Primary database we need to provide the credentials to access the secondary database:\n\n`\n_10\ncreate extension postgres_fdw;\n_10\n_10\ncreate server docs_server\n_10\nforeign data wrapper postgres_fdw\n_10\noptions (host 'db.xxx.supabase.co', port '5432', dbname 'postgres');\n_10\n_10\ncreate user mapping for docs_user\n_10\nserver docs_server\n_10\noptions (user 'postgres', password 'password');\n`\n\n#### Create a foreign table [\\#](\\#create-a-foreign-table)\n\nWe can now create a foreign table to access the data in our secondary project.\n\n`\n_10\ncreate foreign table docs (\n_10\nid text not null,\n_10\nembedding vector(384),\n_10\nmetadata jsonb,\n_10\nurl text\n_10\n)\n_10\nserver docs_server\n_10\noptions (schema_name 'public', table_name 'docs');\n`\n\nThis looks very similar to our View example above, and you can continue to use the client libraries to access your collections through the foreign table:\n\n`\n_10\nconst { data, error } = await supabase\n_10\n.from('docs')\n_10\n.select('id, embedding, metadata')\n_10\n.eq('url', '/hello-world')\n`\n\n### Enterprise architecture [\\#](\\#enterprise-architecture)\n\nThis diagram provides an example architecture that allows you to access the collections either with our client libraries or using Vecs. You can add as many secondary databases as you need (in this example we only show one):",
      "metadata": {
        "ogUrl": "https://supabase.com/docs/guides/ai/engineering-for-scale",
        "title": "Engineering for Scale | Supabase Docs",
        "robots": "index, follow",
        "ogImage": "https://obuldanrptloktxcffvn.supabase.co/functions/v1/og-images?site=docs&type=ai&title=Engineering%20for%20Scale&description=Building%20an%20enterprise-grade%20vector%20architecture",
        "ogTitle": "Engineering for Scale | Supabase Docs",
        "language": "en",
        "sourceURL": "https://supabase.com/docs/guides/ai/engineering-for-scale",
        "description": "Building an enterprise-grade vector architecture",
        "modifiedTime": "2024-09-06T19:20:40.691Z",
        "ogDescription": "Building an enterprise-grade vector architecture",
        "publishedTime": "2024-09-06T19:20:40.691Z",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "AI & Vectors\n\n# Keyword search\n\n## Learn how to search by words or phrases.\n\n* * *\n\nKeyword search involves locating documents or records that contain specific words or phrases, primarily based on the exact match between the search terms and the text within the data. It differs from [semantic search](/docs/guides/ai/semantic-search), which interprets the meaning behind the query to provide results that are contextually related, even if the exact words aren't present in the text. Semantic search considers synonyms, intent, and natural language nuances to provide a more nuanced approach to information retrieval.\n\nIn Postgres, keyword search is implemented using [full-text search](/docs/guides/database/full-text-search). It supports indexing and text analysis for data retrieval, focusing on records that match the search criteria. Postgres' full-text search extends beyond simple keyword matching to address linguistic nuances, making it effective for applications that require precise text queries.\n\n## Why would I want to use keyword search? [\\#](\\#why-would-i-want-to-use-keyword-search)\n\nKeyword search is particularly useful in scenarios where precision and specificity matter. It's more effective than semantic search when users are looking for information using exact terminology or specific identifiers. It ensures that results directly contain those terms, reducing the chance of retrieving irrelevant information that might be semantically related but not what the user seeks.\n\nFor example in technical or academic research databases, researchers often search for specific studies, compounds, or concepts identified by certain terms or codes. Searching for a specific chemical compound using its exact molecular formula or a unique identifier will yield more focused and relevant results compared to a semantic search, which could return a wide range of documents discussing the compound in different contexts. Keyword search ensures documents that explicitly mention the exact term are found, allowing users to access the precise data they need efficiently.\n\nIt's also possible to combine keyword search with semantic search to get the best of both worlds. See [Hybrid search](/docs/guides/ai/hybrid-search) for more details.\n\n## Using full-text search [\\#](\\#using-full-text-search)\n\nFor an in-depth guide to Postgres' full-text search, including how to store, index, and query records, see [Full text search](/docs/guides/database/full-text-search).\n\n## See also [\\#](\\#see-also)\n\n- [Semantic search](/docs/guides/ai/semantic-search)\n- [Hybrid search](/docs/guides/ai/hybrid-search)",
      "metadata": {
        "ogUrl": "https://supabase.com/docs/guides/ai/keyword-search",
        "title": "Keyword search | Supabase Docs",
        "robots": "index, follow",
        "ogImage": "https://obuldanrptloktxcffvn.supabase.co/functions/v1/og-images?site=docs&type=ai&title=Keyword%20search&description=Learn%20how%20to%20search%20by%20words%20or%20phrases.",
        "ogTitle": "Keyword search | Supabase Docs",
        "language": "en",
        "sourceURL": "https://supabase.com/docs/guides/ai/keyword-search",
        "description": "Learn how to search by words or phrases.",
        "modifiedTime": "2024-09-06T19:20:41.221Z",
        "ogDescription": "Learn how to search by words or phrases.",
        "publishedTime": "2024-09-06T19:20:41.221Z",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "AI & Vectors\n\n# Vector indexes\n\n* * *\n\nOnce your vector table starts to grow, you will likely want to add an index to speed up queries. Without indexes, you'll be performing a sequential scan which can be a resource-intensive operation when you have many records.\n\n## Choosing an index [\\#](\\#choosing-an-index)\n\nToday `pgvector` supports two types of indexes:\n\n- [HNSW](/docs/guides/ai/vector-indexes/hnsw-indexes)\n- [IVFFlat](/docs/guides/ai/vector-indexes/ivf-indexes)\n\nIn general we recommend using [HNSW](/docs/guides/ai/vector-indexes/hnsw-indexes) because of its [performance](https://supabase.com/blog/increase-performance-pgvector-hnsw#hnsw-performance-1536-dimensions) and [robustness against changing data](/docs/guides/ai/vector-indexes/hnsw-indexes#when-should-you-create-hnsw-indexes).\n\n## Distance operators [\\#](\\#distance-operators)\n\nIndexes can be used to improve performance of nearest neighbor search using various distance measures. `pgvector` includes 3 distance operators:\n\n| Operator | Description | [**Operator class**](https://www.postgresql.org/docs/current/sql-createopclass.html) |\n| --- | --- | --- |\n| `<->` | Euclidean distance | `vector_l2_ops` |\n| `<#>` | negative inner product | `vector_ip_ops` |\n| `<=>` | cosine distance | `vector_cosine_ops` |\n\nCurrently vectors with up to 2,000 dimensions can be indexed.\n\n## Resources [\\#](\\#resources)\n\nRead more about indexing on `pgvector`'s [GitHub page](https://github.com/pgvector/pgvector#indexing).",
      "metadata": {
        "ogUrl": "https://supabase.com/docs/guides/ai/vector-indexes",
        "title": "Vector indexes | Supabase Docs",
        "robots": "index, follow",
        "ogImage": "https://obuldanrptloktxcffvn.supabase.co/functions/v1/og-images?site=docs&type=ai&title=Vector%20indexes&description=Understanding%20vector%20indexes",
        "ogTitle": "Vector indexes | Supabase Docs",
        "language": "en",
        "sourceURL": "https://supabase.com/docs/guides/ai/vector-indexes",
        "description": "Understanding vector indexes",
        "modifiedTime": "2024-09-06T19:20:40.691Z",
        "ogDescription": "Understanding vector indexes",
        "publishedTime": "2024-09-06T19:20:40.691Z",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "AI & Vectors\n\n# Concepts\n\n* * *\n\nEmbeddings are core to many AI and vector applications. This guide covers these concepts. If you prefer to get started right away, see our guide on [Generating Embeddings](/docs/guides/ai/quickstarts/generate-text-embeddings).\n\n## What are embeddings? [\\#](\\#what-are-embeddings)\n\nEmbeddings capture the \"relatedness\" of text, images, video, or other types of information. This relatedness is most commonly used for:\n\n- **Search:** how similar is a search term to a body of text?\n- **Recommendations:** how similar are two products?\n- **Classifications:** how do we categorize a body of text?\n- **Clustering:** how do we identify trends?\n\nLet's explore an example of text embeddings. Say we have three phrases:\n\n1. \"The cat chases the mouse\"\n2. \"The kitten hunts rodents\"\n3. \"I like ham sandwiches\"\n\nYour job is to group phrases with similar meaning. If you are a human, this should be obvious. Phrases 1 and 2 are almost identical, while phrase 3 has a completely different meaning.\n\nAlthough phrases 1 and 2 are similar, they share no common vocabulary (besides \"the\"). Yet their meanings are nearly identical. How can we teach a computer that these are the same?\n\n## Human language [\\#](\\#human-language)\n\nHumans use words and symbols to communicate language. But words in isolation are mostly meaningless - we need to draw from shared knowledge & experience in order to make sense of them. The phrase \u201cYou should Google it\u201d only makes sense if you know that Google is a search engine and that people have been using it as a verb.\n\nIn the same way, we need to train a neural network model to understand human language. An effective model should be trained on millions of different examples to understand what each word, phrase, sentence, or paragraph could mean in different contexts.\n\nSo how does this relate to embeddings?\n\n## How do embeddings work? [\\#](\\#how-do-embeddings-work)\n\nEmbeddings compress discrete information (words & symbols) into distributed continuous-valued data (vectors). If we took our phrases from before and plot them on a chart, it might look something like this:\n\n![Vector similarity](https://supabase.com/docs/img/ai/vector-similarity.png)\n\nPhrases 1 and 2 would be plotted close to each other, since their meanings are similar. We would expect phrase 3 to live somewhere far away since it isn't related. If we had a fourth phrase, \u201cSally ate Swiss cheese\u201d, this might exist somewhere between phrase 3 (cheese can go on sandwiches) and phrase 1 (mice like Swiss cheese).\n\nIn this example we only have 2 dimensions: the X and Y axis. In reality, we would need many more dimensions to effectively capture the complexities of human language.\n\n## Using embeddings [\\#](\\#using-embeddings)\n\nCompared to our 2-dimensional example above, most embedding models will output many more dimensions. For example the open source [`gte-small`](https://huggingface.co/Supabase/gte-small) model outputs 384 dimensions.\n\nWhy is this useful? Once we have generated embeddings on multiple texts, it is trivial to calculate how similar they are using vector math operations like cosine distance. A common use case for this is search. Your process might look something like this:\n\n1. Pre-process your knowledge base and generate embeddings for each page\n2. Store your embeddings to be referenced later\n3. Build a search page that prompts your user for input\n4. Take user's input, generate a one-time embedding, then perform a similarity search against your pre-processed embeddings.\n5. Return the most similar pages to the user\n\n## See also [\\#](\\#see-also)\n\n- [Structured and Unstructured embeddings](/docs/guides/ai/structured-unstructured)",
      "metadata": {
        "ogUrl": "https://supabase.com/docs/guides/ai/concepts",
        "title": "Concepts | Supabase Docs",
        "robots": "index, follow",
        "ogImage": "https://obuldanrptloktxcffvn.supabase.co/functions/v1/og-images?site=docs&type=ai&title=Concepts&description=Learn%20about%20embeddings%20within%20AI%20and%20vector%20applications.",
        "ogTitle": "Concepts | Supabase Docs",
        "language": "en",
        "sourceURL": "https://supabase.com/docs/guides/ai/concepts",
        "description": "Learn about embeddings within AI and vector applications.",
        "modifiedTime": "2024-09-06T19:20:41.221Z",
        "ogDescription": "Learn about embeddings within AI and vector applications.",
        "publishedTime": "2024-09-06T19:20:41.221Z",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "AI & Vectors\n\n# Going to Production\n\n## Going to production checklist for AI applications.\n\n* * *\n\nThis guide will help you to prepare your application for production. We'll provide actionable steps to help you scale your application, ensure that it is reliable, can handle the load, and provide optimal accuracy for your use case.\n\nSee our [Engineering for Scale](/docs/guides/ai/engineering-for-scale) guide for more information about engineering at scale.\n\n## Do you need indexes? [\\#](\\#do-you-need-indexes)\n\nSequential scans will result in significantly higher latencies and lower throughput, guaranteeing 100% accuracy and not being RAM bound.\n\nThere are a couple of cases where you might not need indexes:\n\n- You have a small dataset and don't need to scale it.\n- You are not expecting high amounts of vector search queries per second.\n- You need to guarantee 100% accuracy.\n\nYou don't have to create indexes in these cases and can use sequential scans instead. This type of workload will not be RAM bound and will not require any additional resources but will result in higher latencies and lower throughput. Extra CPU cores may help to improve queries per second, but it will not help to improve latency.\n\nOn the other hand, if you need to scale your application, you will need to [create indexes](/docs/guides/ai/vector-indexes). This will result in lower latencies and higher throughput, but will require additional RAM to make use of Postgres Caching. Also, using indexes will result in lower accuracy, since you are replacing exact (KNN) search with approximate (ANN) search.\n\n## HNSW vs IVFFlat indexes [\\#](\\#hnsw-vs-ivfflat-indexes)\n\n`pgvector` supports two types of indexes: HNSW and IVFFlat. We recommend using [HNSW](/docs/guides/ai/vector-indexes/hnsw-indexes) because of its [performance](https://supabase.com/blog/increase-performance-pgvector-hnsw#hnsw-performance-1536-dimensions) and [robustness against changing data](/docs/guides/ai/vector-indexes/hnsw-indexes#when-should-you-create-hnsw-indexes).\n\n## HNSW, understanding `ef_construction`, `ef_search`, and `m` [\\#](\\#hnsw-understanding-efconstruction--efsearch--and-m)\n\nIndex build parameters:\n\n- `m` is the number of bi-directional links created for every new element during construction. Higher `m` is suitable for datasets with high dimensionality and/or high accuracy requirements. Reasonable values for `m` are between 2 and 100. Range 12-48 is a good starting point for most use cases (16 is the default value).\n\n- `ef_construction` is the size of the dynamic list for the nearest neighbors (used during the construction algorithm). Higher `ef_construction` will result in better index quality and higher accuracy, but it will also increase the time required to build the index. `ef_construction` has to be at least 2 \\* `m` (64 is the default value). At some point, increasing `ef_construction` does not improve the quality of the index. You can measure accuracy when `ef_search` = `ef_construction`: if accuracy is lower than 0.9, then there is room for improvement.\n\n\nSearch parameters:\n\n- `ef_search` is the size of the dynamic list for the nearest neighbors (used during the search). Increasing `ef_search` will result in better accuracy, but it will also increase the time required to execute a query (40 is the default value).\n\n## IVFFlat, understanding `probes` and `lists` [\\#](\\#ivfflat-understanding-probes-and-lists)\n\nIndexes used for approximate vector similarity search in pgvector divides a dataset into partitions. The number of these partitions is defined by the `lists` constant. The `probes` controls how many lists are going to be searched during a query.\n\nThe values of lists and probes directly affect accuracy and queries per second (QPS).\n\n- Higher `lists` means an index will be built slower, but you can achieve better QPS and accuracy.\n- Higher `probes` means that select queries will be slower, but you can achieve better accuracy.\n- `lists` and `probes` are not independent. Higher `lists` means that you will have to use higher `probes` to achieve the same accuracy.\n\nYou can find more examples of how `lists` and `probes` constants affect accuracy and QPS in [pgvector 0.4.0 performance](https://supabase.com/blog/pgvector-performance) blogpost.\n\n## Performance tips when using indexes [\\#](\\#performance-tips-when-using-indexes)\n\nFirst, a few generic tips which you can pick and choose from:\n\n1. The Supabase managed platform will automatically optimize Postgres configs for you based on your compute addon. But if you self-host, consider **adjusting your Postgres config** based on RAM & CPU cores. See [example optimizations](https://gist.github.com/egor-romanov/323e2847851bbd758081511785573c08) for more details.\n2. Prefer `inner-product` to `L2` or `Cosine` distances if your vectors are normalized (like `text-embedding-ada-002`). If embeddings are not normalized, `Cosine` distance should give the best results with an index.\n3. **Pre-warm your database.** Implement the warm-up technique before transitioning to production or running benchmarks.\n   - Use [pg\\_prewarm](https://www.postgresql.org/docs/current/pgprewarm.html) to load the index into RAM `select pg_prewarm('vecs.docs_vec_idx');`. This will help to avoid cold cache issues.\n   - Execute 10,000 to 50,000 \"warm-up\" queries before each benchmark/prod. This will help to utilize cache and buffers more efficiently.\n4. **Establish your workload.** Finetune `m` and `ef_construction` or `lists` constants for the pgvector index to accelerate your queries (at the expense of a slower build times). For instance, for benchmarks with 1,000,000 OpenAI embeddings, we set `m` and `ef_construction` to 32 and 80, and it resulted in 35% higher QPS than 24 and 56 values respectively.\n5. **Benchmark your own specific workloads.** Doing this during cache warm-up helps gauge the best value for the index build parameters, balancing accuracy with queries per second (QPS).\n\n## Going into production [\\#](\\#going-into-production)\n\n1. Decide if you are going to use indexes or not. You can skip the rest of this guide if you do not use indexes.\n2. Over-provision RAM during preparation. You can scale down in step `5`, but it's better to start with a larger size to get the best results for RAM requirements. (We'd recommend at least 8XL if you're using Supabase.)\n3. Upload your data to the database. If you use the [`vecs`](/docs/guides/ai/python/api) library, it will automatically generate an index with default parameters.\n4. Run a benchmark using randomly generated queries and observe the results. Again, you can use the `vecs` library with the `ann-benchmarks` tool. Do it with default values for index build parameters, you can later adjust them to get the best results.\n5. Monitor the RAM usage, and save it as a note for yourself. You would likely want to use a compute add-on in the future that has the same amount of RAM that was used at the moment (both actual RAM usage and RAM used for cache and buffers).\n6. Scale down your compute add-on to the one that would have the same amount of RAM used at the moment.\n7. Repeat step 3 to load the data into RAM. You should see QPS increase on subsequent runs, and stop when it no longer increases.\n8. Run a benchmark using real queries and observe the results. You can use the `vecs` library for that as well with `ann-benchmarks` tool. Tweak `ef_search` for HNSW or `probes` for IVFFlat until you see that both accuracy and QPS match your requirements.\n9. If you want higher QPS you can increase `m` and `ef_construction` for HNSW or `lists` for IVFFlat parameters (consider switching from IVF to HNSW). You have to rebuild the index with a higher `m` and `ef_construction` values and repeat steps 6-7 to find the best combination of `m`, `ef_construction` and `ef_search` constants to achieve the best QPS and accuracy values. Higher `m`, `ef_construction` mean that index will build slower, but you can achieve better QPS and accuracy. Higher `ef_search` mean that select queries will be slower, but you can achieve better accuracy.\n\n## Useful links [\\#](\\#useful-links)\n\nDon't forget to check out the general [Production Checklist](/docs/guides/platform/going-into-prod) to ensure your project is secure, performant, and will remain available for your users.\n\nYou can look at our [Choosing Compute Add-on](/docs/guides/ai/choosing-compute-addon) guide to get a basic understanding of how much compute you might need for your workload.\n\nOr take a look at our [pgvector 0.5.0 performance](https://supabase.com/blog/increase-performance-pgvector-hnsw) and [pgvector 0.4.0 performance](https://supabase.com/blog/pgvector-performance) blog posts to see what pgvector is capable of and how the above technique can be used to achieve the best results.",
      "metadata": {
        "ogUrl": "https://supabase.com/docs/guides/ai/going-to-prod",
        "title": "Going to Production | Supabase Docs",
        "robots": "index, follow",
        "ogImage": "https://obuldanrptloktxcffvn.supabase.co/functions/v1/og-images?site=docs&type=ai&title=Going%20to%20Production&description=Checklist%20for%20going%20to%20production%20with%20your%20AI%20application.",
        "ogTitle": "Going to Production | Supabase Docs",
        "language": "en",
        "sourceURL": "https://supabase.com/docs/guides/ai/going-to-prod",
        "description": "Checklist for going to production with your AI application.",
        "modifiedTime": "2024-09-06T19:20:40.301Z",
        "ogDescription": "Checklist for going to production with your AI application.",
        "publishedTime": "2024-09-06T19:20:40.300Z",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "AI & Vectors\n\n# Creating and managing collections\n\n## Connecting to your database with Colab.\n\n* * *\n\nThis guide will walk you through a basic [\"Hello World\"](https://github.com/supabase/supabase/blob/master/examples/ai/vector_hello_world.ipynb) example using Colab and Supabase Vecs. You'll learn how to:\n\n1. Launch a Postgres database that uses pgvector to store embeddings\n2. Launch a notebook that connects to your database\n3. Create a vector collection\n4. Add data to the collection\n5. Query the collection\n\n## Project setup [\\#](\\#project-setup)\n\nLet's create a new Postgres database. This is as simple as starting a new Project in Supabase:\n\n1. [Create a new project](https://database.new/) in the Supabase dashboard.\n2. Enter your project details. Remember to store your password somewhere safe.\n\nYour database will be available in less than a minute.\n\n**Finding your credentials:**\n\nYou can find your project credentials inside the project [settings](https://supabase.com/dashboard/project/_/settings/), including:\n\n- [Database credentials](https://supabase.com/dashboard/project/_/settings/database): connection strings and connection pooler details.\n- [API credentials](https://supabase.com/dashboard/project/_/settings/database): your serverless API URL and `anon` / `service_role` keys.\n\n## Launching a notebook [\\#](\\#launching-a-notebook)\n\nLaunch our [`vector_hello_world`](https://github.com/supabase/supabase/blob/master/examples/ai/vector_hello_world.ipynb) notebook in Colab:\n\n[![](https://supabase.com/docs/img/ai/colab-badge.svg)](https://colab.research.google.com/github/supabase/supabase/blob/master/examples/ai/vector_hello_world.ipynb)\n\nAt the top of the notebook, you'll see a button `Copy to Drive`. Click this button to copy the notebook to your Google Drive.\n\n## Connecting to your database [\\#](\\#connecting-to-your-database)\n\nInside the Notebook, find the cell which specifies the `DB_CONNECTION`. It will contain some code like this:\n\n`\n_10\nimport vecs\n_10\n_10\nDB_CONNECTION = \"postgresql://<user>:<password>@<host>:<port>/<db_name>\"\n_10\n_10\n# create vector store client\n_10\nvx = vecs.create_client(DB_CONNECTION)\n`\n\nReplace the `DB_CONNECTION` with your own connection string for your database. You can find the Postgres connection string in the [Database Settings](https://supabase.com/dashboard/project/_/settings/database) of your Supabase project.\n\nSQLAlchemy requires the connection string to start with `postgresql://` (instead of `postgres://`). Don't forget to rename this after copying the string from the dashboard.\n\nYou must use the \"connection pooling\" string (domain ending in `*.pooler.supabase.com`) with Google Colab since Colab does not support IPv6.\n\n## Stepping through the notebook [\\#](\\#stepping-through-the-notebook)\n\nNow all that's left is to step through the notebook. You can do this by clicking the \"execute\" button ( `ctrl+enter`) at the top left of each code cell. The notebook guides you through the process of creating a collection, adding data to it, and querying it.\n\nYou can view the inserted items in the [Table Editor](https://supabase.com/dashboard/project/_/editor/), by selecting the `vecs` schema from the schema dropdown.\n\n![Colab documents](https://supabase.com/docs/img/ai/google-colab/colab-documents.png)\n\n## Next steps [\\#](\\#next-steps)\n\nYou can now start building your own applications with Vecs. Check our [examples](/docs/guides/ai#examples) for ideas.",
      "metadata": {
        "ogUrl": "https://supabase.com/docs/guides/ai/quickstarts/hello-world",
        "title": "Creating and managing collections | Supabase Docs",
        "robots": "index, follow",
        "ogImage": "https://obuldanrptloktxcffvn.supabase.co/functions/v1/og-images?site=docs&type=ai&title=Creating%20and%20managing%20collections&description=undefined",
        "ogTitle": "Creating and managing collections | Supabase Docs",
        "language": "en",
        "sourceURL": "https://supabase.com/docs/guides/ai/quickstarts/hello-world",
        "description": "Connecting to your database with Colab.",
        "modifiedTime": "2024-09-06T19:20:41.221Z",
        "ogDescription": "Connecting to your database with Colab.",
        "publishedTime": "2024-09-06T19:20:41.221Z",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    }
  ]
}