{
  "input_url": "https://docs.llamaindex.ai/en/stable",
  "total_pages": 100,
  "unique_links": [
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/bedrock/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/azure_openai/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/mistralai/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/ibm/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/upstage/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/databricks/",
    "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/aim/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/fireworks/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/huggingface/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/vertex/",
    "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/uptrain/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/sagemaker_endpoint/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/nvidia/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/gemini/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/cohere/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/azure_inference/",
    "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/wandb/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/yandexgpt/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/xinference/",
    "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/agentops/",
    "https://docs.llamaindex.ai/en/stable/api_reference/evaluation/response/",
    "https://docs.llamaindex.ai/en/stable/api_reference/evaluation/query_response/",
    "https://docs.llamaindex.ai/en/stable/api_reference/evaluation/correctness/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/llamafile/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/octoai/",
    "https://docs.llamaindex.ai/en/stable/api_reference/evaluation/pairwise_comparison/",
    "https://docs.llamaindex.ai/en/stable/api_reference/evaluation/retrieval/",
    "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/argilla/",
    "https://docs.llamaindex.ai/en/stable/api_reference/evaluation/context_relevancy/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/premai/",
    "https://docs.llamaindex.ai/en/stable/api_reference/agent/lats/",
    "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/literalai/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/cloudflare_workersai/",
    "https://docs.llamaindex.ai/en/stable/api_reference/evaluation/metrics/",
    "https://docs.llamaindex.ai/en/stable/api_reference/evaluation/faithfullness/",
    "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/promptlayer/",
    "https://docs.llamaindex.ai/en/stable/api_reference/evaluation/tonic_validate/",
    "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/llama_debug/",
    "https://docs.llamaindex.ai/en/stable/api_reference/agent/coa/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/huggingface_openvino/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/huggingface_itrex/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/alephalpha/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/huggingface_optimum_intel/",
    "https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/ipex_llm/",
    "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/token_counter/",
    "https://docs.llamaindex.ai/en/stable/api_reference/agent/llm_compiler/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/llm_rails/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/huggingface_api/",
    "https://docs.llamaindex.ai/en/stable/api_reference/agent/",
    "https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/clip/",
    "https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/simple/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/alibabacloud_aisearch/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/oci_genai/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/litellm/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/textembed/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/google/",
    "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/arize_phoenix/",
    "https://docs.llamaindex.ai/en/stable/api_reference/evaluation/semantic_similarity/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/voyageai/",
    "https://docs.llamaindex.ai/en/stable/api_reference/evaluation/answer_relevancy/",
    "https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/context/",
    "https://docs.llamaindex.ai/en/stable/api_reference/evaluation/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/gigachat/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/anyscale/",
    "https://docs.llamaindex.ai/en/stable/api_reference/evaluation/multi_modal/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/fastembed/",
    "https://docs.llamaindex.ai/en/stable/api_reference/extractors/entity/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/openai/",
    "https://docs.llamaindex.ai/en/stable/api_reference/evaluation/dataset_generation/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/adapter/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/nomic/",
    "https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/condense_plus_context/",
    "https://docs.llamaindex.ai/en/stable/api_reference/evaluation/guideline/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/ollama/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/mixedbreadai/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/instructor/",
    "https://docs.llamaindex.ai/en/stable/api_reference/agent/dashscope/",
    "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/langfuse/",
    "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/openinference/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/huggingface_optimum/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/together/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/dashscope/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/clarifai/",
    "https://docs.llamaindex.ai/en/stable/api_reference/agent/openai_legacy/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/jinaai/",
    "https://docs.llamaindex.ai/en/stable/api_reference/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/deepinfra/",
    "https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/condense_question/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/elasticsearch/",
    "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/deepeval/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/text_embeddings_inference/",
    "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/",
    "https://docs.llamaindex.ai/en/stable/api_reference/agent/react/",
    "https://docs.llamaindex.ai/en/stable/api_reference/extractors/",
    "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/langchain/",
    "https://docs.llamaindex.ai/en/stable/api_reference/agent/introspective/",
    "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/honeyhive/"
  ],
  "data": [
    {
      "markdown": "\n\n# Core Agent Classes [\\#](\\#core-agent-classes \"Permanent link\")\n\n## Base Types [\\#](\\#base-types \"Permanent link\")\n\nBase agent types.\n\n### BaseAgent [\\#](\\#llama_index.core.agent.types.BaseAgent \"Permanent link\")\n\nBases: `BaseChatEngine`, `BaseQueryEngine`\n\nBase Agent.\n\nSource code in `llama-index-core/llama_index/core/base/agent/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>``` | ```<br>class BaseAgent(BaseChatEngine, BaseQueryEngine):<br>    \"\"\"Base Agent.\"\"\"<br>    def _get_prompts(self) -> PromptDictType:<br>        \"\"\"Get prompts.\"\"\"<br>        # TODO: the ReAct agent does not explicitly specify prompts, would need a<br>        # refactor to expose those prompts<br>        return {}<br>    def _get_prompt_modules(self) -> PromptMixinType:<br>        \"\"\"Get prompt modules.\"\"\"<br>        return {}<br>    def _update_prompts(self, prompts: PromptDictType) -> None:<br>        \"\"\"Update prompts.\"\"\"<br>    # ===== Query Engine Interface =====<br>    @trace_method(\"query\")<br>    def _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:<br>        agent_response = self.chat(<br>            query_bundle.query_str,<br>            chat_history=[],<br>        )<br>        return Response(<br>            response=str(agent_response), source_nodes=agent_response.source_nodes<br>        )<br>    @trace_method(\"query\")<br>    async def _aquery(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:<br>        agent_response = await self.achat(<br>            query_bundle.query_str,<br>            chat_history=[],<br>        )<br>        return Response(<br>            response=str(agent_response), source_nodes=agent_response.source_nodes<br>        )<br>    def stream_chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> StreamingAgentChatResponse:<br>        raise NotImplementedError(\"stream_chat not implemented\")<br>    async def astream_chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> StreamingAgentChatResponse:<br>        raise NotImplementedError(\"astream_chat not implemented\")<br>``` |\n\n### BaseAgentWorker [\\#](\\#llama_index.core.agent.types.BaseAgentWorker \"Permanent link\")\n\nBases: `PromptMixin`, `DispatcherSpanMixin`\n\nBase agent worker.\n\nSource code in `llama-index-core/llama_index/core/base/agent/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>``` | ```<br>class BaseAgentWorker(PromptMixin, DispatcherSpanMixin):<br>    \"\"\"Base agent worker.\"\"\"<br>    def _get_prompts(self) -> PromptDictType:<br>        \"\"\"Get prompts.\"\"\"<br>        # TODO: the ReAct agent does not explicitly specify prompts, would need a<br>        # refactor to expose those prompts<br>        return {}<br>    def _get_prompt_modules(self) -> PromptMixinType:<br>        \"\"\"Get prompt modules.\"\"\"<br>        return {}<br>    def _update_prompts(self, prompts: PromptDictType) -> None:<br>        \"\"\"Update prompts.\"\"\"<br>    @abstractmethod<br>    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>        \"\"\"Initialize step from task.\"\"\"<br>    @abstractmethod<br>    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>    @abstractmethod<br>    async def arun_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        raise NotImplementedError<br>    @abstractmethod<br>    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        # TODO: figure out if we need a different type for TaskStepOutput<br>        raise NotImplementedError<br>    @abstractmethod<br>    async def astream_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async stream).\"\"\"<br>        raise NotImplementedError<br>    @abstractmethod<br>    def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>        \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>    def set_callback_manager(self, callback_manager: CallbackManager) -> None:<br>        \"\"\"Set callback manager.\"\"\"<br>        # TODO: make this abstractmethod (right now will break some agent impls)<br>    def as_agent(self, **kwargs: Any) -> \"AgentRunner\":<br>        \"\"\"Return as an agent runner.\"\"\"<br>        from llama_index.core.agent.runner.base import AgentRunner<br>        return AgentRunner(self, **kwargs)<br>``` |\n\n#### initialize\\_step`abstractmethod`[\\#](\\#llama_index.core.agent.types.BaseAgentWorker.initialize_step \"Permanent link\")\n\n```\ninitialize_step(task: Task, **kwargs: Any) -> TaskStep\n\n```\n\nInitialize step from task.\n\nSource code in `llama-index-core/llama_index/core/base/agent/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>208<br>209<br>210<br>``` | ```<br>@abstractmethod<br>def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>    \"\"\"Initialize step from task.\"\"\"<br>``` |\n\n#### run\\_step`abstractmethod`[\\#](\\#llama_index.core.agent.types.BaseAgentWorker.run_step \"Permanent link\")\n\n```\nrun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step.\n\nSource code in `llama-index-core/llama_index/core/base/agent/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>212<br>213<br>214<br>``` | ```<br>@abstractmethod<br>def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step.\"\"\"<br>``` |\n\n#### arun\\_step`abstractmethod``async`[\\#](\\#llama_index.core.agent.types.BaseAgentWorker.arun_step \"Permanent link\")\n\n```\narun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async).\n\nSource code in `llama-index-core/llama_index/core/base/agent/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>216<br>217<br>218<br>219<br>220<br>221<br>``` | ```<br>@abstractmethod<br>async def arun_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async).\"\"\"<br>    raise NotImplementedError<br>``` |\n\n#### stream\\_step`abstractmethod`[\\#](\\#llama_index.core.agent.types.BaseAgentWorker.stream_step \"Permanent link\")\n\n```\nstream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (stream).\n\nSource code in `llama-index-core/llama_index/core/base/agent/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>223<br>224<br>225<br>226<br>227<br>``` | ```<br>@abstractmethod<br>def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step (stream).\"\"\"<br>    # TODO: figure out if we need a different type for TaskStepOutput<br>    raise NotImplementedError<br>``` |\n\n#### astream\\_step`abstractmethod``async`[\\#](\\#llama_index.core.agent.types.BaseAgentWorker.astream_step \"Permanent link\")\n\n```\nastream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async stream).\n\nSource code in `llama-index-core/llama_index/core/base/agent/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>229<br>230<br>231<br>232<br>233<br>234<br>``` | ```<br>@abstractmethod<br>async def astream_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async stream).\"\"\"<br>    raise NotImplementedError<br>``` |\n\n#### finalize\\_task`abstractmethod`[\\#](\\#llama_index.core.agent.types.BaseAgentWorker.finalize_task \"Permanent link\")\n\n```\nfinalize_task(task: Task, **kwargs: Any) -> None\n\n```\n\nFinalize task, after all the steps are completed.\n\nSource code in `llama-index-core/llama_index/core/base/agent/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>236<br>237<br>238<br>``` | ```<br>@abstractmethod<br>def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>    \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>``` |\n\n#### set\\_callback\\_manager [\\#](\\#llama_index.core.agent.types.BaseAgentWorker.set_callback_manager \"Permanent link\")\n\n```\nset_callback_manager(callback_manager: CallbackManager) -> None\n\n```\n\nSet callback manager.\n\nSource code in `llama-index-core/llama_index/core/base/agent/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>240<br>241<br>``` | ```<br>def set_callback_manager(self, callback_manager: CallbackManager) -> None:<br>    \"\"\"Set callback manager.\"\"\"<br>``` |\n\n#### as\\_agent [\\#](\\#llama_index.core.agent.types.BaseAgentWorker.as_agent \"Permanent link\")\n\n```\nas_agent(**kwargs: Any) -> AgentRunner\n\n```\n\nReturn as an agent runner.\n\nSource code in `llama-index-core/llama_index/core/base/agent/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>244<br>245<br>246<br>247<br>248<br>``` | ```<br>def as_agent(self, **kwargs: Any) -> \"AgentRunner\":<br>    \"\"\"Return as an agent runner.\"\"\"<br>    from llama_index.core.agent.runner.base import AgentRunner<br>    return AgentRunner(self, **kwargs)<br>``` |\n\n### Task [\\#](\\#llama_index.core.agent.types.Task \"Permanent link\")\n\nBases: `BaseModel`\n\nAgent Task.\n\nRepresents a \"run\" of an agent given a user input.\n\nSource code in `llama-index-core/llama_index/core/base/agent/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>``` | ```<br>class Task(BaseModel):<br>    \"\"\"Agent Task.<br>    Represents a \"run\" of an agent given a user input.<br>    \"\"\"<br>    model_config = ConfigDict(arbitrary_types_allowed=True)<br>    task_id: str = Field(<br>        default_factory=lambda: str(uuid.uuid4()), description=\"Task ID\"<br>    )<br>    input: str = Field(..., description=\"User input\")<br>    # NOTE: this is state that may be modified throughout the course of execution of the task<br>    memory: SerializeAsAny[BaseMemory] = Field(<br>        ...,<br>        description=(<br>            \"Conversational Memory. Maintains state before execution of this task.\"<br>        ),<br>    )<br>    callback_manager: CallbackManager = Field(<br>        default_factory=lambda: CallbackManager([]),<br>        exclude=True,<br>        description=\"Callback manager for the task.\",<br>    )<br>    extra_state: Dict[str, Any] = Field(<br>        default_factory=dict,<br>        description=(<br>            \"Additional user-specified state for a given task. \"<br>            \"Can be modified throughout the execution of a task.\"<br>        ),<br>    )<br>``` |\n\n### TaskStep [\\#](\\#llama_index.core.agent.types.TaskStep \"Permanent link\")\n\nBases: `BaseModel`\n\nAgent task step.\n\nRepresents a single input step within the execution run (\"Task\") of an agent\ngiven a user input.\n\nThe output is returned as a `TaskStepOutput`.\n\nSource code in `llama-index-core/llama_index/core/base/agent/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>``` | ```<br>class TaskStep(BaseModel):<br>    \"\"\"Agent task step.<br>    Represents a single input step within the execution run (\"Task\") of an agent<br>    given a user input.<br>    The output is returned as a `TaskStepOutput`.<br>    \"\"\"<br>    task_id: str = Field(..., description=\"Task ID\")<br>    step_id: str = Field(..., description=\"Step ID\")<br>    input: Optional[str] = Field(default=None, description=\"User input\")<br>    # memory: BaseMemory = Field(<br>    #     ..., description=\"Conversational Memory\"<br>    # )<br>    step_state: Dict[str, Any] = Field(<br>        default_factory=dict, description=\"Additional state for a given step.\"<br>    )<br>    # NOTE: the state below may change throughout the course of execution<br>    # this tracks the relationships to other steps<br>    next_steps: Dict[str, \"TaskStep\"] = Field(<br>        default_factory=dict, description=\"Next steps to be executed.\"<br>    )<br>    prev_steps: Dict[str, \"TaskStep\"] = Field(<br>        default_factory=dict,<br>        description=\"Previous steps that were dependencies for this step.\",<br>    )<br>    is_ready: bool = Field(<br>        default=True, description=\"Is this step ready to be executed?\"<br>    )<br>    def get_next_step(<br>        self,<br>        step_id: str,<br>        input: Optional[str] = None,<br>        step_state: Optional[Dict[str, Any]] = None,<br>    ) -> \"TaskStep\":<br>        \"\"\"Convenience function to get next step.<br>        Preserve task_id, memory, step_state.<br>        \"\"\"<br>        return TaskStep(<br>            task_id=self.task_id,<br>            step_id=step_id,<br>            input=input,<br>            # memory=self.memory,<br>            step_state=step_state or self.step_state,<br>        )<br>    def link_step(<br>        self,<br>        next_step: \"TaskStep\",<br>    ) -> None:<br>        \"\"\"Link to next step.<br>        Add link from this step to next, and from next step to current.<br>        \"\"\"<br>        self.next_steps[next_step.step_id] = next_step<br>        next_step.prev_steps[self.step_id] = self<br>``` |\n\n#### get\\_next\\_step [\\#](\\#llama_index.core.agent.types.TaskStep.get_next_step \"Permanent link\")\n\n```\nget_next_step(step_id: str, input: Optional[str] = None, step_state: Optional[Dict[str, Any]] = None) -> TaskStep\n\n```\n\nConvenience function to get next step.\n\nPreserve task\\_id, memory, step\\_state.\n\nSource code in `llama-index-core/llama_index/core/base/agent/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>``` | ```<br>def get_next_step(<br>    self,<br>    step_id: str,<br>    input: Optional[str] = None,<br>    step_state: Optional[Dict[str, Any]] = None,<br>) -> \"TaskStep\":<br>    \"\"\"Convenience function to get next step.<br>    Preserve task_id, memory, step_state.<br>    \"\"\"<br>    return TaskStep(<br>        task_id=self.task_id,<br>        step_id=step_id,<br>        input=input,<br>        # memory=self.memory,<br>        step_state=step_state or self.step_state,<br>    )<br>``` |\n\n#### link\\_step [\\#](\\#llama_index.core.agent.types.TaskStep.link_step \"Permanent link\")\n\n```\nlink_step(next_step: TaskStep) -> None\n\n```\n\nLink to next step.\n\nAdd link from this step to next, and from next step to current.\n\nSource code in `llama-index-core/llama_index/core/base/agent/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>``` | ```<br>def link_step(<br>    self,<br>    next_step: \"TaskStep\",<br>) -> None:<br>    \"\"\"Link to next step.<br>    Add link from this step to next, and from next step to current.<br>    \"\"\"<br>    self.next_steps[next_step.step_id] = next_step<br>    next_step.prev_steps[self.step_id] = self<br>``` |\n\n### TaskStepOutput [\\#](\\#llama_index.core.agent.types.TaskStepOutput \"Permanent link\")\n\nBases: `BaseModel`\n\nAgent task step output.\n\nSource code in `llama-index-core/llama_index/core/base/agent/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>``` | ```<br>class TaskStepOutput(BaseModel):<br>    \"\"\"Agent task step output.\"\"\"<br>    output: Any = Field(..., description=\"Task step output\")<br>    task_step: TaskStep = Field(..., description=\"Task step input\")<br>    next_steps: List[TaskStep] = Field(..., description=\"Next steps to be executed.\")<br>    is_last: bool = Field(default=False, description=\"Is this the last step?\")<br>    def __str__(self) -> str:<br>        \"\"\"String representation.\"\"\"<br>        return str(self.output)<br>``` |\n\n## Runners [\\#](\\#runners \"Permanent link\")\n\n### AgentRunner [\\#](\\#llama_index.core.agent.AgentRunner \"Permanent link\")\n\nBases: `BaseAgentRunner`\n\nAgent runner.\n\nTop-level agent orchestrator that can create tasks, run each step in a task,\nor run a task e2e. Stores state and keeps track of tasks.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `agent_worker` | `BaseAgentWorker` | step executor | _required_ |\n| `chat_history` | `Optional[List[ChatMessage]]` | chat history. Defaults to None. | `None` |\n| `state` | `Optional[AgentState]` | agent state. Defaults to None. | `None` |\n| `memory` | `Optional[BaseMemory]` | memory. Defaults to None. | `None` |\n| `llm` | `Optional[LLM]` | LLM. Defaults to None. | `None` |\n| `callback_manager` | `Optional[CallbackManager]` | callback manager. Defaults to None. | `None` |\n| `init_task_state_kwargs` | `Optional[dict]` | init task state kwargs. Defaults to None. | `None` |\n\nSource code in `llama-index-core/llama_index/core/agent/runner/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>472<br>473<br>474<br>475<br>476<br>477<br>478<br>479<br>480<br>481<br>482<br>483<br>484<br>485<br>486<br>487<br>488<br>489<br>490<br>491<br>492<br>493<br>494<br>495<br>496<br>497<br>498<br>499<br>500<br>501<br>502<br>503<br>504<br>505<br>506<br>507<br>508<br>509<br>510<br>511<br>512<br>513<br>514<br>515<br>516<br>517<br>518<br>519<br>520<br>521<br>522<br>523<br>524<br>525<br>526<br>527<br>528<br>529<br>530<br>531<br>532<br>533<br>534<br>535<br>536<br>537<br>538<br>539<br>540<br>541<br>542<br>543<br>544<br>545<br>546<br>547<br>548<br>549<br>550<br>551<br>552<br>553<br>554<br>555<br>556<br>557<br>558<br>559<br>560<br>561<br>562<br>563<br>564<br>565<br>566<br>567<br>568<br>569<br>570<br>571<br>572<br>573<br>574<br>575<br>576<br>577<br>578<br>579<br>580<br>581<br>582<br>583<br>584<br>585<br>586<br>587<br>588<br>589<br>590<br>591<br>592<br>593<br>594<br>595<br>596<br>597<br>598<br>599<br>600<br>601<br>602<br>603<br>604<br>605<br>606<br>607<br>608<br>609<br>610<br>611<br>612<br>613<br>614<br>615<br>616<br>617<br>618<br>619<br>620<br>621<br>622<br>623<br>624<br>625<br>626<br>627<br>628<br>629<br>630<br>631<br>632<br>633<br>634<br>635<br>636<br>637<br>638<br>639<br>640<br>641<br>642<br>643<br>644<br>645<br>646<br>647<br>648<br>649<br>650<br>651<br>652<br>653<br>654<br>655<br>656<br>657<br>658<br>659<br>660<br>661<br>662<br>663<br>664<br>665<br>666<br>667<br>668<br>669<br>670<br>671<br>672<br>673<br>674<br>675<br>676<br>677<br>678<br>679<br>680<br>681<br>682<br>683<br>684<br>685<br>686<br>687<br>688<br>689<br>690<br>691<br>692<br>693<br>694<br>695<br>696<br>697<br>698<br>699<br>700<br>701<br>702<br>703<br>704<br>705<br>706<br>707<br>708<br>709<br>710<br>711<br>712<br>713<br>714<br>715<br>716<br>717<br>718<br>719<br>720<br>721<br>722<br>723<br>724<br>725<br>726<br>727<br>728<br>729<br>730<br>731<br>732<br>733<br>734<br>735<br>736<br>``` | ```<br>class AgentRunner(BaseAgentRunner):<br>    \"\"\"Agent runner.<br>    Top-level agent orchestrator that can create tasks, run each step in a task,<br>    or run a task e2e. Stores state and keeps track of tasks.<br>    Args:<br>        agent_worker (BaseAgentWorker): step executor<br>        chat_history (Optional[List[ChatMessage]], optional): chat history. Defaults to None.<br>        state (Optional[AgentState], optional): agent state. Defaults to None.<br>        memory (Optional[BaseMemory], optional): memory. Defaults to None.<br>        llm (Optional[LLM], optional): LLM. Defaults to None.<br>        callback_manager (Optional[CallbackManager], optional): callback manager. Defaults to None.<br>        init_task_state_kwargs (Optional[dict], optional): init task state kwargs. Defaults to None.<br>    \"\"\"<br>    # # TODO: implement this in Pydantic<br>    def __init__(<br>        self,<br>        agent_worker: BaseAgentWorker,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        state: Optional[AgentState] = None,<br>        memory: Optional[BaseMemory] = None,<br>        llm: Optional[LLM] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        init_task_state_kwargs: Optional[dict] = None,<br>        delete_task_on_finish: bool = False,<br>        default_tool_choice: str = \"auto\",<br>        verbose: bool = False,<br>    ) -> None:<br>        \"\"\"Initialize.\"\"\"<br>        self.agent_worker = agent_worker<br>        self.state = state or AgentState()<br>        self.memory = memory or ChatMemoryBuffer.from_defaults(chat_history, llm=llm)<br>        # get and set callback manager<br>        if callback_manager is not None:<br>            self.agent_worker.set_callback_manager(callback_manager)<br>            self.callback_manager = callback_manager<br>        else:<br>            # TODO: This is *temporary*<br>            # Stopgap before having a callback on the BaseAgentWorker interface.<br>            # Doing that requires a bit more refactoring to make sure existing code<br>            # doesn't break.<br>            if hasattr(self.agent_worker, \"callback_manager\"):<br>                self.callback_manager = (<br>                    self.agent_worker.callback_manager or CallbackManager()<br>                )<br>            else:<br>                self.callback_manager = CallbackManager()<br>        self.init_task_state_kwargs = init_task_state_kwargs or {}<br>        self.delete_task_on_finish = delete_task_on_finish<br>        self.default_tool_choice = default_tool_choice<br>        self.verbose = verbose<br>    @staticmethod<br>    def from_llm(<br>        tools: Optional[List[BaseTool]] = None,<br>        llm: Optional[LLM] = None,<br>        **kwargs: Any,<br>    ) -> \"AgentRunner\":<br>        from llama_index.core.agent import ReActAgent<br>        if os.getenv(\"IS_TESTING\"):<br>            return ReActAgent.from_tools(<br>                tools=tools,<br>                llm=llm,<br>                **kwargs,<br>            )<br>        try:<br>            from llama_index.llms.openai import OpenAI  # pants: no-infer-dep<br>            from llama_index.llms.openai.utils import (<br>                is_function_calling_model,<br>            )  # pants: no-infer-dep<br>        except ImportError:<br>            raise ImportError(<br>                \"`llama-index-llms-openai` package not found. Please \"<br>                \"install by running `pip install llama-index-llms-openai`.\"<br>            )<br>        if isinstance(llm, OpenAI) and is_function_calling_model(llm.model):<br>            from llama_index.agent.openai import OpenAIAgent  # pants: no-infer-dep<br>            return OpenAIAgent.from_tools(<br>                tools=tools,<br>                llm=llm,<br>                **kwargs,<br>            )<br>        else:<br>            return ReActAgent.from_tools(<br>                tools=tools,<br>                llm=llm,<br>                **kwargs,<br>            )<br>    @property<br>    def chat_history(self) -> List[ChatMessage]:<br>        return self.memory.get_all()<br>    def reset(self) -> None:<br>        self.memory.reset()<br>        self.state.reset()<br>    def create_task(self, input: str, **kwargs: Any) -> Task:<br>        \"\"\"Create task.\"\"\"<br>        if not self.init_task_state_kwargs:<br>            extra_state = kwargs.pop(\"extra_state\", {})<br>        else:<br>            if \"extra_state\" in kwargs:<br>                raise ValueError(<br>                    \"Cannot specify both `extra_state` and `init_task_state_kwargs`\"<br>                )<br>            else:<br>                extra_state = self.init_task_state_kwargs<br>        callback_manager = kwargs.pop(\"callback_manager\", self.callback_manager)<br>        task = Task(<br>            input=input,<br>            memory=self.memory,<br>            extra_state=extra_state,<br>            callback_manager=callback_manager,<br>            **kwargs,<br>        )<br>        # # put input into memory<br>        # self.memory.put(ChatMessage(content=input, role=MessageRole.USER))<br>        # get initial step from task, and put it in the step queue<br>        initial_step = self.agent_worker.initialize_step(task)<br>        task_state = TaskState(<br>            task=task,<br>            step_queue=deque([initial_step]),<br>        )<br>        # add it to state<br>        self.state.task_dict[task.task_id] = task_state<br>        return task<br>    def delete_task(<br>        self,<br>        task_id: str,<br>    ) -> None:<br>        \"\"\"Delete task.<br>        NOTE: this will not delete any previous executions from memory.<br>        \"\"\"<br>        self.state.task_dict.pop(task_id)<br>    def list_tasks(self, **kwargs: Any) -> List[Task]:<br>        \"\"\"List tasks.\"\"\"<br>        return [task_state.task for task_state in self.state.task_dict.values()]<br>    def get_task(self, task_id: str, **kwargs: Any) -> Task:<br>        \"\"\"Get task.\"\"\"<br>        return self.state.get_task(task_id)<br>    def get_upcoming_steps(self, task_id: str, **kwargs: Any) -> List[TaskStep]:<br>        \"\"\"Get upcoming steps.\"\"\"<br>        return list(self.state.get_step_queue(task_id))<br>    def get_completed_steps(self, task_id: str, **kwargs: Any) -> List[TaskStepOutput]:<br>        \"\"\"Get completed steps.\"\"\"<br>        return self.state.get_completed_steps(task_id)<br>    def get_task_output(self, task_id: str, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Get task output.\"\"\"<br>        completed_steps = self.get_completed_steps(task_id)<br>        if len(completed_steps) == 0:<br>            raise ValueError(f\"No completed steps for task_id: {task_id}\")<br>        return completed_steps[-1]<br>    def get_completed_tasks(self, **kwargs: Any) -> List[Task]:<br>        \"\"\"Get completed tasks.\"\"\"<br>        task_states = list(self.state.task_dict.values())<br>        completed_tasks = []<br>        for task_state in task_states:<br>            completed_steps = self.get_completed_steps(task_state.task.task_id)<br>            if len(completed_steps) > 0 and completed_steps[-1].is_last:<br>                completed_tasks.append(task_state.task)<br>        return completed_tasks<br>    @dispatcher.span<br>    def _run_step(<br>        self,<br>        task_id: str,<br>        step: Optional[TaskStep] = None,<br>        input: Optional[str] = None,<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Execute step.\"\"\"<br>        task = self.state.get_task(task_id)<br>        step_queue = self.state.get_step_queue(task_id)<br>        step = step or step_queue.popleft()<br>        if input is not None:<br>            step.input = input<br>        dispatcher.event(<br>            AgentRunStepStartEvent(task_id=task_id, step=step, input=input)<br>        )<br>        if self.verbose:<br>            print(f\"> Running step {step.step_id}. Step input: {step.input}\")<br>        # TODO: figure out if you can dynamically swap in different step executors<br>        # not clear when you would do that by theoretically possible<br>        if mode == ChatResponseMode.WAIT:<br>            cur_step_output = self.agent_worker.run_step(step, task, **kwargs)<br>        elif mode == ChatResponseMode.STREAM:<br>            cur_step_output = self.agent_worker.stream_step(step, task, **kwargs)<br>        else:<br>            raise ValueError(f\"Invalid mode: {mode}\")<br>        # append cur_step_output next steps to queue<br>        next_steps = cur_step_output.next_steps<br>        step_queue.extend(next_steps)<br>        # add cur_step_output to completed steps<br>        completed_steps = self.state.get_completed_steps(task_id)<br>        completed_steps.append(cur_step_output)<br>        dispatcher.event(AgentRunStepEndEvent(step_output=cur_step_output))<br>        return cur_step_output<br>    @dispatcher.span<br>    async def _arun_step(<br>        self,<br>        task_id: str,<br>        step: Optional[TaskStep] = None,<br>        input: Optional[str] = None,<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Execute step.\"\"\"<br>        dispatcher.event(<br>            AgentRunStepStartEvent(task_id=task_id, step=step, input=input)<br>        )<br>        task = self.state.get_task(task_id)<br>        step_queue = self.state.get_step_queue(task_id)<br>        step = step or step_queue.popleft()<br>        if input is not None:<br>            step.input = input<br>        if self.verbose:<br>            print(f\"> Running step {step.step_id}. Step input: {step.input}\")<br>        # TODO: figure out if you can dynamically swap in different step executors<br>        # not clear when you would do that by theoretically possible<br>        if mode == ChatResponseMode.WAIT:<br>            cur_step_output = await self.agent_worker.arun_step(step, task, **kwargs)<br>        elif mode == ChatResponseMode.STREAM:<br>            cur_step_output = await self.agent_worker.astream_step(step, task, **kwargs)<br>        else:<br>            raise ValueError(f\"Invalid mode: {mode}\")<br>        # append cur_step_output next steps to queue<br>        next_steps = cur_step_output.next_steps<br>        step_queue.extend(next_steps)<br>        # add cur_step_output to completed steps<br>        completed_steps = self.state.get_completed_steps(task_id)<br>        completed_steps.append(cur_step_output)<br>        dispatcher.event(AgentRunStepEndEvent(step_output=cur_step_output))<br>        return cur_step_output<br>    @dispatcher.span<br>    def run_step(<br>        self,<br>        task_id: str,<br>        input: Optional[str] = None,<br>        step: Optional[TaskStep] = None,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        step = validate_step_from_args(task_id, input, step, **kwargs)<br>        return self._run_step(<br>            task_id, step, input=input, mode=ChatResponseMode.WAIT, **kwargs<br>        )<br>    @dispatcher.span<br>    async def arun_step(<br>        self,<br>        task_id: str,<br>        input: Optional[str] = None,<br>        step: Optional[TaskStep] = None,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        step = validate_step_from_args(task_id, input, step, **kwargs)<br>        return await self._arun_step(<br>            task_id, step, input=input, mode=ChatResponseMode.WAIT, **kwargs<br>        )<br>    @dispatcher.span<br>    def stream_step(<br>        self,<br>        task_id: str,<br>        input: Optional[str] = None,<br>        step: Optional[TaskStep] = None,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        step = validate_step_from_args(task_id, input, step, **kwargs)<br>        return self._run_step(<br>            task_id, step, input=input, mode=ChatResponseMode.STREAM, **kwargs<br>        )<br>    @dispatcher.span<br>    async def astream_step(<br>        self,<br>        task_id: str,<br>        input: Optional[str] = None,<br>        step: Optional[TaskStep] = None,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async stream).\"\"\"<br>        step = validate_step_from_args(task_id, input, step, **kwargs)<br>        return await self._arun_step(<br>            task_id, step, input=input, mode=ChatResponseMode.STREAM, **kwargs<br>        )<br>    @dispatcher.span<br>    def finalize_response(<br>        self,<br>        task_id: str,<br>        step_output: Optional[TaskStepOutput] = None,<br>    ) -> AGENT_CHAT_RESPONSE_TYPE:<br>        \"\"\"Finalize response.\"\"\"<br>        if step_output is None:<br>            step_output = self.state.get_completed_steps(task_id)[-1]<br>        if not step_output.is_last:<br>            raise ValueError(<br>                \"finalize_response can only be called on the last step output\"<br>            )<br>        if not isinstance(<br>            step_output.output,<br>            (AgentChatResponse, StreamingAgentChatResponse),<br>        ):<br>            raise ValueError(<br>                \"When `is_last` is True, cur_step_output.output must be \"<br>                f\"AGENT_CHAT_RESPONSE_TYPE: {step_output.output}\"<br>            )<br>        # finalize task<br>        self.agent_worker.finalize_task(self.state.get_task(task_id))<br>        if self.delete_task_on_finish:<br>            self.delete_task(task_id)<br>        # Attach all sources generated across all steps<br>        step_output.output.sources = self.get_task(task_id).extra_state.get(<br>            \"sources\", []<br>        )<br>        step_output.output.set_source_nodes()<br>        return cast(AGENT_CHAT_RESPONSE_TYPE, step_output.output)<br>    @dispatcher.span<br>    def _chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Union[str, dict] = \"auto\",<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>    ) -> AGENT_CHAT_RESPONSE_TYPE:<br>        \"\"\"Chat with step executor.\"\"\"<br>        if chat_history is not None:<br>            self.memory.set(chat_history)<br>        task = self.create_task(message)<br>        result_output = None<br>        dispatcher.event(AgentChatWithStepStartEvent(user_msg=message))<br>        while True:<br>            # pass step queue in as argument, assume step executor is stateless<br>            cur_step_output = self._run_step(<br>                task.task_id, mode=mode, tool_choice=tool_choice<br>            )<br>            if cur_step_output.is_last:<br>                result_output = cur_step_output<br>                break<br>            # ensure tool_choice does not cause endless loops<br>            tool_choice = \"auto\"<br>        result = self.finalize_response(<br>            task.task_id,<br>            result_output,<br>        )<br>        dispatcher.event(AgentChatWithStepEndEvent(response=result))<br>        return result<br>    @dispatcher.span<br>    async def _achat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Union[str, dict] = \"auto\",<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>    ) -> AGENT_CHAT_RESPONSE_TYPE:<br>        \"\"\"Chat with step executor.\"\"\"<br>        if chat_history is not None:<br>            self.memory.set(chat_history)<br>        task = self.create_task(message)<br>        result_output = None<br>        dispatcher.event(AgentChatWithStepStartEvent(user_msg=message))<br>        while True:<br>            # pass step queue in as argument, assume step executor is stateless<br>            cur_step_output = await self._arun_step(<br>                task.task_id, mode=mode, tool_choice=tool_choice<br>            )<br>            if cur_step_output.is_last:<br>                result_output = cur_step_output<br>                break<br>            # ensure tool_choice does not cause endless loops<br>            tool_choice = \"auto\"<br>        result = self.finalize_response(<br>            task.task_id,<br>            result_output,<br>        )<br>        dispatcher.event(AgentChatWithStepEndEvent(response=result))<br>        return result<br>    @dispatcher.span<br>    @trace_method(\"chat\")<br>    def chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Optional[Union[str, dict]] = None,<br>    ) -> AgentChatResponse:<br>        # override tool choice is provided as input.<br>        if tool_choice is None:<br>            tool_choice = self.default_tool_choice<br>        with self.callback_manager.event(<br>            CBEventType.AGENT_STEP,<br>            payload={EventPayload.MESSAGES: [message]},<br>        ) as e:<br>            chat_response = self._chat(<br>                message=message,<br>                chat_history=chat_history,<br>                tool_choice=tool_choice,<br>                mode=ChatResponseMode.WAIT,<br>            )<br>            assert isinstance(chat_response, AgentChatResponse)<br>            e.on_end(payload={EventPayload.RESPONSE: chat_response})<br>        return chat_response<br>    @dispatcher.span<br>    @trace_method(\"chat\")<br>    async def achat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Optional[Union[str, dict]] = None,<br>    ) -> AgentChatResponse:<br>        # override tool choice is provided as input.<br>        if tool_choice is None:<br>            tool_choice = self.default_tool_choice<br>        with self.callback_manager.event(<br>            CBEventType.AGENT_STEP,<br>            payload={EventPayload.MESSAGES: [message]},<br>        ) as e:<br>            chat_response = await self._achat(<br>                message=message,<br>                chat_history=chat_history,<br>                tool_choice=tool_choice,<br>                mode=ChatResponseMode.WAIT,<br>            )<br>            assert isinstance(chat_response, AgentChatResponse)<br>            e.on_end(payload={EventPayload.RESPONSE: chat_response})<br>        return chat_response<br>    @dispatcher.span<br>    @trace_method(\"chat\")<br>    def stream_chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Optional[Union[str, dict]] = None,<br>    ) -> StreamingAgentChatResponse:<br>        # override tool choice is provided as input.<br>        if tool_choice is None:<br>            tool_choice = self.default_tool_choice<br>        with self.callback_manager.event(<br>            CBEventType.AGENT_STEP,<br>            payload={EventPayload.MESSAGES: [message]},<br>        ) as e:<br>            chat_response = self._chat(<br>                message, chat_history, tool_choice, mode=ChatResponseMode.STREAM<br>            )<br>            assert isinstance(chat_response, StreamingAgentChatResponse) or (<br>                isinstance(chat_response, AgentChatResponse)<br>                and chat_response.is_dummy_stream<br>            )<br>            e.on_end(payload={EventPayload.RESPONSE: chat_response})<br>        return chat_response  # type: ignore<br>    @dispatcher.span<br>    @trace_method(\"chat\")<br>    async def astream_chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Optional[Union[str, dict]] = None,<br>    ) -> StreamingAgentChatResponse:<br>        # override tool choice is provided as input.<br>        if tool_choice is None:<br>            tool_choice = self.default_tool_choice<br>        with self.callback_manager.event(<br>            CBEventType.AGENT_STEP,<br>            payload={EventPayload.MESSAGES: [message]},<br>        ) as e:<br>            chat_response = await self._achat(<br>                message, chat_history, tool_choice, mode=ChatResponseMode.STREAM<br>            )<br>            assert isinstance(chat_response, StreamingAgentChatResponse) or (<br>                isinstance(chat_response, AgentChatResponse)<br>                and chat_response.is_dummy_stream<br>            )<br>            e.on_end(payload={EventPayload.RESPONSE: chat_response})<br>        return chat_response  # type: ignore<br>    def undo_step(self, task_id: str) -> None:<br>        \"\"\"Undo previous step.\"\"\"<br>        raise NotImplementedError(\"undo_step not implemented\")<br>``` |\n\n#### create\\_task [\\#](\\#llama_index.core.agent.AgentRunner.create_task \"Permanent link\")\n\n```\ncreate_task(input: str, **kwargs: Any) -> Task\n\n```\n\nCreate task.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>``` | ```<br>def create_task(self, input: str, **kwargs: Any) -> Task:<br>    \"\"\"Create task.\"\"\"<br>    if not self.init_task_state_kwargs:<br>        extra_state = kwargs.pop(\"extra_state\", {})<br>    else:<br>        if \"extra_state\" in kwargs:<br>            raise ValueError(<br>                \"Cannot specify both `extra_state` and `init_task_state_kwargs`\"<br>            )<br>        else:<br>            extra_state = self.init_task_state_kwargs<br>    callback_manager = kwargs.pop(\"callback_manager\", self.callback_manager)<br>    task = Task(<br>        input=input,<br>        memory=self.memory,<br>        extra_state=extra_state,<br>        callback_manager=callback_manager,<br>        **kwargs,<br>    )<br>    # # put input into memory<br>    # self.memory.put(ChatMessage(content=input, role=MessageRole.USER))<br>    # get initial step from task, and put it in the step queue<br>    initial_step = self.agent_worker.initialize_step(task)<br>    task_state = TaskState(<br>        task=task,<br>        step_queue=deque([initial_step]),<br>    )<br>    # add it to state<br>    self.state.task_dict[task.task_id] = task_state<br>    return task<br>``` |\n\n#### delete\\_task [\\#](\\#llama_index.core.agent.AgentRunner.delete_task \"Permanent link\")\n\n```\ndelete_task(task_id: str) -> None\n\n```\n\nDelete task.\n\nNOTE: this will not delete any previous executions from memory.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>``` | ```<br>def delete_task(<br>    self,<br>    task_id: str,<br>) -> None:<br>    \"\"\"Delete task.<br>    NOTE: this will not delete any previous executions from memory.<br>    \"\"\"<br>    self.state.task_dict.pop(task_id)<br>``` |\n\n#### list\\_tasks [\\#](\\#llama_index.core.agent.AgentRunner.list_tasks \"Permanent link\")\n\n```\nlist_tasks(**kwargs: Any) -> List[Task]\n\n```\n\nList tasks.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>351<br>352<br>353<br>``` | ```<br>def list_tasks(self, **kwargs: Any) -> List[Task]:<br>    \"\"\"List tasks.\"\"\"<br>    return [task_state.task for task_state in self.state.task_dict.values()]<br>``` |\n\n#### get\\_task [\\#](\\#llama_index.core.agent.AgentRunner.get_task \"Permanent link\")\n\n```\nget_task(task_id: str, **kwargs: Any) -> Task\n\n```\n\nGet task.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>355<br>356<br>357<br>``` | ```<br>def get_task(self, task_id: str, **kwargs: Any) -> Task:<br>    \"\"\"Get task.\"\"\"<br>    return self.state.get_task(task_id)<br>``` |\n\n#### get\\_upcoming\\_steps [\\#](\\#llama_index.core.agent.AgentRunner.get_upcoming_steps \"Permanent link\")\n\n```\nget_upcoming_steps(task_id: str, **kwargs: Any) -> List[TaskStep]\n\n```\n\nGet upcoming steps.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>359<br>360<br>361<br>``` | ```<br>def get_upcoming_steps(self, task_id: str, **kwargs: Any) -> List[TaskStep]:<br>    \"\"\"Get upcoming steps.\"\"\"<br>    return list(self.state.get_step_queue(task_id))<br>``` |\n\n#### get\\_completed\\_steps [\\#](\\#llama_index.core.agent.AgentRunner.get_completed_steps \"Permanent link\")\n\n```\nget_completed_steps(task_id: str, **kwargs: Any) -> List[TaskStepOutput]\n\n```\n\nGet completed steps.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>363<br>364<br>365<br>``` | ```<br>def get_completed_steps(self, task_id: str, **kwargs: Any) -> List[TaskStepOutput]:<br>    \"\"\"Get completed steps.\"\"\"<br>    return self.state.get_completed_steps(task_id)<br>``` |\n\n#### get\\_task\\_output [\\#](\\#llama_index.core.agent.AgentRunner.get_task_output \"Permanent link\")\n\n```\nget_task_output(task_id: str, **kwargs: Any) -> TaskStepOutput\n\n```\n\nGet task output.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>367<br>368<br>369<br>370<br>371<br>372<br>``` | ```<br>def get_task_output(self, task_id: str, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Get task output.\"\"\"<br>    completed_steps = self.get_completed_steps(task_id)<br>    if len(completed_steps) == 0:<br>        raise ValueError(f\"No completed steps for task_id: {task_id}\")<br>    return completed_steps[-1]<br>``` |\n\n#### get\\_completed\\_tasks [\\#](\\#llama_index.core.agent.AgentRunner.get_completed_tasks \"Permanent link\")\n\n```\nget_completed_tasks(**kwargs: Any) -> List[Task]\n\n```\n\nGet completed tasks.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>``` | ```<br>def get_completed_tasks(self, **kwargs: Any) -> List[Task]:<br>    \"\"\"Get completed tasks.\"\"\"<br>    task_states = list(self.state.task_dict.values())<br>    completed_tasks = []<br>    for task_state in task_states:<br>        completed_steps = self.get_completed_steps(task_state.task.task_id)<br>        if len(completed_steps) > 0 and completed_steps[-1].is_last:<br>            completed_tasks.append(task_state.task)<br>    return completed_tasks<br>``` |\n\n#### run\\_step [\\#](\\#llama_index.core.agent.AgentRunner.run_step \"Permanent link\")\n\n```\nrun_step(task_id: str, input: Optional[str] = None, step: Optional[TaskStep] = None, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>469<br>470<br>471<br>472<br>473<br>474<br>475<br>476<br>477<br>478<br>479<br>480<br>481<br>``` | ```<br>@dispatcher.span<br>def run_step(<br>    self,<br>    task_id: str,<br>    input: Optional[str] = None,<br>    step: Optional[TaskStep] = None,<br>    **kwargs: Any,<br>) -> TaskStepOutput:<br>    \"\"\"Run step.\"\"\"<br>    step = validate_step_from_args(task_id, input, step, **kwargs)<br>    return self._run_step(<br>        task_id, step, input=input, mode=ChatResponseMode.WAIT, **kwargs<br>    )<br>``` |\n\n#### arun\\_step`async`[\\#](\\#llama_index.core.agent.AgentRunner.arun_step \"Permanent link\")\n\n```\narun_step(task_id: str, input: Optional[str] = None, step: Optional[TaskStep] = None, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async).\n\nSource code in `llama-index-core/llama_index/core/agent/runner/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>483<br>484<br>485<br>486<br>487<br>488<br>489<br>490<br>491<br>492<br>493<br>494<br>495<br>``` | ```<br>@dispatcher.span<br>async def arun_step(<br>    self,<br>    task_id: str,<br>    input: Optional[str] = None,<br>    step: Optional[TaskStep] = None,<br>    **kwargs: Any,<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async).\"\"\"<br>    step = validate_step_from_args(task_id, input, step, **kwargs)<br>    return await self._arun_step(<br>        task_id, step, input=input, mode=ChatResponseMode.WAIT, **kwargs<br>    )<br>``` |\n\n#### stream\\_step [\\#](\\#llama_index.core.agent.AgentRunner.stream_step \"Permanent link\")\n\n```\nstream_step(task_id: str, input: Optional[str] = None, step: Optional[TaskStep] = None, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (stream).\n\nSource code in `llama-index-core/llama_index/core/agent/runner/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>497<br>498<br>499<br>500<br>501<br>502<br>503<br>504<br>505<br>506<br>507<br>508<br>509<br>``` | ```<br>@dispatcher.span<br>def stream_step(<br>    self,<br>    task_id: str,<br>    input: Optional[str] = None,<br>    step: Optional[TaskStep] = None,<br>    **kwargs: Any,<br>) -> TaskStepOutput:<br>    \"\"\"Run step (stream).\"\"\"<br>    step = validate_step_from_args(task_id, input, step, **kwargs)<br>    return self._run_step(<br>        task_id, step, input=input, mode=ChatResponseMode.STREAM, **kwargs<br>    )<br>``` |\n\n#### astream\\_step`async`[\\#](\\#llama_index.core.agent.AgentRunner.astream_step \"Permanent link\")\n\n```\nastream_step(task_id: str, input: Optional[str] = None, step: Optional[TaskStep] = None, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async stream).\n\nSource code in `llama-index-core/llama_index/core/agent/runner/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>511<br>512<br>513<br>514<br>515<br>516<br>517<br>518<br>519<br>520<br>521<br>522<br>523<br>``` | ```<br>@dispatcher.span<br>async def astream_step(<br>    self,<br>    task_id: str,<br>    input: Optional[str] = None,<br>    step: Optional[TaskStep] = None,<br>    **kwargs: Any,<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async stream).\"\"\"<br>    step = validate_step_from_args(task_id, input, step, **kwargs)<br>    return await self._arun_step(<br>        task_id, step, input=input, mode=ChatResponseMode.STREAM, **kwargs<br>    )<br>``` |\n\n#### finalize\\_response [\\#](\\#llama_index.core.agent.AgentRunner.finalize_response \"Permanent link\")\n\n```\nfinalize_response(task_id: str, step_output: Optional[TaskStepOutput] = None) -> AGENT_CHAT_RESPONSE_TYPE\n\n```\n\nFinalize response.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>525<br>526<br>527<br>528<br>529<br>530<br>531<br>532<br>533<br>534<br>535<br>536<br>537<br>538<br>539<br>540<br>541<br>542<br>543<br>544<br>545<br>546<br>547<br>548<br>549<br>550<br>551<br>552<br>553<br>554<br>555<br>556<br>557<br>558<br>559<br>560<br>``` | ```<br>@dispatcher.span<br>def finalize_response(<br>    self,<br>    task_id: str,<br>    step_output: Optional[TaskStepOutput] = None,<br>) -> AGENT_CHAT_RESPONSE_TYPE:<br>    \"\"\"Finalize response.\"\"\"<br>    if step_output is None:<br>        step_output = self.state.get_completed_steps(task_id)[-1]<br>    if not step_output.is_last:<br>        raise ValueError(<br>            \"finalize_response can only be called on the last step output\"<br>        )<br>    if not isinstance(<br>        step_output.output,<br>        (AgentChatResponse, StreamingAgentChatResponse),<br>    ):<br>        raise ValueError(<br>            \"When `is_last` is True, cur_step_output.output must be \"<br>            f\"AGENT_CHAT_RESPONSE_TYPE: {step_output.output}\"<br>        )<br>    # finalize task<br>    self.agent_worker.finalize_task(self.state.get_task(task_id))<br>    if self.delete_task_on_finish:<br>        self.delete_task(task_id)<br>    # Attach all sources generated across all steps<br>    step_output.output.sources = self.get_task(task_id).extra_state.get(<br>        \"sources\", []<br>    )<br>    step_output.output.set_source_nodes()<br>    return cast(AGENT_CHAT_RESPONSE_TYPE, step_output.output)<br>``` |\n\n#### undo\\_step [\\#](\\#llama_index.core.agent.AgentRunner.undo_step \"Permanent link\")\n\n```\nundo_step(task_id: str) -> None\n\n```\n\nUndo previous step.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>734<br>735<br>736<br>``` | ```<br>def undo_step(self, task_id: str) -> None:<br>    \"\"\"Undo previous step.\"\"\"<br>    raise NotImplementedError(\"undo_step not implemented\")<br>``` |\n\n### ParallelAgentRunner [\\#](\\#llama_index.core.agent.ParallelAgentRunner \"Permanent link\")\n\nBases: `BaseAgentRunner`\n\nParallel agent runner.\n\nExecutes steps in queue in parallel. Requires async support.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/parallel.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>472<br>473<br>474<br>475<br>476<br>477<br>478<br>479<br>480<br>481<br>482<br>483<br>484<br>485<br>486<br>487<br>488<br>489<br>490<br>491<br>492<br>493<br>494<br>495<br>496<br>``` | ```<br>class ParallelAgentRunner(BaseAgentRunner):<br>    \"\"\"Parallel agent runner.<br>    Executes steps in queue in parallel. Requires async support.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        agent_worker: BaseAgentWorker,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        state: Optional[DAGAgentState] = None,<br>        memory: Optional[BaseMemory] = None,<br>        llm: Optional[LLM] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        init_task_state_kwargs: Optional[dict] = None,<br>        delete_task_on_finish: bool = False,<br>    ) -> None:<br>        \"\"\"Initialize.\"\"\"<br>        self.memory = memory or ChatMemoryBuffer.from_defaults(chat_history, llm=llm)<br>        self.state = state or DAGAgentState()<br>        self.callback_manager = callback_manager or CallbackManager([])<br>        self.init_task_state_kwargs = init_task_state_kwargs or {}<br>        self.agent_worker = agent_worker<br>        self.delete_task_on_finish = delete_task_on_finish<br>    @property<br>    def chat_history(self) -> List[ChatMessage]:<br>        return self.memory.get_all()<br>    def reset(self) -> None:<br>        self.memory.reset()<br>    def create_task(self, input: str, **kwargs: Any) -> Task:<br>        \"\"\"Create task.\"\"\"<br>        task = Task(<br>            input=input,<br>            memory=self.memory,<br>            extra_state=self.init_task_state_kwargs,<br>            **kwargs,<br>        )<br>        # # put input into memory<br>        # self.memory.put(ChatMessage(content=input, role=MessageRole.USER))<br>        # add it to state<br>        # get initial step from task, and put it in the step queue<br>        initial_step = self.agent_worker.initialize_step(task)<br>        task_state = DAGTaskState(<br>            task=task,<br>            root_step=initial_step,<br>            step_queue=deque([initial_step]),<br>        )<br>        self.state.task_dict[task.task_id] = task_state<br>        return task<br>    def delete_task(<br>        self,<br>        task_id: str,<br>    ) -> None:<br>        \"\"\"Delete task.<br>        NOTE: this will not delete any previous executions from memory.<br>        \"\"\"<br>        self.state.task_dict.pop(task_id)<br>    def get_completed_tasks(self, **kwargs: Any) -> List[Task]:<br>        \"\"\"Get completed tasks.\"\"\"<br>        task_states = list(self.state.task_dict.values())<br>        return [<br>            task_state.task<br>            for task_state in task_states<br>            if len(task_state.completed_steps) > 0<br>            and task_state.completed_steps[-1].is_last<br>        ]<br>    def get_task_output(self, task_id: str, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Get task output.\"\"\"<br>        task_state = self.state.task_dict[task_id]<br>        if len(task_state.completed_steps) == 0:<br>            raise ValueError(f\"No completed steps for task_id: {task_id}\")<br>        return task_state.completed_steps[-1]<br>    def list_tasks(self, **kwargs: Any) -> List[Task]:<br>        \"\"\"List tasks.\"\"\"<br>        task_states = list(self.state.task_dict.values())<br>        return [task_state.task for task_state in task_states]<br>    def get_task(self, task_id: str, **kwargs: Any) -> Task:<br>        \"\"\"Get task.\"\"\"<br>        return self.state.get_task(task_id)<br>    def get_upcoming_steps(self, task_id: str, **kwargs: Any) -> List[TaskStep]:<br>        \"\"\"Get upcoming steps.\"\"\"<br>        return list(self.state.get_step_queue(task_id))<br>    def get_completed_steps(self, task_id: str, **kwargs: Any) -> List[TaskStepOutput]:<br>        \"\"\"Get completed steps.\"\"\"<br>        return self.state.get_completed_steps(task_id)<br>    def run_steps_in_queue(<br>        self,<br>        task_id: str,<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>        **kwargs: Any,<br>    ) -> List[TaskStepOutput]:<br>        \"\"\"Execute steps in queue.<br>        Run all steps in queue, clearing it out.<br>        Assume that all steps can be run in parallel.<br>        \"\"\"<br>        return asyncio_run(self.arun_steps_in_queue(task_id, mode=mode, **kwargs))<br>    async def arun_steps_in_queue(<br>        self,<br>        task_id: str,<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>        **kwargs: Any,<br>    ) -> List[TaskStepOutput]:<br>        \"\"\"Execute all steps in queue.<br>        All steps in queue are assumed to be ready.<br>        \"\"\"<br>        # first pop all steps from step_queue<br>        steps: List[TaskStep] = []<br>        while len(self.state.get_step_queue(task_id)) > 0:<br>            steps.append(self.state.get_step_queue(task_id).popleft())<br>        # take every item in the queue, and run it<br>        tasks = []<br>        for step in steps:<br>            tasks.append(self._arun_step(task_id, step=step, mode=mode, **kwargs))<br>        return await asyncio.gather(*tasks)<br>    def _run_step(<br>        self,<br>        task_id: str,<br>        step: Optional[TaskStep] = None,<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Execute step.\"\"\"<br>        task = self.state.get_task(task_id)<br>        task_queue = self.state.get_step_queue(task_id)<br>        step = step or task_queue.popleft()<br>        if not step.is_ready:<br>            raise ValueError(f\"Step {step.step_id} is not ready\")<br>        if mode == ChatResponseMode.WAIT:<br>            cur_step_output: TaskStepOutput = self.agent_worker.run_step(<br>                step, task, **kwargs<br>            )<br>        elif mode == ChatResponseMode.STREAM:<br>            cur_step_output = self.agent_worker.stream_step(step, task, **kwargs)<br>        else:<br>            raise ValueError(f\"Invalid mode: {mode}\")<br>        for next_step in cur_step_output.next_steps:<br>            if next_step.is_ready:<br>                task_queue.append(next_step)<br>        # add cur_step_output to completed steps<br>        completed_steps = self.state.get_completed_steps(task_id)<br>        completed_steps.append(cur_step_output)<br>        return cur_step_output<br>    async def _arun_step(<br>        self,<br>        task_id: str,<br>        step: Optional[TaskStep] = None,<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Execute step.\"\"\"<br>        task = self.state.get_task(task_id)<br>        task_queue = self.state.get_step_queue(task_id)<br>        step = step or task_queue.popleft()<br>        if not step.is_ready:<br>            raise ValueError(f\"Step {step.step_id} is not ready\")<br>        if mode == ChatResponseMode.WAIT:<br>            cur_step_output = await self.agent_worker.arun_step(step, task, **kwargs)<br>        elif mode == ChatResponseMode.STREAM:<br>            cur_step_output = await self.agent_worker.astream_step(step, task, **kwargs)<br>        else:<br>            raise ValueError(f\"Invalid mode: {mode}\")<br>        for next_step in cur_step_output.next_steps:<br>            if next_step.is_ready:<br>                task_queue.append(next_step)<br>        # add cur_step_output to completed steps<br>        completed_steps = self.state.get_completed_steps(task_id)<br>        completed_steps.append(cur_step_output)<br>        return cur_step_output<br>    def run_step(<br>        self,<br>        task_id: str,<br>        input: Optional[str] = None,<br>        step: Optional[TaskStep] = None,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        return self._run_step(task_id, step, mode=ChatResponseMode.WAIT, **kwargs)<br>    async def arun_step(<br>        self,<br>        task_id: str,<br>        input: Optional[str] = None,<br>        step: Optional[TaskStep] = None,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        return await self._arun_step(<br>            task_id, step, mode=ChatResponseMode.WAIT, **kwargs<br>        )<br>    def stream_step(<br>        self,<br>        task_id: str,<br>        input: Optional[str] = None,<br>        step: Optional[TaskStep] = None,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        return self._run_step(task_id, step, mode=ChatResponseMode.STREAM, **kwargs)<br>    async def astream_step(<br>        self,<br>        task_id: str,<br>        input: Optional[str] = None,<br>        step: Optional[TaskStep] = None,<br>        **kwargs: Any,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async stream).\"\"\"<br>        return await self._arun_step(<br>            task_id, step, mode=ChatResponseMode.STREAM, **kwargs<br>        )<br>    def finalize_response(<br>        self,<br>        task_id: str,<br>        step_output: Optional[TaskStepOutput] = None,<br>    ) -> AGENT_CHAT_RESPONSE_TYPE:<br>        \"\"\"Finalize response.\"\"\"<br>        if step_output is None:<br>            step_output = self.state.get_completed_steps(task_id)[-1]<br>        if not step_output.is_last:<br>            raise ValueError(<br>                \"finalize_response can only be called on the last step output\"<br>            )<br>        if not isinstance(<br>            step_output.output,<br>            (AgentChatResponse, StreamingAgentChatResponse),<br>        ):<br>            raise ValueError(<br>                \"When `is_last` is True, cur_step_output.output must be \"<br>                f\"AGENT_CHAT_RESPONSE_TYPE: {step_output.output}\"<br>            )<br>        # finalize task<br>        self.agent_worker.finalize_task(self.state.get_task(task_id))<br>        if self.delete_task_on_finish:<br>            self.delete_task(task_id)<br>        return cast(AGENT_CHAT_RESPONSE_TYPE, step_output.output)<br>    def _chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Union[str, dict] = \"auto\",<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>    ) -> AGENT_CHAT_RESPONSE_TYPE:<br>        \"\"\"Chat with step executor.\"\"\"<br>        if chat_history is not None:<br>            self.memory.set(chat_history)<br>        task = self.create_task(message)<br>        result_output = None<br>        while True:<br>            # pass step queue in as argument, assume step executor is stateless<br>            cur_step_outputs = self.run_steps_in_queue(task.task_id, mode=mode)<br>            # check if a step output is_last<br>            is_last = any(<br>                cur_step_output.is_last for cur_step_output in cur_step_outputs<br>            )<br>            if is_last:<br>                if len(cur_step_outputs) > 1:<br>                    raise ValueError(<br>                        \"More than one step output returned in final step.\"<br>                    )<br>                cur_step_output = cur_step_outputs[0]<br>                result_output = cur_step_output<br>                break<br>        return self.finalize_response(<br>            task.task_id,<br>            result_output,<br>        )<br>    async def _achat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Union[str, dict] = \"auto\",<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>    ) -> AGENT_CHAT_RESPONSE_TYPE:<br>        \"\"\"Chat with step executor.\"\"\"<br>        if chat_history is not None:<br>            self.memory.set(chat_history)<br>        task = self.create_task(message)<br>        result_output = None<br>        while True:<br>            # pass step queue in as argument, assume step executor is stateless<br>            cur_step_outputs = await self.arun_steps_in_queue(task.task_id, mode=mode)<br>            # check if a step output is_last<br>            is_last = any(<br>                cur_step_output.is_last for cur_step_output in cur_step_outputs<br>            )<br>            if is_last:<br>                if len(cur_step_outputs) > 1:<br>                    raise ValueError(<br>                        \"More than one step output returned in final step.\"<br>                    )<br>                cur_step_output = cur_step_outputs[0]<br>                result_output = cur_step_output<br>                break<br>        return self.finalize_response(<br>            task.task_id,<br>            result_output,<br>        )<br>    @trace_method(\"chat\")<br>    def chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Union[str, dict] = \"auto\",<br>    ) -> AgentChatResponse:<br>        with self.callback_manager.event(<br>            CBEventType.AGENT_STEP,<br>            payload={EventPayload.MESSAGES: [message]},<br>        ) as e:<br>            chat_response = self._chat(<br>                message, chat_history, tool_choice, mode=ChatResponseMode.WAIT<br>            )<br>            assert isinstance(chat_response, AgentChatResponse)<br>            e.on_end(payload={EventPayload.RESPONSE: chat_response})<br>        return chat_response<br>    @trace_method(\"chat\")<br>    async def achat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Union[str, dict] = \"auto\",<br>    ) -> AgentChatResponse:<br>        with self.callback_manager.event(<br>            CBEventType.AGENT_STEP,<br>            payload={EventPayload.MESSAGES: [message]},<br>        ) as e:<br>            chat_response = await self._achat(<br>                message, chat_history, tool_choice, mode=ChatResponseMode.WAIT<br>            )<br>            assert isinstance(chat_response, AgentChatResponse)<br>            e.on_end(payload={EventPayload.RESPONSE: chat_response})<br>        return chat_response<br>    @trace_method(\"chat\")<br>    def stream_chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Union[str, dict] = \"auto\",<br>    ) -> StreamingAgentChatResponse:<br>        with self.callback_manager.event(<br>            CBEventType.AGENT_STEP,<br>            payload={EventPayload.MESSAGES: [message]},<br>        ) as e:<br>            chat_response = self._chat(<br>                message, chat_history, tool_choice, mode=ChatResponseMode.STREAM<br>            )<br>            e.on_end(payload={EventPayload.RESPONSE: chat_response})<br>        return chat_response  # type: ignore<br>    @trace_method(\"chat\")<br>    async def astream_chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Union[str, dict] = \"auto\",<br>    ) -> StreamingAgentChatResponse:<br>        with self.callback_manager.event(<br>            CBEventType.AGENT_STEP,<br>            payload={EventPayload.MESSAGES: [message]},<br>        ) as e:<br>            chat_response = await self._achat(<br>                message, chat_history, tool_choice, mode=ChatResponseMode.STREAM<br>            )<br>            e.on_end(payload={EventPayload.RESPONSE: chat_response})<br>        return chat_response  # type: ignore<br>    def undo_step(self, task_id: str) -> None:<br>        \"\"\"Undo previous step.\"\"\"<br>        raise NotImplementedError(\"undo_step not implemented\")<br>``` |\n\n#### create\\_task [\\#](\\#llama_index.core.agent.ParallelAgentRunner.create_task \"Permanent link\")\n\n```\ncreate_task(input: str, **kwargs: Any) -> Task\n\n```\n\nCreate task.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/parallel.py`\n\n|     |     |\n| --- | --- |\n| ```<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>``` | ```<br>def create_task(self, input: str, **kwargs: Any) -> Task:<br>    \"\"\"Create task.\"\"\"<br>    task = Task(<br>        input=input,<br>        memory=self.memory,<br>        extra_state=self.init_task_state_kwargs,<br>        **kwargs,<br>    )<br>    # # put input into memory<br>    # self.memory.put(ChatMessage(content=input, role=MessageRole.USER))<br>    # add it to state<br>    # get initial step from task, and put it in the step queue<br>    initial_step = self.agent_worker.initialize_step(task)<br>    task_state = DAGTaskState(<br>        task=task,<br>        root_step=initial_step,<br>        step_queue=deque([initial_step]),<br>    )<br>    self.state.task_dict[task.task_id] = task_state<br>    return task<br>``` |\n\n#### delete\\_task [\\#](\\#llama_index.core.agent.ParallelAgentRunner.delete_task \"Permanent link\")\n\n```\ndelete_task(task_id: str) -> None\n\n```\n\nDelete task.\n\nNOTE: this will not delete any previous executions from memory.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/parallel.py`\n\n|     |     |\n| --- | --- |\n| ```<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>``` | ```<br>def delete_task(<br>    self,<br>    task_id: str,<br>) -> None:<br>    \"\"\"Delete task.<br>    NOTE: this will not delete any previous executions from memory.<br>    \"\"\"<br>    self.state.task_dict.pop(task_id)<br>``` |\n\n#### get\\_completed\\_tasks [\\#](\\#llama_index.core.agent.ParallelAgentRunner.get_completed_tasks \"Permanent link\")\n\n```\nget_completed_tasks(**kwargs: Any) -> List[Task]\n\n```\n\nGet completed tasks.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/parallel.py`\n\n|     |     |\n| --- | --- |\n| ```<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>``` | ```<br>def get_completed_tasks(self, **kwargs: Any) -> List[Task]:<br>    \"\"\"Get completed tasks.\"\"\"<br>    task_states = list(self.state.task_dict.values())<br>    return [<br>        task_state.task<br>        for task_state in task_states<br>        if len(task_state.completed_steps) > 0<br>        and task_state.completed_steps[-1].is_last<br>    ]<br>``` |\n\n#### get\\_task\\_output [\\#](\\#llama_index.core.agent.ParallelAgentRunner.get_task_output \"Permanent link\")\n\n```\nget_task_output(task_id: str, **kwargs: Any) -> TaskStepOutput\n\n```\n\nGet task output.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/parallel.py`\n\n|     |     |\n| --- | --- |\n| ```<br>150<br>151<br>152<br>153<br>154<br>155<br>``` | ```<br>def get_task_output(self, task_id: str, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Get task output.\"\"\"<br>    task_state = self.state.task_dict[task_id]<br>    if len(task_state.completed_steps) == 0:<br>        raise ValueError(f\"No completed steps for task_id: {task_id}\")<br>    return task_state.completed_steps[-1]<br>``` |\n\n#### list\\_tasks [\\#](\\#llama_index.core.agent.ParallelAgentRunner.list_tasks \"Permanent link\")\n\n```\nlist_tasks(**kwargs: Any) -> List[Task]\n\n```\n\nList tasks.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/parallel.py`\n\n|     |     |\n| --- | --- |\n| ```<br>157<br>158<br>159<br>160<br>``` | ```<br>def list_tasks(self, **kwargs: Any) -> List[Task]:<br>    \"\"\"List tasks.\"\"\"<br>    task_states = list(self.state.task_dict.values())<br>    return [task_state.task for task_state in task_states]<br>``` |\n\n#### get\\_task [\\#](\\#llama_index.core.agent.ParallelAgentRunner.get_task \"Permanent link\")\n\n```\nget_task(task_id: str, **kwargs: Any) -> Task\n\n```\n\nGet task.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/parallel.py`\n\n|     |     |\n| --- | --- |\n| ```<br>162<br>163<br>164<br>``` | ```<br>def get_task(self, task_id: str, **kwargs: Any) -> Task:<br>    \"\"\"Get task.\"\"\"<br>    return self.state.get_task(task_id)<br>``` |\n\n#### get\\_upcoming\\_steps [\\#](\\#llama_index.core.agent.ParallelAgentRunner.get_upcoming_steps \"Permanent link\")\n\n```\nget_upcoming_steps(task_id: str, **kwargs: Any) -> List[TaskStep]\n\n```\n\nGet upcoming steps.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/parallel.py`\n\n|     |     |\n| --- | --- |\n| ```<br>166<br>167<br>168<br>``` | ```<br>def get_upcoming_steps(self, task_id: str, **kwargs: Any) -> List[TaskStep]:<br>    \"\"\"Get upcoming steps.\"\"\"<br>    return list(self.state.get_step_queue(task_id))<br>``` |\n\n#### get\\_completed\\_steps [\\#](\\#llama_index.core.agent.ParallelAgentRunner.get_completed_steps \"Permanent link\")\n\n```\nget_completed_steps(task_id: str, **kwargs: Any) -> List[TaskStepOutput]\n\n```\n\nGet completed steps.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/parallel.py`\n\n|     |     |\n| --- | --- |\n| ```<br>170<br>171<br>172<br>``` | ```<br>def get_completed_steps(self, task_id: str, **kwargs: Any) -> List[TaskStepOutput]:<br>    \"\"\"Get completed steps.\"\"\"<br>    return self.state.get_completed_steps(task_id)<br>``` |\n\n#### run\\_steps\\_in\\_queue [\\#](\\#llama_index.core.agent.ParallelAgentRunner.run_steps_in_queue \"Permanent link\")\n\n```\nrun_steps_in_queue(task_id: str, mode: ChatResponseMode = ChatResponseMode.WAIT, **kwargs: Any) -> List[TaskStepOutput]\n\n```\n\nExecute steps in queue.\n\nRun all steps in queue, clearing it out.\n\nAssume that all steps can be run in parallel.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/parallel.py`\n\n|     |     |\n| --- | --- |\n| ```<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>``` | ```<br>def run_steps_in_queue(<br>    self,<br>    task_id: str,<br>    mode: ChatResponseMode = ChatResponseMode.WAIT,<br>    **kwargs: Any,<br>) -> List[TaskStepOutput]:<br>    \"\"\"Execute steps in queue.<br>    Run all steps in queue, clearing it out.<br>    Assume that all steps can be run in parallel.<br>    \"\"\"<br>    return asyncio_run(self.arun_steps_in_queue(task_id, mode=mode, **kwargs))<br>``` |\n\n#### arun\\_steps\\_in\\_queue`async`[\\#](\\#llama_index.core.agent.ParallelAgentRunner.arun_steps_in_queue \"Permanent link\")\n\n```\narun_steps_in_queue(task_id: str, mode: ChatResponseMode = ChatResponseMode.WAIT, **kwargs: Any) -> List[TaskStepOutput]\n\n```\n\nExecute all steps in queue.\n\nAll steps in queue are assumed to be ready.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/parallel.py`\n\n|     |     |\n| --- | --- |\n| ```<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>``` | ```<br>async def arun_steps_in_queue(<br>    self,<br>    task_id: str,<br>    mode: ChatResponseMode = ChatResponseMode.WAIT,<br>    **kwargs: Any,<br>) -> List[TaskStepOutput]:<br>    \"\"\"Execute all steps in queue.<br>    All steps in queue are assumed to be ready.<br>    \"\"\"<br>    # first pop all steps from step_queue<br>    steps: List[TaskStep] = []<br>    while len(self.state.get_step_queue(task_id)) > 0:<br>        steps.append(self.state.get_step_queue(task_id).popleft())<br>    # take every item in the queue, and run it<br>    tasks = []<br>    for step in steps:<br>        tasks.append(self._arun_step(task_id, step=step, mode=mode, **kwargs))<br>    return await asyncio.gather(*tasks)<br>``` |\n\n#### run\\_step [\\#](\\#llama_index.core.agent.ParallelAgentRunner.run_step \"Permanent link\")\n\n```\nrun_step(task_id: str, input: Optional[str] = None, step: Optional[TaskStep] = None, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/parallel.py`\n\n|     |     |\n| --- | --- |\n| ```<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>``` | ```<br>def run_step(<br>    self,<br>    task_id: str,<br>    input: Optional[str] = None,<br>    step: Optional[TaskStep] = None,<br>    **kwargs: Any,<br>) -> TaskStepOutput:<br>    \"\"\"Run step.\"\"\"<br>    return self._run_step(task_id, step, mode=ChatResponseMode.WAIT, **kwargs)<br>``` |\n\n#### arun\\_step`async`[\\#](\\#llama_index.core.agent.ParallelAgentRunner.arun_step \"Permanent link\")\n\n```\narun_step(task_id: str, input: Optional[str] = None, step: Optional[TaskStep] = None, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async).\n\nSource code in `llama-index-core/llama_index/core/agent/runner/parallel.py`\n\n|     |     |\n| --- | --- |\n| ```<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>``` | ```<br>async def arun_step(<br>    self,<br>    task_id: str,<br>    input: Optional[str] = None,<br>    step: Optional[TaskStep] = None,<br>    **kwargs: Any,<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async).\"\"\"<br>    return await self._arun_step(<br>        task_id, step, mode=ChatResponseMode.WAIT, **kwargs<br>    )<br>``` |\n\n#### stream\\_step [\\#](\\#llama_index.core.agent.ParallelAgentRunner.stream_step \"Permanent link\")\n\n```\nstream_step(task_id: str, input: Optional[str] = None, step: Optional[TaskStep] = None, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (stream).\n\nSource code in `llama-index-core/llama_index/core/agent/runner/parallel.py`\n\n|     |     |\n| --- | --- |\n| ```<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>``` | ```<br>def stream_step(<br>    self,<br>    task_id: str,<br>    input: Optional[str] = None,<br>    step: Optional[TaskStep] = None,<br>    **kwargs: Any,<br>) -> TaskStepOutput:<br>    \"\"\"Run step (stream).\"\"\"<br>    return self._run_step(task_id, step, mode=ChatResponseMode.STREAM, **kwargs)<br>``` |\n\n#### astream\\_step`async`[\\#](\\#llama_index.core.agent.ParallelAgentRunner.astream_step \"Permanent link\")\n\n```\nastream_step(task_id: str, input: Optional[str] = None, step: Optional[TaskStep] = None, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async stream).\n\nSource code in `llama-index-core/llama_index/core/agent/runner/parallel.py`\n\n|     |     |\n| --- | --- |\n| ```<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>``` | ```<br>async def astream_step(<br>    self,<br>    task_id: str,<br>    input: Optional[str] = None,<br>    step: Optional[TaskStep] = None,<br>    **kwargs: Any,<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async stream).\"\"\"<br>    return await self._arun_step(<br>        task_id, step, mode=ChatResponseMode.STREAM, **kwargs<br>    )<br>``` |\n\n#### finalize\\_response [\\#](\\#llama_index.core.agent.ParallelAgentRunner.finalize_response \"Permanent link\")\n\n```\nfinalize_response(task_id: str, step_output: Optional[TaskStepOutput] = None) -> AGENT_CHAT_RESPONSE_TYPE\n\n```\n\nFinalize response.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/parallel.py`\n\n|     |     |\n| --- | --- |\n| ```<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>``` | ```<br>def finalize_response(<br>    self,<br>    task_id: str,<br>    step_output: Optional[TaskStepOutput] = None,<br>) -> AGENT_CHAT_RESPONSE_TYPE:<br>    \"\"\"Finalize response.\"\"\"<br>    if step_output is None:<br>        step_output = self.state.get_completed_steps(task_id)[-1]<br>    if not step_output.is_last:<br>        raise ValueError(<br>            \"finalize_response can only be called on the last step output\"<br>        )<br>    if not isinstance(<br>        step_output.output,<br>        (AgentChatResponse, StreamingAgentChatResponse),<br>    ):<br>        raise ValueError(<br>            \"When `is_last` is True, cur_step_output.output must be \"<br>            f\"AGENT_CHAT_RESPONSE_TYPE: {step_output.output}\"<br>        )<br>    # finalize task<br>    self.agent_worker.finalize_task(self.state.get_task(task_id))<br>    if self.delete_task_on_finish:<br>        self.delete_task(task_id)<br>    return cast(AGENT_CHAT_RESPONSE_TYPE, step_output.output)<br>``` |\n\n#### undo\\_step [\\#](\\#llama_index.core.agent.ParallelAgentRunner.undo_step \"Permanent link\")\n\n```\nundo_step(task_id: str) -> None\n\n```\n\nUndo previous step.\n\nSource code in `llama-index-core/llama_index/core/agent/runner/parallel.py`\n\n|     |     |\n| --- | --- |\n| ```<br>494<br>495<br>496<br>``` | ```<br>def undo_step(self, task_id: str) -> None:<br>    \"\"\"Undo previous step.\"\"\"<br>    raise NotImplementedError(\"undo_step not implemented\")<br>``` |\n\n## Workers [\\#](\\#workers \"Permanent link\")\n\n### CustomSimpleAgentWorker [\\#](\\#llama_index.core.agent.CustomSimpleAgentWorker \"Permanent link\")\n\nBases: `BaseModel`, `BaseAgentWorker`\n\nCustom simple agent worker.\n\nThis is \"simple\" in the sense that some of the scaffolding is setup already.\nAssumptions:\n\\- assumes that the agent has tools, llm, callback manager, and tool retriever\n\\- has a `from_tools` convenience function\n\\- assumes that the agent is sequential, and doesn't take in any additional\nintermediate inputs.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `tools` | `Sequence[BaseTool]` | Tools to use for reasoning | _required_ |\n| `llm` | `LLM` | LLM to use | _required_ |\n| `callback_manager` | `CallbackManager` | Callback manager | `None` |\n| `tool_retriever` | `Optional[ObjectRetriever[BaseTool]]` | Tool retriever | `None` |\n| `verbose` | `bool` | Whether to print out reasoning steps | `False` |\n\nSource code in `llama-index-core/llama_index/core/agent/custom/simple.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>``` | ```<br>class CustomSimpleAgentWorker(BaseModel, BaseAgentWorker):<br>    \"\"\"Custom simple agent worker.<br>    This is \"simple\" in the sense that some of the scaffolding is setup already.<br>    Assumptions:<br>    - assumes that the agent has tools, llm, callback manager, and tool retriever<br>    - has a `from_tools` convenience function<br>    - assumes that the agent is sequential, and doesn't take in any additional<br>    intermediate inputs.<br>    Args:<br>        tools (Sequence[BaseTool]): Tools to use for reasoning<br>        llm (LLM): LLM to use<br>        callback_manager (CallbackManager): Callback manager<br>        tool_retriever (Optional[ObjectRetriever[BaseTool]]): Tool retriever<br>        verbose (bool): Whether to print out reasoning steps<br>    \"\"\"<br>    model_config = ConfigDict(arbitrary_types_allowed=True)<br>    tools: Sequence[BaseTool] = Field(..., description=\"Tools to use for reasoning\")<br>    llm: LLM = Field(..., description=\"LLM to use\")<br>    callback_manager: CallbackManager = Field(<br>        default_factory=lambda: CallbackManager([]), exclude=True<br>    )<br>    tool_retriever: Optional[ObjectRetriever[BaseTool]] = Field(<br>        default=None, description=\"Tool retriever\"<br>    )<br>    verbose: bool = Field(False, description=\"Whether to print out reasoning steps\")<br>    _get_tools: Callable[[str], Sequence[BaseTool]] = PrivateAttr()<br>    def __init__(<br>        self,<br>        tools: Sequence[BaseTool],<br>        llm: LLM,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        **kwargs: Any,<br>    ) -> None:<br>        callback_manager = callback_manager or CallbackManager([])<br>        super().__init__(<br>            tools=tools,<br>            llm=llm,<br>            callback_manager=callback_manager or CallbackManager([]),<br>            tool_retriever=tool_retriever,<br>            verbose=verbose,<br>            **kwargs,<br>        )<br>        if len(tools) > 0 and tool_retriever is not None:<br>            raise ValueError(\"Cannot specify both tools and tool_retriever\")<br>        elif len(tools) > 0:<br>            self._get_tools = lambda _: tools<br>        elif tool_retriever is not None:<br>            tool_retriever_c = cast(ObjectRetriever[BaseTool], tool_retriever)<br>            self._get_tools = lambda message: tool_retriever_c.retrieve(message)<br>        else:<br>            self._get_tools = lambda _: []<br>    @classmethod<br>    def from_tools(<br>        cls,<br>        tools: Optional[Sequence[BaseTool]] = None,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        llm: Optional[LLM] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        **kwargs: Any,<br>    ) -> \"CustomSimpleAgentWorker\":<br>        \"\"\"Convenience constructor method from set of of BaseTools (Optional).\"\"\"<br>        llm = llm or Settings.llm<br>        if callback_manager is not None:<br>            llm.callback_manager = callback_manager<br>        return cls(<br>            tools=tools or [],<br>            tool_retriever=tool_retriever,<br>            llm=llm,<br>            callback_manager=callback_manager or CallbackManager([]),<br>            verbose=verbose,<br>            **kwargs,<br>        )<br>    @abstractmethod<br>    def _initialize_state(self, task: Task, **kwargs: Any) -> Dict[str, Any]:<br>        \"\"\"Initialize state.\"\"\"<br>    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>        \"\"\"Initialize step from task.\"\"\"<br>        sources: List[ToolOutput] = []<br>        # temporary memory for new messages<br>        new_memory = ChatMemoryBuffer.from_defaults()<br>        # initialize initial state<br>        initial_state = {<br>            \"sources\": sources,<br>            \"memory\": new_memory,<br>        }<br>        step_state = self._initialize_state(task, **kwargs)<br>        # if intersecting keys, error<br>        if set(step_state.keys()).intersection(set(initial_state.keys())):<br>            raise ValueError(<br>                f\"Step state keys {step_state.keys()} and initial state keys {initial_state.keys()} intersect.\"<br>                f\"*NOTE*: initial state keys {initial_state.keys()} are reserved.\"<br>            )<br>        step_state.update(initial_state)<br>        return TaskStep(<br>            task_id=task.task_id,<br>            step_id=str(uuid.uuid4()),<br>            input=task.input,<br>            step_state=step_state,<br>        )<br>    def get_tools(self, input: str) -> List[AsyncBaseTool]:<br>        \"\"\"Get tools.\"\"\"<br>        return [adapt_to_async_tool(t) for t in self._get_tools(input)]<br>    def _get_task_step_response(<br>        self, agent_response: AGENT_CHAT_RESPONSE_TYPE, step: TaskStep, is_done: bool<br>    ) -> TaskStepOutput:<br>        \"\"\"Get task step response.\"\"\"<br>        if is_done:<br>            new_steps = []<br>        else:<br>            new_steps = [<br>                step.get_next_step(<br>                    step_id=str(uuid.uuid4()),<br>                    # NOTE: input is unused<br>                    input=None,<br>                )<br>            ]<br>        return TaskStepOutput(<br>            output=agent_response,<br>            task_step=step,<br>            is_last=is_done,<br>            next_steps=new_steps,<br>        )<br>    @abstractmethod<br>    def _run_step(<br>        self, state: Dict[str, Any], task: Task, input: Optional[str] = None<br>    ) -> Tuple[AgentChatResponse, bool]:<br>        \"\"\"Run step.<br>        Returns:<br>            Tuple of (agent_response, is_done)<br>        \"\"\"<br>    async def _arun_step(<br>        self, state: Dict[str, Any], task: Task, input: Optional[str] = None<br>    ) -> Tuple[AgentChatResponse, bool]:<br>        \"\"\"Run step (async).<br>        Can override this method if you want to run the step asynchronously.<br>        Returns:<br>            Tuple of (agent_response, is_done)<br>        \"\"\"<br>        raise NotImplementedError(<br>            \"This agent does not support async.\" \"Please implement _arun_step.\"<br>        )<br>    @trace_method(\"run_step\")<br>    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        agent_response, is_done = self._run_step(<br>            step.step_state, task, input=step.input<br>        )<br>        response = self._get_task_step_response(agent_response, step, is_done)<br>        # sync step state with task state<br>        task.extra_state.update(step.step_state)<br>        return response<br>    @trace_method(\"run_step\")<br>    async def arun_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        agent_response, is_done = await self._arun_step(<br>            step.step_state, task, input=step.input<br>        )<br>        response = self._get_task_step_response(agent_response, step, is_done)<br>        task.extra_state.update(step.step_state)<br>        return response<br>    @trace_method(\"run_step\")<br>    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        raise NotImplementedError(\"This agent does not support streaming.\")<br>    @trace_method(\"run_step\")<br>    async def astream_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async stream).\"\"\"<br>        raise NotImplementedError(\"This agent does not support streaming.\")<br>    @abstractmethod<br>    def _finalize_task(self, state: Dict[str, Any], **kwargs: Any) -> None:<br>        \"\"\"Finalize task, after all the steps are completed.<br>        State is all the step states.<br>        \"\"\"<br>    def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>        \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>        # add new messages to memory<br>        task.memory.set(task.memory.get() + task.extra_state[\"memory\"].get_all())<br>        # reset new memory<br>        task.extra_state[\"memory\"].reset()<br>        self._finalize_task(task.extra_state, **kwargs)<br>    def set_callback_manager(self, callback_manager: CallbackManager) -> None:<br>        \"\"\"Set callback manager.\"\"\"<br>        # TODO: make this abstractmethod (right now will break some agent impls)<br>        self.callback_manager = callback_manager<br>``` |\n\n#### from\\_tools`classmethod`[\\#](\\#llama_index.core.agent.CustomSimpleAgentWorker.from_tools \"Permanent link\")\n\n```\nfrom_tools(tools: Optional[Sequence[BaseTool]] = None, tool_retriever: Optional[ObjectRetriever[BaseTool]] = None, llm: Optional[LLM] = None, callback_manager: Optional[CallbackManager] = None, verbose: bool = False, **kwargs: Any) -> CustomSimpleAgentWorker\n\n```\n\nConvenience constructor method from set of of BaseTools (Optional).\n\nSource code in `llama-index-core/llama_index/core/agent/custom/simple.py`\n\n|     |     |\n| --- | --- |\n| ```<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>``` | ```<br>@classmethod<br>def from_tools(<br>    cls,<br>    tools: Optional[Sequence[BaseTool]] = None,<br>    tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>    llm: Optional[LLM] = None,<br>    callback_manager: Optional[CallbackManager] = None,<br>    verbose: bool = False,<br>    **kwargs: Any,<br>) -> \"CustomSimpleAgentWorker\":<br>    \"\"\"Convenience constructor method from set of of BaseTools (Optional).\"\"\"<br>    llm = llm or Settings.llm<br>    if callback_manager is not None:<br>        llm.callback_manager = callback_manager<br>    return cls(<br>        tools=tools or [],<br>        tool_retriever=tool_retriever,<br>        llm=llm,<br>        callback_manager=callback_manager or CallbackManager([]),<br>        verbose=verbose,<br>        **kwargs,<br>    )<br>``` |\n\n#### initialize\\_step [\\#](\\#llama_index.core.agent.CustomSimpleAgentWorker.initialize_step \"Permanent link\")\n\n```\ninitialize_step(task: Task, **kwargs: Any) -> TaskStep\n\n```\n\nInitialize step from task.\n\nSource code in `llama-index-core/llama_index/core/agent/custom/simple.py`\n\n|     |     |\n| --- | --- |\n| ```<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>``` | ```<br>def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>    \"\"\"Initialize step from task.\"\"\"<br>    sources: List[ToolOutput] = []<br>    # temporary memory for new messages<br>    new_memory = ChatMemoryBuffer.from_defaults()<br>    # initialize initial state<br>    initial_state = {<br>        \"sources\": sources,<br>        \"memory\": new_memory,<br>    }<br>    step_state = self._initialize_state(task, **kwargs)<br>    # if intersecting keys, error<br>    if set(step_state.keys()).intersection(set(initial_state.keys())):<br>        raise ValueError(<br>            f\"Step state keys {step_state.keys()} and initial state keys {initial_state.keys()} intersect.\"<br>            f\"*NOTE*: initial state keys {initial_state.keys()} are reserved.\"<br>        )<br>    step_state.update(initial_state)<br>    return TaskStep(<br>        task_id=task.task_id,<br>        step_id=str(uuid.uuid4()),<br>        input=task.input,<br>        step_state=step_state,<br>    )<br>``` |\n\n#### get\\_tools [\\#](\\#llama_index.core.agent.CustomSimpleAgentWorker.get_tools \"Permanent link\")\n\n```\nget_tools(input: str) -> List[AsyncBaseTool]\n\n```\n\nGet tools.\n\nSource code in `llama-index-core/llama_index/core/agent/custom/simple.py`\n\n|     |     |\n| --- | --- |\n| ```<br>155<br>156<br>157<br>``` | ```<br>def get_tools(self, input: str) -> List[AsyncBaseTool]:<br>    \"\"\"Get tools.\"\"\"<br>    return [adapt_to_async_tool(t) for t in self._get_tools(input)]<br>``` |\n\n#### run\\_step [\\#](\\#llama_index.core.agent.CustomSimpleAgentWorker.run_step \"Permanent link\")\n\n```\nrun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step.\n\nSource code in `llama-index-core/llama_index/core/agent/custom/simple.py`\n\n|     |     |\n| --- | --- |\n| ```<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>``` | ```<br>@trace_method(\"run_step\")<br>def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step.\"\"\"<br>    agent_response, is_done = self._run_step(<br>        step.step_state, task, input=step.input<br>    )<br>    response = self._get_task_step_response(agent_response, step, is_done)<br>    # sync step state with task state<br>    task.extra_state.update(step.step_state)<br>    return response<br>``` |\n\n#### arun\\_step`async`[\\#](\\#llama_index.core.agent.CustomSimpleAgentWorker.arun_step \"Permanent link\")\n\n```\narun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async).\n\nSource code in `llama-index-core/llama_index/core/agent/custom/simple.py`\n\n|     |     |\n| --- | --- |\n| ```<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>``` | ```<br>@trace_method(\"run_step\")<br>async def arun_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async).\"\"\"<br>    agent_response, is_done = await self._arun_step(<br>        step.step_state, task, input=step.input<br>    )<br>    response = self._get_task_step_response(agent_response, step, is_done)<br>    task.extra_state.update(step.step_state)<br>    return response<br>``` |\n\n#### stream\\_step [\\#](\\#llama_index.core.agent.CustomSimpleAgentWorker.stream_step \"Permanent link\")\n\n```\nstream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (stream).\n\nSource code in `llama-index-core/llama_index/core/agent/custom/simple.py`\n\n|     |     |\n| --- | --- |\n| ```<br>230<br>231<br>232<br>233<br>``` | ```<br>@trace_method(\"run_step\")<br>def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step (stream).\"\"\"<br>    raise NotImplementedError(\"This agent does not support streaming.\")<br>``` |\n\n#### astream\\_step`async`[\\#](\\#llama_index.core.agent.CustomSimpleAgentWorker.astream_step \"Permanent link\")\n\n```\nastream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async stream).\n\nSource code in `llama-index-core/llama_index/core/agent/custom/simple.py`\n\n|     |     |\n| --- | --- |\n| ```<br>235<br>236<br>237<br>238<br>239<br>240<br>``` | ```<br>@trace_method(\"run_step\")<br>async def astream_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async stream).\"\"\"<br>    raise NotImplementedError(\"This agent does not support streaming.\")<br>``` |\n\n#### finalize\\_task [\\#](\\#llama_index.core.agent.CustomSimpleAgentWorker.finalize_task \"Permanent link\")\n\n```\nfinalize_task(task: Task, **kwargs: Any) -> None\n\n```\n\nFinalize task, after all the steps are completed.\n\nSource code in `llama-index-core/llama_index/core/agent/custom/simple.py`\n\n|     |     |\n| --- | --- |\n| ```<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>``` | ```<br>def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>    \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>    # add new messages to memory<br>    task.memory.set(task.memory.get() + task.extra_state[\"memory\"].get_all())<br>    # reset new memory<br>    task.extra_state[\"memory\"].reset()<br>    self._finalize_task(task.extra_state, **kwargs)<br>``` |\n\n#### set\\_callback\\_manager [\\#](\\#llama_index.core.agent.CustomSimpleAgentWorker.set_callback_manager \"Permanent link\")\n\n```\nset_callback_manager(callback_manager: CallbackManager) -> None\n\n```\n\nSet callback manager.\n\nSource code in `llama-index-core/llama_index/core/agent/custom/simple.py`\n\n|     |     |\n| --- | --- |\n| ```<br>258<br>259<br>260<br>261<br>``` | ```<br>def set_callback_manager(self, callback_manager: CallbackManager) -> None:<br>    \"\"\"Set callback manager.\"\"\"<br>    # TODO: make this abstractmethod (right now will break some agent impls)<br>    self.callback_manager = callback_manager<br>``` |\n\n### MultimodalReActAgentWorker [\\#](\\#llama_index.core.agent.MultimodalReActAgentWorker \"Permanent link\")\n\nBases: `BaseAgentWorker`\n\nMultimodal ReAct Agent worker.\n\n**NOTE**: This is a BETA feature.\n\nSource code in `llama-index-core/llama_index/core/agent/react_multimodal/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>472<br>473<br>474<br>475<br>476<br>477<br>478<br>479<br>480<br>481<br>482<br>483<br>484<br>485<br>486<br>487<br>488<br>489<br>490<br>491<br>492<br>493<br>494<br>495<br>496<br>497<br>498<br>499<br>500<br>501<br>502<br>503<br>504<br>505<br>506<br>507<br>508<br>509<br>510<br>511<br>512<br>513<br>514<br>515<br>516<br>517<br>518<br>519<br>520<br>521<br>522<br>523<br>524<br>``` | ```<br>class MultimodalReActAgentWorker(BaseAgentWorker):<br>    \"\"\"Multimodal ReAct Agent worker.<br>    **NOTE**: This is a BETA feature.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        tools: Sequence[BaseTool],<br>        multi_modal_llm: MultiModalLLM,<br>        max_iterations: int = 10,<br>        react_chat_formatter: Optional[ReActChatFormatter] = None,<br>        output_parser: Optional[ReActOutputParser] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>    ) -> None:<br>        self._multi_modal_llm = multi_modal_llm<br>        self.callback_manager = callback_manager or CallbackManager([])<br>        self._max_iterations = max_iterations<br>        self._react_chat_formatter = react_chat_formatter or ReActChatFormatter(<br>            system_header=REACT_MM_CHAT_SYSTEM_HEADER<br>        )<br>        self._output_parser = output_parser or ReActOutputParser()<br>        self._verbose = verbose<br>        try:<br>            from llama_index.multi_modal_llms.openai.utils import (<br>                generate_openai_multi_modal_chat_message,<br>            )  # pants: no-infer-dep<br>            self._add_user_step_to_reasoning = partial(<br>                add_user_step_to_reasoning,<br>                generate_chat_message_fn=generate_openai_multi_modal_chat_message,  # type: ignore<br>            )<br>        except ImportError:<br>            raise ImportError(<br>                \"`llama-index-multi-modal-llms-openai` package cannot be found. \"<br>                \"Please install it by using `pip install `llama-index-multi-modal-llms-openai`\"<br>            )<br>        if len(tools) > 0 and tool_retriever is not None:<br>            raise ValueError(\"Cannot specify both tools and tool_retriever\")<br>        elif len(tools) > 0:<br>            self._get_tools = lambda _: tools<br>        elif tool_retriever is not None:<br>            tool_retriever_c = cast(ObjectRetriever[BaseTool], tool_retriever)<br>            self._get_tools = lambda message: tool_retriever_c.retrieve(message)<br>        else:<br>            self._get_tools = lambda _: []<br>    @classmethod<br>    def from_tools(<br>        cls,<br>        tools: Optional[Sequence[BaseTool]] = None,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        multi_modal_llm: Optional[MultiModalLLM] = None,<br>        max_iterations: int = 10,<br>        react_chat_formatter: Optional[ReActChatFormatter] = None,<br>        output_parser: Optional[ReActOutputParser] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        **kwargs: Any,<br>    ) -> \"MultimodalReActAgentWorker\":<br>        \"\"\"Convenience constructor method from set of of BaseTools (Optional).<br>        NOTE: kwargs should have been exhausted by this point. In other words<br>        the various upstream components such as BaseSynthesizer (response synthesizer)<br>        or BaseRetriever should have picked up off their respective kwargs in their<br>        constructions.<br>        Returns:<br>            ReActAgent<br>        \"\"\"<br>        if multi_modal_llm is None:<br>            try:<br>                from llama_index.multi_modal_llms.openai import (<br>                    OpenAIMultiModal,<br>                )  # pants: no-infer-dep<br>                multi_modal_llm = multi_modal_llm or OpenAIMultiModal(<br>                    model=\"gpt-4-vision-preview\", max_new_tokens=1000<br>                )<br>            except ImportError:<br>                raise ImportError(<br>                    \"`llama-index-multi-modal-llms-openai` package cannot be found. \"<br>                    \"Please install it by using `pip install `llama-index-multi-modal-llms-openai`\"<br>                )<br>        return cls(<br>            tools=tools or [],<br>            tool_retriever=tool_retriever,<br>            multi_modal_llm=multi_modal_llm,<br>            max_iterations=max_iterations,<br>            react_chat_formatter=react_chat_formatter,<br>            output_parser=output_parser,<br>            callback_manager=callback_manager,<br>            verbose=verbose,<br>        )<br>    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>        \"\"\"Initialize step from task.\"\"\"<br>        sources: List[ToolOutput] = []<br>        current_reasoning: List[BaseReasoningStep] = []<br>        # temporary memory for new messages<br>        new_memory = ChatMemoryBuffer.from_defaults()<br>        # validation<br>        if \"image_docs\" not in task.extra_state:<br>            raise ValueError(\"Image docs not found in task extra state.\")<br>        # initialize task state<br>        task_state = {<br>            \"sources\": sources,<br>            \"current_reasoning\": current_reasoning,<br>            \"new_memory\": new_memory,<br>        }<br>        task.extra_state.update(task_state)<br>        return TaskStep(<br>            task_id=task.task_id,<br>            step_id=str(uuid.uuid4()),<br>            input=task.input,<br>            step_state={\"is_first\": True, \"image_docs\": task.extra_state[\"image_docs\"]},<br>        )<br>    def get_tools(self, input: str) -> List[AsyncBaseTool]:<br>        \"\"\"Get tools.\"\"\"<br>        return [adapt_to_async_tool(t) for t in self._get_tools(input)]<br>    def _extract_reasoning_step(<br>        self, output: ChatResponse, is_streaming: bool = False<br>    ) -> Tuple[str, List[BaseReasoningStep], bool]:<br>        \"\"\"<br>        Extracts the reasoning step from the given output.<br>        This method parses the message content from the output,<br>        extracts the reasoning step, and determines whether the processing is<br>        complete. It also performs validation checks on the output and<br>        handles possible errors.<br>        \"\"\"<br>        if output.message.content is None:<br>            raise ValueError(\"Got empty message.\")<br>        message_content = output.message.content<br>        current_reasoning = []<br>        try:<br>            reasoning_step = self._output_parser.parse(message_content, is_streaming)<br>        except BaseException as exc:<br>            raise ValueError(f\"Could not parse output: {message_content}\") from exc<br>        if self._verbose:<br>            print_text(f\"{reasoning_step.get_content()}\\n\", color=\"pink\")<br>        current_reasoning.append(reasoning_step)<br>        if reasoning_step.is_done:<br>            return message_content, current_reasoning, True<br>        reasoning_step = cast(ActionReasoningStep, reasoning_step)<br>        if not isinstance(reasoning_step, ActionReasoningStep):<br>            raise ValueError(f\"Expected ActionReasoningStep, got {reasoning_step}\")<br>        return message_content, current_reasoning, False<br>    def _process_actions(<br>        self,<br>        task: Task,<br>        tools: Sequence[AsyncBaseTool],<br>        output: ChatResponse,<br>        is_streaming: bool = False,<br>    ) -> Tuple[List[BaseReasoningStep], bool]:<br>        tools_dict: Dict[str, AsyncBaseTool] = {<br>            tool.metadata.get_name(): tool for tool in tools<br>        }<br>        _, current_reasoning, is_done = self._extract_reasoning_step(<br>            output, is_streaming<br>        )<br>        if is_done:<br>            return current_reasoning, True<br>        # call tool with input<br>        reasoning_step = cast(ActionReasoningStep, current_reasoning[-1])<br>        tool = tools_dict[reasoning_step.action]<br>        with self.callback_manager.event(<br>            CBEventType.FUNCTION_CALL,<br>            payload={<br>                EventPayload.FUNCTION_CALL: reasoning_step.action_input,<br>                EventPayload.TOOL: tool.metadata,<br>            },<br>        ) as event:<br>            tool_output = tool.call(**reasoning_step.action_input)<br>            event.on_end(payload={EventPayload.FUNCTION_OUTPUT: str(tool_output)})<br>        task.extra_state[\"sources\"].append(tool_output)<br>        observation_step = ObservationReasoningStep(<br>            observation=str(tool_output), return_direct=tool.metadata.return_direct<br>        )<br>        current_reasoning.append(observation_step)<br>        if self._verbose:<br>            print_text(f\"{observation_step.get_content()}\\n\", color=\"blue\")<br>        return current_reasoning, tool.metadata.return_direct<br>    async def _aprocess_actions(<br>        self,<br>        task: Task,<br>        tools: Sequence[AsyncBaseTool],<br>        output: ChatResponse,<br>        is_streaming: bool = False,<br>    ) -> Tuple[List[BaseReasoningStep], bool]:<br>        tools_dict = {tool.metadata.name: tool for tool in tools}<br>        _, current_reasoning, is_done = self._extract_reasoning_step(<br>            output, is_streaming<br>        )<br>        if is_done:<br>            return current_reasoning, True<br>        # call tool with input<br>        reasoning_step = cast(ActionReasoningStep, current_reasoning[-1])<br>        tool = tools_dict[reasoning_step.action]<br>        with self.callback_manager.event(<br>            CBEventType.FUNCTION_CALL,<br>            payload={<br>                EventPayload.FUNCTION_CALL: reasoning_step.action_input,<br>                EventPayload.TOOL: tool.metadata,<br>            },<br>        ) as event:<br>            tool_output = await tool.acall(**reasoning_step.action_input)<br>            event.on_end(payload={EventPayload.FUNCTION_OUTPUT: str(tool_output)})<br>        task.extra_state[\"sources\"].append(tool_output)<br>        observation_step = ObservationReasoningStep(<br>            observation=str(tool_output), return_direct=tool.metadata.return_direct<br>        )<br>        current_reasoning.append(observation_step)<br>        if self._verbose:<br>            print_text(f\"{observation_step.get_content()}\\n\", color=\"blue\")<br>        return current_reasoning, tool.metadata.return_direct<br>    def _get_response(<br>        self,<br>        current_reasoning: List[BaseReasoningStep],<br>        sources: List[ToolOutput],<br>    ) -> AgentChatResponse:<br>        \"\"\"Get response from reasoning steps.\"\"\"<br>        if len(current_reasoning) == 0:<br>            raise ValueError(\"No reasoning steps were taken.\")<br>        elif len(current_reasoning) == self._max_iterations:<br>            raise ValueError(\"Reached max iterations.\")<br>        if isinstance(current_reasoning[-1], ResponseReasoningStep):<br>            response_step = cast(ResponseReasoningStep, current_reasoning[-1])<br>            response_str = response_step.response<br>        elif (<br>            isinstance(current_reasoning[-1], ObservationReasoningStep)<br>            and current_reasoning[-1].return_direct<br>        ):<br>            response_str = current_reasoning[-1].observation<br>        else:<br>            response_str = current_reasoning[-1].get_content()<br>        # TODO: add sources from reasoning steps<br>        return AgentChatResponse(response=response_str, sources=sources)<br>    def _get_task_step_response(<br>        self, agent_response: AGENT_CHAT_RESPONSE_TYPE, step: TaskStep, is_done: bool<br>    ) -> TaskStepOutput:<br>        \"\"\"Get task step response.\"\"\"<br>        if is_done:<br>            new_steps = []<br>        else:<br>            new_steps = [<br>                step.get_next_step(<br>                    step_id=str(uuid.uuid4()),<br>                    # NOTE: input is unused<br>                    input=None,<br>                )<br>            ]<br>        return TaskStepOutput(<br>            output=agent_response,<br>            task_step=step,<br>            is_last=is_done,<br>            next_steps=new_steps,<br>        )<br>    def _run_step(<br>        self,<br>        step: TaskStep,<br>        task: Task,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        # This is either not None on the first step or if the user specifies<br>        # an intermediate step in the middle<br>        if step.input is not None:<br>            self._add_user_step_to_reasoning(<br>                step=step,<br>                memory=task.extra_state[\"new_memory\"],<br>                current_reasoning=task.extra_state[\"current_reasoning\"],<br>                verbose=self._verbose,<br>            )<br>        # TODO: see if we want to do step-based inputs<br>        tools = self.get_tools(task.input)<br>        input_chat = self._react_chat_formatter.format(<br>            tools,<br>            chat_history=task.memory.get_all()<br>            + task.extra_state[\"new_memory\"].get_all(),<br>            current_reasoning=task.extra_state[\"current_reasoning\"],<br>        )<br>        # send prompt<br>        chat_response = self._multi_modal_llm.chat(input_chat)<br>        # given react prompt outputs, call tools or return response<br>        reasoning_steps, is_done = self._process_actions(<br>            task, tools, output=chat_response<br>        )<br>        task.extra_state[\"current_reasoning\"].extend(reasoning_steps)<br>        agent_response = self._get_response(<br>            task.extra_state[\"current_reasoning\"], task.extra_state[\"sources\"]<br>        )<br>        if is_done:<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(content=agent_response.response, role=MessageRole.ASSISTANT)<br>            )<br>        return self._get_task_step_response(agent_response, step, is_done)<br>    async def _arun_step(<br>        self,<br>        step: TaskStep,<br>        task: Task,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        if step.input is not None:<br>            self._add_user_step_to_reasoning(<br>                step=step,<br>                memory=task.extra_state[\"new_memory\"],<br>                current_reasoning=task.extra_state[\"current_reasoning\"],<br>                verbose=self._verbose,<br>            )<br>        # TODO: see if we want to do step-based inputs<br>        tools = self.get_tools(task.input)<br>        input_chat = self._react_chat_formatter.format(<br>            tools,<br>            chat_history=task.memory.get_all()<br>            + task.extra_state[\"new_memory\"].get_all(),<br>            current_reasoning=task.extra_state[\"current_reasoning\"],<br>        )<br>        # send prompt<br>        chat_response = await self._multi_modal_llm.achat(input_chat)<br>        # given react prompt outputs, call tools or return response<br>        reasoning_steps, is_done = await self._aprocess_actions(<br>            task, tools, output=chat_response<br>        )<br>        task.extra_state[\"current_reasoning\"].extend(reasoning_steps)<br>        agent_response = self._get_response(<br>            task.extra_state[\"current_reasoning\"], task.extra_state[\"sources\"]<br>        )<br>        if is_done:<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(content=agent_response.response, role=MessageRole.ASSISTANT)<br>            )<br>        return self._get_task_step_response(agent_response, step, is_done)<br>    def _run_step_stream(<br>        self,<br>        step: TaskStep,<br>        task: Task,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        raise NotImplementedError(\"Stream step not implemented yet.\")<br>    async def _arun_step_stream(<br>        self,<br>        step: TaskStep,<br>        task: Task,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        raise NotImplementedError(\"Stream step not implemented yet.\")<br>    @trace_method(\"run_step\")<br>    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        return self._run_step(step, task)<br>    @trace_method(\"run_step\")<br>    async def arun_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        return await self._arun_step(step, task)<br>    @trace_method(\"run_step\")<br>    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        # TODO: figure out if we need a different type for TaskStepOutput<br>        return self._run_step_stream(step, task)<br>    @trace_method(\"run_step\")<br>    async def astream_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async stream).\"\"\"<br>        return await self._arun_step_stream(step, task)<br>    def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>        \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>        # add new messages to memory<br>        task.memory.set(<br>            task.memory.get_all() + task.extra_state[\"new_memory\"].get_all()<br>        )<br>        # reset new memory<br>        task.extra_state[\"new_memory\"].reset()<br>    def set_callback_manager(self, callback_manager: CallbackManager) -> None:<br>        \"\"\"Set callback manager.\"\"\"<br>        # TODO: make this abstractmethod (right now will break some agent impls)<br>        self.callback_manager = callback_manager<br>``` |\n\n#### from\\_tools`classmethod`[\\#](\\#llama_index.core.agent.MultimodalReActAgentWorker.from_tools \"Permanent link\")\n\n```\nfrom_tools(tools: Optional[Sequence[BaseTool]] = None, tool_retriever: Optional[ObjectRetriever[BaseTool]] = None, multi_modal_llm: Optional[MultiModalLLM] = None, max_iterations: int = 10, react_chat_formatter: Optional[ReActChatFormatter] = None, output_parser: Optional[ReActOutputParser] = None, callback_manager: Optional[CallbackManager] = None, verbose: bool = False, **kwargs: Any) -> MultimodalReActAgentWorker\n\n```\n\nConvenience constructor method from set of of BaseTools (Optional).\n\nNOTE: kwargs should have been exhausted by this point. In other words\nthe various upstream components such as BaseSynthesizer (response synthesizer)\nor BaseRetriever should have picked up off their respective kwargs in their\nconstructions.\n\n**Returns:**\n\n| Type | Description |\n| --- | --- |\n| `MultimodalReActAgentWorker` | ReActAgent |\n\nSource code in `llama-index-core/llama_index/core/agent/react_multimodal/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>``` | ```<br>@classmethod<br>def from_tools(<br>    cls,<br>    tools: Optional[Sequence[BaseTool]] = None,<br>    tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>    multi_modal_llm: Optional[MultiModalLLM] = None,<br>    max_iterations: int = 10,<br>    react_chat_formatter: Optional[ReActChatFormatter] = None,<br>    output_parser: Optional[ReActOutputParser] = None,<br>    callback_manager: Optional[CallbackManager] = None,<br>    verbose: bool = False,<br>    **kwargs: Any,<br>) -> \"MultimodalReActAgentWorker\":<br>    \"\"\"Convenience constructor method from set of of BaseTools (Optional).<br>    NOTE: kwargs should have been exhausted by this point. In other words<br>    the various upstream components such as BaseSynthesizer (response synthesizer)<br>    or BaseRetriever should have picked up off their respective kwargs in their<br>    constructions.<br>    Returns:<br>        ReActAgent<br>    \"\"\"<br>    if multi_modal_llm is None:<br>        try:<br>            from llama_index.multi_modal_llms.openai import (<br>                OpenAIMultiModal,<br>            )  # pants: no-infer-dep<br>            multi_modal_llm = multi_modal_llm or OpenAIMultiModal(<br>                model=\"gpt-4-vision-preview\", max_new_tokens=1000<br>            )<br>        except ImportError:<br>            raise ImportError(<br>                \"`llama-index-multi-modal-llms-openai` package cannot be found. \"<br>                \"Please install it by using `pip install `llama-index-multi-modal-llms-openai`\"<br>            )<br>    return cls(<br>        tools=tools or [],<br>        tool_retriever=tool_retriever,<br>        multi_modal_llm=multi_modal_llm,<br>        max_iterations=max_iterations,<br>        react_chat_formatter=react_chat_formatter,<br>        output_parser=output_parser,<br>        callback_manager=callback_manager,<br>        verbose=verbose,<br>    )<br>``` |\n\n#### initialize\\_step [\\#](\\#llama_index.core.agent.MultimodalReActAgentWorker.initialize_step \"Permanent link\")\n\n```\ninitialize_step(task: Task, **kwargs: Any) -> TaskStep\n\n```\n\nInitialize step from task.\n\nSource code in `llama-index-core/llama_index/core/agent/react_multimodal/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>``` | ```<br>def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>    \"\"\"Initialize step from task.\"\"\"<br>    sources: List[ToolOutput] = []<br>    current_reasoning: List[BaseReasoningStep] = []<br>    # temporary memory for new messages<br>    new_memory = ChatMemoryBuffer.from_defaults()<br>    # validation<br>    if \"image_docs\" not in task.extra_state:<br>        raise ValueError(\"Image docs not found in task extra state.\")<br>    # initialize task state<br>    task_state = {<br>        \"sources\": sources,<br>        \"current_reasoning\": current_reasoning,<br>        \"new_memory\": new_memory,<br>    }<br>    task.extra_state.update(task_state)<br>    return TaskStep(<br>        task_id=task.task_id,<br>        step_id=str(uuid.uuid4()),<br>        input=task.input,<br>        step_state={\"is_first\": True, \"image_docs\": task.extra_state[\"image_docs\"]},<br>    )<br>``` |\n\n#### get\\_tools [\\#](\\#llama_index.core.agent.MultimodalReActAgentWorker.get_tools \"Permanent link\")\n\n```\nget_tools(input: str) -> List[AsyncBaseTool]\n\n```\n\nGet tools.\n\nSource code in `llama-index-core/llama_index/core/agent/react_multimodal/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>229<br>230<br>231<br>``` | ```<br>def get_tools(self, input: str) -> List[AsyncBaseTool]:<br>    \"\"\"Get tools.\"\"\"<br>    return [adapt_to_async_tool(t) for t in self._get_tools(input)]<br>``` |\n\n#### run\\_step [\\#](\\#llama_index.core.agent.MultimodalReActAgentWorker.run_step \"Permanent link\")\n\n```\nrun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step.\n\nSource code in `llama-index-core/llama_index/core/agent/react_multimodal/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>487<br>488<br>489<br>490<br>``` | ```<br>@trace_method(\"run_step\")<br>def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step.\"\"\"<br>    return self._run_step(step, task)<br>``` |\n\n#### arun\\_step`async`[\\#](\\#llama_index.core.agent.MultimodalReActAgentWorker.arun_step \"Permanent link\")\n\n```\narun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async).\n\nSource code in `llama-index-core/llama_index/core/agent/react_multimodal/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>492<br>493<br>494<br>495<br>496<br>497<br>``` | ```<br>@trace_method(\"run_step\")<br>async def arun_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async).\"\"\"<br>    return await self._arun_step(step, task)<br>``` |\n\n#### stream\\_step [\\#](\\#llama_index.core.agent.MultimodalReActAgentWorker.stream_step \"Permanent link\")\n\n```\nstream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (stream).\n\nSource code in `llama-index-core/llama_index/core/agent/react_multimodal/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>499<br>500<br>501<br>502<br>503<br>``` | ```<br>@trace_method(\"run_step\")<br>def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step (stream).\"\"\"<br>    # TODO: figure out if we need a different type for TaskStepOutput<br>    return self._run_step_stream(step, task)<br>``` |\n\n#### astream\\_step`async`[\\#](\\#llama_index.core.agent.MultimodalReActAgentWorker.astream_step \"Permanent link\")\n\n```\nastream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async stream).\n\nSource code in `llama-index-core/llama_index/core/agent/react_multimodal/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>505<br>506<br>507<br>508<br>509<br>510<br>``` | ```<br>@trace_method(\"run_step\")<br>async def astream_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async stream).\"\"\"<br>    return await self._arun_step_stream(step, task)<br>``` |\n\n#### finalize\\_task [\\#](\\#llama_index.core.agent.MultimodalReActAgentWorker.finalize_task \"Permanent link\")\n\n```\nfinalize_task(task: Task, **kwargs: Any) -> None\n\n```\n\nFinalize task, after all the steps are completed.\n\nSource code in `llama-index-core/llama_index/core/agent/react_multimodal/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>512<br>513<br>514<br>515<br>516<br>517<br>518<br>519<br>``` | ```<br>def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>    \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>    # add new messages to memory<br>    task.memory.set(<br>        task.memory.get_all() + task.extra_state[\"new_memory\"].get_all()<br>    )<br>    # reset new memory<br>    task.extra_state[\"new_memory\"].reset()<br>``` |\n\n#### set\\_callback\\_manager [\\#](\\#llama_index.core.agent.MultimodalReActAgentWorker.set_callback_manager \"Permanent link\")\n\n```\nset_callback_manager(callback_manager: CallbackManager) -> None\n\n```\n\nSet callback manager.\n\nSource code in `llama-index-core/llama_index/core/agent/react_multimodal/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>521<br>522<br>523<br>524<br>``` | ```<br>def set_callback_manager(self, callback_manager: CallbackManager) -> None:<br>    \"\"\"Set callback manager.\"\"\"<br>    # TODO: make this abstractmethod (right now will break some agent impls)<br>    self.callback_manager = callback_manager<br>``` |\n\n### QueryPipelineAgentWorker [\\#](\\#llama_index.core.agent.QueryPipelineAgentWorker \"Permanent link\")\n\nBases: `BaseModel`, `BaseAgentWorker`\n\nQuery Pipeline agent worker.\n\nNOTE: This is now deprecated. Use `FnAgentWorker` instead to build a stateful agent.\n\nBarebones agent worker that takes in a query pipeline.\n\n**Default Workflow**: The default workflow assumes that you compose\na query pipeline with `StatefulFnComponent` objects. This allows you to store, update\nand retrieve state throughout the executions of the query pipeline by the agent.\n\nThe task and step state of the agent are stored in this `state` variable via a special key.\nOf course you can choose to store other variables in this state as well.\n\n**Deprecated Workflow**: The deprecated workflow assumes that the first component in the\nquery pipeline is an `AgentInputComponent` and last is `AgentFnComponent`.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `pipeline` | `QueryPipeline` | Query pipeline | _required_ |\n\nSource code in `llama-index-core/llama_index/core/agent/custom/pipeline_worker.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>``` | ```<br>@deprecated(\"Use `FnAgentWorker` instead to build a stateful agent.\")<br>class QueryPipelineAgentWorker(BaseModel, BaseAgentWorker):<br>    \"\"\"Query Pipeline agent worker.<br>    NOTE: This is now deprecated. Use `FnAgentWorker` instead to build a stateful agent.<br>    Barebones agent worker that takes in a query pipeline.<br>    **Default Workflow**: The default workflow assumes that you compose<br>    a query pipeline with `StatefulFnComponent` objects. This allows you to store, update<br>    and retrieve state throughout the executions of the query pipeline by the agent.<br>    The task and step state of the agent are stored in this `state` variable via a special key.<br>    Of course you can choose to store other variables in this state as well.<br>    **Deprecated Workflow**: The deprecated workflow assumes that the first component in the<br>    query pipeline is an `AgentInputComponent` and last is `AgentFnComponent`.<br>    Args:<br>        pipeline (QueryPipeline): Query pipeline<br>    \"\"\"<br>    model_config = ConfigDict(arbitrary_types_allowed=True)<br>    pipeline: QueryPipeline = Field(..., description=\"Query pipeline\")<br>    callback_manager: CallbackManager = Field(..., exclude=True)<br>    task_key: str = Field(\"task\", description=\"Key to store task in state\")<br>    step_state_key: str = Field(\"step_state\", description=\"Key to store step in state\")<br>    def __init__(<br>        self,<br>        pipeline: QueryPipeline,<br>        callback_manager: Optional[CallbackManager] = None,<br>        **kwargs: Any,<br>    ) -> None:<br>        \"\"\"Initialize.\"\"\"<br>        if callback_manager is not None:<br>            # set query pipeline callback<br>            pipeline.set_callback_manager(callback_manager)<br>        else:<br>            callback_manager = pipeline.callback_manager<br>        super().__init__(<br>            pipeline=pipeline,<br>            callback_manager=callback_manager,<br>            **kwargs,<br>        )<br>        # validate query pipeline<br>        # self.agent_input_component<br>        self.agent_components<br>    @property<br>    def agent_input_component(self) -> AgentInputComponent:<br>        \"\"\"Get agent input component.<br>        NOTE: This is deprecated and will be removed in the future.<br>        \"\"\"<br>        root_key = self.pipeline.get_root_keys()[0]<br>        if not isinstance(self.pipeline.module_dict[root_key], AgentInputComponent):<br>            raise ValueError(<br>                \"Query pipeline first component must be AgentInputComponent, got \"<br>                f\"{self.pipeline.module_dict[root_key]}\"<br>            )<br>        return cast(AgentInputComponent, self.pipeline.module_dict[root_key])<br>    @property<br>    def agent_components(self) -> Sequence[BaseAgentComponent]:<br>        \"\"\"Get agent output component.\"\"\"<br>        return _get_agent_components(self.pipeline)<br>    def preprocess(self, task: Task, step: TaskStep) -> None:<br>        \"\"\"Preprocessing flow.<br>        This runs preprocessing to propagate the task and step as variables<br>        to relevant components in the query pipeline.<br>        Contains deprecated flow of updating agent components.<br>        But also contains main flow of updating StatefulFnComponent components.<br>        \"\"\"<br>        # NOTE: this is deprecated<br>        # partial agent output component with task and step<br>        for agent_fn_component in self.agent_components:<br>            agent_fn_component.partial(task=task, state=step.step_state)<br>        # update stateful components<br>        self.pipeline.update_state(<br>            {self.task_key: task, self.step_state_key: step.step_state}<br>        )<br>    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>        \"\"\"Initialize step from task.\"\"\"<br>        sources: List[ToolOutput] = []<br>        # temporary memory for new messages<br>        new_memory = ChatMemoryBuffer.from_defaults()<br>        # initialize initial state<br>        initial_state = {<br>            \"sources\": sources,<br>            \"memory\": new_memory,<br>        }<br>        return TaskStep(<br>            task_id=task.task_id,<br>            step_id=str(uuid.uuid4()),<br>            input=task.input,<br>            step_state=initial_state,<br>        )<br>    def _get_task_step_response(<br>        self, agent_response: AGENT_CHAT_RESPONSE_TYPE, step: TaskStep, is_done: bool<br>    ) -> TaskStepOutput:<br>        \"\"\"Get task step response.\"\"\"<br>        if is_done:<br>            new_steps = []<br>        else:<br>            new_steps = [<br>                step.get_next_step(<br>                    step_id=str(uuid.uuid4()),<br>                    # NOTE: input is unused<br>                    input=None,<br>                )<br>            ]<br>        return TaskStepOutput(<br>            output=agent_response,<br>            task_step=step,<br>            is_last=is_done,<br>            next_steps=new_steps,<br>        )<br>    @trace_method(\"run_step\")<br>    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        self.preprocess(task, step)<br>        # HACK: do a try/except for now. Fine since old agent components are deprecated<br>        try:<br>            self.agent_input_component<br>            uses_deprecated = True<br>        except ValueError:<br>            uses_deprecated = False<br>        if uses_deprecated:<br>            agent_response, is_done = self.pipeline.run(<br>                state=step.step_state, task=task<br>            )<br>        else:<br>            agent_response, is_done = self.pipeline.run()<br>        response = self._get_task_step_response(agent_response, step, is_done)<br>        # sync step state with task state<br>        task.extra_state.update(step.step_state)<br>        return response<br>    @trace_method(\"run_step\")<br>    async def arun_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        self.preprocess(task, step)<br>        # HACK: do a try/except for now. Fine since old agent components are deprecated<br>        try:<br>            self.agent_input_component<br>            uses_deprecated = True<br>        except ValueError:<br>            uses_deprecated = False<br>        if uses_deprecated:<br>            agent_response, is_done = await self.pipeline.arun(<br>                state=step.step_state, task=task<br>            )<br>        else:<br>            agent_response, is_done = await self.pipeline.arun()<br>        response = self._get_task_step_response(agent_response, step, is_done)<br>        task.extra_state.update(step.step_state)<br>        return response<br>    @trace_method(\"run_step\")<br>    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        raise NotImplementedError(\"This agent does not support streaming.\")<br>    @trace_method(\"run_step\")<br>    async def astream_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async stream).\"\"\"<br>        raise NotImplementedError(\"This agent does not support streaming.\")<br>    def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>        \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>        # add new messages to memory<br>        task.memory.set(task.memory.get() + task.extra_state[\"memory\"].get_all())<br>        # reset new memory<br>        task.extra_state[\"memory\"].reset()<br>    def set_callback_manager(self, callback_manager: CallbackManager) -> None:<br>        \"\"\"Set callback manager.\"\"\"<br>        # TODO: make this abstractmethod (right now will break some agent impls)<br>        self.callback_manager = callback_manager<br>        self.pipeline.set_callback_manager(callback_manager)<br>``` |\n\n#### agent\\_input\\_component`property`[\\#](\\#llama_index.core.agent.QueryPipelineAgentWorker.agent_input_component \"Permanent link\")\n\n```\nagent_input_component: AgentInputComponent\n\n```\n\nGet agent input component.\n\nNOTE: This is deprecated and will be removed in the future.\n\n#### agent\\_components`property`[\\#](\\#llama_index.core.agent.QueryPipelineAgentWorker.agent_components \"Permanent link\")\n\n```\nagent_components: Sequence[BaseAgentComponent]\n\n```\n\nGet agent output component.\n\n#### preprocess [\\#](\\#llama_index.core.agent.QueryPipelineAgentWorker.preprocess \"Permanent link\")\n\n```\npreprocess(task: Task, step: TaskStep) -> None\n\n```\n\nPreprocessing flow.\n\nThis runs preprocessing to propagate the task and step as variables\nto relevant components in the query pipeline.\n\nContains deprecated flow of updating agent components.\nBut also contains main flow of updating StatefulFnComponent components.\n\nSource code in `llama-index-core/llama_index/core/agent/custom/pipeline_worker.py`\n\n|     |     |\n| --- | --- |\n| ```<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>``` | ```<br>def preprocess(self, task: Task, step: TaskStep) -> None:<br>    \"\"\"Preprocessing flow.<br>    This runs preprocessing to propagate the task and step as variables<br>    to relevant components in the query pipeline.<br>    Contains deprecated flow of updating agent components.<br>    But also contains main flow of updating StatefulFnComponent components.<br>    \"\"\"<br>    # NOTE: this is deprecated<br>    # partial agent output component with task and step<br>    for agent_fn_component in self.agent_components:<br>        agent_fn_component.partial(task=task, state=step.step_state)<br>    # update stateful components<br>    self.pipeline.update_state(<br>        {self.task_key: task, self.step_state_key: step.step_state}<br>    )<br>``` |\n\n#### initialize\\_step [\\#](\\#llama_index.core.agent.QueryPipelineAgentWorker.initialize_step \"Permanent link\")\n\n```\ninitialize_step(task: Task, **kwargs: Any) -> TaskStep\n\n```\n\nInitialize step from task.\n\nSource code in `llama-index-core/llama_index/core/agent/custom/pipeline_worker.py`\n\n|     |     |\n| --- | --- |\n| ```<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>``` | ```<br>def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>    \"\"\"Initialize step from task.\"\"\"<br>    sources: List[ToolOutput] = []<br>    # temporary memory for new messages<br>    new_memory = ChatMemoryBuffer.from_defaults()<br>    # initialize initial state<br>    initial_state = {<br>        \"sources\": sources,<br>        \"memory\": new_memory,<br>    }<br>    return TaskStep(<br>        task_id=task.task_id,<br>        step_id=str(uuid.uuid4()),<br>        input=task.input,<br>        step_state=initial_state,<br>    )<br>``` |\n\n#### run\\_step [\\#](\\#llama_index.core.agent.QueryPipelineAgentWorker.run_step \"Permanent link\")\n\n```\nrun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step.\n\nSource code in `llama-index-core/llama_index/core/agent/custom/pipeline_worker.py`\n\n|     |     |\n| --- | --- |\n| ```<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>``` | ```<br>@trace_method(\"run_step\")<br>def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step.\"\"\"<br>    self.preprocess(task, step)<br>    # HACK: do a try/except for now. Fine since old agent components are deprecated<br>    try:<br>        self.agent_input_component<br>        uses_deprecated = True<br>    except ValueError:<br>        uses_deprecated = False<br>    if uses_deprecated:<br>        agent_response, is_done = self.pipeline.run(<br>            state=step.step_state, task=task<br>        )<br>    else:<br>        agent_response, is_done = self.pipeline.run()<br>    response = self._get_task_step_response(agent_response, step, is_done)<br>    # sync step state with task state<br>    task.extra_state.update(step.step_state)<br>    return response<br>``` |\n\n#### arun\\_step`async`[\\#](\\#llama_index.core.agent.QueryPipelineAgentWorker.arun_step \"Permanent link\")\n\n```\narun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async).\n\nSource code in `llama-index-core/llama_index/core/agent/custom/pipeline_worker.py`\n\n|     |     |\n| --- | --- |\n| ```<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>``` | ```<br>@trace_method(\"run_step\")<br>async def arun_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async).\"\"\"<br>    self.preprocess(task, step)<br>    # HACK: do a try/except for now. Fine since old agent components are deprecated<br>    try:<br>        self.agent_input_component<br>        uses_deprecated = True<br>    except ValueError:<br>        uses_deprecated = False<br>    if uses_deprecated:<br>        agent_response, is_done = await self.pipeline.arun(<br>            state=step.step_state, task=task<br>        )<br>    else:<br>        agent_response, is_done = await self.pipeline.arun()<br>    response = self._get_task_step_response(agent_response, step, is_done)<br>    task.extra_state.update(step.step_state)<br>    return response<br>``` |\n\n#### stream\\_step [\\#](\\#llama_index.core.agent.QueryPipelineAgentWorker.stream_step \"Permanent link\")\n\n```\nstream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (stream).\n\nSource code in `llama-index-core/llama_index/core/agent/custom/pipeline_worker.py`\n\n|     |     |\n| --- | --- |\n| ```<br>233<br>234<br>235<br>236<br>``` | ```<br>@trace_method(\"run_step\")<br>def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step (stream).\"\"\"<br>    raise NotImplementedError(\"This agent does not support streaming.\")<br>``` |\n\n#### astream\\_step`async`[\\#](\\#llama_index.core.agent.QueryPipelineAgentWorker.astream_step \"Permanent link\")\n\n```\nastream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async stream).\n\nSource code in `llama-index-core/llama_index/core/agent/custom/pipeline_worker.py`\n\n|     |     |\n| --- | --- |\n| ```<br>238<br>239<br>240<br>241<br>242<br>243<br>``` | ```<br>@trace_method(\"run_step\")<br>async def astream_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async stream).\"\"\"<br>    raise NotImplementedError(\"This agent does not support streaming.\")<br>``` |\n\n#### finalize\\_task [\\#](\\#llama_index.core.agent.QueryPipelineAgentWorker.finalize_task \"Permanent link\")\n\n```\nfinalize_task(task: Task, **kwargs: Any) -> None\n\n```\n\nFinalize task, after all the steps are completed.\n\nSource code in `llama-index-core/llama_index/core/agent/custom/pipeline_worker.py`\n\n|     |     |\n| --- | --- |\n| ```<br>245<br>246<br>247<br>248<br>249<br>250<br>``` | ```<br>def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>    \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>    # add new messages to memory<br>    task.memory.set(task.memory.get() + task.extra_state[\"memory\"].get_all())<br>    # reset new memory<br>    task.extra_state[\"memory\"].reset()<br>``` |\n\n#### set\\_callback\\_manager [\\#](\\#llama_index.core.agent.QueryPipelineAgentWorker.set_callback_manager \"Permanent link\")\n\n```\nset_callback_manager(callback_manager: CallbackManager) -> None\n\n```\n\nSet callback manager.\n\nSource code in `llama-index-core/llama_index/core/agent/custom/pipeline_worker.py`\n\n|     |     |\n| --- | --- |\n| ```<br>252<br>253<br>254<br>255<br>256<br>``` | ```<br>def set_callback_manager(self, callback_manager: CallbackManager) -> None:<br>    \"\"\"Set callback manager.\"\"\"<br>    # TODO: make this abstractmethod (right now will break some agent impls)<br>    self.callback_manager = callback_manager<br>    self.pipeline.set_callback_manager(callback_manager)<br>``` |\n\nBack to top",
      "metadata": {
        "title": "Core Agent Classes - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/agent/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "\n\n# Wandb\n\n## WandbCallbackHandler [\\#](\\#llama_index.callbacks.wandb.WandbCallbackHandler \"Permanent link\")\n\nBases: `BaseCallbackHandler`\n\nCallback handler that logs events to wandb.\n\nNOTE: this is a beta feature. The usage within our codebase, and the interface\nmay change.\n\nUse the `WandbCallbackHandler` to log trace events to wandb. This handler is\nuseful for debugging and visualizing the trace events. It captures the payload of\nthe events and logs them to wandb. The handler also tracks the start and end of\nevents. This is particularly useful for debugging your LLM calls.\n\nThe `WandbCallbackHandler` can also be used to log the indices and graphs to wandb\nusing the `persist_index` method. This will save the indexes as artifacts in wandb.\nThe `load_storage_context` method can be used to load the indexes from wandb\nartifacts. This method will return a `StorageContext` object that can be used to\nbuild the index, using `load_index_from_storage`, `load_indices_from_storage` or\n`load_graph_from_storage` functions.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `event_starts_to_ignore` | `Optional[List[CBEventType]]` | list of event types to<br>ignore when tracking event starts. | `None` |\n| `event_ends_to_ignore` | `Optional[List[CBEventType]]` | list of event types to<br>ignore when tracking event ends. | `None` |\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-wandb/llama_index/callbacks/wandb/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>472<br>473<br>474<br>475<br>476<br>477<br>478<br>479<br>480<br>481<br>482<br>483<br>484<br>485<br>486<br>487<br>488<br>489<br>490<br>491<br>492<br>493<br>494<br>495<br>496<br>497<br>498<br>499<br>500<br>501<br>502<br>503<br>504<br>505<br>506<br>507<br>508<br>509<br>510<br>511<br>512<br>513<br>514<br>515<br>516<br>517<br>518<br>519<br>520<br>521<br>522<br>523<br>524<br>525<br>526<br>527<br>528<br>529<br>530<br>531<br>532<br>533<br>534<br>535<br>536<br>537<br>538<br>539<br>540<br>541<br>542<br>543<br>544<br>545<br>546<br>547<br>548<br>549<br>550<br>551<br>552<br>553<br>554<br>555<br>556<br>557<br>558<br>559<br>560<br>561<br>562<br>563<br>564<br>565<br>566<br>567<br>568<br>569<br>570<br>``` | ```<br>class WandbCallbackHandler(BaseCallbackHandler):<br>    \"\"\"Callback handler that logs events to wandb.<br>    NOTE: this is a beta feature. The usage within our codebase, and the interface<br>    may change.<br>    Use the `WandbCallbackHandler` to log trace events to wandb. This handler is<br>    useful for debugging and visualizing the trace events. It captures the payload of<br>    the events and logs them to wandb. The handler also tracks the start and end of<br>    events. This is particularly useful for debugging your LLM calls.<br>    The `WandbCallbackHandler` can also be used to log the indices and graphs to wandb<br>    using the `persist_index` method. This will save the indexes as artifacts in wandb.<br>    The `load_storage_context` method can be used to load the indexes from wandb<br>    artifacts. This method will return a `StorageContext` object that can be used to<br>    build the index, using `load_index_from_storage`, `load_indices_from_storage` or<br>    `load_graph_from_storage` functions.<br>    Args:<br>        event_starts_to_ignore (Optional[List[CBEventType]]): list of event types to<br>            ignore when tracking event starts.<br>        event_ends_to_ignore (Optional[List[CBEventType]]): list of event types to<br>            ignore when tracking event ends.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        run_args: Optional[WandbRunArgs] = None,<br>        tokenizer: Optional[Callable[[str], List]] = None,<br>        event_starts_to_ignore: Optional[List[CBEventType]] = None,<br>        event_ends_to_ignore: Optional[List[CBEventType]] = None,<br>    ) -> None:<br>        try:<br>            import wandb<br>            from wandb.sdk.data_types import trace_tree<br>            self._wandb = wandb<br>            self._trace_tree = trace_tree<br>        except ImportError:<br>            raise ImportError(<br>                \"WandbCallbackHandler requires wandb. \"<br>                \"Please install it with `pip install wandb`.\"<br>            )<br>        from llama_index.core.indices import (<br>            ComposableGraph,<br>            GPTEmptyIndex,<br>            GPTKeywordTableIndex,<br>            GPTRAKEKeywordTableIndex,<br>            GPTSimpleKeywordTableIndex,<br>            GPTSQLStructStoreIndex,<br>            GPTTreeIndex,<br>            GPTVectorStoreIndex,<br>            SummaryIndex,<br>        )<br>        self._IndexType = (<br>            ComposableGraph,<br>            GPTKeywordTableIndex,<br>            GPTSimpleKeywordTableIndex,<br>            GPTRAKEKeywordTableIndex,<br>            SummaryIndex,<br>            GPTEmptyIndex,<br>            GPTTreeIndex,<br>            GPTVectorStoreIndex,<br>            GPTSQLStructStoreIndex,<br>        )<br>        self._run_args = run_args<br>        # Check if a W&B run is already initialized; if not, initialize one<br>        self._ensure_run(should_print_url=(self._wandb.run is None))  # type: ignore[attr-defined]<br>        self._event_pairs_by_id: Dict[str, List[CBEvent]] = defaultdict(list)<br>        self._cur_trace_id: Optional[str] = None<br>        self._trace_map: Dict[str, List[str]] = defaultdict(list)<br>        self.tokenizer = tokenizer or get_tokenizer()<br>        self._token_counter = TokenCounter(tokenizer=self.tokenizer)<br>        event_starts_to_ignore = (<br>            event_starts_to_ignore if event_starts_to_ignore else []<br>        )<br>        event_ends_to_ignore = event_ends_to_ignore if event_ends_to_ignore else []<br>        super().__init__(<br>            event_starts_to_ignore=event_starts_to_ignore,<br>            event_ends_to_ignore=event_ends_to_ignore,<br>        )<br>    def on_event_start(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        parent_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> str:<br>        \"\"\"Store event start data by event type.<br>        Args:<br>            event_type (CBEventType): event type to store.<br>            payload (Optional[Dict[str, Any]]): payload to store.<br>            event_id (str): event id to store.<br>            parent_id (str): parent event id.<br>        \"\"\"<br>        event = CBEvent(event_type, payload=payload, id_=event_id)<br>        self._event_pairs_by_id[event.id_].append(event)<br>        return event.id_<br>    def on_event_end(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> None:<br>        \"\"\"Store event end data by event type.<br>        Args:<br>            event_type (CBEventType): event type to store.<br>            payload (Optional[Dict[str, Any]]): payload to store.<br>            event_id (str): event id to store.<br>        \"\"\"<br>        event = CBEvent(event_type, payload=payload, id_=event_id)<br>        self._event_pairs_by_id[event.id_].append(event)<br>        self._trace_map = defaultdict(list)<br>    def start_trace(self, trace_id: Optional[str] = None) -> None:<br>        \"\"\"Launch a trace.\"\"\"<br>        self._trace_map = defaultdict(list)<br>        self._cur_trace_id = trace_id<br>        self._start_time = datetime.now()<br>    def end_trace(<br>        self,<br>        trace_id: Optional[str] = None,<br>        trace_map: Optional[Dict[str, List[str]]] = None,<br>    ) -> None:<br>        # Ensure W&B run is initialized<br>        self._ensure_run()<br>        self._trace_map = trace_map or defaultdict(list)<br>        self._end_time = datetime.now()<br>        # Log the trace map to wandb<br>        # We can control what trace ids we want to log here.<br>        self.log_trace_tree()<br>        # TODO (ayulockin): Log the LLM token counts to wandb when weave is ready<br>    def log_trace_tree(self) -> None:<br>        \"\"\"Log the trace tree to wandb.\"\"\"<br>        try:<br>            child_nodes = self._trace_map[\"root\"]<br>            root_span = self._convert_event_pair_to_wb_span(<br>                self._event_pairs_by_id[child_nodes[0]],<br>                trace_id=self._cur_trace_id if len(child_nodes) > 1 else None,<br>            )<br>            if len(child_nodes) == 1:<br>                child_nodes = self._trace_map[child_nodes[0]]<br>                root_span = self._build_trace_tree(child_nodes, root_span)<br>            else:<br>                root_span = self._build_trace_tree(child_nodes, root_span)<br>            if root_span:<br>                root_trace = self._trace_tree.WBTraceTree(root_span)<br>                if self._wandb.run:  # type: ignore[attr-defined]<br>                    self._wandb.run.log({\"trace\": root_trace})  # type: ignore[attr-defined]<br>                self._wandb.termlog(\"Logged trace tree to W&B.\")  # type: ignore[attr-defined]<br>        except Exception as e:<br>            print(f\"Failed to log trace tree to W&B: {e}\")<br>            # ignore errors to not break user code<br>    def persist_index(<br>        self, index: \"IndexType\", index_name: str, persist_dir: Union[str, None] = None<br>    ) -> None:<br>        \"\"\"Upload an index to wandb as an artifact. You can learn more about W&B<br>        artifacts here: https://docs.wandb.ai/guides/artifacts.<br>        For the `ComposableGraph` index, the root id is stored as artifact metadata.<br>        Args:<br>            index (IndexType): index to upload.<br>            index_name (str): name of the index. This will be used as the artifact name.<br>            persist_dir (Union[str, None]): directory to persist the index. If None, a<br>                temporary directory will be created and used.<br>        \"\"\"<br>        if persist_dir is None:<br>            persist_dir = f\"{self._wandb.run.dir}/storage\"  # type: ignore<br>            _default_persist_dir = True<br>        if not os.path.exists(persist_dir):<br>            os.makedirs(persist_dir)<br>        if isinstance(index, self._IndexType):<br>            try:<br>                index.storage_context.persist(persist_dir)  # type: ignore<br>                metadata = None<br>                # For the `ComposableGraph` index, store the root id as metadata<br>                if isinstance(index, self._IndexType[0]):<br>                    root_id = index.root_id<br>                    metadata = {\"root_id\": root_id}<br>                self._upload_index_as_wb_artifact(persist_dir, index_name, metadata)<br>            except Exception as e:<br>                # Silently ignore errors to not break user code<br>                self._print_upload_index_fail_message(e)<br>        # clear the default storage dir<br>        if _default_persist_dir:<br>            shutil.rmtree(persist_dir, ignore_errors=True)<br>    def load_storage_context(<br>        self, artifact_url: str, index_download_dir: Union[str, None] = None<br>    ) -> \"StorageContext\":<br>        \"\"\"Download an index from wandb and return a storage context.<br>        Use this storage context to load the index into memory using<br>        `load_index_from_storage`, `load_indices_from_storage` or<br>        `load_graph_from_storage` functions.<br>        Args:<br>            artifact_url (str): url of the artifact to download. The artifact url will<br>                be of the form: `entity/project/index_name:version` and can be found in<br>                the W&B UI.<br>            index_download_dir (Union[str, None]): directory to download the index to.<br>        \"\"\"<br>        from llama_index.core.storage.storage_context import StorageContext<br>        artifact = self._wandb.use_artifact(artifact_url, type=\"storage_context\")  # type: ignore[attr-defined]<br>        artifact_dir = artifact.download(root=index_download_dir)<br>        return StorageContext.from_defaults(persist_dir=artifact_dir)<br>    def _upload_index_as_wb_artifact(<br>        self, dir_path: str, artifact_name: str, metadata: Optional[Dict]<br>    ) -> None:<br>        \"\"\"Utility function to upload a dir to W&B as an artifact.\"\"\"<br>        artifact = self._wandb.Artifact(artifact_name, type=\"storage_context\")  # type: ignore[attr-defined]<br>        if metadata:<br>            artifact.metadata = metadata<br>        artifact.add_dir(dir_path)<br>        self._wandb.run.log_artifact(artifact)  # type: ignore<br>    def _build_trace_tree(<br>        self, events: List[str], span: \"trace_tree.Span\"<br>    ) -> \"trace_tree.Span\":<br>        \"\"\"Build the trace tree from the trace map.\"\"\"<br>        for child_event in events:<br>            child_span = self._convert_event_pair_to_wb_span(<br>                self._event_pairs_by_id[child_event]<br>            )<br>            child_span = self._build_trace_tree(<br>                self._trace_map[child_event], child_span<br>            )<br>            span.add_child_span(child_span)<br>        return span<br>    def _convert_event_pair_to_wb_span(<br>        self,<br>        event_pair: List[CBEvent],<br>        trace_id: Optional[str] = None,<br>    ) -> \"trace_tree.Span\":<br>        \"\"\"Convert a pair of events to a wandb trace tree span.\"\"\"<br>        start_time_ms, end_time_ms = self._get_time_in_ms(event_pair)<br>        if trace_id is None:<br>            event_type = event_pair[0].event_type<br>            span_kind = self._map_event_type_to_span_kind(event_type)<br>        else:<br>            event_type = trace_id  # type: ignore<br>            span_kind = None<br>        wb_span = self._trace_tree.Span(<br>            name=f\"{event_type}\",<br>            span_kind=span_kind,<br>            start_time_ms=start_time_ms,<br>            end_time_ms=end_time_ms,<br>        )<br>        inputs, outputs, wb_span = self._add_payload_to_span(wb_span, event_pair)<br>        wb_span.add_named_result(inputs=inputs, outputs=outputs)  # type: ignore<br>        return wb_span<br>    def _map_event_type_to_span_kind(<br>        self, event_type: CBEventType<br>    ) -> Union[None, \"trace_tree.SpanKind\"]:<br>        \"\"\"Map a CBEventType to a wandb trace tree SpanKind.\"\"\"<br>        if event_type == CBEventType.CHUNKING:<br>            span_kind = None<br>        elif event_type == CBEventType.NODE_PARSING:<br>            span_kind = None<br>        elif event_type == CBEventType.EMBEDDING:<br>            # TODO: add span kind for EMBEDDING when it's available<br>            span_kind = None<br>        elif event_type == CBEventType.LLM:<br>            span_kind = self._trace_tree.SpanKind.LLM<br>        elif event_type == CBEventType.QUERY:<br>            span_kind = self._trace_tree.SpanKind.AGENT<br>        elif event_type == CBEventType.AGENT_STEP:<br>            span_kind = self._trace_tree.SpanKind.AGENT<br>        elif event_type == CBEventType.RETRIEVE:<br>            span_kind = self._trace_tree.SpanKind.TOOL<br>        elif event_type == CBEventType.SYNTHESIZE:<br>            span_kind = self._trace_tree.SpanKind.CHAIN<br>        elif event_type == CBEventType.TREE:<br>            span_kind = self._trace_tree.SpanKind.CHAIN<br>        elif event_type == CBEventType.SUB_QUESTION:<br>            span_kind = self._trace_tree.SpanKind.CHAIN<br>        elif event_type == CBEventType.RERANKING:<br>            span_kind = self._trace_tree.SpanKind.CHAIN<br>        elif event_type == CBEventType.FUNCTION_CALL:<br>            span_kind = self._trace_tree.SpanKind.TOOL<br>        else:<br>            span_kind = None<br>        return span_kind<br>    def _add_payload_to_span(<br>        self, span: \"trace_tree.Span\", event_pair: List[CBEvent]<br>    ) -> Tuple[Optional[Dict[str, Any]], Optional[Dict[str, Any]], \"trace_tree.Span\"]:<br>        \"\"\"Add the event's payload to the span.\"\"\"<br>        assert len(event_pair) == 2<br>        event_type = event_pair[0].event_type<br>        inputs = None<br>        outputs = None<br>        if event_type == CBEventType.NODE_PARSING:<br>            # TODO: disabled full detailed inputs/outputs due to UI lag<br>            inputs, outputs = self._handle_node_parsing_payload(event_pair)<br>        elif event_type == CBEventType.LLM:<br>            inputs, outputs, span = self._handle_llm_payload(event_pair, span)<br>        elif event_type == CBEventType.QUERY:<br>            inputs, outputs = self._handle_query_payload(event_pair)<br>        elif event_type == CBEventType.EMBEDDING:<br>            inputs, outputs = self._handle_embedding_payload(event_pair)<br>        return inputs, outputs, span<br>    def _handle_node_parsing_payload(<br>        self, event_pair: List[CBEvent]<br>    ) -> Tuple[Dict[str, Any], Dict[str, Any]]:<br>        \"\"\"Handle the payload of a NODE_PARSING event.\"\"\"<br>        inputs = event_pair[0].payload<br>        outputs = event_pair[-1].payload<br>        if inputs and EventPayload.DOCUMENTS in inputs:<br>            documents = inputs.pop(EventPayload.DOCUMENTS)<br>            inputs[\"num_documents\"] = len(documents)<br>        if outputs and EventPayload.NODES in outputs:<br>            nodes = outputs.pop(EventPayload.NODES)<br>            outputs[\"num_nodes\"] = len(nodes)<br>        return inputs or {}, outputs or {}<br>    def _handle_llm_payload(<br>        self, event_pair: List[CBEvent], span: \"trace_tree.Span\"<br>    ) -> Tuple[Dict[str, Any], Dict[str, Any], \"trace_tree.Span\"]:<br>        \"\"\"Handle the payload of a LLM event.\"\"\"<br>        inputs = event_pair[0].payload<br>        outputs = event_pair[-1].payload<br>        assert isinstance(inputs, dict) and isinstance(outputs, dict)<br>        # Get `original_template` from Prompt<br>        if EventPayload.PROMPT in inputs:<br>            inputs[EventPayload.PROMPT] = inputs[EventPayload.PROMPT]<br>        # Format messages<br>        if EventPayload.MESSAGES in inputs:<br>            inputs[EventPayload.MESSAGES] = \"\\n\".join(<br>                [str(x) for x in inputs[EventPayload.MESSAGES]]<br>            )<br>        token_counts = get_llm_token_counts(self._token_counter, outputs)<br>        metadata = {<br>            \"formatted_prompt_tokens_count\": token_counts.prompt_token_count,<br>            \"prediction_tokens_count\": token_counts.completion_token_count,<br>            \"total_tokens_used\": token_counts.total_token_count,<br>        }<br>        span.attributes = metadata<br>        # Make `response` part of `outputs`<br>        outputs = {EventPayload.RESPONSE: str(outputs[EventPayload.RESPONSE])}<br>        return inputs, outputs, span<br>    def _handle_query_payload(<br>        self, event_pair: List[CBEvent]<br>    ) -> Tuple[Optional[Dict[str, Any]], Dict[str, Any]]:<br>        \"\"\"Handle the payload of a QUERY event.\"\"\"<br>        inputs = event_pair[0].payload<br>        outputs = event_pair[-1].payload<br>        if outputs:<br>            response_obj = outputs[EventPayload.RESPONSE]<br>            response = str(outputs[EventPayload.RESPONSE])<br>            if type(response).__name__ == \"Response\":<br>                response = response_obj.response<br>            elif type(response).__name__ == \"StreamingResponse\":<br>                response = response_obj.get_response().response<br>        else:<br>            response = \" \"<br>        outputs = {\"response\": response}<br>        return inputs, outputs<br>    def _handle_embedding_payload(<br>        self,<br>        event_pair: List[CBEvent],<br>    ) -> Tuple[Optional[Dict[str, Any]], Dict[str, Any]]:<br>        event_pair[0].payload<br>        outputs = event_pair[-1].payload<br>        chunks = []<br>        if outputs:<br>            chunks = outputs.get(EventPayload.CHUNKS, [])<br>        return {}, {\"num_chunks\": len(chunks)}<br>    def _get_time_in_ms(self, event_pair: List[CBEvent]) -> Tuple[int, int]:<br>        \"\"\"Get the start and end time of an event pair in milliseconds.\"\"\"<br>        start_time = datetime.strptime(event_pair[0].time, TIMESTAMP_FORMAT)<br>        end_time = datetime.strptime(event_pair[1].time, TIMESTAMP_FORMAT)<br>        start_time_in_ms = int(<br>            (start_time - datetime(1970, 1, 1)).total_seconds() * 1000<br>        )<br>        end_time_in_ms = int((end_time - datetime(1970, 1, 1)).total_seconds() * 1000)<br>        return start_time_in_ms, end_time_in_ms<br>    def _ensure_run(self, should_print_url: bool = False) -> None:<br>        \"\"\"Ensures an active W&B run exists.<br>        If not, will start a new run with the provided run_args.<br>        \"\"\"<br>        if self._wandb.run is None:  # type: ignore[attr-defined]<br>            # Make a shallow copy of the run args, so we don't modify the original<br>            run_args = self._run_args or {}  # type: ignore<br>            run_args: dict = {**run_args}  # type: ignore<br>            # Prefer to run in silent mode since W&B has a lot of output<br>            # which can be undesirable when dealing with text-based models.<br>            if \"settings\" not in run_args:  # type: ignore<br>                run_args[\"settings\"] = {\"silent\": True}  # type: ignore<br>            # Start the run and add the stream table<br>            self._wandb.init(**run_args)  # type: ignore[attr-defined]<br>            self._wandb.run._label(repo=\"llama_index\")  # type: ignore<br>            if should_print_url:<br>                self._print_wandb_init_message(<br>                    self._wandb.run.settings.run_url  # type: ignore<br>                )<br>    def _print_wandb_init_message(self, run_url: str) -> None:<br>        \"\"\"Print a message to the terminal when W&B is initialized.\"\"\"<br>        self._wandb.termlog(  # type: ignore[attr-defined]<br>            f\"Streaming LlamaIndex events to W&B at {run_url}\\n\"<br>            \"`WandbCallbackHandler` is currently in beta.\\n\"<br>            \"Please report any issues to https://github.com/wandb/wandb/issues \"<br>            \"with the tag `llamaindex`.\"<br>        )<br>    def _print_upload_index_fail_message(self, e: Exception) -> None:<br>        \"\"\"Print a message to the terminal when uploading the index fails.\"\"\"<br>        self._wandb.termlog(  # type: ignore[attr-defined]<br>            f\"Failed to upload index to W&B with the following error: {e}\\n\"<br>        )<br>    def finish(self) -> None:<br>        \"\"\"Finish the callback handler.\"\"\"<br>        self._wandb.finish()  # type: ignore[attr-defined]<br>``` |\n\n### on\\_event\\_start [\\#](\\#llama_index.callbacks.wandb.WandbCallbackHandler.on_event_start \"Permanent link\")\n\n```\non_event_start(event_type: CBEventType, payload: Optional[Dict[str, Any]] = None, event_id: str = '', parent_id: str = '', **kwargs: Any) -> str\n\n```\n\nStore event start data by event type.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `event_type` | `CBEventType` | event type to store. | _required_ |\n| `payload` | `Optional[Dict[str, Any]]` | payload to store. | `None` |\n| `event_id` | `str` | event id to store. | `''` |\n| `parent_id` | `str` | parent event id. | `''` |\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-wandb/llama_index/callbacks/wandb/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>``` | ```<br>def on_event_start(<br>    self,<br>    event_type: CBEventType,<br>    payload: Optional[Dict[str, Any]] = None,<br>    event_id: str = \"\",<br>    parent_id: str = \"\",<br>    **kwargs: Any,<br>) -> str:<br>    \"\"\"Store event start data by event type.<br>    Args:<br>        event_type (CBEventType): event type to store.<br>        payload (Optional[Dict[str, Any]]): payload to store.<br>        event_id (str): event id to store.<br>        parent_id (str): parent event id.<br>    \"\"\"<br>    event = CBEvent(event_type, payload=payload, id_=event_id)<br>    self._event_pairs_by_id[event.id_].append(event)<br>    return event.id_<br>``` |\n\n### on\\_event\\_end [\\#](\\#llama_index.callbacks.wandb.WandbCallbackHandler.on_event_end \"Permanent link\")\n\n```\non_event_end(event_type: CBEventType, payload: Optional[Dict[str, Any]] = None, event_id: str = '', **kwargs: Any) -> None\n\n```\n\nStore event end data by event type.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `event_type` | `CBEventType` | event type to store. | _required_ |\n| `payload` | `Optional[Dict[str, Any]]` | payload to store. | `None` |\n| `event_id` | `str` | event id to store. | `''` |\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-wandb/llama_index/callbacks/wandb/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>``` | ```<br>def on_event_end(<br>    self,<br>    event_type: CBEventType,<br>    payload: Optional[Dict[str, Any]] = None,<br>    event_id: str = \"\",<br>    **kwargs: Any,<br>) -> None:<br>    \"\"\"Store event end data by event type.<br>    Args:<br>        event_type (CBEventType): event type to store.<br>        payload (Optional[Dict[str, Any]]): payload to store.<br>        event_id (str): event id to store.<br>    \"\"\"<br>    event = CBEvent(event_type, payload=payload, id_=event_id)<br>    self._event_pairs_by_id[event.id_].append(event)<br>    self._trace_map = defaultdict(list)<br>``` |\n\n### start\\_trace [\\#](\\#llama_index.callbacks.wandb.WandbCallbackHandler.start_trace \"Permanent link\")\n\n```\nstart_trace(trace_id: Optional[str] = None) -> None\n\n```\n\nLaunch a trace.\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-wandb/llama_index/callbacks/wandb/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>216<br>217<br>218<br>219<br>220<br>``` | ```<br>def start_trace(self, trace_id: Optional[str] = None) -> None:<br>    \"\"\"Launch a trace.\"\"\"<br>    self._trace_map = defaultdict(list)<br>    self._cur_trace_id = trace_id<br>    self._start_time = datetime.now()<br>``` |\n\n### log\\_trace\\_tree [\\#](\\#llama_index.callbacks.wandb.WandbCallbackHandler.log_trace_tree \"Permanent link\")\n\n```\nlog_trace_tree() -> None\n\n```\n\nLog the trace tree to wandb.\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-wandb/llama_index/callbacks/wandb/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>``` | ```<br>def log_trace_tree(self) -> None:<br>    \"\"\"Log the trace tree to wandb.\"\"\"<br>    try:<br>        child_nodes = self._trace_map[\"root\"]<br>        root_span = self._convert_event_pair_to_wb_span(<br>            self._event_pairs_by_id[child_nodes[0]],<br>            trace_id=self._cur_trace_id if len(child_nodes) > 1 else None,<br>        )<br>        if len(child_nodes) == 1:<br>            child_nodes = self._trace_map[child_nodes[0]]<br>            root_span = self._build_trace_tree(child_nodes, root_span)<br>        else:<br>            root_span = self._build_trace_tree(child_nodes, root_span)<br>        if root_span:<br>            root_trace = self._trace_tree.WBTraceTree(root_span)<br>            if self._wandb.run:  # type: ignore[attr-defined]<br>                self._wandb.run.log({\"trace\": root_trace})  # type: ignore[attr-defined]<br>            self._wandb.termlog(\"Logged trace tree to W&B.\")  # type: ignore[attr-defined]<br>    except Exception as e:<br>        print(f\"Failed to log trace tree to W&B: {e}\")<br>``` |\n\n### persist\\_index [\\#](\\#llama_index.callbacks.wandb.WandbCallbackHandler.persist_index \"Permanent link\")\n\n```\npersist_index(index: IndexType, index_name: str, persist_dir: Union[str, None] = None) -> None\n\n```\n\nUpload an index to wandb as an artifact. You can learn more about W&B\nartifacts here: https://docs.wandb.ai/guides/artifacts.\n\nFor the `ComposableGraph` index, the root id is stored as artifact metadata.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `index` | `IndexType` | index to upload. | _required_ |\n| `index_name` | `str` | name of the index. This will be used as the artifact name. | _required_ |\n| `persist_dir` | `Union[str, None]` | directory to persist the index. If None, a<br>temporary directory will be created and used. | `None` |\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-wandb/llama_index/callbacks/wandb/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>``` | ```<br>def persist_index(<br>    self, index: \"IndexType\", index_name: str, persist_dir: Union[str, None] = None<br>) -> None:<br>    \"\"\"Upload an index to wandb as an artifact. You can learn more about W&B<br>    artifacts here: https://docs.wandb.ai/guides/artifacts.<br>    For the `ComposableGraph` index, the root id is stored as artifact metadata.<br>    Args:<br>        index (IndexType): index to upload.<br>        index_name (str): name of the index. This will be used as the artifact name.<br>        persist_dir (Union[str, None]): directory to persist the index. If None, a<br>            temporary directory will be created and used.<br>    \"\"\"<br>    if persist_dir is None:<br>        persist_dir = f\"{self._wandb.run.dir}/storage\"  # type: ignore<br>        _default_persist_dir = True<br>    if not os.path.exists(persist_dir):<br>        os.makedirs(persist_dir)<br>    if isinstance(index, self._IndexType):<br>        try:<br>            index.storage_context.persist(persist_dir)  # type: ignore<br>            metadata = None<br>            # For the `ComposableGraph` index, store the root id as metadata<br>            if isinstance(index, self._IndexType[0]):<br>                root_id = index.root_id<br>                metadata = {\"root_id\": root_id}<br>            self._upload_index_as_wb_artifact(persist_dir, index_name, metadata)<br>        except Exception as e:<br>            # Silently ignore errors to not break user code<br>            self._print_upload_index_fail_message(e)<br>    # clear the default storage dir<br>    if _default_persist_dir:<br>        shutil.rmtree(persist_dir, ignore_errors=True)<br>``` |\n\n### load\\_storage\\_context [\\#](\\#llama_index.callbacks.wandb.WandbCallbackHandler.load_storage_context \"Permanent link\")\n\n```\nload_storage_context(artifact_url: str, index_download_dir: Union[str, None] = None) -> StorageContext\n\n```\n\nDownload an index from wandb and return a storage context.\n\nUse this storage context to load the index into memory using\n`load_index_from_storage`, `load_indices_from_storage` or\n`load_graph_from_storage` functions.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `artifact_url` | `str` | url of the artifact to download. The artifact url will<br>be of the form: `entity/project/index_name:version` and can be found in<br>the W&B UI. | _required_ |\n| `index_download_dir` | `Union[str, None]` | directory to download the index to. | `None` |\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-wandb/llama_index/callbacks/wandb/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>``` | ```<br>def load_storage_context(<br>    self, artifact_url: str, index_download_dir: Union[str, None] = None<br>) -> \"StorageContext\":<br>    \"\"\"Download an index from wandb and return a storage context.<br>    Use this storage context to load the index into memory using<br>    `load_index_from_storage`, `load_indices_from_storage` or<br>    `load_graph_from_storage` functions.<br>    Args:<br>        artifact_url (str): url of the artifact to download. The artifact url will<br>            be of the form: `entity/project/index_name:version` and can be found in<br>            the W&B UI.<br>        index_download_dir (Union[str, None]): directory to download the index to.<br>    \"\"\"<br>    from llama_index.core.storage.storage_context import StorageContext<br>    artifact = self._wandb.use_artifact(artifact_url, type=\"storage_context\")  # type: ignore[attr-defined]<br>    artifact_dir = artifact.download(root=index_download_dir)<br>    return StorageContext.from_defaults(persist_dir=artifact_dir)<br>``` |\n\n### finish [\\#](\\#llama_index.callbacks.wandb.WandbCallbackHandler.finish \"Permanent link\")\n\n```\nfinish() -> None\n\n```\n\nFinish the callback handler.\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-wandb/llama_index/callbacks/wandb/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>568<br>569<br>570<br>``` | ```<br>def finish(self) -> None:<br>    \"\"\"Finish the callback handler.\"\"\"<br>    self._wandb.finish()  # type: ignore[attr-defined]<br>``` |\n\nBack to top",
      "metadata": {
        "title": "Wandb - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/wandb/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/google/#llama_index.embeddings.google.GeminiEmbedding)\n\n# Google\n\n## GeminiEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/google/\\#llama_index.embeddings.google.GeminiEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nGoogle Gemini embeddings.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model_name` | `str` | Model for embedding.<br>Defaults to \"models/embedding-001\". | `'models/embedding-001'` |\n| `api_key` | `Optional[str]` | API key to access the model. Defaults to None. | `None` |\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-google/llama_index/embeddings/google/gemini.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>``` | ```<br>class GeminiEmbedding(BaseEmbedding):<br>    \"\"\"Google Gemini embeddings.<br>    Args:<br>        model_name (str): Model for embedding.<br>            Defaults to \"models/embedding-001\".<br>        api_key (Optional[str]): API key to access the model. Defaults to None.<br>    \"\"\"<br>    _model: Any = PrivateAttr()<br>    title: Optional[str] = Field(<br>        default=\"\",<br>        description=\"Title is only applicable for retrieval_document tasks, and is used to represent a document title. For other tasks, title is invalid.\",<br>    )<br>    task_type: Optional[str] = Field(<br>        default=\"retrieval_document\",<br>        description=\"The task for embedding model.\",<br>    )<br>    def __init__(<br>        self,<br>        model_name: str = \"models/embedding-001\",<br>        task_type: Optional[str] = \"retrieval_document\",<br>        api_key: Optional[str] = None,<br>        title: Optional[str] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>        **kwargs: Any,<br>    ):<br>        super().__init__(<br>            model_name=model_name,<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            title=title,<br>            task_type=task_type,<br>            **kwargs,<br>        )<br>        gemini.configure(api_key=api_key)<br>        self._model = gemini<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"GeminiEmbedding\"<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self._model.embed_content(<br>            model=self.model_name,<br>            content=query,<br>            title=self.title,<br>            task_type=self.task_type,<br>        )[\"embedding\"]<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._model.embed_content(<br>            model=self.model_name,<br>            content=text,<br>            title=self.title,<br>            task_type=self.task_type,<br>        )[\"embedding\"]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        return [<br>            self._model.embed_content(<br>                model=self.model_name,<br>                content=text,<br>                title=self.title,<br>                task_type=self.task_type,<br>            )[\"embedding\"]<br>            for text in texts<br>        ]<br>    ### Async methods ###<br>    # need to wait async calls from Gemini side to be implemented.<br>    # Issue: https://github.com/google/generative-ai-python/issues/125<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        return self._get_query_embedding(query)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously get text embedding.\"\"\"<br>        return self._get_text_embedding(text)<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Asynchronously get text embeddings.\"\"\"<br>        return self._get_text_embeddings(texts)<br>``` |\n\n## GooglePaLMEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/google/\\#llama_index.embeddings.google.GooglePaLMEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nClass for Google PaLM embeddings.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model_name` | `str` | Model for embedding.<br>Defaults to \"models/embedding-gecko-001\". | `'models/embedding-gecko-001'` |\n| `api_key` | `Optional[str]` | API key to access the model. Defaults to None. | `None` |\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-google/llama_index/embeddings/google/palm.py`\n\n|     |     |\n| --- | --- |\n| ```<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>``` | ```<br>class GooglePaLMEmbedding(BaseEmbedding):<br>    \"\"\"Class for Google PaLM embeddings.<br>    Args:<br>        model_name (str): Model for embedding.<br>            Defaults to \"models/embedding-gecko-001\".<br>        api_key (Optional[str]): API key to access the model. Defaults to None.<br>    \"\"\"<br>    _model: Any = PrivateAttr()<br>    def __init__(<br>        self,<br>        model_name: str = \"models/embedding-gecko-001\",<br>        api_key: Optional[str] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>        **kwargs: Any,<br>    ):<br>        super().__init__(<br>            model_name=model_name,<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            **kwargs,<br>        )<br>        palm.configure(api_key=api_key)<br>        self._model = palm<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"PaLMEmbedding\"<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self._model.generate_embeddings(model=self.model_name, text=query)[<br>            \"embedding\"<br>        ]<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        return await self._model.aget_embedding(query)<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._model.generate_embeddings(model=self.model_name, text=text)[<br>            \"embedding\"<br>        ]<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously get text embedding.\"\"\"<br>        return self._model._get_text_embedding(text)<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        return self._model.generate_embeddings(model=self.model_name, text=texts)[<br>            \"embedding\"<br>        ]<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Asynchronously get text embeddings.\"\"\"<br>        return await self._model._get_embeddings(texts)<br>``` |\n\n## GoogleUnivSentEncoderEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/google/\\#llama_index.embeddings.google.GoogleUnivSentEncoderEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-google/llama_index/embeddings/google/univ_sent_encoder.py`\n\n|     |     |\n| --- | --- |\n| ```<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>``` | ```<br>class GoogleUnivSentEncoderEmbedding(BaseEmbedding):<br>    _model: Any = PrivateAttr()<br>    def __init__(<br>        self,<br>        handle: Optional[str] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>    ):<br>        \"\"\"Init params.\"\"\"<br>        handle = handle or DEFAULT_HANDLE<br>        try:<br>            import tensorflow_hub as hub<br>            model = hub.load(handle)<br>        except ImportError:<br>            raise ImportError(<br>                \"Please install tensorflow_hub: `pip install tensorflow_hub`\"<br>            )<br>        super().__init__(<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            model_name=handle,<br>        )<br>        self._model = model<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"GoogleUnivSentEncoderEmbedding\"<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self._get_embedding(query)<br>    # TODO: use proper async methods<br>    async def _aget_text_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._get_embedding(query)<br>    # TODO: user proper async methods<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self._get_embedding(query)<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._get_embedding(text)<br>    def _get_embedding(self, text: str) -> List[float]:<br>        vectors = self._model([text]).numpy().tolist()<br>        return vectors[0]<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Google - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/google/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "# Textembed\n\nBack to top",
      "metadata": {
        "title": "Textembed - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/textembed/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "# Nvidia\n\nBack to top",
      "metadata": {
        "title": "Nvidia - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/nvidia/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "\n\n# Upstage\n\n## UpstageEmbedding [\\#](\\#llama_index.embeddings.upstage.UpstageEmbedding \"Permanent link\")\n\nBases: `OpenAIEmbedding`\n\nClass for Upstage embeddings.\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-upstage/llama_index/embeddings/upstage/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>``` | ```<br>class UpstageEmbedding(OpenAIEmbedding):<br>    \"\"\"<br>    Class for Upstage embeddings.<br>    \"\"\"<br>    additional_kwargs: Dict[str, Any] = Field(<br>        default_factory=dict, description=\"Additional kwargs for the Upstage API.\"<br>    )<br>    api_key: str = Field(description=\"The Upstage API key.\")<br>    api_base: Optional[str] = Field(<br>        default=DEFAULT_UPSTAGE_API_BASE, description=\"The base URL for Upstage API.\"<br>    )<br>    dimensions: Optional[int] = Field(<br>        None,<br>        description=\"Not supported yet. The number of dimensions the resulting output embeddings should have.\",<br>    )<br>    def __init__(<br>        self,<br>        model: str = \"solar-embedding-1-large\",<br>        embed_batch_size: int = 100,<br>        dimensions: Optional[int] = None,<br>        additional_kwargs: Dict[str, Any] = None,<br>        api_key: Optional[str] = None,<br>        api_base: Optional[str] = None,<br>        max_retries: int = 10,<br>        timeout: float = 60.0,<br>        reuse_client: bool = True,<br>        callback_manager: Optional[CallbackManager] = None,<br>        default_headers: Optional[Dict[str, str]] = None,<br>        http_client: Optional[httpx.Client] = None,<br>        **kwargs: Any,<br>    ) -> None:<br>        additional_kwargs = additional_kwargs or {}<br>        if dimensions is not None:<br>            warnings.warn(\"Received dimensions argument. This is not supported yet.\")<br>            additional_kwargs[\"dimensions\"] = dimensions<br>        if embed_batch_size > MAX_EMBED_BATCH_SIZE:<br>            raise ValueError(<br>                f\"embed_batch_size should be less than or equal to {MAX_EMBED_BATCH_SIZE}.\"<br>            )<br>        if \"upstage_api_key\" in kwargs:<br>            api_key = kwargs.pop(\"upstage_api_key\")<br>        api_key, api_base = resolve_upstage_credentials(<br>            api_key=api_key, api_base=api_base<br>        )<br>        if \"model_name\" in kwargs:<br>            model = kwargs.pop(\"model_name\")<br>        # if model endswith with \"-query\" or \"-passage\", remove the suffix and print a warning<br>        if model.endswith((\"-query\", \"-passage\")):<br>            model = model.rsplit(\"-\", 1)[0]<br>            logger.warning(<br>                f\"Model name should not end with '-query' or '-passage'. The suffix has been removed. \"<br>                f\"Model name: {model}\"<br>            )<br>        super().__init__(<br>            embed_batch_size=embed_batch_size,<br>            dimensions=dimensions,<br>            callback_manager=callback_manager,<br>            model_name=model,<br>            additional_kwargs=additional_kwargs,<br>            api_key=api_key,<br>            api_base=api_base,<br>            max_retries=max_retries,<br>            reuse_client=reuse_client,<br>            timeout=timeout,<br>            default_headers=default_headers,<br>            **kwargs,<br>        )<br>        self._client = None<br>        self._aclient = None<br>        self._http_client = http_client<br>        self._query_engine, self._text_engine = get_engine(model)<br>    def class_name(cls) -> str:<br>        return \"UpstageEmbedding\"<br>    def _get_credential_kwargs(self, is_async: bool = False) -> Dict[str, Any]:<br>        return {<br>            \"api_key\": self.api_key,<br>            \"base_url\": self.api_base,<br>            \"max_retries\": self.max_retries,<br>            \"timeout\": self.timeout,<br>            \"default_headers\": self.default_headers,<br>            \"http_client\": self._async_http_client if is_async else self._http_client,<br>        }<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        client = self._get_client()<br>        text = query.replace(\"\\n\", \" \")<br>        return (<br>            client.embeddings.create(<br>                input=text, model=self._query_engine, **self.additional_kwargs<br>            )<br>            .data[0]<br>            .embedding<br>        )<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        client = self._get_aclient()<br>        text = query.replace(\"\\n\", \" \")<br>        return (<br>            (<br>                await client.embeddings.create(<br>                    input=text, model=self._query_engine, **self.additional_kwargs<br>                )<br>            )<br>            .data[0]<br>            .embedding<br>        )<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        client = self._get_client()<br>        return (<br>            client.embeddings.create(<br>                input=text, model=self._text_engine, **self.additional_kwargs<br>            )<br>            .data[0]<br>            .embedding<br>        )<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously get text embedding.\"\"\"<br>        client = self._get_aclient()<br>        return (<br>            (<br>                await client.embeddings.create(<br>                    input=text, model=self._text_engine, **self.additional_kwargs<br>                )<br>            )<br>            .data[0]<br>            .embedding<br>        )<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        client = self._get_client()<br>        batch_size = min(self.embed_batch_size, len(texts))<br>        texts = [text.replace(\"\\n\", \" \") for text in texts]<br>        embeddings = []<br>        for i in range(0, len(texts), batch_size):<br>            batch = texts[i : i + batch_size]<br>            response = client.embeddings.create(<br>                input=batch, model=self._text_engine, **self.additional_kwargs<br>            )<br>            embeddings.extend([r.embedding for r in response.data])<br>        return embeddings<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Asynchronously get text embeddings.\"\"\"<br>        client = self._get_aclient()<br>        batch_size = min(self.embed_batch_size, len(texts))<br>        texts = [text.replace(\"\\n\", \" \") for text in texts]<br>        embeddings = []<br>        for i in range(0, len(texts), batch_size):<br>            batch = texts[i : i + batch_size]<br>            response = await client.embeddings.create(<br>                input=batch, model=self._text_engine, **self.additional_kwargs<br>            )<br>            embeddings.extend([r.embedding for r in response.data])<br>        return embeddings<br>``` |\n\nBack to top",
      "metadata": {
        "title": "Upstage - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/upstage/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/azure_openai/#llama_index.embeddings.azure_openai.AzureOpenAIEmbedding)\n\n# Azure openai\n\n## AzureOpenAIEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/azure_openai/\\#llama_index.embeddings.azure_openai.AzureOpenAIEmbedding \"Permanent link\")\n\nBases: `OpenAIEmbedding`\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-azure-openai/llama_index/embeddings/azure_openai/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>``` | ```<br>class AzureOpenAIEmbedding(OpenAIEmbedding):<br>    azure_endpoint: Optional[str] = Field(<br>        default=None, description=\"The Azure endpoint to use.\", validate_default=True<br>    )<br>    azure_deployment: Optional[str] = Field(<br>        default=None, description=\"The Azure deployment to use.\", validate_default=True<br>    )<br>    api_base: str = Field(<br>        default=\"\",<br>        description=\"The base URL for Azure deployment.\",<br>        validate_default=True,<br>    )<br>    api_version: str = Field(<br>        default=\"\",<br>        description=\"The version for Azure OpenAI API.\",<br>        validate_default=True,<br>    )<br>    azure_ad_token_provider: Optional[AnnotatedProvider] = Field(<br>        default=None,<br>        description=\"Callback function to provide Azure AD token.\",<br>        exclude=True,<br>    )<br>    use_azure_ad: bool = Field(<br>        description=\"Indicates if Microsoft Entra ID (former Azure AD) is used for token authentication\"<br>    )<br>    _azure_ad_token: Any = PrivateAttr(default=None)<br>    _client: AzureOpenAI = PrivateAttr()<br>    _aclient: AsyncAzureOpenAI = PrivateAttr()<br>    def __init__(<br>        self,<br>        mode: str = OpenAIEmbeddingMode.TEXT_SEARCH_MODE,<br>        model: str = OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        additional_kwargs: Optional[Dict[str, Any]] = None,<br>        api_key: Optional[str] = None,<br>        api_version: Optional[str] = None,<br>        # azure specific<br>        azure_endpoint: Optional[str] = None,<br>        azure_deployment: Optional[str] = None,<br>        azure_ad_token_provider: Optional[AzureADTokenProvider] = None,<br>        use_azure_ad: bool = False,<br>        deployment_name: Optional[str] = None,<br>        max_retries: int = 10,<br>        reuse_client: bool = True,<br>        callback_manager: Optional[CallbackManager] = None,<br>        num_workers: Optional[int] = None,<br>        # custom httpx client<br>        http_client: Optional[httpx.Client] = None,<br>        async_http_client: Optional[httpx.AsyncClient] = None,<br>        **kwargs: Any,<br>    ):<br>        azure_endpoint = get_from_param_or_env(<br>            \"azure_endpoint\", azure_endpoint, \"AZURE_OPENAI_ENDPOINT\", \"\"<br>        )<br>        azure_deployment = resolve_from_aliases(<br>            azure_deployment,<br>            deployment_name,<br>        )<br>        super().__init__(<br>            mode=mode,<br>            model=model,<br>            embed_batch_size=embed_batch_size,<br>            additional_kwargs=additional_kwargs,<br>            api_key=api_key,<br>            api_version=api_version,<br>            azure_endpoint=azure_endpoint,<br>            azure_deployment=azure_deployment,<br>            azure_ad_token_provider=azure_ad_token_provider,<br>            use_azure_ad=use_azure_ad,<br>            max_retries=max_retries,<br>            reuse_client=reuse_client,<br>            callback_manager=callback_manager,<br>            http_client=http_client,<br>            async_http_client=async_http_client,<br>            num_workers=num_workers,<br>            **kwargs,<br>        )<br>    @model_validator(mode=\"before\")<br>    @classmethod<br>    def validate_env(cls, values: Dict[str, Any]) -> Dict[str, Any]:<br>        \"\"\"Validate necessary credentials are set.\"\"\"<br>        if (<br>            values.get(\"api_base\") == \"https://api.openai.com/v1\"<br>            and values.get(\"azure_endpoint\") is None<br>        ):<br>            raise ValueError(<br>                \"You must set OPENAI_API_BASE to your Azure endpoint. \"<br>                \"It should look like https://YOUR_RESOURCE_NAME.openai.azure.com/\"<br>            )<br>        if values.get(\"api_version\") is None:<br>            raise ValueError(\"You must set OPENAI_API_VERSION for Azure OpenAI.\")<br>        return values<br>    def _get_client(self) -> AzureOpenAI:<br>        if not self.reuse_client:<br>            return AzureOpenAI(**self._get_credential_kwargs())<br>        if self._client is None:<br>            self._client = AzureOpenAI(**self._get_credential_kwargs())<br>        return self._client<br>    def _get_aclient(self) -> AsyncAzureOpenAI:<br>        if not self.reuse_client:<br>            return AsyncAzureOpenAI(**self._get_credential_kwargs(is_async=True))<br>        if self._aclient is None:<br>            self._aclient = AsyncAzureOpenAI(<br>                **self._get_credential_kwargs(is_async=True)<br>            )<br>        return self._aclient<br>    def _get_credential_kwargs(self, is_async: bool = False) -> Dict[str, Any]:<br>        if self.use_azure_ad:<br>            self._azure_ad_token = refresh_openai_azuread_token(self._azure_ad_token)<br>            self.api_key = self._azure_ad_token.token<br>        else:<br>            self.api_key = get_from_param_or_env(<br>                \"api_key\", self.api_key, \"AZURE_OPENAI_API_KEY\"<br>            )<br>        return {<br>            \"api_key\": self.api_key,<br>            \"azure_ad_token_provider\": self.azure_ad_token_provider,<br>            \"azure_endpoint\": self.azure_endpoint,<br>            \"azure_deployment\": self.azure_deployment,<br>            \"api_version\": self.api_version,<br>            \"default_headers\": self.default_headers,<br>            \"http_client\": self._async_http_client if is_async else self._http_client,<br>        }<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"AzureOpenAIEmbedding\"<br>``` |\n\n### validate\\_env`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/azure_openai/\\#llama_index.embeddings.azure_openai.AzureOpenAIEmbedding.validate_env \"Permanent link\")\n\n```\nvalidate_env(values: Dict[str, Any]) -> Dict[str, Any]\n\n```\n\nValidate necessary credentials are set.\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-azure-openai/llama_index/embeddings/azure_openai/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>``` | ```<br>@model_validator(mode=\"before\")<br>@classmethod<br>def validate_env(cls, values: Dict[str, Any]) -> Dict[str, Any]:<br>    \"\"\"Validate necessary credentials are set.\"\"\"<br>    if (<br>        values.get(\"api_base\") == \"https://api.openai.com/v1\"<br>        and values.get(\"azure_endpoint\") is None<br>    ):<br>        raise ValueError(<br>            \"You must set OPENAI_API_BASE to your Azure endpoint. \"<br>            \"It should look like https://YOUR_RESOURCE_NAME.openai.azure.com/\"<br>        )<br>    if values.get(\"api_version\") is None:<br>        raise ValueError(\"You must set OPENAI_API_VERSION for Azure OpenAI.\")<br>    return values<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Azure openai - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/azure_openai/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "# Huggingface itrex\n\nBack to top",
      "metadata": {
        "title": "Huggingface itrex - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/huggingface_itrex/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "\n\n# Databricks\n\n## DatabricksEmbedding [\\#](\\#llama_index.embeddings.databricks.DatabricksEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nDatabricks class for text embedding.\n\nDatabricks adheres to the OpenAI API, so this integration aligns closely with the existing OpenAIEmbedding class.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model` | `str` | The unique ID of the embedding model as served by the Databricks endpoint. | _required_ |\n| `endpoint` | `Optional[str]` | The url of the Databricks endpoint. Can be set as an environment variable ( `DATABRICKS_SERVING_ENDPOINT`). | `None` |\n| `api_key` | `Optional[str]` | The Databricks API key to use. Can be set as an environment variable ( `DATABRICKS_TOKEN`). | `None` |\n\n**Examples:**\n\n`pip install llama-index-embeddings-databricks`\n\n```\nimport os\nfrom llama_index.core import Settings\nfrom llama_index.embeddings.databricks import DatabricksEmbedding\n\n# Set up the DatabricksEmbedding class with the required model, API key and serving endpoint\nos.environ[\"DATABRICKS_TOKEN\"] = \"<MY TOKEN>\"\nos.environ[\"DATABRICKS_SERVING_ENDPOINT\"] = \"<MY ENDPOINT>\"\nembed_model  = DatabricksEmbedding(model=\"databricks-bge-large-en\")\nSettings.embed_model = embed_model\n\n# Embed some text\nembeddings = embed_model.get_text_embedding(\"The DatabricksEmbedding integration works great.\")\n\n```\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-databricks/llama_index/embeddings/databricks/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>``` | ````<br>class DatabricksEmbedding(BaseEmbedding):<br>    \"\"\"Databricks class for text embedding.<br>    Databricks adheres to the OpenAI API, so this integration aligns closely with the existing OpenAIEmbedding class.<br>    Args:<br>        model (str): The unique ID of the embedding model as served by the Databricks endpoint.<br>        endpoint (Optional[str]): The url of the Databricks endpoint. Can be set as an environment variable (`DATABRICKS_SERVING_ENDPOINT`).<br>        api_key (Optional[str]): The Databricks API key to use. Can be set as an environment variable (`DATABRICKS_TOKEN`).<br>    Examples:<br>        `pip install llama-index-embeddings-databricks`<br>        ```python<br>        import os<br>        from llama_index.core import Settings<br>        from llama_index.embeddings.databricks import DatabricksEmbedding<br>        # Set up the DatabricksEmbedding class with the required model, API key and serving endpoint<br>        os.environ[\"DATABRICKS_TOKEN\"] = \"<MY TOKEN>\"<br>        os.environ[\"DATABRICKS_SERVING_ENDPOINT\"] = \"<MY ENDPOINT>\"<br>        embed_model  = DatabricksEmbedding(model=\"databricks-bge-large-en\")<br>        Settings.embed_model = embed_model<br>        # Embed some text<br>        embeddings = embed_model.get_text_embedding(\"The DatabricksEmbedding integration works great.\")<br>        ```<br>    \"\"\"<br>    additional_kwargs: Dict[str, Any] = Field(<br>        default_factory=dict, description=\"Additional kwargs as for the OpenAI API.\"<br>    )<br>    model: str = Field(<br>        description=\"The ID of a model hosted on the databricks endpoint.\"<br>    )<br>    api_key: str = Field(description=\"The Databricks API key.\")<br>    endpoint: str = Field(description=\"The Databricks API endpoint.\")<br>    max_retries: int = Field(<br>        default=10, description=\"Maximum number of retries.\", gte=0<br>    )<br>    timeout: float = Field(default=60.0, description=\"Timeout for each request.\", gte=0)<br>    default_headers: Optional[Dict[str, str]] = Field(<br>        default=None, description=\"The default headers for API requests.\"<br>    )<br>    reuse_client: bool = Field(<br>        default=True,<br>        description=(<br>            \"Reuse the client between requests. When doing anything with large \"<br>            \"volumes of async API calls, setting this to false can improve stability.\"<br>        ),<br>    )<br>    _query_engine: str = PrivateAttr()<br>    _text_engine: str = PrivateAttr()<br>    _client: Optional[OpenAI] = PrivateAttr()<br>    _aclient: Optional[AsyncOpenAI] = PrivateAttr()<br>    _http_client: Optional[httpx.Client] = PrivateAttr()<br>    def __init__(<br>        self,<br>        model: str,<br>        endpoint: Optional[str] = None,<br>        embed_batch_size: int = 100,<br>        additional_kwargs: Optional[Dict[str, Any]] = None,<br>        api_key: Optional[str] = None,<br>        max_retries: int = 10,<br>        timeout: float = 60.0,<br>        reuse_client: bool = True,<br>        callback_manager: Optional[CallbackManager] = None,<br>        default_headers: Optional[Dict[str, str]] = None,<br>        http_client: Optional[httpx.Client] = None,<br>        num_workers: Optional[int] = None,<br>        **kwargs: Any,<br>    ) -> None:<br>        additional_kwargs = additional_kwargs or {}<br>        api_key = get_from_param_or_env(\"api_key\", api_key, \"DATABRICKS_TOKEN\")<br>        endpoint = get_from_param_or_env(<br>            \"endpoint\", endpoint, \"DATABRICKS_SERVING_ENDPOINT\"<br>        )<br>        super().__init__(<br>            model=model,<br>            endpoint=endpoint,<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            model_name=model,<br>            additional_kwargs=additional_kwargs,<br>            api_key=api_key,<br>            max_retries=max_retries,<br>            reuse_client=reuse_client,<br>            timeout=timeout,<br>            default_headers=default_headers,<br>            num_workers=num_workers,<br>            **kwargs,<br>        )<br>        self._client = None<br>        self._aclient = None<br>        self._http_client = http_client<br>    def _get_client(self) -> OpenAI:<br>        if not self.reuse_client:<br>            return OpenAI(**self._get_credential_kwargs())<br>        if self._client is None:<br>            self._client = OpenAI(**self._get_credential_kwargs())<br>        return self._client<br>    def _get_aclient(self) -> AsyncOpenAI:<br>        if not self.reuse_client:<br>            return AsyncOpenAI(**self._get_credential_kwargs())<br>        if self._aclient is None:<br>            self._aclient = AsyncOpenAI(**self._get_credential_kwargs())<br>        return self._aclient<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"DatabricksEmbedding\"<br>    def _get_credential_kwargs(self) -> Dict[str, Any]:<br>        return {<br>            \"api_key\": self.api_key,<br>            \"base_url\": self.endpoint,<br>            \"max_retries\": self.max_retries,<br>            \"timeout\": self.timeout,<br>            \"default_headers\": self.default_headers,<br>            \"http_client\": self._http_client,<br>        }<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        client = self._get_client()<br>        return get_embedding(<br>            client,<br>            query,<br>            engine=self.model,<br>            **self.additional_kwargs,<br>        )<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        aclient = self._get_aclient()<br>        return await aget_embedding(<br>            aclient,<br>            query,<br>            engine=self.model,<br>            **self.additional_kwargs,<br>        )<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        client = self._get_client()<br>        return get_embedding(<br>            client,<br>            text,<br>            engine=self.model,<br>            **self.additional_kwargs,<br>        )<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously get text embedding.\"\"\"<br>        aclient = self._get_aclient()<br>        return await aget_embedding(<br>            aclient,<br>            text,<br>            engine=self.model,<br>            **self.additional_kwargs,<br>        )<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.<br>        By default, this is a wrapper around _get_text_embedding.<br>        Can be overridden for batch queries.<br>        \"\"\"<br>        client = self._get_client()<br>        return get_embeddings(<br>            client,<br>            texts,<br>            engine=self.model,<br>            **self.additional_kwargs,<br>        )<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Asynchronously get text embeddings.\"\"\"<br>        aclient = self._get_aclient()<br>        return await aget_embeddings(<br>            aclient,<br>            texts,<br>            engine=self.model,<br>            **self.additional_kwargs,<br>        )<br>```` |\n\nBack to top",
      "metadata": {
        "title": "Databricks - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/databricks/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/tonic_validate/#llama_index.evaluation.tonic_validate.AnswerConsistencyBinaryEvaluator)\n\n# Tonic validate\n\n## AnswerConsistencyBinaryEvaluator [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/tonic_validate/\\#llama_index.evaluation.tonic_validate.AnswerConsistencyBinaryEvaluator \"Permanent link\")\n\nBases: `BaseEvaluator`\n\nTonic Validate's answer consistency binary metric.\n\nThe output score is a float that is either 0.0 or 1.0.\n\nSee https://docs.tonic.ai/validate/ for more details.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `openai_service(OpenAIService)` |  | The OpenAI service to use. Specifies the chat<br>completion model to use as the LLM evaluator. Defaults to \"gpt-4\". | _required_ |\n\nSource code in `llama-index-integrations/evaluation/llama-index-evaluation-tonic-validate/llama_index/evaluation/tonic_validate/answer_consistency_binary.py`\n\n|     |     |\n| --- | --- |\n| ```<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>``` | ```<br>class AnswerConsistencyBinaryEvaluator(BaseEvaluator):<br>    \"\"\"Tonic Validate's answer consistency binary metric.<br>    The output score is a float that is either 0.0 or 1.0.<br>    See https://docs.tonic.ai/validate/ for more details.<br>    Args:<br>        openai_service(OpenAIService): The OpenAI service to use. Specifies the chat<br>            completion model to use as the LLM evaluator. Defaults to \"gpt-4\".<br>    \"\"\"<br>    def __init__(self, openai_service: Optional[Any] = None):<br>        if openai_service is None:<br>            openai_service = OpenAIService(\"gpt-4\")<br>        self.openai_service = openai_service<br>        self.metric = AnswerConsistencyBinaryMetric()<br>    async def aevaluate(<br>        self,<br>        query: Optional[str] = None,<br>        response: Optional[str] = None,<br>        contexts: Optional[Sequence[str]] = None,<br>        **kwargs: Any<br>    ) -> EvaluationResult:<br>        from tonic_validate.classes.benchmark import BenchmarkItem<br>        from tonic_validate.classes.llm_response import LLMResponse<br>        benchmark_item = BenchmarkItem(question=query)<br>        llm_response = LLMResponse(<br>            llm_answer=response,<br>            llm_context_list=contexts,<br>            benchmark_item=benchmark_item,<br>        )<br>        score = self.metric.score(llm_response, self.openai_service)<br>        return EvaluationResult(<br>            query=query, contexts=contexts, response=response, score=score<br>        )<br>    def _get_prompts(self) -> PromptDictType:<br>        return {}<br>    def _get_prompt_modules(self) -> PromptMixinType:<br>        return {}<br>    def _update_prompts(self, prompts_dict: PromptDictType) -> None:<br>        return<br>``` |\n\n## AnswerConsistencyEvaluator [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/tonic_validate/\\#llama_index.evaluation.tonic_validate.AnswerConsistencyEvaluator \"Permanent link\")\n\nBases: `BaseEvaluator`\n\nTonic Validate's answer consistency metric.\n\nThe output score is a float between 0.0 and 1.0.\n\nSee https://docs.tonic.ai/validate/ for more details.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `openai_service(OpenAIService)` |  | The OpenAI service to use. Specifies the chat<br>completion model to use as the LLM evaluator. Defaults to \"gpt-4\". | _required_ |\n\nSource code in `llama-index-integrations/evaluation/llama-index-evaluation-tonic-validate/llama_index/evaluation/tonic_validate/answer_consistency.py`\n\n|     |     |\n| --- | --- |\n| ```<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>``` | ```<br>class AnswerConsistencyEvaluator(BaseEvaluator):<br>    \"\"\"Tonic Validate's answer consistency metric.<br>    The output score is a float between 0.0 and 1.0.<br>    See https://docs.tonic.ai/validate/ for more details.<br>    Args:<br>        openai_service(OpenAIService): The OpenAI service to use. Specifies the chat<br>            completion model to use as the LLM evaluator. Defaults to \"gpt-4\".<br>    \"\"\"<br>    def __init__(self, openai_service: Optional[Any] = None):<br>        if openai_service is None:<br>            openai_service = OpenAIService(\"gpt-4\")<br>        self.openai_service = openai_service<br>        self.metric = AnswerConsistencyMetric()<br>    async def aevaluate(<br>        self,<br>        query: Optional[str] = None,<br>        response: Optional[str] = None,<br>        contexts: Optional[Sequence[str]] = None,<br>        **kwargs: Any<br>    ) -> EvaluationResult:<br>        from tonic_validate.classes.benchmark import BenchmarkItem<br>        from tonic_validate.classes.llm_response import LLMResponse<br>        benchmark_item = BenchmarkItem(question=query)<br>        llm_response = LLMResponse(<br>            llm_answer=response,<br>            llm_context_list=contexts,<br>            benchmark_item=benchmark_item,<br>        )<br>        score = self.metric.score(llm_response, self.openai_service)<br>        return EvaluationResult(<br>            query=query, contexts=contexts, response=response, score=score<br>        )<br>    def _get_prompts(self) -> PromptDictType:<br>        return {}<br>    def _get_prompt_modules(self) -> PromptMixinType:<br>        return {}<br>    def _update_prompts(self, prompts_dict: PromptDictType) -> None:<br>        return<br>``` |\n\n## AnswerSimilarityEvaluator [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/tonic_validate/\\#llama_index.evaluation.tonic_validate.AnswerSimilarityEvaluator \"Permanent link\")\n\nBases: `BaseEvaluator`\n\nTonic Validate's answer similarity metric.\n\nThe output score is a float between 0.0 and 5.0.\n\nSee https://docs.tonic.ai/validate/ for more details.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `openai_service(OpenAIService)` |  | The OpenAI service to use. Specifies the chat<br>completion model to use as the LLM evaluator. Defaults to \"gpt-4\". | _required_ |\n\nSource code in `llama-index-integrations/evaluation/llama-index-evaluation-tonic-validate/llama_index/evaluation/tonic_validate/answer_similarity.py`\n\n|     |     |\n| --- | --- |\n| ```<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>``` | ```<br>class AnswerSimilarityEvaluator(BaseEvaluator):<br>    \"\"\"Tonic Validate's answer similarity metric.<br>    The output score is a float between 0.0 and 5.0.<br>    See https://docs.tonic.ai/validate/ for more details.<br>    Args:<br>        openai_service(OpenAIService): The OpenAI service to use. Specifies the chat<br>            completion model to use as the LLM evaluator. Defaults to \"gpt-4\".<br>    \"\"\"<br>    def __init__(self, openai_service: Optional[Any] = None):<br>        if openai_service is None:<br>            openai_service = OpenAIService(\"gpt-4\")<br>        self.openai_service = openai_service<br>        self.metric = AnswerSimilarityMetric()<br>    async def aevaluate(<br>        self,<br>        query: Optional[str] = None,<br>        response: Optional[str] = None,<br>        contexts: Optional[Sequence[str]] = None,<br>        reference_response: Optional[str] = None,<br>        **kwargs: Any<br>    ) -> EvaluationResult:<br>        from tonic_validate.classes.benchmark import BenchmarkItem<br>        from tonic_validate.classes.llm_response import LLMResponse<br>        benchmark_item = BenchmarkItem(question=query, answer=reference_response)<br>        llm_response = LLMResponse(<br>            llm_answer=response,<br>            llm_context_list=contexts,<br>            benchmark_item=benchmark_item,<br>        )<br>        score = self.metric.score(llm_response, self.openai_service)<br>        return EvaluationResult(<br>            query=query, contexts=contexts, response=response, score=score<br>        )<br>    def _get_prompts(self) -> PromptDictType:<br>        return {}<br>    def _get_prompt_modules(self) -> PromptMixinType:<br>        return {}<br>    def _update_prompts(self, prompts_dict: PromptDictType) -> None:<br>        return<br>``` |\n\n## AugmentationAccuracyEvaluator [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/tonic_validate/\\#llama_index.evaluation.tonic_validate.AugmentationAccuracyEvaluator \"Permanent link\")\n\nBases: `BaseEvaluator`\n\nTonic Validate's augmentation accuracy metric.\n\nThe output score is a float between 0.0 and 1.0.\n\nSee https://docs.tonic.ai/validate/ for more details.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `openai_service(OpenAIService)` |  | The OpenAI service to use. Specifies the chat<br>completion model to use as the LLM evaluator. Defaults to \"gpt-4\". | _required_ |\n\nSource code in `llama-index-integrations/evaluation/llama-index-evaluation-tonic-validate/llama_index/evaluation/tonic_validate/augmentation_accuracy.py`\n\n|     |     |\n| --- | --- |\n| ```<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>``` | ```<br>class AugmentationAccuracyEvaluator(BaseEvaluator):<br>    \"\"\"Tonic Validate's augmentation accuracy metric.<br>    The output score is a float between 0.0 and 1.0.<br>    See https://docs.tonic.ai/validate/ for more details.<br>    Args:<br>        openai_service(OpenAIService): The OpenAI service to use. Specifies the chat<br>            completion model to use as the LLM evaluator. Defaults to \"gpt-4\".<br>    \"\"\"<br>    def __init__(self, openai_service: Optional[Any] = None):<br>        if openai_service is None:<br>            openai_service = OpenAIService(\"gpt-4\")<br>        self.openai_service = openai_service<br>        self.metric = AugmentationAccuracyMetric()<br>    async def aevaluate(<br>        self,<br>        query: Optional[str] = None,<br>        response: Optional[str] = None,<br>        contexts: Optional[Sequence[str]] = None,<br>        **kwargs: Any<br>    ) -> EvaluationResult:<br>        from tonic_validate.classes.benchmark import BenchmarkItem<br>        from tonic_validate.classes.llm_response import LLMResponse<br>        benchmark_item = BenchmarkItem(question=query)<br>        llm_response = LLMResponse(<br>            llm_answer=response,<br>            llm_context_list=contexts,<br>            benchmark_item=benchmark_item,<br>        )<br>        score = self.metric.score(llm_response, self.openai_service)<br>        return EvaluationResult(<br>            query=query, contexts=contexts, response=response, score=score<br>        )<br>    def _get_prompts(self) -> PromptDictType:<br>        return {}<br>    def _get_prompt_modules(self) -> PromptMixinType:<br>        return {}<br>    def _update_prompts(self, prompts_dict: PromptDictType) -> None:<br>        return<br>``` |\n\n## AugmentationPrecisionEvaluator [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/tonic_validate/\\#llama_index.evaluation.tonic_validate.AugmentationPrecisionEvaluator \"Permanent link\")\n\nBases: `BaseEvaluator`\n\nTonic Validate's augmentation precision metric.\n\nThe output score is a float between 0.0 and 1.0.\n\nSee https://docs.tonic.ai/validate/ for more details.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `openai_service(OpenAIService)` |  | The OpenAI service to use. Specifies the chat<br>completion model to use as the LLM evaluator. Defaults to \"gpt-4\". | _required_ |\n\nSource code in `llama-index-integrations/evaluation/llama-index-evaluation-tonic-validate/llama_index/evaluation/tonic_validate/augmentation_precision.py`\n\n|     |     |\n| --- | --- |\n| ```<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>``` | ```<br>class AugmentationPrecisionEvaluator(BaseEvaluator):<br>    \"\"\"Tonic Validate's augmentation precision metric.<br>    The output score is a float between 0.0 and 1.0.<br>    See https://docs.tonic.ai/validate/ for more details.<br>    Args:<br>        openai_service(OpenAIService): The OpenAI service to use. Specifies the chat<br>            completion model to use as the LLM evaluator. Defaults to \"gpt-4\".<br>    \"\"\"<br>    def __init__(self, openai_service: Optional[Any] = None):<br>        if openai_service is None:<br>            openai_service = OpenAIService(\"gpt-4\")<br>        self.openai_service = openai_service<br>        self.metric = AugmentationPrecisionMetric()<br>    async def aevaluate(<br>        self,<br>        query: Optional[str] = None,<br>        response: Optional[str] = None,<br>        contexts: Optional[Sequence[str]] = None,<br>        **kwargs: Any<br>    ) -> EvaluationResult:<br>        from tonic_validate.classes.benchmark import BenchmarkItem<br>        from tonic_validate.classes.llm_response import LLMResponse<br>        benchmark_item = BenchmarkItem(question=query)<br>        llm_response = LLMResponse(<br>            llm_answer=response,<br>            llm_context_list=contexts,<br>            benchmark_item=benchmark_item,<br>        )<br>        score = self.metric.score(llm_response, self.openai_service)<br>        return EvaluationResult(<br>            query=query, contexts=contexts, response=response, score=score<br>        )<br>    def _get_prompts(self) -> PromptDictType:<br>        return {}<br>    def _get_prompt_modules(self) -> PromptMixinType:<br>        return {}<br>    def _update_prompts(self, prompts_dict: PromptDictType) -> None:<br>        return<br>``` |\n\n## RetrievalPrecisionEvaluator [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/tonic_validate/\\#llama_index.evaluation.tonic_validate.RetrievalPrecisionEvaluator \"Permanent link\")\n\nBases: `BaseEvaluator`\n\nTonic Validate's retrieval precision metric.\n\nThe output score is a float between 0.0 and 1.0.\n\nSee https://docs.tonic.ai/validate/ for more details.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `openai_service(OpenAIService)` |  | The OpenAI service to use. Specifies the chat<br>completion model to use as the LLM evaluator. Defaults to \"gpt-4\". | _required_ |\n\nSource code in `llama-index-integrations/evaluation/llama-index-evaluation-tonic-validate/llama_index/evaluation/tonic_validate/retrieval_precision.py`\n\n|     |     |\n| --- | --- |\n| ```<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>``` | ```<br>class RetrievalPrecisionEvaluator(BaseEvaluator):<br>    \"\"\"Tonic Validate's retrieval precision metric.<br>    The output score is a float between 0.0 and 1.0.<br>    See https://docs.tonic.ai/validate/ for more details.<br>    Args:<br>        openai_service(OpenAIService): The OpenAI service to use. Specifies the chat<br>            completion model to use as the LLM evaluator. Defaults to \"gpt-4\".<br>    \"\"\"<br>    def __init__(self, openai_service: Optional[Any] = None):<br>        if openai_service is None:<br>            openai_service = OpenAIService(\"gpt-4\")<br>        self.openai_service = openai_service<br>        self.metric = RetrievalPrecisionMetric()<br>    async def aevaluate(<br>        self,<br>        query: Optional[str] = None,<br>        response: Optional[str] = None,<br>        contexts: Optional[Sequence[str]] = None,<br>        **kwargs: Any<br>    ) -> EvaluationResult:<br>        from tonic_validate.classes.benchmark import BenchmarkItem<br>        from tonic_validate.classes.llm_response import LLMResponse<br>        benchmark_item = BenchmarkItem(question=query, answer=response)<br>        llm_response = LLMResponse(<br>            llm_answer=response,<br>            llm_context_list=contexts,<br>            benchmark_item=benchmark_item,<br>        )<br>        score = self.metric.score(llm_response, self.openai_service)<br>        return EvaluationResult(<br>            query=query, contexts=contexts, response=response, score=score<br>        )<br>    def _get_prompts(self) -> PromptDictType:<br>        return {}<br>    def _get_prompt_modules(self) -> PromptMixinType:<br>        return {}<br>    def _update_prompts(self, prompts_dict: PromptDictType) -> None:<br>        return<br>``` |\n\n## TonicValidateEvaluator [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/tonic_validate/\\#llama_index.evaluation.tonic_validate.TonicValidateEvaluator \"Permanent link\")\n\nBases: `BaseEvaluator`\n\nTonic Validate's validate scorer. Calculates all of Tonic Validate's metrics.\n\nSee https://docs.tonic.ai/validate/ for more details.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `metrics(List[Metric])` |  | The metrics to use. Defaults to all of Tonic Validate's<br>metrics. | _required_ |\n| `model_evaluator(str)` |  | The OpenAI service to use. Specifies the chat completion<br>model to use as the LLM evaluator. Defaults to \"gpt-4\". | _required_ |\n\nSource code in `llama-index-integrations/evaluation/llama-index-evaluation-tonic-validate/llama_index/evaluation/tonic_validate/tonic_validate_evaluator.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>``` | ```<br>class TonicValidateEvaluator(BaseEvaluator):<br>    \"\"\"Tonic Validate's validate scorer. Calculates all of Tonic Validate's metrics.<br>    See https://docs.tonic.ai/validate/ for more details.<br>    Args:<br>        metrics(List[Metric]): The metrics to use. Defaults to all of Tonic Validate's<br>            metrics.<br>        model_evaluator(str): The OpenAI service to use. Specifies the chat completion<br>            model to use as the LLM evaluator. Defaults to \"gpt-4\".<br>    \"\"\"<br>    def __init__(<br>        self, metrics: Optional[List[Any]] = None, model_evaluator: str = \"gpt-4\"<br>    ):<br>        if metrics is None:<br>            metrics = [<br>                AnswerConsistencyMetric(),<br>                AnswerSimilarityMetric(),<br>                AugmentationAccuracyMetric(),<br>                AugmentationPrecisionMetric(),<br>                RetrievalPrecisionMetric(),<br>            ]<br>        self.metrics = metrics<br>        self.model_evaluator = model_evaluator<br>        self.validate_scorer = ValidateScorer(metrics, model_evaluator)<br>    def _calculate_average_score(self, run: Any) -> float:<br>        from tonic_validate.metrics.answer_similarity_metric import (<br>            AnswerSimilarityMetric,<br>        )<br>        ave_score = 0.0<br>        metric_cnt = 0<br>        for metric_name, score in run.overall_scores.items():<br>            if metric_name == AnswerSimilarityMetric.name:<br>                ave_score += score / 5<br>            else:<br>                ave_score += score<br>            metric_cnt += 1<br>        return ave_score / metric_cnt<br>    async def aevaluate(<br>        self,<br>        query: Optional[str] = None,<br>        response: Optional[str] = None,<br>        contexts: Optional[Sequence[str]] = None,<br>        reference_response: Optional[str] = None,<br>        **kwargs: Any,<br>    ) -> TonicValidateEvaluationResult:<br>        from tonic_validate.classes.benchmark import BenchmarkItem<br>        from tonic_validate.classes.llm_response import LLMResponse<br>        benchmark_item = BenchmarkItem(question=query, answer=reference_response)<br>        llm_response = LLMResponse(<br>            llm_answer=response,<br>            llm_context_list=contexts,<br>            benchmark_item=benchmark_item,<br>        )<br>        responses = [llm_response]<br>        run = self.validate_scorer.score_run(responses)<br>        ave_score = self._calculate_average_score(run)<br>        return TonicValidateEvaluationResult(<br>            query=query,<br>            contexts=contexts,<br>            response=response,<br>            score=ave_score,<br>            score_dict=run.run_data[0].scores,<br>        )<br>    async def aevaluate_run(<br>        self,<br>        queries: List[str],<br>        responses: List[str],<br>        contexts_list: List[List[str]],<br>        reference_responses: List[str],<br>        **kwargs: Any,<br>    ) -> Any:<br>        \"\"\"Evaluates a batch of responses.<br>        Returns a Tonic Validate Run object, which can be logged to the Tonic Validate<br>        UI. See https://docs.tonic.ai/validate/ for more details.<br>        \"\"\"<br>        from tonic_validate.classes.benchmark import BenchmarkItem<br>        from tonic_validate.classes.llm_response import LLMResponse<br>        llm_responses = []<br>        for query, response, contexts, reference_response in zip(<br>            queries, responses, contexts_list, reference_responses<br>        ):<br>            benchmark_item = BenchmarkItem(question=query, answer=reference_response)<br>            llm_response = LLMResponse(<br>                llm_answer=response,<br>                llm_context_list=contexts,<br>                benchmark_item=benchmark_item,<br>            )<br>            llm_responses.append(llm_response)<br>        return self.validate_scorer.score_run(llm_responses)<br>    def evaluate_run(<br>        self,<br>        queries: List[str],<br>        responses: List[str],<br>        contexts_list: List[List[str]],<br>        reference_responses: List[str],<br>        **kwargs: Any,<br>    ) -> Any:<br>        \"\"\"Evaluates a batch of responses.<br>        Returns a Tonic Validate Run object, which can be logged to the Tonic Validate<br>        UI. See https://docs.tonic.ai/validate/ for more details.<br>        \"\"\"<br>        return asyncio.run(<br>            self.aevaluate_run(<br>                queries=queries,<br>                responses=responses,<br>                contexts_list=contexts_list,<br>                reference_responses=reference_responses,<br>                **kwargs,<br>            )<br>        )<br>    def _get_prompts(self) -> PromptDictType:<br>        return {}<br>    def _get_prompt_modules(self) -> PromptMixinType:<br>        return {}<br>    def _update_prompts(self, prompts_dict: PromptDictType) -> None:<br>        return<br>``` |\n\n### aevaluate\\_run`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/tonic_validate/\\#llama_index.evaluation.tonic_validate.TonicValidateEvaluator.aevaluate_run \"Permanent link\")\n\n```\naevaluate_run(queries: List[str], responses: List[str], contexts_list: List[List[str]], reference_responses: List[str], **kwargs: Any) -> Any\n\n```\n\nEvaluates a batch of responses.\n\nReturns a Tonic Validate Run object, which can be logged to the Tonic Validate\nUI. See https://docs.tonic.ai/validate/ for more details.\n\nSource code in `llama-index-integrations/evaluation/llama-index-evaluation-tonic-validate/llama_index/evaluation/tonic_validate/tonic_validate_evaluator.py`\n\n|     |     |\n| --- | --- |\n| ```<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>``` | ```<br>async def aevaluate_run(<br>    self,<br>    queries: List[str],<br>    responses: List[str],<br>    contexts_list: List[List[str]],<br>    reference_responses: List[str],<br>    **kwargs: Any,<br>) -> Any:<br>    \"\"\"Evaluates a batch of responses.<br>    Returns a Tonic Validate Run object, which can be logged to the Tonic Validate<br>    UI. See https://docs.tonic.ai/validate/ for more details.<br>    \"\"\"<br>    from tonic_validate.classes.benchmark import BenchmarkItem<br>    from tonic_validate.classes.llm_response import LLMResponse<br>    llm_responses = []<br>    for query, response, contexts, reference_response in zip(<br>        queries, responses, contexts_list, reference_responses<br>    ):<br>        benchmark_item = BenchmarkItem(question=query, answer=reference_response)<br>        llm_response = LLMResponse(<br>            llm_answer=response,<br>            llm_context_list=contexts,<br>            benchmark_item=benchmark_item,<br>        )<br>        llm_responses.append(llm_response)<br>    return self.validate_scorer.score_run(llm_responses)<br>``` |\n\n### evaluate\\_run [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/tonic_validate/\\#llama_index.evaluation.tonic_validate.TonicValidateEvaluator.evaluate_run \"Permanent link\")\n\n```\nevaluate_run(queries: List[str], responses: List[str], contexts_list: List[List[str]], reference_responses: List[str], **kwargs: Any) -> Any\n\n```\n\nEvaluates a batch of responses.\n\nReturns a Tonic Validate Run object, which can be logged to the Tonic Validate\nUI. See https://docs.tonic.ai/validate/ for more details.\n\nSource code in `llama-index-integrations/evaluation/llama-index-evaluation-tonic-validate/llama_index/evaluation/tonic_validate/tonic_validate_evaluator.py`\n\n|     |     |\n| --- | --- |\n| ```<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>``` | ```<br>def evaluate_run(<br>    self,<br>    queries: List[str],<br>    responses: List[str],<br>    contexts_list: List[List[str]],<br>    reference_responses: List[str],<br>    **kwargs: Any,<br>) -> Any:<br>    \"\"\"Evaluates a batch of responses.<br>    Returns a Tonic Validate Run object, which can be logged to the Tonic Validate<br>    UI. See https://docs.tonic.ai/validate/ for more details.<br>    \"\"\"<br>    return asyncio.run(<br>        self.aevaluate_run(<br>            queries=queries,<br>            responses=responses,<br>            contexts_list=contexts_list,<br>            reference_responses=reference_responses,<br>            **kwargs,<br>        )<br>    )<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Tonic validate - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/evaluation/tonic_validate/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "# Octoai\n\nBack to top",
      "metadata": {
        "title": "Octoai - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/octoai/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/fastembed/#llama_index.embeddings.fastembed.FastEmbedEmbedding)\n\n# Fastembed\n\n## FastEmbedEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/fastembed/\\#llama_index.embeddings.fastembed.FastEmbedEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nQdrant FastEmbedding models.\nFastEmbed is a lightweight, fast, Python library built for embedding generation.\nSee more documentation at:\n\\\\* https://github.com/qdrant/fastembed/\n\\\\* https://qdrant.github.io/fastembed/.\n\nTo use this class, you must install the `fastembed` Python package.\n\n`pip install fastembed`\nExample:\nfrom llama\\_index.embeddings.fastembed import FastEmbedEmbedding\nfastembed = FastEmbedEmbedding()\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-fastembed/llama_index/embeddings/fastembed/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>89<br>90<br>91<br>92<br>93<br>94<br>95<br>96<br>97<br>98<br>99<br>``` | ```<br>class FastEmbedEmbedding(BaseEmbedding):<br>    \"\"\"<br>    Qdrant FastEmbedding models.<br>    FastEmbed is a lightweight, fast, Python library built for embedding generation.<br>    See more documentation at:<br>    * https://github.com/qdrant/fastembed/<br>    * https://qdrant.github.io/fastembed/.<br>    To use this class, you must install the `fastembed` Python package.<br>    `pip install fastembed`<br>    Example:<br>        from llama_index.embeddings.fastembed import FastEmbedEmbedding<br>        fastembed = FastEmbedEmbedding()<br>    \"\"\"<br>    model_name: str = Field(<br>        \"BAAI/bge-small-en-v1.5\",<br>        description=\"Name of the FastEmbedding model to use.\\n\"<br>        \"Defaults to 'BAAI/bge-small-en-v1.5'.\\n\"<br>        \"Find the list of supported models at \"<br>        \"https://qdrant.github.io/fastembed/examples/Supported_Models/\",<br>    )<br>    max_length: int = Field(<br>        512,<br>        description=\"The maximum number of tokens. Defaults to 512.\\n\"<br>        \"Unknown behavior for values > 512.\",<br>    )<br>    cache_dir: Optional[str] = Field(<br>        None,<br>        description=\"The path to the cache directory.\\n\"<br>        \"Defaults to `local_cache` in the parent directory\",<br>    )<br>    threads: Optional[int] = Field(<br>        None,<br>        description=\"The number of threads single onnxruntime session can use.\\n\"<br>        \"Defaults to None\",<br>    )<br>    doc_embed_type: Literal[\"default\", \"passage\"] = Field(<br>        \"default\",<br>        description=\"Type of embedding method to use for documents.\\n\"<br>        \"Available options are 'default' and 'passage'.\",<br>    )<br>    _model: Any = PrivateAttr()<br>    @classmethod<br>    def class_name(self) -> str:<br>        return \"FastEmbedEmbedding\"<br>    def __init__(<br>        self,<br>        model_name: Optional[str] = \"BAAI/bge-small-en-v1.5\",<br>        max_length: Optional[int] = 512,<br>        cache_dir: Optional[str] = None,<br>        threads: Optional[int] = None,<br>        doc_embed_type: Literal[\"default\", \"passage\"] = \"default\",<br>    ):<br>        super().__init__(<br>            model_name=model_name,<br>            max_length=max_length,<br>            threads=threads,<br>            doc_embed_type=doc_embed_type,<br>        )<br>        self._model = TextEmbedding(<br>            model_name=model_name,<br>            max_length=max_length,<br>            cache_dir=cache_dir,<br>            threads=threads,<br>        )<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        embeddings: List[np.ndarray]<br>        if self.doc_embed_type == \"passage\":<br>            embeddings = list(self._model.passage_embed(text))<br>        else:<br>            embeddings = list(self._model.embed(text))<br>        return embeddings[0].tolist()<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        query_embeddings: np.ndarray = next(self._model.query_embed(query))<br>        return query_embeddings.tolist()<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        return self._get_query_embedding(query)<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Fastembed - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/fastembed/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/#llama_index.core.chat_engine.types.ChatResponseMode)\n\n# Index\n\n## ChatResponseMode [\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.ChatResponseMode \"Permanent link\")\n\nBases: `str`, `Enum`\n\nFlag toggling waiting/streaming in `Agent._chat`.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>40<br>41<br>42<br>43<br>44<br>``` | ```<br>class ChatResponseMode(str, Enum):<br>    \"\"\"Flag toggling waiting/streaming in `Agent._chat`.\"\"\"<br>    WAIT = \"wait\"<br>    STREAM = \"stream\"<br>``` |\n\n## AgentChatResponse`dataclass`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.AgentChatResponse \"Permanent link\")\n\nAgent chat response.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>89<br>90<br>91<br>92<br>``` | ```<br>@dataclass<br>class AgentChatResponse:<br>    \"\"\"Agent chat response.\"\"\"<br>    response: str = \"\"<br>    sources: List[ToolOutput] = field(default_factory=list)<br>    source_nodes: List[NodeWithScore] = field(default_factory=list)<br>    is_dummy_stream: bool = False<br>    metadata: Optional[Dict[str, Any]] = None<br>    def set_source_nodes(self) -> None:<br>        if self.sources and not self.source_nodes:<br>            for tool_output in self.sources:<br>                if isinstance(tool_output.raw_output, (Response, StreamingResponse)):<br>                    self.source_nodes.extend(tool_output.raw_output.source_nodes)<br>    def __post_init__(self) -> None:<br>        self.set_source_nodes()<br>    def __str__(self) -> str:<br>        return self.response<br>    @property<br>    def response_gen(self) -> Generator[str, None, None]:<br>        \"\"\"Used for fake streaming, i.e. with tool outputs.\"\"\"<br>        if not self.is_dummy_stream:<br>            raise ValueError(<br>                \"response_gen is only available for streaming responses. \"<br>                \"Set is_dummy_stream=True if you still want a generator.\"<br>            )<br>        for token in self.response.split(\" \"):<br>            yield token + \" \"<br>            time.sleep(0.1)<br>    async def async_response_gen(self) -> AsyncGenerator[str, None]:<br>        \"\"\"Used for fake streaming, i.e. with tool outputs.\"\"\"<br>        if not self.is_dummy_stream:<br>            raise ValueError(<br>                \"response_gen is only available for streaming responses. \"<br>                \"Set is_dummy_stream=True if you still want a generator.\"<br>            )<br>        for token in self.response.split(\" \"):<br>            yield token + \" \"<br>            await asyncio.sleep(0.1)<br>``` |\n\n### response\\_gen`property`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.AgentChatResponse.response_gen \"Permanent link\")\n\n```\nresponse_gen: Generator[str, None, None]\n\n```\n\nUsed for fake streaming, i.e. with tool outputs.\n\n### async\\_response\\_gen`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.AgentChatResponse.async_response_gen \"Permanent link\")\n\n```\nasync_response_gen() -> AsyncGenerator[str, None]\n\n```\n\nUsed for fake streaming, i.e. with tool outputs.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>89<br>90<br>91<br>92<br>``` | ```<br>async def async_response_gen(self) -> AsyncGenerator[str, None]:<br>    \"\"\"Used for fake streaming, i.e. with tool outputs.\"\"\"<br>    if not self.is_dummy_stream:<br>        raise ValueError(<br>            \"response_gen is only available for streaming responses. \"<br>            \"Set is_dummy_stream=True if you still want a generator.\"<br>        )<br>    for token in self.response.split(\" \"):<br>        yield token + \" \"<br>        await asyncio.sleep(0.1)<br>``` |\n\n## StreamingAgentChatResponse`dataclass`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.StreamingAgentChatResponse \"Permanent link\")\n\nStreaming chat response to user and writing to chat history.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>``` | ```<br>@dataclass<br>class StreamingAgentChatResponse:<br>    \"\"\"Streaming chat response to user and writing to chat history.\"\"\"<br>    response: str = \"\"<br>    sources: List[ToolOutput] = field(default_factory=list)<br>    chat_stream: Optional[ChatResponseGen] = None<br>    achat_stream: Optional[ChatResponseAsyncGen] = None<br>    source_nodes: List[NodeWithScore] = field(default_factory=list)<br>    unformatted_response: str = \"\"<br>    queue: Queue = field(default_factory=Queue)<br>    aqueue: Optional[asyncio.Queue] = None<br>    # flag when chat message is a function call<br>    is_function: Optional[bool] = None<br>    # flag when processing done<br>    is_done = False<br>    # signal when a new item is added to the queue<br>    new_item_event: Optional[asyncio.Event] = None<br>    # NOTE: async code uses two events rather than one since it yields<br>    # control when waiting for queue item<br>    # signal when the OpenAI functions stop executing<br>    is_function_false_event: Optional[asyncio.Event] = None<br>    # signal when an OpenAI function is being executed<br>    is_function_not_none_thread_event: Event = field(default_factory=Event)<br>    # Track if an exception occurred<br>    exception: Optional[Exception] = None<br>    def set_source_nodes(self) -> None:<br>        if self.sources and not self.source_nodes:<br>            for tool_output in self.sources:<br>                if isinstance(tool_output.raw_output, (Response, StreamingResponse)):<br>                    self.source_nodes.extend(tool_output.raw_output.source_nodes)<br>    def __post_init__(self) -> None:<br>        self.set_source_nodes()<br>    def __str__(self) -> str:<br>        if self.is_done and not self.queue.empty() and not self.is_function:<br>            while self.queue.queue:<br>                delta = self.queue.queue.popleft()<br>                self.unformatted_response += delta<br>            self.response = self.unformatted_response.strip()<br>        return self.response<br>    def _ensure_async_setup(self) -> None:<br>        if self.aqueue is None:<br>            self.aqueue = asyncio.Queue()<br>        if self.new_item_event is None:<br>            self.new_item_event = asyncio.Event()<br>        if self.is_function_false_event is None:<br>            self.is_function_false_event = asyncio.Event()<br>    def put_in_queue(self, delta: Optional[str]) -> None:<br>        self.queue.put_nowait(delta)<br>        self.is_function_not_none_thread_event.set()<br>    def aput_in_queue(self, delta: Optional[str]) -> None:<br>        assert self.aqueue is not None<br>        assert self.new_item_event is not None<br>        self.aqueue.put_nowait(delta)<br>        self.new_item_event.set()<br>    @dispatcher.span<br>    def write_response_to_history(<br>        self,<br>        memory: BaseMemory,<br>        on_stream_end_fn: Optional[Callable] = None,<br>    ) -> None:<br>        if self.chat_stream is None:<br>            raise ValueError(<br>                \"chat_stream is None. Cannot write to history without chat_stream.\"<br>            )<br>        # try/except to prevent hanging on error<br>        dispatcher.event(StreamChatStartEvent())<br>        try:<br>            final_text = \"\"<br>            for chat in self.chat_stream:<br>                self.is_function = is_function(chat.message)<br>                if chat.delta:<br>                    dispatcher.event(<br>                        StreamChatDeltaReceivedEvent(<br>                            delta=chat.delta,<br>                        )<br>                    )<br>                    self.put_in_queue(chat.delta)<br>                final_text += chat.delta or \"\"<br>            if self.is_function is not None:  # if loop has gone through iteration<br>                # NOTE: this is to handle the special case where we consume some of the<br>                # chat stream, but not all of it (e.g. in react agent)<br>                chat.message.content = final_text.strip()  # final message<br>                memory.put(chat.message)<br>        except Exception as e:<br>            dispatcher.event(StreamChatErrorEvent(exception=e))<br>            self.exception = e<br>            # This act as is_done events for any consumers waiting<br>            self.is_function_not_none_thread_event.set()<br>            # force the queue reader to see the exception<br>            self.put_in_queue(\"\")<br>            raise<br>        dispatcher.event(StreamChatEndEvent())<br>        self.is_done = True<br>        # This act as is_done events for any consumers waiting<br>        self.is_function_not_none_thread_event.set()<br>        if on_stream_end_fn is not None and not self.is_function:<br>            on_stream_end_fn()<br>    @dispatcher.span<br>    async def awrite_response_to_history(<br>        self,<br>        memory: BaseMemory,<br>        on_stream_end_fn: Optional[Callable] = None,<br>    ) -> None:<br>        self._ensure_async_setup()<br>        assert self.aqueue is not None<br>        assert self.is_function_false_event is not None<br>        assert self.new_item_event is not None<br>        if self.achat_stream is None:<br>            raise ValueError(<br>                \"achat_stream is None. Cannot asynchronously write to \"<br>                \"history without achat_stream.\"<br>            )<br>        # try/except to prevent hanging on error<br>        dispatcher.event(StreamChatStartEvent())<br>        try:<br>            final_text = \"\"<br>            async for chat in self.achat_stream:<br>                self.is_function = is_function(chat.message)<br>                if chat.delta:<br>                    dispatcher.event(<br>                        StreamChatDeltaReceivedEvent(<br>                            delta=chat.delta,<br>                        )<br>                    )<br>                    self.aput_in_queue(chat.delta)<br>                final_text += chat.delta or \"\"<br>                self.new_item_event.set()<br>                if self.is_function is False:<br>                    self.is_function_false_event.set()<br>            if self.is_function is not None:  # if loop has gone through iteration<br>                # NOTE: this is to handle the special case where we consume some of the<br>                # chat stream, but not all of it (e.g. in react agent)<br>                chat.message.content = final_text.strip()  # final message<br>                memory.put(chat.message)<br>        except Exception as e:<br>            dispatcher.event(StreamChatErrorEvent(exception=e))<br>            self.exception = e<br>            # These act as is_done events for any consumers waiting<br>            self.is_function_false_event.set()<br>            self.new_item_event.set()<br>            # force the queue reader to see the exception<br>            self.aput_in_queue(\"\")<br>            raise<br>        dispatcher.event(StreamChatEndEvent())<br>        self.is_done = True<br>        # These act as is_done events for any consumers waiting<br>        self.is_function_false_event.set()<br>        self.new_item_event.set()<br>        if on_stream_end_fn is not None and not self.is_function:<br>            on_stream_end_fn()<br>    @property<br>    def response_gen(self) -> Generator[str, None, None]:<br>        while not self.is_done or not self.queue.empty():<br>            if self.exception is not None:<br>                raise self.exception<br>            try:<br>                delta = self.queue.get(block=False)<br>                self.unformatted_response += delta<br>                yield delta<br>            except Empty:<br>                # Queue is empty, but we're not done yet. Sleep for 0 secs to release the GIL and allow other threads to run.<br>                time.sleep(0)<br>        self.response = self.unformatted_response.strip()<br>    async def async_response_gen(self) -> AsyncGenerator[str, None]:<br>        self._ensure_async_setup()<br>        assert self.aqueue is not None<br>        while True:<br>            if not self.aqueue.empty() or not self.is_done:<br>                if self.exception is not None:<br>                    raise self.exception<br>                try:<br>                    delta = await asyncio.wait_for(self.aqueue.get(), timeout=0.1)<br>                except asyncio.TimeoutError:<br>                    if self.is_done:<br>                        break<br>                    continue<br>                if delta is not None:<br>                    self.unformatted_response += delta<br>                    yield delta<br>            else:<br>                break<br>        self.response = self.unformatted_response.strip()<br>    def print_response_stream(self) -> None:<br>        for token in self.response_gen:<br>            print(token, end=\"\", flush=True)<br>    async def aprint_response_stream(self) -> None:<br>        async for token in self.async_response_gen():<br>            print(token, end=\"\", flush=True)<br>``` |\n\n## BaseChatEngine [\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.BaseChatEngine \"Permanent link\")\n\nBases: `DispatcherSpanMixin`, `ABC`\n\nBase Chat Engine.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>``` | ```<br>class BaseChatEngine(DispatcherSpanMixin, ABC):<br>    \"\"\"Base Chat Engine.\"\"\"<br>    @abstractmethod<br>    def reset(self) -> None:<br>        \"\"\"Reset conversation state.\"\"\"<br>    @abstractmethod<br>    def chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> AGENT_CHAT_RESPONSE_TYPE:<br>        \"\"\"Main chat interface.\"\"\"<br>    @abstractmethod<br>    def stream_chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> StreamingAgentChatResponse:<br>        \"\"\"Stream chat interface.\"\"\"<br>    @abstractmethod<br>    async def achat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> AGENT_CHAT_RESPONSE_TYPE:<br>        \"\"\"Async version of main chat interface.\"\"\"<br>    @abstractmethod<br>    async def astream_chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> StreamingAgentChatResponse:<br>        \"\"\"Async version of main chat interface.\"\"\"<br>    def chat_repl(self) -> None:<br>        \"\"\"Enter interactive chat REPL.\"\"\"<br>        print(\"===== Entering Chat REPL =====\")<br>        print('Type \"exit\" to exit.\\n')<br>        self.reset()<br>        message = input(\"Human: \")<br>        while message != \"exit\":<br>            response = self.chat(message)<br>            print(f\"Assistant: {response}\\n\")<br>            message = input(\"Human: \")<br>    def streaming_chat_repl(self) -> None:<br>        \"\"\"Enter interactive chat REPL with streaming responses.\"\"\"<br>        print(\"===== Entering Chat REPL =====\")<br>        print('Type \"exit\" to exit.\\n')<br>        self.reset()<br>        message = input(\"Human: \")<br>        while message != \"exit\":<br>            response = self.stream_chat(message)<br>            print(\"Assistant: \", end=\"\", flush=True)<br>            response.print_response_stream()<br>            print(\"\\n\")<br>            message = input(\"Human: \")<br>    @property<br>    @abstractmethod<br>    def chat_history(self) -> List[ChatMessage]:<br>        pass<br>``` |\n\n### reset`abstractmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.BaseChatEngine.reset \"Permanent link\")\n\n```\nreset() -> None\n\n```\n\nReset conversation state.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>318<br>319<br>320<br>``` | ```<br>@abstractmethod<br>def reset(self) -> None:<br>    \"\"\"Reset conversation state.\"\"\"<br>``` |\n\n### chat`abstractmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.BaseChatEngine.chat \"Permanent link\")\n\n```\nchat(message: str, chat_history: Optional[List[ChatMessage]] = None) -> AGENT_CHAT_RESPONSE_TYPE\n\n```\n\nMain chat interface.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>322<br>323<br>324<br>325<br>326<br>``` | ```<br>@abstractmethod<br>def chat(<br>    self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>) -> AGENT_CHAT_RESPONSE_TYPE:<br>    \"\"\"Main chat interface.\"\"\"<br>``` |\n\n### stream\\_chat`abstractmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.BaseChatEngine.stream_chat \"Permanent link\")\n\n```\nstream_chat(message: str, chat_history: Optional[List[ChatMessage]] = None) -> StreamingAgentChatResponse\n\n```\n\nStream chat interface.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>328<br>329<br>330<br>331<br>332<br>``` | ```<br>@abstractmethod<br>def stream_chat(<br>    self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>) -> StreamingAgentChatResponse:<br>    \"\"\"Stream chat interface.\"\"\"<br>``` |\n\n### achat`abstractmethod``async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.BaseChatEngine.achat \"Permanent link\")\n\n```\nachat(message: str, chat_history: Optional[List[ChatMessage]] = None) -> AGENT_CHAT_RESPONSE_TYPE\n\n```\n\nAsync version of main chat interface.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>334<br>335<br>336<br>337<br>338<br>``` | ```<br>@abstractmethod<br>async def achat(<br>    self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>) -> AGENT_CHAT_RESPONSE_TYPE:<br>    \"\"\"Async version of main chat interface.\"\"\"<br>``` |\n\n### astream\\_chat`abstractmethod``async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.BaseChatEngine.astream_chat \"Permanent link\")\n\n```\nastream_chat(message: str, chat_history: Optional[List[ChatMessage]] = None) -> StreamingAgentChatResponse\n\n```\n\nAsync version of main chat interface.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>340<br>341<br>342<br>343<br>344<br>``` | ```<br>@abstractmethod<br>async def astream_chat(<br>    self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>) -> StreamingAgentChatResponse:<br>    \"\"\"Async version of main chat interface.\"\"\"<br>``` |\n\n### chat\\_repl [\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.BaseChatEngine.chat_repl \"Permanent link\")\n\n```\nchat_repl() -> None\n\n```\n\nEnter interactive chat REPL.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>``` | ```<br>def chat_repl(self) -> None:<br>    \"\"\"Enter interactive chat REPL.\"\"\"<br>    print(\"===== Entering Chat REPL =====\")<br>    print('Type \"exit\" to exit.\\n')<br>    self.reset()<br>    message = input(\"Human: \")<br>    while message != \"exit\":<br>        response = self.chat(message)<br>        print(f\"Assistant: {response}\\n\")<br>        message = input(\"Human: \")<br>``` |\n\n### streaming\\_chat\\_repl [\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.BaseChatEngine.streaming_chat_repl \"Permanent link\")\n\n```\nstreaming_chat_repl() -> None\n\n```\n\nEnter interactive chat REPL with streaming responses.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>``` | ```<br>def streaming_chat_repl(self) -> None:<br>    \"\"\"Enter interactive chat REPL with streaming responses.\"\"\"<br>    print(\"===== Entering Chat REPL =====\")<br>    print('Type \"exit\" to exit.\\n')<br>    self.reset()<br>    message = input(\"Human: \")<br>    while message != \"exit\":<br>        response = self.stream_chat(message)<br>        print(\"Assistant: \", end=\"\", flush=True)<br>        response.print_response_stream()<br>        print(\"\\n\")<br>        message = input(\"Human: \")<br>``` |\n\n## ChatMode [\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.ChatMode \"Permanent link\")\n\nBases: `str`, `Enum`\n\nChat Engine Modes.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>``` | ```<br>class ChatMode(str, Enum):<br>    \"\"\"Chat Engine Modes.\"\"\"<br>    SIMPLE = \"simple\"<br>    \"\"\"Corresponds to `SimpleChatEngine`.<br>    Chat with LLM, without making use of a knowledge base.<br>    \"\"\"<br>    CONDENSE_QUESTION = \"condense_question\"<br>    \"\"\"Corresponds to `CondenseQuestionChatEngine`.<br>    First generate a standalone question from conversation context and last message,<br>    then query the query engine for a response.<br>    \"\"\"<br>    CONTEXT = \"context\"<br>    \"\"\"Corresponds to `ContextChatEngine`.<br>    First retrieve text from the index using the user's message, then use the context<br>    in the system prompt to generate a response.<br>    \"\"\"<br>    CONDENSE_PLUS_CONTEXT = \"condense_plus_context\"<br>    \"\"\"Corresponds to `CondensePlusContextChatEngine`.<br>    First condense a conversation and latest user message to a standalone question.<br>    Then build a context for the standalone question from a retriever,<br>    Then pass the context along with prompt and user message to LLM to generate a response.<br>    \"\"\"<br>    REACT = \"react\"<br>    \"\"\"Corresponds to `ReActAgent`.<br>    Use a ReAct agent loop with query engine tools.<br>    \"\"\"<br>    OPENAI = \"openai\"<br>    \"\"\"Corresponds to `OpenAIAgent`.<br>    Use an OpenAI function calling agent loop.<br>    NOTE: only works with OpenAI models that support function calling API.<br>    \"\"\"<br>    BEST = \"best\"<br>    \"\"\"Select the best chat engine based on the current LLM.<br>    Corresponds to `OpenAIAgent` if using an OpenAI model that supports<br>    function calling API, otherwise, corresponds to `ReActAgent`.<br>    \"\"\"<br>``` |\n\n### SIMPLE`class-attribute``instance-attribute`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.ChatMode.SIMPLE \"Permanent link\")\n\n```\nSIMPLE = 'simple'\n\n```\n\nCorresponds to `SimpleChatEngine`.\n\nChat with LLM, without making use of a knowledge base.\n\n### CONDENSE\\_QUESTION`class-attribute``instance-attribute`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.ChatMode.CONDENSE_QUESTION \"Permanent link\")\n\n```\nCONDENSE_QUESTION = 'condense_question'\n\n```\n\nCorresponds to `CondenseQuestionChatEngine`.\n\nFirst generate a standalone question from conversation context and last message,\nthen query the query engine for a response.\n\n### CONTEXT`class-attribute``instance-attribute`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.ChatMode.CONTEXT \"Permanent link\")\n\n```\nCONTEXT = 'context'\n\n```\n\nCorresponds to `ContextChatEngine`.\n\nFirst retrieve text from the index using the user's message, then use the context\nin the system prompt to generate a response.\n\n### CONDENSE\\_PLUS\\_CONTEXT`class-attribute``instance-attribute`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.ChatMode.CONDENSE_PLUS_CONTEXT \"Permanent link\")\n\n```\nCONDENSE_PLUS_CONTEXT = 'condense_plus_context'\n\n```\n\nCorresponds to `CondensePlusContextChatEngine`.\n\nFirst condense a conversation and latest user message to a standalone question.\nThen build a context for the standalone question from a retriever,\nThen pass the context along with prompt and user message to LLM to generate a response.\n\n### REACT`class-attribute``instance-attribute`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.ChatMode.REACT \"Permanent link\")\n\n```\nREACT = 'react'\n\n```\n\nCorresponds to `ReActAgent`.\n\nUse a ReAct agent loop with query engine tools.\n\n### OPENAI`class-attribute``instance-attribute`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.ChatMode.OPENAI \"Permanent link\")\n\n```\nOPENAI = 'openai'\n\n```\n\nCorresponds to `OpenAIAgent`.\n\nUse an OpenAI function calling agent loop.\n\nNOTE: only works with OpenAI models that support function calling API.\n\n### BEST`class-attribute``instance-attribute`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.ChatMode.BEST \"Permanent link\")\n\n```\nBEST = 'best'\n\n```\n\nSelect the best chat engine based on the current LLM.\n\nCorresponds to `OpenAIAgent` if using an OpenAI model that supports\nfunction calling API, otherwise, corresponds to `ReActAgent`.\n\n## is\\_function [\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/\\#llama_index.core.chat_engine.types.is_function \"Permanent link\")\n\n```\nis_function(message: ChatMessage) -> bool\n\n```\n\nUtility for ChatMessage responses from OpenAI models.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/types.py`\n\n|     |     |\n| --- | --- |\n| ```<br>35<br>36<br>37<br>``` | ```<br>def is_function(message: ChatMessage) -> bool:<br>    \"\"\"Utility for ChatMessage responses from OpenAI models.\"\"\"<br>    return \"tool_calls\" in message.additional_kwargs<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Index - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/clarifai/#llama_index.embeddings.clarifai.ClarifaiEmbedding)\n\n# Clarifai\n\n## ClarifaiEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/clarifai/\\#llama_index.embeddings.clarifai.ClarifaiEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nClarifai embeddings class.\n\nClarifai uses Personal Access Tokens(PAT) to validate requests.\nYou can create and manage PATs under your Clarifai account security settings.\nExport your PAT as an environment variable by running `export CLARIFAI_PAT={PAT}`\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-clarifai/llama_index/embeddings/clarifai/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>``` | ```<br>class ClarifaiEmbedding(BaseEmbedding):<br>    \"\"\"Clarifai embeddings class.<br>    Clarifai uses Personal Access Tokens(PAT) to validate requests.<br>    You can create and manage PATs under your Clarifai account security settings.<br>    Export your PAT as an environment variable by running `export CLARIFAI_PAT={PAT}`<br>    \"\"\"<br>    model_url: Optional[str] = Field(<br>        description=f\"Full URL of the model. e.g. `{EXAMPLE_URL}`\"<br>    )<br>    model_id: Optional[str] = Field(description=\"Model ID.\")<br>    model_version_id: Optional[str] = Field(description=\"Model Version ID.\")<br>    app_id: Optional[str] = Field(description=\"Clarifai application ID of the model.\")<br>    user_id: Optional[str] = Field(description=\"Clarifai user ID of the model.\")<br>    pat: Optional[str] = Field(<br>        description=\"Personal Access Tokens(PAT) to validate requests.\"<br>    )<br>    _model: Any = PrivateAttr()<br>    def __init__(<br>        self,<br>        model_name: Optional[str] = None,<br>        model_url: Optional[str] = None,<br>        model_version_id: Optional[str] = \"\",<br>        app_id: Optional[str] = None,<br>        user_id: Optional[str] = None,<br>        pat: Optional[str] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>    ):<br>        embed_batch_size = min(128, embed_batch_size)<br>        if pat is None and os.environ.get(\"CLARIFAI_PAT\") is not None:<br>            pat = os.environ.get(\"CLARIFAI_PAT\")<br>        if not pat and os.environ.get(\"CLARIFAI_PAT\") is None:<br>            raise ValueError(<br>                \"Set `CLARIFAI_PAT` as env variable or pass `pat` as constructor argument\"<br>            )<br>        if model_url is not None and model_name is not None:<br>            raise ValueError(\"You can only specify one of model_url or model_name.\")<br>        if model_url is None and model_name is None:<br>            raise ValueError(\"You must specify one of model_url or model_name.\")<br>        if model_name is not None:<br>            if app_id is None or user_id is None:<br>                raise ValueError(<br>                    f\"Missing one app ID or user ID of the model: {app_id=}, {user_id=}\"<br>                )<br>            else:<br>                model = Model(<br>                    user_id=user_id,<br>                    app_id=app_id,<br>                    model_id=model_name,<br>                    model_version={\"id\": model_version_id},<br>                    pat=pat,<br>                )<br>        if model_url is not None:<br>            model = Model(model_url, pat=pat)<br>            model_name = model.id<br>        super().__init__(<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            model_name=model_name,<br>        )<br>        self._model = model<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"ClarifaiEmbedding\"<br>    def _embed(self, sentences: List[str]) -> List[List[float]]:<br>        \"\"\"Embed sentences.\"\"\"<br>        try:<br>            from clarifai.client.input import Inputs<br>        except ImportError:<br>            raise ImportError(\"ClarifaiEmbedding requires `pip install clarifai`.\")<br>        embeddings = []<br>        try:<br>            for i in range(0, len(sentences), self.embed_batch_size):<br>                batch = sentences[i : i + self.embed_batch_size]<br>                input_batch = [<br>                    Inputs.get_text_input(input_id=str(id), raw_text=inp)<br>                    for id, inp in enumerate(batch)<br>                ]<br>                predict_response = self._model.predict(input_batch)<br>                embeddings.extend(<br>                    [<br>                        list(output.data.embeddings[0].vector)<br>                        for output in predict_response.outputs<br>                    ]<br>                )<br>        except Exception as e:<br>            logger.error(f\"Predict failed, exception: {e}\")<br>        return embeddings<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self._embed([query])[0]<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding async.\"\"\"<br>        return self._get_query_embedding(query)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding async.\"\"\"<br>        return self._get_text_embedding(text)<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._embed([text])[0]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        return self._embed(texts)<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Clarifai - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/clarifai/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/#llama_index.core.evaluation.BaseEvaluator)\n\n# Index\n\nEvaluation modules.\n\n## BaseEvaluator [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/\\#llama_index.core.evaluation.BaseEvaluator \"Permanent link\")\n\nBases: `PromptMixin`\n\nBase Evaluator class.\n\nSource code in `llama-index-core/llama_index/core/evaluation/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>``` | ```<br>class BaseEvaluator(PromptMixin):<br>    \"\"\"Base Evaluator class.\"\"\"<br>    def _get_prompt_modules(self) -> PromptMixinType:<br>        \"\"\"Get prompt modules.\"\"\"<br>        return {}<br>    def evaluate(<br>        self,<br>        query: Optional[str] = None,<br>        response: Optional[str] = None,<br>        contexts: Optional[Sequence[str]] = None,<br>        **kwargs: Any,<br>    ) -> EvaluationResult:<br>        \"\"\"Run evaluation with query string, retrieved contexts,<br>        and generated response string.<br>        Subclasses can override this method to provide custom evaluation logic and<br>        take in additional arguments.<br>        \"\"\"<br>        return asyncio_run(<br>            self.aevaluate(<br>                query=query,<br>                response=response,<br>                contexts=contexts,<br>                **kwargs,<br>            )<br>        )<br>    @abstractmethod<br>    async def aevaluate(<br>        self,<br>        query: Optional[str] = None,<br>        response: Optional[str] = None,<br>        contexts: Optional[Sequence[str]] = None,<br>        **kwargs: Any,<br>    ) -> EvaluationResult:<br>        \"\"\"Run evaluation with query string, retrieved contexts,<br>        and generated response string.<br>        Subclasses can override this method to provide custom evaluation logic and<br>        take in additional arguments.<br>        \"\"\"<br>        raise NotImplementedError<br>    def evaluate_response(<br>        self,<br>        query: Optional[str] = None,<br>        response: Optional[Response] = None,<br>        **kwargs: Any,<br>    ) -> EvaluationResult:<br>        \"\"\"Run evaluation with query string and generated Response object.<br>        Subclasses can override this method to provide custom evaluation logic and<br>        take in additional arguments.<br>        \"\"\"<br>        response_str: Optional[str] = None<br>        contexts: Optional[Sequence[str]] = None<br>        if response is not None:<br>            response_str = response.response<br>            contexts = [node.get_content() for node in response.source_nodes]<br>        return self.evaluate(<br>            query=query, response=response_str, contexts=contexts, **kwargs<br>        )<br>    async def aevaluate_response(<br>        self,<br>        query: Optional[str] = None,<br>        response: Optional[Response] = None,<br>        **kwargs: Any,<br>    ) -> EvaluationResult:<br>        \"\"\"Run evaluation with query string and generated Response object.<br>        Subclasses can override this method to provide custom evaluation logic and<br>        take in additional arguments.<br>        \"\"\"<br>        response_str: Optional[str] = None<br>        contexts: Optional[Sequence[str]] = None<br>        if response is not None:<br>            response_str = response.response<br>            contexts = [node.get_content() for node in response.source_nodes]<br>        return await self.aevaluate(<br>            query=query, response=response_str, contexts=contexts, **kwargs<br>        )<br>``` |\n\n### evaluate [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/\\#llama_index.core.evaluation.BaseEvaluator.evaluate \"Permanent link\")\n\n```\nevaluate(query: Optional[str] = None, response: Optional[str] = None, contexts: Optional[Sequence[str]] = None, **kwargs: Any) -> EvaluationResult\n\n```\n\nRun evaluation with query string, retrieved contexts,\nand generated response string.\n\nSubclasses can override this method to provide custom evaluation logic and\ntake in additional arguments.\n\nSource code in `llama-index-core/llama_index/core/evaluation/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>``` | ```<br>def evaluate(<br>    self,<br>    query: Optional[str] = None,<br>    response: Optional[str] = None,<br>    contexts: Optional[Sequence[str]] = None,<br>    **kwargs: Any,<br>) -> EvaluationResult:<br>    \"\"\"Run evaluation with query string, retrieved contexts,<br>    and generated response string.<br>    Subclasses can override this method to provide custom evaluation logic and<br>    take in additional arguments.<br>    \"\"\"<br>    return asyncio_run(<br>        self.aevaluate(<br>            query=query,<br>            response=response,<br>            contexts=contexts,<br>            **kwargs,<br>        )<br>    )<br>``` |\n\n### aevaluate`abstractmethod``async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/\\#llama_index.core.evaluation.BaseEvaluator.aevaluate \"Permanent link\")\n\n```\naevaluate(query: Optional[str] = None, response: Optional[str] = None, contexts: Optional[Sequence[str]] = None, **kwargs: Any) -> EvaluationResult\n\n```\n\nRun evaluation with query string, retrieved contexts,\nand generated response string.\n\nSubclasses can override this method to provide custom evaluation logic and\ntake in additional arguments.\n\nSource code in `llama-index-core/llama_index/core/evaluation/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>``` | ```<br>@abstractmethod<br>async def aevaluate(<br>    self,<br>    query: Optional[str] = None,<br>    response: Optional[str] = None,<br>    contexts: Optional[Sequence[str]] = None,<br>    **kwargs: Any,<br>) -> EvaluationResult:<br>    \"\"\"Run evaluation with query string, retrieved contexts,<br>    and generated response string.<br>    Subclasses can override this method to provide custom evaluation logic and<br>    take in additional arguments.<br>    \"\"\"<br>    raise NotImplementedError<br>``` |\n\n### evaluate\\_response [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/\\#llama_index.core.evaluation.BaseEvaluator.evaluate_response \"Permanent link\")\n\n```\nevaluate_response(query: Optional[str] = None, response: Optional[Response] = None, **kwargs: Any) -> EvaluationResult\n\n```\n\nRun evaluation with query string and generated Response object.\n\nSubclasses can override this method to provide custom evaluation logic and\ntake in additional arguments.\n\nSource code in `llama-index-core/llama_index/core/evaluation/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>``` | ```<br>def evaluate_response(<br>    self,<br>    query: Optional[str] = None,<br>    response: Optional[Response] = None,<br>    **kwargs: Any,<br>) -> EvaluationResult:<br>    \"\"\"Run evaluation with query string and generated Response object.<br>    Subclasses can override this method to provide custom evaluation logic and<br>    take in additional arguments.<br>    \"\"\"<br>    response_str: Optional[str] = None<br>    contexts: Optional[Sequence[str]] = None<br>    if response is not None:<br>        response_str = response.response<br>        contexts = [node.get_content() for node in response.source_nodes]<br>    return self.evaluate(<br>        query=query, response=response_str, contexts=contexts, **kwargs<br>    )<br>``` |\n\n### aevaluate\\_response`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/\\#llama_index.core.evaluation.BaseEvaluator.aevaluate_response \"Permanent link\")\n\n```\naevaluate_response(query: Optional[str] = None, response: Optional[Response] = None, **kwargs: Any) -> EvaluationResult\n\n```\n\nRun evaluation with query string and generated Response object.\n\nSubclasses can override this method to provide custom evaluation logic and\ntake in additional arguments.\n\nSource code in `llama-index-core/llama_index/core/evaluation/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>``` | ```<br>async def aevaluate_response(<br>    self,<br>    query: Optional[str] = None,<br>    response: Optional[Response] = None,<br>    **kwargs: Any,<br>) -> EvaluationResult:<br>    \"\"\"Run evaluation with query string and generated Response object.<br>    Subclasses can override this method to provide custom evaluation logic and<br>    take in additional arguments.<br>    \"\"\"<br>    response_str: Optional[str] = None<br>    contexts: Optional[Sequence[str]] = None<br>    if response is not None:<br>        response_str = response.response<br>        contexts = [node.get_content() for node in response.source_nodes]<br>    return await self.aevaluate(<br>        query=query, response=response_str, contexts=contexts, **kwargs<br>    )<br>``` |\n\n## EvaluationResult [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/\\#llama_index.core.evaluation.EvaluationResult \"Permanent link\")\n\nBases: `BaseModel`\n\nEvaluation result.\n\nOutput of an BaseEvaluator.\n\nSource code in `llama-index-core/llama_index/core/evaluation/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>``` | ```<br>class EvaluationResult(BaseModel):<br>    \"\"\"Evaluation result.<br>    Output of an BaseEvaluator.<br>    \"\"\"<br>    query: Optional[str] = Field(default=None, description=\"Query string\")<br>    contexts: Optional[Sequence[str]] = Field(<br>        default=None, description=\"Context strings\"<br>    )<br>    response: Optional[str] = Field(default=None, description=\"Response string\")<br>    passing: Optional[bool] = Field(<br>        default=None, description=\"Binary evaluation result (passing or not)\"<br>    )<br>    feedback: Optional[str] = Field(<br>        default=None, description=\"Feedback or reasoning for the response\"<br>    )<br>    score: Optional[float] = Field(default=None, description=\"Score for the response\")<br>    pairwise_source: Optional[str] = Field(<br>        default=None,<br>        description=(<br>            \"Used only for pairwise and specifies whether it is from original order of\"<br>            \" presented answers or flipped order\"<br>        ),<br>    )<br>    invalid_result: bool = Field(<br>        default=False, description=\"Whether the evaluation result is an invalid one.\"<br>    )<br>    invalid_reason: Optional[str] = Field(<br>        default=None, description=\"Reason for invalid evaluation.\"<br>    )<br>``` |\n\n## BatchEvalRunner [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/\\#llama_index.core.evaluation.BatchEvalRunner \"Permanent link\")\n\nBatch evaluation runner.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `evaluators` | `Dict[str, BaseEvaluator]` | Dictionary of evaluators. | _required_ |\n| `workers` | `int` | Number of workers to use for parallelization.<br>Defaults to 2. | `2` |\n| `show_progress` | `bool` | Whether to show progress bars. Defaults to False. | `False` |\n\nSource code in `llama-index-core/llama_index/core/evaluation/batch_runner.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>``` | ````<br>class BatchEvalRunner:<br>    \"\"\"<br>    Batch evaluation runner.<br>    Args:<br>        evaluators (Dict[str, BaseEvaluator]): Dictionary of evaluators.<br>        workers (int): Number of workers to use for parallelization.<br>            Defaults to 2.<br>        show_progress (bool): Whether to show progress bars. Defaults to False.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        evaluators: Dict[str, BaseEvaluator],<br>        workers: int = 2,<br>        show_progress: bool = False,<br>    ):<br>        self.evaluators = evaluators<br>        self.workers = workers<br>        self.semaphore = asyncio.Semaphore(self.workers)<br>        self.show_progress = show_progress<br>        self.asyncio_mod = asyncio_module(show_progress=self.show_progress)<br>    def _format_results(<br>        self, results: List[Tuple[str, EvaluationResult]]<br>    ) -> Dict[str, List[EvaluationResult]]:<br>        \"\"\"Format results.\"\"\"<br>        # Format results<br>        results_dict: Dict[str, List[EvaluationResult]] = {<br>            name: [] for name in self.evaluators<br>        }<br>        for name, result in results:<br>            results_dict[name].append(result)<br>        return results_dict<br>    def _validate_and_clean_inputs(<br>        self,<br>        *inputs_list: Any,<br>    ) -> List[Any]:<br>        \"\"\"<br>        Validate and clean input lists.<br>        Enforce that at least one of the inputs is not None.<br>        Make sure that all inputs have the same length.<br>        Make sure that None inputs are replaced with [None] * len(inputs).<br>        \"\"\"<br>        assert len(inputs_list) > 0<br>        # first, make sure at least one of queries or response_strs is not None<br>        input_len: Optional[int] = None<br>        for inputs in inputs_list:<br>            if inputs is not None:<br>                input_len = len(inputs)<br>                break<br>        if input_len is None:<br>            raise ValueError(\"At least one item in inputs_list must be provided.\")<br>        new_inputs_list = []<br>        for inputs in inputs_list:<br>            if inputs is None:<br>                new_inputs_list.append([None] * input_len)<br>            else:<br>                if len(inputs) != input_len:<br>                    raise ValueError(\"All inputs must have the same length.\")<br>                new_inputs_list.append(inputs)<br>        return new_inputs_list<br>    def _validate_nested_eval_kwargs_types(<br>        self, eval_kwargs_lists: Dict[str, Any]<br>    ) -> Dict[str, Any]:<br>        \"\"\"<br>        Ensure eval kwargs are acceptable format.<br>            either a Dict[str, List] or a Dict[str, Dict[str, List]].<br>        Allows use of different kwargs (e.g. references) with different evaluators<br>            while keeping backwards compatibility for single evaluators<br>        \"\"\"<br>        if not isinstance(eval_kwargs_lists, dict):<br>            raise ValueError(<br>                f\"eval_kwargs_lists must be a dict. Got {eval_kwargs_lists}\"<br>            )<br>        for evaluator, eval_kwargs in eval_kwargs_lists.items():<br>            if isinstance(eval_kwargs, list):<br>                # maintain backwards compatibility - for use with single evaluator<br>                eval_kwargs_lists[evaluator] = self._validate_and_clean_inputs(<br>                    eval_kwargs<br>                )[0]<br>            elif isinstance(eval_kwargs, dict):<br>                # for use with multiple evaluators<br>                for k in eval_kwargs:<br>                    v = eval_kwargs[k]<br>                    if not isinstance(v, list):<br>                        raise ValueError(<br>                            f\"nested inner values in eval_kwargs must be a list. Got {evaluator}: {k}: {v}\"<br>                        )<br>                    eval_kwargs_lists[evaluator][k] = self._validate_and_clean_inputs(<br>                        v<br>                    )[0]<br>            else:<br>                raise ValueError(<br>                    f\"eval_kwargs must be a list or a dict. Got {evaluator}: {eval_kwargs}\"<br>                )<br>        return eval_kwargs_lists<br>    def _get_eval_kwargs(<br>        self, eval_kwargs_lists: Dict[str, Any], idx: int<br>    ) -> Dict[str, Any]:<br>        \"\"\"<br>        Get eval kwargs from eval_kwargs_lists at a given idx.<br>        Since eval_kwargs_lists is a dict of lists, we need to get the<br>        value at idx for each key.<br>        \"\"\"<br>        return {k: v[idx] for k, v in eval_kwargs_lists.items()}<br>    async def aevaluate_response_strs(<br>        self,<br>        queries: Optional[List[str]] = None,<br>        response_strs: Optional[List[str]] = None,<br>        contexts_list: Optional[List[List[str]]] = None,<br>        **eval_kwargs_lists: Dict[str, Any],<br>    ) -> Dict[str, List[EvaluationResult]]:<br>        \"\"\"<br>        Evaluate query, response pairs.<br>        This evaluates queries, responses, contexts as string inputs.<br>        Can supply additional kwargs to the evaluator in eval_kwargs_lists.<br>        Args:<br>            queries (Optional[List[str]]): List of query strings. Defaults to None.<br>            response_strs (Optional[List[str]]): List of response strings.<br>                Defaults to None.<br>            contexts_list (Optional[List[List[str]]]): List of context lists.<br>                Defaults to None.<br>            **eval_kwargs_lists (Dict[str, Any]): Dict of either dicts or lists<br>                of kwargs to pass to evaluator. Defaults to None.<br>                    multiple evaluators: {evaluator: {kwarg: [list of values]},...}<br>                    single evaluator:    {kwarg: [list of values]}<br>        \"\"\"<br>        queries, response_strs, contexts_list = self._validate_and_clean_inputs(<br>            queries, response_strs, contexts_list<br>        )<br>        eval_kwargs_lists = self._validate_nested_eval_kwargs_types(eval_kwargs_lists)<br>        # boolean to check if using multi kwarg evaluator<br>        multi_kwargs = len(eval_kwargs_lists) > 0 and isinstance(<br>            next(iter(eval_kwargs_lists.values())), dict<br>        )<br>        # run evaluations<br>        eval_jobs = []<br>        for idx, query in enumerate(cast(List[str], queries)):<br>            response_str = cast(List, response_strs)[idx]<br>            contexts = cast(List, contexts_list)[idx]<br>            for name, evaluator in self.evaluators.items():<br>                if multi_kwargs:<br>                    # multi-evaluator - get appropriate runtime kwargs if present<br>                    kwargs = (<br>                        eval_kwargs_lists[name] if name in eval_kwargs_lists else {}<br>                    )<br>                else:<br>                    # single evaluator (maintain backwards compatibility)<br>                    kwargs = eval_kwargs_lists<br>                eval_kwargs = self._get_eval_kwargs(kwargs, idx)<br>                eval_jobs.append(<br>                    eval_worker(<br>                        self.semaphore,<br>                        evaluator,<br>                        name,<br>                        query=query,<br>                        response_str=response_str,<br>                        contexts=contexts,<br>                        eval_kwargs=eval_kwargs,<br>                    )<br>                )<br>        results = await self.asyncio_mod.gather(*eval_jobs)<br>        # Format results<br>        return self._format_results(results)<br>    async def aevaluate_responses(<br>        self,<br>        queries: Optional[List[str]] = None,<br>        responses: Optional[List[Response]] = None,<br>        **eval_kwargs_lists: Dict[str, Any],<br>    ) -> Dict[str, List[EvaluationResult]]:<br>        \"\"\"<br>        Evaluate query, response pairs.<br>        This evaluates queries and response objects.<br>        Args:<br>            queries (Optional[List[str]]): List of query strings. Defaults to None.<br>            responses (Optional[List[Response]]): List of response objects.<br>                Defaults to None.<br>            **eval_kwargs_lists (Dict[str, Any]): Dict of either dicts or lists<br>                of kwargs to pass to evaluator. Defaults to None.<br>                    multiple evaluators: {evaluator: {kwarg: [list of values]},...}<br>                    single evaluator:    {kwarg: [list of values]}<br>        \"\"\"<br>        queries, responses = self._validate_and_clean_inputs(queries, responses)<br>        eval_kwargs_lists = self._validate_nested_eval_kwargs_types(eval_kwargs_lists)<br>        # boolean to check if using multi kwarg evaluator<br>        multi_kwargs = len(eval_kwargs_lists) > 0 and isinstance(<br>            next(iter(eval_kwargs_lists.values())), dict<br>        )<br>        # run evaluations<br>        eval_jobs = []<br>        for idx, query in enumerate(cast(List[str], queries)):<br>            response = cast(List, responses)[idx]<br>            for name, evaluator in self.evaluators.items():<br>                if multi_kwargs:<br>                    # multi-evaluator - get appropriate runtime kwargs if present<br>                    kwargs = (<br>                        eval_kwargs_lists[name] if name in eval_kwargs_lists else {}<br>                    )<br>                else:<br>                    # single evaluator (maintain backwards compatibility)<br>                    kwargs = eval_kwargs_lists<br>                eval_kwargs = self._get_eval_kwargs(kwargs, idx)<br>                eval_jobs.append(<br>                    eval_response_worker(<br>                        self.semaphore,<br>                        evaluator,<br>                        name,<br>                        query=query,<br>                        response=response,<br>                        eval_kwargs=eval_kwargs,<br>                    )<br>                )<br>        results = await self.asyncio_mod.gather(*eval_jobs)<br>        # Format results<br>        return self._format_results(results)<br>    async def aevaluate_queries(<br>        self,<br>        query_engine: BaseQueryEngine,<br>        queries: Optional[List[str]] = None,<br>        **eval_kwargs_lists: Dict[str, Any],<br>    ) -> Dict[str, List[EvaluationResult]]:<br>        \"\"\"<br>        Evaluate queries.<br>        Args:<br>            query_engine (BaseQueryEngine): Query engine.<br>            queries (Optional[List[str]]): List of query strings. Defaults to None.<br>            **eval_kwargs_lists (Dict[str, Any]): Dict of lists of kwargs to<br>                pass to evaluator. Defaults to None.<br>        \"\"\"<br>        if queries is None:<br>            raise ValueError(\"`queries` must be provided\")<br>        # gather responses<br>        response_jobs = []<br>        for query in queries:<br>            response_jobs.append(response_worker(self.semaphore, query_engine, query))<br>        responses = await self.asyncio_mod.gather(*response_jobs)<br>        return await self.aevaluate_responses(<br>            queries=queries,<br>            responses=responses,<br>            **eval_kwargs_lists,<br>        )<br>    def evaluate_response_strs(<br>        self,<br>        queries: Optional[List[str]] = None,<br>        response_strs: Optional[List[str]] = None,<br>        contexts_list: Optional[List[List[str]]] = None,<br>        **eval_kwargs_lists: Dict[str, Any],<br>    ) -> Dict[str, List[EvaluationResult]]:<br>        \"\"\"<br>        Evaluate query, response pairs.<br>        Sync version of aevaluate_response_strs.<br>        \"\"\"<br>        return asyncio_run(<br>            self.aevaluate_response_strs(<br>                queries=queries,<br>                response_strs=response_strs,<br>                contexts_list=contexts_list,<br>                **eval_kwargs_lists,<br>            )<br>        )<br>    def evaluate_responses(<br>        self,<br>        queries: Optional[List[str]] = None,<br>        responses: Optional[List[Response]] = None,<br>        **eval_kwargs_lists: Dict[str, Any],<br>    ) -> Dict[str, List[EvaluationResult]]:<br>        \"\"\"<br>        Evaluate query, response objs.<br>        Sync version of aevaluate_responses.<br>        \"\"\"<br>        return asyncio_run(<br>            self.aevaluate_responses(<br>                queries=queries,<br>                responses=responses,<br>                **eval_kwargs_lists,<br>            )<br>        )<br>    def evaluate_queries(<br>        self,<br>        query_engine: BaseQueryEngine,<br>        queries: Optional[List[str]] = None,<br>        **eval_kwargs_lists: Dict[str, Any],<br>    ) -> Dict[str, List[EvaluationResult]]:<br>        \"\"\"<br>        Evaluate queries.<br>        Sync version of aevaluate_queries.<br>        \"\"\"<br>        return asyncio_run(<br>            self.aevaluate_queries(<br>                query_engine=query_engine,<br>                queries=queries,<br>                **eval_kwargs_lists,<br>            )<br>        )<br>    def upload_eval_results(<br>        self,<br>        project_name: str,<br>        app_name: str,<br>        results: Dict[str, List[EvaluationResult]],<br>    ) -> None:<br>        \"\"\"<br>        Upload the evaluation results to LlamaCloud.<br>        Args:<br>            project_name (str): The name of the project.<br>            app_name (str): The name of the app.<br>            results (Dict[str, List[EvaluationResult]]):<br>                The evaluation results, a mapping of metric name to a list of EvaluationResult objects.<br>        Examples:<br>            ```python<br>            results = batch_runner.evaluate_responses(...)<br>            batch_runner.upload_eval_results(<br>                project_name=\"my_project\",<br>                app_name=\"my_app\",<br>                results=results<br>            )<br>            ```<br>        \"\"\"<br>        from llama_index.core.evaluation.eval_utils import upload_eval_results<br>        upload_eval_results(<br>            project_name=project_name, app_name=app_name, results=results<br>        )<br>```` |\n\n### aevaluate\\_response\\_strs`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/\\#llama_index.core.evaluation.BatchEvalRunner.aevaluate_response_strs \"Permanent link\")\n\n```\naevaluate_response_strs(queries: Optional[List[str]] = None, response_strs: Optional[List[str]] = None, contexts_list: Optional[List[List[str]]] = None, **eval_kwargs_lists: Dict[str, Any]) -> Dict[str, List[EvaluationResult]]\n\n```\n\nEvaluate query, response pairs.\n\nThis evaluates queries, responses, contexts as string inputs.\nCan supply additional kwargs to the evaluator in eval\\_kwargs\\_lists.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `queries` | `Optional[List[str]]` | List of query strings. Defaults to None. | `None` |\n| `response_strs` | `Optional[List[str]]` | List of response strings.<br>Defaults to None. | `None` |\n| `contexts_list` | `Optional[List[List[str]]]` | List of context lists.<br>Defaults to None. | `None` |\n| `**eval_kwargs_lists` | `Dict[str, Any]` | Dict of either dicts or lists<br>of kwargs to pass to evaluator. Defaults to None.<br>multiple evaluators: {evaluator: {kwarg: \\[list of values\\]},...}<br>single evaluator: {kwarg: \\[list of values\\]} | `{}` |\n\nSource code in `llama-index-core/llama_index/core/evaluation/batch_runner.py`\n\n|     |     |\n| --- | --- |\n| ```<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>``` | ```<br>async def aevaluate_response_strs(<br>    self,<br>    queries: Optional[List[str]] = None,<br>    response_strs: Optional[List[str]] = None,<br>    contexts_list: Optional[List[List[str]]] = None,<br>    **eval_kwargs_lists: Dict[str, Any],<br>) -> Dict[str, List[EvaluationResult]]:<br>    \"\"\"<br>    Evaluate query, response pairs.<br>    This evaluates queries, responses, contexts as string inputs.<br>    Can supply additional kwargs to the evaluator in eval_kwargs_lists.<br>    Args:<br>        queries (Optional[List[str]]): List of query strings. Defaults to None.<br>        response_strs (Optional[List[str]]): List of response strings.<br>            Defaults to None.<br>        contexts_list (Optional[List[List[str]]]): List of context lists.<br>            Defaults to None.<br>        **eval_kwargs_lists (Dict[str, Any]): Dict of either dicts or lists<br>            of kwargs to pass to evaluator. Defaults to None.<br>                multiple evaluators: {evaluator: {kwarg: [list of values]},...}<br>                single evaluator:    {kwarg: [list of values]}<br>    \"\"\"<br>    queries, response_strs, contexts_list = self._validate_and_clean_inputs(<br>        queries, response_strs, contexts_list<br>    )<br>    eval_kwargs_lists = self._validate_nested_eval_kwargs_types(eval_kwargs_lists)<br>    # boolean to check if using multi kwarg evaluator<br>    multi_kwargs = len(eval_kwargs_lists) > 0 and isinstance(<br>        next(iter(eval_kwargs_lists.values())), dict<br>    )<br>    # run evaluations<br>    eval_jobs = []<br>    for idx, query in enumerate(cast(List[str], queries)):<br>        response_str = cast(List, response_strs)[idx]<br>        contexts = cast(List, contexts_list)[idx]<br>        for name, evaluator in self.evaluators.items():<br>            if multi_kwargs:<br>                # multi-evaluator - get appropriate runtime kwargs if present<br>                kwargs = (<br>                    eval_kwargs_lists[name] if name in eval_kwargs_lists else {}<br>                )<br>            else:<br>                # single evaluator (maintain backwards compatibility)<br>                kwargs = eval_kwargs_lists<br>            eval_kwargs = self._get_eval_kwargs(kwargs, idx)<br>            eval_jobs.append(<br>                eval_worker(<br>                    self.semaphore,<br>                    evaluator,<br>                    name,<br>                    query=query,<br>                    response_str=response_str,<br>                    contexts=contexts,<br>                    eval_kwargs=eval_kwargs,<br>                )<br>            )<br>    results = await self.asyncio_mod.gather(*eval_jobs)<br>    # Format results<br>    return self._format_results(results)<br>``` |\n\n### aevaluate\\_responses`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/\\#llama_index.core.evaluation.BatchEvalRunner.aevaluate_responses \"Permanent link\")\n\n```\naevaluate_responses(queries: Optional[List[str]] = None, responses: Optional[List[Response]] = None, **eval_kwargs_lists: Dict[str, Any]) -> Dict[str, List[EvaluationResult]]\n\n```\n\nEvaluate query, response pairs.\n\nThis evaluates queries and response objects.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `queries` | `Optional[List[str]]` | List of query strings. Defaults to None. | `None` |\n| `responses` | `Optional[List[Response]]` | List of response objects.<br>Defaults to None. | `None` |\n| `**eval_kwargs_lists` | `Dict[str, Any]` | Dict of either dicts or lists<br>of kwargs to pass to evaluator. Defaults to None.<br>multiple evaluators: {evaluator: {kwarg: \\[list of values\\]},...}<br>single evaluator: {kwarg: \\[list of values\\]} | `{}` |\n\nSource code in `llama-index-core/llama_index/core/evaluation/batch_runner.py`\n\n|     |     |\n| --- | --- |\n| ```<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>``` | ```<br>async def aevaluate_responses(<br>    self,<br>    queries: Optional[List[str]] = None,<br>    responses: Optional[List[Response]] = None,<br>    **eval_kwargs_lists: Dict[str, Any],<br>) -> Dict[str, List[EvaluationResult]]:<br>    \"\"\"<br>    Evaluate query, response pairs.<br>    This evaluates queries and response objects.<br>    Args:<br>        queries (Optional[List[str]]): List of query strings. Defaults to None.<br>        responses (Optional[List[Response]]): List of response objects.<br>            Defaults to None.<br>        **eval_kwargs_lists (Dict[str, Any]): Dict of either dicts or lists<br>            of kwargs to pass to evaluator. Defaults to None.<br>                multiple evaluators: {evaluator: {kwarg: [list of values]},...}<br>                single evaluator:    {kwarg: [list of values]}<br>    \"\"\"<br>    queries, responses = self._validate_and_clean_inputs(queries, responses)<br>    eval_kwargs_lists = self._validate_nested_eval_kwargs_types(eval_kwargs_lists)<br>    # boolean to check if using multi kwarg evaluator<br>    multi_kwargs = len(eval_kwargs_lists) > 0 and isinstance(<br>        next(iter(eval_kwargs_lists.values())), dict<br>    )<br>    # run evaluations<br>    eval_jobs = []<br>    for idx, query in enumerate(cast(List[str], queries)):<br>        response = cast(List, responses)[idx]<br>        for name, evaluator in self.evaluators.items():<br>            if multi_kwargs:<br>                # multi-evaluator - get appropriate runtime kwargs if present<br>                kwargs = (<br>                    eval_kwargs_lists[name] if name in eval_kwargs_lists else {}<br>                )<br>            else:<br>                # single evaluator (maintain backwards compatibility)<br>                kwargs = eval_kwargs_lists<br>            eval_kwargs = self._get_eval_kwargs(kwargs, idx)<br>            eval_jobs.append(<br>                eval_response_worker(<br>                    self.semaphore,<br>                    evaluator,<br>                    name,<br>                    query=query,<br>                    response=response,<br>                    eval_kwargs=eval_kwargs,<br>                )<br>            )<br>    results = await self.asyncio_mod.gather(*eval_jobs)<br>    # Format results<br>    return self._format_results(results)<br>``` |\n\n### aevaluate\\_queries`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/\\#llama_index.core.evaluation.BatchEvalRunner.aevaluate_queries \"Permanent link\")\n\n```\naevaluate_queries(query_engine: BaseQueryEngine, queries: Optional[List[str]] = None, **eval_kwargs_lists: Dict[str, Any]) -> Dict[str, List[EvaluationResult]]\n\n```\n\nEvaluate queries.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `query_engine` | `BaseQueryEngine` | Query engine. | _required_ |\n| `queries` | `Optional[List[str]]` | List of query strings. Defaults to None. | `None` |\n| `**eval_kwargs_lists` | `Dict[str, Any]` | Dict of lists of kwargs to<br>pass to evaluator. Defaults to None. | `{}` |\n\nSource code in `llama-index-core/llama_index/core/evaluation/batch_runner.py`\n\n|     |     |\n| --- | --- |\n| ```<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>``` | ```<br>async def aevaluate_queries(<br>    self,<br>    query_engine: BaseQueryEngine,<br>    queries: Optional[List[str]] = None,<br>    **eval_kwargs_lists: Dict[str, Any],<br>) -> Dict[str, List[EvaluationResult]]:<br>    \"\"\"<br>    Evaluate queries.<br>    Args:<br>        query_engine (BaseQueryEngine): Query engine.<br>        queries (Optional[List[str]]): List of query strings. Defaults to None.<br>        **eval_kwargs_lists (Dict[str, Any]): Dict of lists of kwargs to<br>            pass to evaluator. Defaults to None.<br>    \"\"\"<br>    if queries is None:<br>        raise ValueError(\"`queries` must be provided\")<br>    # gather responses<br>    response_jobs = []<br>    for query in queries:<br>        response_jobs.append(response_worker(self.semaphore, query_engine, query))<br>    responses = await self.asyncio_mod.gather(*response_jobs)<br>    return await self.aevaluate_responses(<br>        queries=queries,<br>        responses=responses,<br>        **eval_kwargs_lists,<br>    )<br>``` |\n\n### evaluate\\_response\\_strs [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/\\#llama_index.core.evaluation.BatchEvalRunner.evaluate_response_strs \"Permanent link\")\n\n```\nevaluate_response_strs(queries: Optional[List[str]] = None, response_strs: Optional[List[str]] = None, contexts_list: Optional[List[List[str]]] = None, **eval_kwargs_lists: Dict[str, Any]) -> Dict[str, List[EvaluationResult]]\n\n```\n\nEvaluate query, response pairs.\n\nSync version of aevaluate\\_response\\_strs.\n\nSource code in `llama-index-core/llama_index/core/evaluation/batch_runner.py`\n\n|     |     |\n| --- | --- |\n| ```<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>``` | ```<br>def evaluate_response_strs(<br>    self,<br>    queries: Optional[List[str]] = None,<br>    response_strs: Optional[List[str]] = None,<br>    contexts_list: Optional[List[List[str]]] = None,<br>    **eval_kwargs_lists: Dict[str, Any],<br>) -> Dict[str, List[EvaluationResult]]:<br>    \"\"\"<br>    Evaluate query, response pairs.<br>    Sync version of aevaluate_response_strs.<br>    \"\"\"<br>    return asyncio_run(<br>        self.aevaluate_response_strs(<br>            queries=queries,<br>            response_strs=response_strs,<br>            contexts_list=contexts_list,<br>            **eval_kwargs_lists,<br>        )<br>    )<br>``` |\n\n### evaluate\\_responses [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/\\#llama_index.core.evaluation.BatchEvalRunner.evaluate_responses \"Permanent link\")\n\n```\nevaluate_responses(queries: Optional[List[str]] = None, responses: Optional[List[Response]] = None, **eval_kwargs_lists: Dict[str, Any]) -> Dict[str, List[EvaluationResult]]\n\n```\n\nEvaluate query, response objs.\n\nSync version of aevaluate\\_responses.\n\nSource code in `llama-index-core/llama_index/core/evaluation/batch_runner.py`\n\n|     |     |\n| --- | --- |\n| ```<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>``` | ```<br>def evaluate_responses(<br>    self,<br>    queries: Optional[List[str]] = None,<br>    responses: Optional[List[Response]] = None,<br>    **eval_kwargs_lists: Dict[str, Any],<br>) -> Dict[str, List[EvaluationResult]]:<br>    \"\"\"<br>    Evaluate query, response objs.<br>    Sync version of aevaluate_responses.<br>    \"\"\"<br>    return asyncio_run(<br>        self.aevaluate_responses(<br>            queries=queries,<br>            responses=responses,<br>            **eval_kwargs_lists,<br>        )<br>    )<br>``` |\n\n### evaluate\\_queries [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/\\#llama_index.core.evaluation.BatchEvalRunner.evaluate_queries \"Permanent link\")\n\n```\nevaluate_queries(query_engine: BaseQueryEngine, queries: Optional[List[str]] = None, **eval_kwargs_lists: Dict[str, Any]) -> Dict[str, List[EvaluationResult]]\n\n```\n\nEvaluate queries.\n\nSync version of aevaluate\\_queries.\n\nSource code in `llama-index-core/llama_index/core/evaluation/batch_runner.py`\n\n|     |     |\n| --- | --- |\n| ```<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>``` | ```<br>def evaluate_queries(<br>    self,<br>    query_engine: BaseQueryEngine,<br>    queries: Optional[List[str]] = None,<br>    **eval_kwargs_lists: Dict[str, Any],<br>) -> Dict[str, List[EvaluationResult]]:<br>    \"\"\"<br>    Evaluate queries.<br>    Sync version of aevaluate_queries.<br>    \"\"\"<br>    return asyncio_run(<br>        self.aevaluate_queries(<br>            query_engine=query_engine,<br>            queries=queries,<br>            **eval_kwargs_lists,<br>        )<br>    )<br>``` |\n\n### upload\\_eval\\_results [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/\\#llama_index.core.evaluation.BatchEvalRunner.upload_eval_results \"Permanent link\")\n\n```\nupload_eval_results(project_name: str, app_name: str, results: Dict[str, List[EvaluationResult]]) -> None\n\n```\n\nUpload the evaluation results to LlamaCloud.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `project_name` | `str` | The name of the project. | _required_ |\n| `app_name` | `str` | The name of the app. | _required_ |\n| `results` | `Dict[str, List[EvaluationResult]]` | The evaluation results, a mapping of metric name to a list of EvaluationResult objects. | _required_ |\n\n**Examples:**\n\n```\nresults = batch_runner.evaluate_responses(...)\n\nbatch_runner.upload_eval_results(\n    project_name=\"my_project\",\n    app_name=\"my_app\",\n    results=results\n)\n\n```\n\nSource code in `llama-index-core/llama_index/core/evaluation/batch_runner.py`\n\n|     |     |\n| --- | --- |\n| ```<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>``` | ````<br>def upload_eval_results(<br>    self,<br>    project_name: str,<br>    app_name: str,<br>    results: Dict[str, List[EvaluationResult]],<br>) -> None:<br>    \"\"\"<br>    Upload the evaluation results to LlamaCloud.<br>    Args:<br>        project_name (str): The name of the project.<br>        app_name (str): The name of the app.<br>        results (Dict[str, List[EvaluationResult]]):<br>            The evaluation results, a mapping of metric name to a list of EvaluationResult objects.<br>    Examples:<br>        ```python<br>        results = batch_runner.evaluate_responses(...)<br>        batch_runner.upload_eval_results(<br>            project_name=\"my_project\",<br>            app_name=\"my_app\",<br>            results=results<br>        )<br>        ```<br>    \"\"\"<br>    from llama_index.core.evaluation.eval_utils import upload_eval_results<br>    upload_eval_results(<br>        project_name=project_name, app_name=app_name, results=results<br>    )<br>```` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Index - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/evaluation/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/fireworks/#llama_index.embeddings.fireworks.FireworksEmbedding)\n\n# Fireworks\n\n## FireworksEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/fireworks/\\#llama_index.embeddings.fireworks.FireworksEmbedding \"Permanent link\")\n\nBases: `OpenAIEmbedding`\n\nFireworks class for embeddings.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model` | `str` | Model for embedding.<br>Defaults to \"nomic-ai/nomic-embed-text-v1.5\" | _required_ |\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-fireworks/llama_index/embeddings/fireworks/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>``` | ```<br>class FireworksEmbedding(OpenAIEmbedding):<br>    \"\"\"<br>    Fireworks class for embeddings.<br>    Args:<br>        model (str): Model for embedding.<br>            Defaults to \"nomic-ai/nomic-embed-text-v1.5\"<br>    \"\"\"<br>    additional_kwargs: Dict[str, Any] = Field(<br>        default_factory=dict, description=\"Additional kwargs for the OpenAI API.\"<br>    )<br>    api_key: str = Field(description=\"The Fireworks API key.\")<br>    api_base: str = Field(description=\"The base URL for Fireworks API.\")<br>    api_version: str = Field(description=\"The version for OpenAI API.\")<br>    def __init__(<br>        self,<br>        model_name: str = DEFAULT_MODEL,<br>        dimensions: Optional[int] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        additional_kwargs: Optional[Dict[str, Any]] = None,<br>        api_key: Optional[str] = None,<br>        api_base: Optional[str] = DEFAULT_API_BASE,<br>        api_version: Optional[str] = None,<br>        max_retries: int = 10,<br>        timeout: float = 60.0,<br>        reuse_client: bool = True,<br>        callback_manager: Optional[CallbackManager] = None,<br>        default_headers: Optional[Dict[str, str]] = None,<br>        http_client: Optional[httpx.Client] = None,<br>        **kwargs: Any,<br>    ) -> None:<br>        api_key, api_base, api_version = resolve_fireworks_credentials(<br>            api_key=api_key,<br>            api_base=api_base,<br>            api_version=api_version,<br>        )<br>        super().__init__(<br>            model_name=model_name,<br>            dimensions=dimensions,<br>            embed_batch_size=embed_batch_size,<br>            additional_kwargs=additional_kwargs,<br>            api_key=api_key,<br>            api_base=api_base,<br>            api_version=api_version,<br>            max_retries=max_retries,<br>            timeout=timeout,<br>            reuse_client=reuse_client,<br>            callback_manager=callback_manager,<br>            default_headers=default_headers,<br>            http_client=http_client,<br>            **kwargs,<br>        )<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"FireworksEmbedding\"<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Fireworks - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/fireworks/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/agent/lats/#llama_index.agent.lats.LATSAgentWorker)\n\n# Lats\n\n## LATSAgentWorker [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/lats/\\#llama_index.agent.lats.LATSAgentWorker \"Permanent link\")\n\nBases: `CustomSimpleAgentWorker`\n\nAgent worker that performs a step of Language Agent Tree Search.\n\nSource paper: https://arxiv.org/pdf/2310.04406v2.pdf.\n\nContinues iterating until there's no errors / task is done.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-lats/llama_index/agent/lats/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>``` | ```<br>class LATSAgentWorker(CustomSimpleAgentWorker):<br>    \"\"\"Agent worker that performs a step of Language Agent Tree Search.<br>    Source paper: https://arxiv.org/pdf/2310.04406v2.pdf.<br>    Continues iterating until there's no errors / task is done.<br>    \"\"\"<br>    num_expansions: int = Field(default=2, description=\"Number of expansions to do.\")<br>    reflection_prompt: PromptTemplate = Field(..., description=\"Reflection prompt.\")<br>    candiate_expansion_prompt: PromptTemplate = Field(<br>        ..., description=\"Candidate expansion prompt.\"<br>    )<br>    max_rollouts: int = Field(<br>        default=5,<br>        description=(<br>            \"Max rollouts. By default, -1 means that we keep going until the first solution is found.\"<br>        ),<br>    )<br>    chat_formatter: ReActChatFormatter = Field(<br>        default_factory=ReActChatFormatter, description=\"Chat formatter.\"<br>    )<br>    def __init__(<br>        self,<br>        tools: List[BaseTool],<br>        llm: Optional[LLM] = None,<br>        num_expansions: int = 2,<br>        max_rollouts: int = 5,<br>        reflection_prompt: Optional[PromptTemplate] = None,<br>        candiate_expansion_prompt: Optional[PromptTemplate] = None,<br>        **kwargs: Any,<br>    ) -> None:<br>        \"\"\"Init params.\"\"\"<br>        # validate that all tools are query engine tools<br>        llm = llm or Settings.llm<br>        super().__init__(<br>            tools=tools,<br>            llm=llm,<br>            num_expansions=num_expansions,<br>            max_rollouts=max_rollouts,<br>            reflection_prompt=reflection_prompt or DEFAULT_REFLECTION_PROMPT,<br>            candiate_expansion_prompt=candiate_expansion_prompt<br>            or DEFAULT_CANDIDATES_PROMPT,<br>            **kwargs,<br>        )<br>    def _initialize_state(self, task: Task, **kwargs: Any) -> Dict[str, Any]:<br>        \"\"\"Initialize state.\"\"\"<br>        # initialize root node<br>        root_node = SearchNode(<br>            current_reasoning=[ObservationReasoningStep(observation=task.input)],<br>            evaluation=Evaluation(score=1),  # evaluation for root node is blank<br>        )<br>        return {\"count\": 0, \"solution_queue\": [], \"root_node\": root_node}<br>    async def _arun_candidate(<br>        self,<br>        node: SearchNode,<br>        task: Task,<br>    ) -> List[BaseReasoningStep]:<br>        \"\"\"Generate candidate for a given node.<br>        Generically we sample the action space to generate new candidate nodes.<br>        Practically since we're using a ReAct powered agent, this means<br>        using generating a ReAct trajectory, running a tool.<br>        \"\"\"<br>        output_parser = ReActOutputParser()<br>        # format react prompt<br>        formatted_prompt = self.chat_formatter.format(<br>            self.tools,<br>            chat_history=task.memory.get(),<br>            current_reasoning=node.current_reasoning,<br>        )<br>        # run LLM<br>        response = await self.llm.achat(formatted_prompt)<br>        # parse output into reasoning step<br>        try:<br>            reasoning_step = output_parser.parse(response.message.content)<br>        except ValueError as e:<br>            reasoning_step = ResponseReasoningStep(<br>                thought=response.message.content,<br>                response=f\"Encountered an error parsing: {e!s}\",<br>            )<br>        # get response or run tool<br>        if reasoning_step.is_done:<br>            reasoning_step = cast(ResponseReasoningStep, reasoning_step)<br>            current_reasoning = [reasoning_step]<br>        else:<br>            reasoning_step = cast(ActionReasoningStep, reasoning_step)<br>            tool_selection = ToolSelection(<br>                tool_id=reasoning_step.action,<br>                tool_name=reasoning_step.action,<br>                tool_kwargs=reasoning_step.action_input,<br>            )<br>            try:<br>                tool_output = await acall_tool_with_selection(<br>                    tool_selection, self.tools, verbose=self.verbose<br>                )<br>            except Exception as e:<br>                tool_output = f\"Encountered error: {e!s}\"<br>            observation_step = ObservationReasoningStep(observation=str(tool_output))<br>            current_reasoning = [reasoning_step, observation_step]<br>        return current_reasoning<br>    async def _aevaluate(<br>        self,<br>        cur_node: SearchNode,<br>        current_reasoning: List[BaseReasoningStep],<br>        input: str,<br>    ) -> float:<br>        \"\"\"Evaluate.\"\"\"<br>        all_reasoning = cur_node.current_reasoning + current_reasoning<br>        history_str = \"\\n\".join([s.get_content() for s in all_reasoning])<br>        evaluation = await self.llm.astructured_predict(<br>            Evaluation,<br>            prompt=self.reflection_prompt,<br>            query=input,<br>            conversation_history=history_str,<br>        )<br>        if self.verbose:<br>            print_text(<br>                f\"> Evaluation for input {input}\\n: {evaluation}\\n\\n\", color=\"pink\"<br>            )<br>        return evaluation<br>    async def _get_next_candidates(<br>        self,<br>        cur_node: SearchNode,<br>        input: str,<br>    ) -> List[str]:<br>        \"\"\"Get next candidates.\"\"\"<br>        # get candidates<br>        history_str = \"\\n\".join([s.get_content() for s in cur_node.current_reasoning])<br>        candidates = await self.llm.astructured_predict(<br>            Candidates,<br>            prompt=self.candiate_expansion_prompt,<br>            query=input,<br>            conversation_history=history_str,<br>            num_candidates=self.num_expansions,<br>        )<br>        candidate_strs = candidates.candidates[: self.num_expansions]<br>        if self.verbose:<br>            print_text(f\"> Got candidates: {candidate_strs}\\n\", color=\"yellow\")<br>        # ensure we have the right number of candidates<br>        if len(candidate_strs) < self.num_expansions:<br>            return (candidate_strs * self.num_expansions)[: self.num_expansions]<br>        else:<br>            return candidate_strs[: self.num_expansions]<br>    def _update_state(<br>        self,<br>        node: SearchNode,<br>        current_reasoning: List[BaseReasoningStep],<br>        evaluation: Evaluation,<br>    ) -> SearchNode:<br>        \"\"\"Update state.\"\"\"<br>        # create child node<br>        new_node = SearchNode(<br>            current_reasoning=node.current_reasoning + current_reasoning,<br>            parent=node,<br>            children=[],<br>            evaluation=evaluation,<br>        )<br>        node.children.append(new_node)<br>        # backpropagate the reward<br>        new_node.backpropagate(evaluation.score)<br>        return new_node<br>    def _run_step(<br>        self, state: Dict[str, Any], task: Task, input: Optional[str] = None<br>    ) -> Tuple[AgentChatResponse, bool]:<br>        \"\"\"Run step.<br>        Returns:<br>            Tuple of (agent_response, is_done)<br>        \"\"\"<br>        return asyncio.run(self._arun_step(state, task, input))<br>    async def _arun_step(<br>        self, state: Dict[str, Any], task: Task, input: Optional[str] = None<br>    ) -> Tuple[AgentChatResponse, bool]:<br>        \"\"\"Run step.<br>        Returns:<br>            Tuple of (agent_response, is_done)<br>        \"\"\"<br>        root_node = state[\"root_node\"]<br>        cur_node = root_node.get_best_leaf()<br>        if self.verbose:<br>            print_text(<br>                f\"> Selecting node to expand: {cur_node.answer}\\n\", color=\"green\"<br>            )<br>        # expand the given node, generate n candidate nodes<br>        new_candidates = await self._get_next_candidates(<br>            cur_node,<br>            task.input,<br>        )<br>        new_nodes = []<br>        for candidate in new_candidates:<br>            new_nodes.append(<br>                self._update_state(<br>                    cur_node,<br>                    [ObservationReasoningStep(observation=candidate)],<br>                    Evaluation(score=1),  # evaluation for candidate node is blank<br>                )<br>            )<br>        # expand the given node, generate n candidates<br>        # for each candidate, run tool, get response<br>        solution_queue: List[SearchNode] = state[\"solution_queue\"]<br>        # first, generate the candidates<br>        candidate_jobs = [<br>            self._arun_candidate(new_node, task) for new_node in new_nodes<br>        ]<br>        all_new_reasoning_steps = await asyncio.gather(*candidate_jobs)<br>        if self.verbose:<br>            for new_reasoning_steps in all_new_reasoning_steps:<br>                out_txt = \"\\n\".join([s.get_content() for s in new_reasoning_steps])<br>                print_text(f\"> Generated new reasoning step: {out_txt}\\n\", color=\"blue\")<br>        # then, evaluate the candidates<br>        eval_jobs = [<br>            self._aevaluate(new_node, new_reasoning_steps, task.input)<br>            for new_node, new_reasoning_steps in zip(new_nodes, all_new_reasoning_steps)<br>        ]<br>        evaluations = await asyncio.gather(*eval_jobs)<br>        # then, update the state<br>        for new_reasoning_steps, cur_new_node, evaluation in zip(<br>            all_new_reasoning_steps, new_nodes, evaluations<br>        ):<br>            new_node = self._update_state(cur_new_node, new_reasoning_steps, evaluation)<br>            if new_node.is_done:<br>                if self.verbose:<br>                    print_text(<br>                        f\"> Found solution node: {new_node.answer}\\n\", color=\"cyan\"<br>                    )<br>                solution_queue.append(new_node)<br>        # check if done<br>        state[\"count\"] += 1<br>        if self.max_rollouts == -1 and solution_queue:<br>            is_done = True<br>        elif self.max_rollouts > 0 and state[\"count\"] >= self.max_rollouts:<br>            is_done = True<br>        else:<br>            is_done = False<br>        # determine response<br>        if solution_queue:<br>            best_solution_node = max(solution_queue, key=lambda x: x.score)<br>            response = best_solution_node.answer<br>        else:<br>            response = \"I am still thinking.\"<br>        if self.verbose:<br>            print_text(f\"> Got final response: {response!s}\\n\", color=\"green\")<br>        # return response<br>        return AgentChatResponse(response=str(response)), is_done<br>    def _finalize_task(self, state: Dict[str, Any], **kwargs) -> None:<br>        \"\"\"Finalize task.\"\"\"<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Lats - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/agent/lats/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/clip/#llama_index.embeddings.clip.ClipEmbedding)\n\n# Clip\n\n## ClipEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/clip/\\#llama_index.embeddings.clip.ClipEmbedding \"Permanent link\")\n\nBases: `MultiModalEmbedding`\n\nCLIP embedding models for encoding text and image for Multi-Modal purpose.\n\nThis class provides an interface to generate embeddings using a model\ndeployed in OpenAI CLIP. At the initialization it requires a model name\nof CLIP.\n\nNote\n\nRequires `clip` package to be available in the PYTHONPATH. It can be installed with\n`pip install git+https://github.com/openai/CLIP.git`.\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-clip/llama_index/embeddings/clip/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>``` | ```<br>class ClipEmbedding(MultiModalEmbedding):<br>    \"\"\"CLIP embedding models for encoding text and image for Multi-Modal purpose.<br>    This class provides an interface to generate embeddings using a model<br>    deployed in OpenAI CLIP. At the initialization it requires a model name<br>    of CLIP.<br>    Note:<br>        Requires `clip` package to be available in the PYTHONPATH. It can be installed with<br>        `pip install git+https://github.com/openai/CLIP.git`.<br>    \"\"\"<br>    embed_batch_size: int = Field(default=DEFAULT_EMBED_BATCH_SIZE, gt=0)<br>    _clip: Any = PrivateAttr()<br>    _model: Any = PrivateAttr()<br>    _preprocess: Any = PrivateAttr()<br>    _device: Any = PrivateAttr()<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"ClipEmbedding\"<br>    def __init__(<br>        self,<br>        *,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        model_name: str = DEFAULT_CLIP_MODEL,<br>        **kwargs: Any,<br>    ):<br>        \"\"\"Initializes the ClipEmbedding class.<br>        During the initialization the `clip` package is imported.<br>        Args:<br>            embed_batch_size (int, optional): The batch size for embedding generation. Defaults to 10,<br>                must be > 0 and <= 100.<br>            model_name (str): The model name of Clip model.<br>        Raises:<br>            ImportError: If the `clip` package is not available in the PYTHONPATH.<br>            ValueError: If the model cannot be fetched from Open AI. or if the embed_batch_size<br>                is not in the range (0, 100].<br>        \"\"\"<br>        if embed_batch_size <= 0:<br>            raise ValueError(f\"Embed batch size {embed_batch_size}  must be > 0.\")<br>        try:<br>            import clip<br>            import torch<br>        except ImportError:<br>            raise ImportError(<br>                \"ClipEmbedding requires `pip install git+https://github.com/openai/CLIP.git` and torch.\"<br>            )<br>        super().__init__(<br>            embed_batch_size=embed_batch_size, model_name=model_name, **kwargs<br>        )<br>        try:<br>            self._device = \"cuda\" if torch.cuda.is_available() else \"cpu\"<br>            is_local_path = os.path.exists(self.model_name)<br>            if not is_local_path and self.model_name not in AVAILABLE_CLIP_MODELS:<br>                raise ValueError(<br>                    f\"Model name {self.model_name} is not available in CLIP.\"<br>                )<br>            self._model, self._preprocess = clip.load(<br>                self.model_name, device=self._device<br>            )<br>        except Exception as e:<br>            logger.error(\"Error while loading clip model.\")<br>            raise ValueError(\"Unable to fetch the requested embeddings model\") from e<br>    # TEXT EMBEDDINGS<br>    async def _aget_query_embedding(self, query: str) -> Embedding:<br>        return self._get_query_embedding(query)<br>    def _get_text_embedding(self, text: str) -> Embedding:<br>        return self._get_text_embeddings([text])[0]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[Embedding]:<br>        results = []<br>        for text in texts:<br>            try:<br>                import clip<br>            except ImportError:<br>                raise ImportError(<br>                    \"ClipEmbedding requires `pip install git+https://github.com/openai/CLIP.git` and torch.\"<br>                )<br>            text_embedding = self._model.encode_text(<br>                clip.tokenize(text).to(self._device)<br>            )<br>            results.append(text_embedding.tolist()[0])<br>        return results<br>    def _get_query_embedding(self, query: str) -> Embedding:<br>        return self._get_text_embedding(query)<br>    # IMAGE EMBEDDINGS<br>    async def _aget_image_embedding(self, img_file_path: ImageType) -> Embedding:<br>        return self._get_image_embedding(img_file_path)<br>    def _get_image_embedding(self, img_file_path: ImageType) -> Embedding:<br>        import torch<br>        with torch.no_grad():<br>            image = (<br>                self._preprocess(Image.open(img_file_path))<br>                .unsqueeze(0)<br>                .to(self._device)<br>            )<br>            return self._model.encode_image(image).tolist()[0]<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Clip - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/clip/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/retrieval/#llama_index.core.evaluation.BaseRetrievalEvaluator)\n\n# Retrieval\n\nEvaluation modules.\n\n## BaseRetrievalEvaluator [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/retrieval/\\#llama_index.core.evaluation.BaseRetrievalEvaluator \"Permanent link\")\n\nBases: `BaseModel`\n\nBase Retrieval Evaluator class.\n\nSource code in `llama-index-core/llama_index/core/evaluation/retrieval/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>``` | ```<br>class BaseRetrievalEvaluator(BaseModel):<br>    \"\"\"Base Retrieval Evaluator class.\"\"\"<br>    model_config = ConfigDict(arbitrary_types_allowed=True)<br>    metrics: List[BaseRetrievalMetric] = Field(<br>        ..., description=\"List of metrics to evaluate\"<br>    )<br>    @classmethod<br>    def from_metric_names(<br>        cls, metric_names: List[str], **kwargs: Any<br>    ) -> \"BaseRetrievalEvaluator\":<br>        \"\"\"Create evaluator from metric names.<br>        Args:<br>            metric_names (List[str]): List of metric names<br>            **kwargs: Additional arguments for the evaluator<br>        \"\"\"<br>        metric_types = resolve_metrics(metric_names)<br>        return cls(metrics=[metric() for metric in metric_types], **kwargs)<br>    @abstractmethod<br>    async def _aget_retrieved_ids_and_texts(<br>        self, query: str, mode: RetrievalEvalMode = RetrievalEvalMode.TEXT<br>    ) -> Tuple[List[str], List[str]]:<br>        \"\"\"Get retrieved ids and texts.\"\"\"<br>        raise NotImplementedError<br>    def evaluate(<br>        self,<br>        query: str,<br>        expected_ids: List[str],<br>        expected_texts: Optional[List[str]] = None,<br>        mode: RetrievalEvalMode = RetrievalEvalMode.TEXT,<br>        **kwargs: Any,<br>    ) -> RetrievalEvalResult:<br>        \"\"\"Run evaluation results with query string and expected ids.<br>        Args:<br>            query (str): Query string<br>            expected_ids (List[str]): Expected ids<br>        Returns:<br>            RetrievalEvalResult: Evaluation result<br>        \"\"\"<br>        return asyncio_run(<br>            self.aevaluate(<br>                query=query,<br>                expected_ids=expected_ids,<br>                expected_texts=expected_texts,<br>                mode=mode,<br>                **kwargs,<br>            )<br>        )<br>    # @abstractmethod<br>    async def aevaluate(<br>        self,<br>        query: str,<br>        expected_ids: List[str],<br>        expected_texts: Optional[List[str]] = None,<br>        mode: RetrievalEvalMode = RetrievalEvalMode.TEXT,<br>        **kwargs: Any,<br>    ) -> RetrievalEvalResult:<br>        \"\"\"Run evaluation with query string, retrieved contexts,<br>        and generated response string.<br>        Subclasses can override this method to provide custom evaluation logic and<br>        take in additional arguments.<br>        \"\"\"<br>        retrieved_ids, retrieved_texts = await self._aget_retrieved_ids_and_texts(<br>            query, mode<br>        )<br>        metric_dict = {}<br>        for metric in self.metrics:<br>            eval_result = metric.compute(<br>                query, expected_ids, retrieved_ids, expected_texts, retrieved_texts<br>            )<br>            metric_dict[metric.metric_name] = eval_result<br>        return RetrievalEvalResult(<br>            query=query,<br>            expected_ids=expected_ids,<br>            expected_texts=expected_texts,<br>            retrieved_ids=retrieved_ids,<br>            retrieved_texts=retrieved_texts,<br>            mode=mode,<br>            metric_dict=metric_dict,<br>        )<br>    async def aevaluate_dataset(<br>        self,<br>        dataset: EmbeddingQAFinetuneDataset,<br>        workers: int = 2,<br>        show_progress: bool = False,<br>        **kwargs: Any,<br>    ) -> List[RetrievalEvalResult]:<br>        \"\"\"Run evaluation with dataset.\"\"\"<br>        semaphore = asyncio.Semaphore(workers)<br>        async def eval_worker(<br>            query: str, expected_ids: List[str], mode: RetrievalEvalMode<br>        ) -> RetrievalEvalResult:<br>            async with semaphore:<br>                return await self.aevaluate(query, expected_ids=expected_ids, mode=mode)<br>        response_jobs = []<br>        mode = RetrievalEvalMode.from_str(dataset.mode)<br>        for query_id, query in dataset.queries.items():<br>            expected_ids = dataset.relevant_docs[query_id]<br>            response_jobs.append(eval_worker(query, expected_ids, mode))<br>        if show_progress:<br>            from tqdm.asyncio import tqdm_asyncio<br>            eval_results = await tqdm_asyncio.gather(*response_jobs)<br>        else:<br>            eval_results = await asyncio.gather(*response_jobs)<br>        return eval_results<br>``` |\n\n### from\\_metric\\_names`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/retrieval/\\#llama_index.core.evaluation.BaseRetrievalEvaluator.from_metric_names \"Permanent link\")\n\n```\nfrom_metric_names(metric_names: List[str], **kwargs: Any) -> BaseRetrievalEvaluator\n\n```\n\nCreate evaluator from metric names.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `metric_names` | `List[str]` | List of metric names | _required_ |\n| `**kwargs` | `Any` | Additional arguments for the evaluator | `{}` |\n\nSource code in `llama-index-core/llama_index/core/evaluation/retrieval/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>84<br>85<br>86<br>87<br>88<br>89<br>90<br>91<br>92<br>93<br>94<br>95<br>96<br>``` | ```<br>@classmethod<br>def from_metric_names(<br>    cls, metric_names: List[str], **kwargs: Any<br>) -> \"BaseRetrievalEvaluator\":<br>    \"\"\"Create evaluator from metric names.<br>    Args:<br>        metric_names (List[str]): List of metric names<br>        **kwargs: Additional arguments for the evaluator<br>    \"\"\"<br>    metric_types = resolve_metrics(metric_names)<br>    return cls(metrics=[metric() for metric in metric_types], **kwargs)<br>``` |\n\n### evaluate [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/retrieval/\\#llama_index.core.evaluation.BaseRetrievalEvaluator.evaluate \"Permanent link\")\n\n```\nevaluate(query: str, expected_ids: List[str], expected_texts: Optional[List[str]] = None, mode: RetrievalEvalMode = RetrievalEvalMode.TEXT, **kwargs: Any) -> RetrievalEvalResult\n\n```\n\nRun evaluation results with query string and expected ids.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `query` | `str` | Query string | _required_ |\n| `expected_ids` | `List[str]` | Expected ids | _required_ |\n\n**Returns:**\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `RetrievalEvalResult` | `RetrievalEvalResult` | Evaluation result |\n\nSource code in `llama-index-core/llama_index/core/evaluation/retrieval/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>``` | ```<br>def evaluate(<br>    self,<br>    query: str,<br>    expected_ids: List[str],<br>    expected_texts: Optional[List[str]] = None,<br>    mode: RetrievalEvalMode = RetrievalEvalMode.TEXT,<br>    **kwargs: Any,<br>) -> RetrievalEvalResult:<br>    \"\"\"Run evaluation results with query string and expected ids.<br>    Args:<br>        query (str): Query string<br>        expected_ids (List[str]): Expected ids<br>    Returns:<br>        RetrievalEvalResult: Evaluation result<br>    \"\"\"<br>    return asyncio_run(<br>        self.aevaluate(<br>            query=query,<br>            expected_ids=expected_ids,<br>            expected_texts=expected_texts,<br>            mode=mode,<br>            **kwargs,<br>        )<br>    )<br>``` |\n\n### aevaluate`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/retrieval/\\#llama_index.core.evaluation.BaseRetrievalEvaluator.aevaluate \"Permanent link\")\n\n```\naevaluate(query: str, expected_ids: List[str], expected_texts: Optional[List[str]] = None, mode: RetrievalEvalMode = RetrievalEvalMode.TEXT, **kwargs: Any) -> RetrievalEvalResult\n\n```\n\nRun evaluation with query string, retrieved contexts,\nand generated response string.\n\nSubclasses can override this method to provide custom evaluation logic and\ntake in additional arguments.\n\nSource code in `llama-index-core/llama_index/core/evaluation/retrieval/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>``` | ```<br>async def aevaluate(<br>    self,<br>    query: str,<br>    expected_ids: List[str],<br>    expected_texts: Optional[List[str]] = None,<br>    mode: RetrievalEvalMode = RetrievalEvalMode.TEXT,<br>    **kwargs: Any,<br>) -> RetrievalEvalResult:<br>    \"\"\"Run evaluation with query string, retrieved contexts,<br>    and generated response string.<br>    Subclasses can override this method to provide custom evaluation logic and<br>    take in additional arguments.<br>    \"\"\"<br>    retrieved_ids, retrieved_texts = await self._aget_retrieved_ids_and_texts(<br>        query, mode<br>    )<br>    metric_dict = {}<br>    for metric in self.metrics:<br>        eval_result = metric.compute(<br>            query, expected_ids, retrieved_ids, expected_texts, retrieved_texts<br>        )<br>        metric_dict[metric.metric_name] = eval_result<br>    return RetrievalEvalResult(<br>        query=query,<br>        expected_ids=expected_ids,<br>        expected_texts=expected_texts,<br>        retrieved_ids=retrieved_ids,<br>        retrieved_texts=retrieved_texts,<br>        mode=mode,<br>        metric_dict=metric_dict,<br>    )<br>``` |\n\n### aevaluate\\_dataset`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/retrieval/\\#llama_index.core.evaluation.BaseRetrievalEvaluator.aevaluate_dataset \"Permanent link\")\n\n```\naevaluate_dataset(dataset: EmbeddingQAFinetuneDataset, workers: int = 2, show_progress: bool = False, **kwargs: Any) -> List[RetrievalEvalResult]\n\n```\n\nRun evaluation with dataset.\n\nSource code in `llama-index-core/llama_index/core/evaluation/retrieval/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>``` | ```<br>async def aevaluate_dataset(<br>    self,<br>    dataset: EmbeddingQAFinetuneDataset,<br>    workers: int = 2,<br>    show_progress: bool = False,<br>    **kwargs: Any,<br>) -> List[RetrievalEvalResult]:<br>    \"\"\"Run evaluation with dataset.\"\"\"<br>    semaphore = asyncio.Semaphore(workers)<br>    async def eval_worker(<br>        query: str, expected_ids: List[str], mode: RetrievalEvalMode<br>    ) -> RetrievalEvalResult:<br>        async with semaphore:<br>            return await self.aevaluate(query, expected_ids=expected_ids, mode=mode)<br>    response_jobs = []<br>    mode = RetrievalEvalMode.from_str(dataset.mode)<br>    for query_id, query in dataset.queries.items():<br>        expected_ids = dataset.relevant_docs[query_id]<br>        response_jobs.append(eval_worker(query, expected_ids, mode))<br>    if show_progress:<br>        from tqdm.asyncio import tqdm_asyncio<br>        eval_results = await tqdm_asyncio.gather(*response_jobs)<br>    else:<br>        eval_results = await asyncio.gather(*response_jobs)<br>    return eval_results<br>``` |\n\n## RetrieverEvaluator [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/retrieval/\\#llama_index.core.evaluation.RetrieverEvaluator \"Permanent link\")\n\nBases: `BaseRetrievalEvaluator`\n\nRetriever evaluator.\n\nThis module will evaluate a retriever using a set of metrics.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `metrics` | `List[BaseRetrievalMetric]` | Sequence of metrics to evaluate | _required_ |\n| `retriever` |  | Retriever to evaluate. | _required_ |\n| `node_postprocessors` | `Optional[List[BaseNodePostprocessor]]` | Post-processor to apply after retrieval. | _required_ |\n\nSource code in `llama-index-core/llama_index/core/evaluation/retrieval/evaluator.py`\n\n|     |     |\n| --- | --- |\n| ```<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>``` | ```<br>class RetrieverEvaluator(BaseRetrievalEvaluator):<br>    \"\"\"Retriever evaluator.<br>    This module will evaluate a retriever using a set of metrics.<br>    Args:<br>        metrics (List[BaseRetrievalMetric]): Sequence of metrics to evaluate<br>        retriever: Retriever to evaluate.<br>        node_postprocessors (Optional[List[BaseNodePostprocessor]]): Post-processor to apply after retrieval.<br>    \"\"\"<br>    retriever: BaseRetriever = Field(..., description=\"Retriever to evaluate\")<br>    node_postprocessors: Optional[List[SerializeAsAny[BaseNodePostprocessor]]] = Field(<br>        default=None, description=\"Optional post-processor\"<br>    )<br>    async def _aget_retrieved_ids_and_texts(<br>        self, query: str, mode: RetrievalEvalMode = RetrievalEvalMode.TEXT<br>    ) -> Tuple[List[str], List[str]]:<br>        \"\"\"Get retrieved ids and texts, potentially applying a post-processor.\"\"\"<br>        retrieved_nodes = await self.retriever.aretrieve(query)<br>        if self.node_postprocessors:<br>            for node_postprocessor in self.node_postprocessors:<br>                retrieved_nodes = node_postprocessor.postprocess_nodes(<br>                    retrieved_nodes, query_str=query<br>                )<br>        return (<br>            [node.node.node_id for node in retrieved_nodes],<br>            [node.node.text for node in retrieved_nodes],<br>        )<br>``` |\n\n## RetrievalEvalResult [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/retrieval/\\#llama_index.core.evaluation.RetrievalEvalResult \"Permanent link\")\n\nBases: `BaseModel`\n\nRetrieval eval result.\n\nNOTE: this abstraction might change in the future.\n\n**Attributes:**\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `query` | `str` | Query string |\n| `expected_ids` | `List[str]` | Expected ids |\n| `retrieved_ids` | `List[str]` | Retrieved ids |\n| `metric_dict` | `Dict[str, BaseRetrievalMetric]` | Metric dictionary for the evaluation |\n\nSource code in `llama-index-core/llama_index/core/evaluation/retrieval/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>``` | ```<br>class RetrievalEvalResult(BaseModel):<br>    \"\"\"Retrieval eval result.<br>    NOTE: this abstraction might change in the future.<br>    Attributes:<br>        query (str): Query string<br>        expected_ids (List[str]): Expected ids<br>        retrieved_ids (List[str]): Retrieved ids<br>        metric_dict (Dict[str, BaseRetrievalMetric]): \\<br>            Metric dictionary for the evaluation<br>    \"\"\"<br>    model_config = ConfigDict(arbitrary_types_allowed=True)<br>    query: str = Field(..., description=\"Query string\")<br>    expected_ids: List[str] = Field(..., description=\"Expected ids\")<br>    expected_texts: Optional[List[str]] = Field(<br>        default=None,<br>        description=\"Expected texts associated with nodes provided in `expected_ids`\",<br>    )<br>    retrieved_ids: List[str] = Field(..., description=\"Retrieved ids\")<br>    retrieved_texts: List[str] = Field(..., description=\"Retrieved texts\")<br>    mode: \"RetrievalEvalMode\" = Field(<br>        default=RetrievalEvalMode.TEXT, description=\"text or image\"<br>    )<br>    metric_dict: Dict[str, RetrievalMetricResult] = Field(<br>        ..., description=\"Metric dictionary for the evaluation\"<br>    )<br>    @property<br>    def metric_vals_dict(self) -> Dict[str, float]:<br>        \"\"\"Dictionary of metric values.\"\"\"<br>        return {k: v.score for k, v in self.metric_dict.items()}<br>    def __str__(self) -> str:<br>        \"\"\"String representation.\"\"\"<br>        return f\"Query: {self.query}\\n\" f\"Metrics: {self.metric_vals_dict!s}\\n\"<br>``` |\n\n### metric\\_vals\\_dict`property`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/retrieval/\\#llama_index.core.evaluation.RetrievalEvalResult.metric_vals_dict \"Permanent link\")\n\n```\nmetric_vals_dict: Dict[str, float]\n\n```\n\nDictionary of metric values.\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Retrieval - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/evaluation/retrieval/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/gigachat/#llama_index.embeddings.gigachat.GigaChatEmbedding)\n\n# Gigachat\n\n## GigaChatEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/gigachat/\\#llama_index.embeddings.gigachat.GigaChatEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nGigaChat encoder class for generating embeddings.\n\n**Attributes:**\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `_client` | `Optional[GigaChat]` | Instance of the GigaChat client. |\n| `type` | `str` | Type identifier for the encoder, which is \"gigachat\". |\n\nExample\n\n.. code-block:: python\nfrom langchain\\_community.embeddings.gigachat import GigaChatEmbeddings\n\n```\nembeddings = GigaChatEmbeddings(\n    credentials=..., scope=..., verify_ssl_certs=False\n)\n\n```\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-gigachat/llama_index/embeddings/gigachat/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 13<br> 14<br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>``` | ```<br>class GigaChatEmbedding(BaseEmbedding):<br>    \"\"\"<br>    GigaChat encoder class for generating embeddings.<br>    Attributes:<br>        _client (Optional[GigaChat]): Instance of the GigaChat client.<br>        type (str): Type identifier for the encoder, which is \"gigachat\".<br>    Example:<br>        .. code-block:: python<br>            from langchain_community.embeddings.gigachat import GigaChatEmbeddings<br>            embeddings = GigaChatEmbeddings(<br>                credentials=..., scope=..., verify_ssl_certs=False<br>            )<br>    \"\"\"<br>    _client: Optional[GigaChat] = PrivateAttr()<br>    type: str = \"gigachat\"<br>    def __init__(<br>        self,<br>        name: Optional[str] = \"Embeddings\",<br>        auth_data: Optional[str] = None,<br>        scope: Optional[str] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>        **kwargs: Any,<br>    ) -> None:<br>        auth_data = get_from_param_or_env(<br>            \"auth_data\", auth_data, \"GIGACHAT_AUTH_DATA\", \"\"<br>        )<br>        if not auth_data:<br>            raise ValueError(<br>                \"You must provide an AUTH DATA to use GigaChat. \"<br>                \"You can either pass it in as an argument or set it `GIGACHAT_AUTH_DATA`.\"<br>            )<br>        if scope is None:<br>            raise ValueError(<br>                \"\"\"<br>                GigaChat scope cannot be 'None'.<br>                Set 'GIGACHAT_API_PERS' for personal use or 'GIGACHAT_API_CORP' for corporate use.<br>                \"\"\"<br>            )<br>        super().__init__(<br>            model_name=name,<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            **kwargs,<br>        )<br>        try:<br>            self._client = GigaChat(<br>                scope=scope, credentials=auth_data, verify_ssl_certs=False<br>            )<br>        except Exception as e:<br>            raise ValueError(f\"GigaChat client failed to initialize. Error: {e}\") from e<br>    @classmethod<br>    def class_name(cls) -> str:<br>        \"\"\"Return the class name.\"\"\"<br>        return \"GigaChatEmbedding\"<br>    def _get_query_embeddings(self, queries: List[str]) -> List[List[float]]:<br>        \"\"\"Synchronously Embed documents using a GigaChat embeddings model.<br>        Args:<br>            queries: The list of documents to embed.<br>        Returns:<br>            List of embeddings, one for each document.<br>        \"\"\"<br>        embeddings = self._client.embeddings(queries).data<br>        return [embeds_obj.embedding for embeds_obj in embeddings]<br>    async def _aget_query_embeddings(self, queries: List[str]) -> List[List[float]]:<br>        \"\"\"Asynchronously embed documents using a GigaChat embeddings model.<br>        Args:<br>            queries: The list of documents to embed.<br>        Returns:<br>            List of embeddings, one for each document.<br>        \"\"\"<br>        embeddings = (await self._client.aembeddings(queries)).data<br>        return [embeds_obj.embedding for embeds_obj in embeddings]<br>    def _get_query_embedding(self, query: List[str]) -> List[float]:<br>        \"\"\"Synchronously embed a document using GigaChat embeddings model.<br>        Args:<br>            query: The document to embed.<br>        Returns:<br>            Embeddings for the document.<br>        \"\"\"<br>        return self._client.embeddings(query).data[0].embedding<br>    async def _aget_query_embedding(self, query: List[str]) -> List[float]:<br>        \"\"\"Asynchronously embed a query using GigaChat embeddings model.<br>        Args:<br>            query: The document to embed.<br>        Returns:<br>            Embeddings for the document.<br>        \"\"\"<br>        return (await self._client.aembeddings(query)).data[0].embedding<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Synchronously embed a text using GigaChat embeddings model.<br>        Args:<br>            text: The text to embed.<br>        Returns:<br>            Embeddings for the text.<br>        \"\"\"<br>        return self._client.embeddings([text]).data[0].embedding<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously embed a text using GigaChat embeddings model.<br>        Args:<br>            text: The text to embed.<br>        Returns:<br>            Embeddings for the text.<br>        \"\"\"<br>        return (await self._client.aembeddings([text])).data[0].embedding<br>``` |\n\n### class\\_name`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/gigachat/\\#llama_index.embeddings.gigachat.GigaChatEmbedding.class_name \"Permanent link\")\n\n```\nclass_name() -> str\n\n```\n\nReturn the class name.\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-gigachat/llama_index/embeddings/gigachat/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>70<br>71<br>72<br>73<br>``` | ```<br>@classmethod<br>def class_name(cls) -> str:<br>    \"\"\"Return the class name.\"\"\"<br>    return \"GigaChatEmbedding\"<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Gigachat - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/gigachat/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/simple/#llama_index.core.chat_engine.SimpleChatEngine)\n\n# Simple\n\n## SimpleChatEngine [\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/simple/\\#llama_index.core.chat_engine.SimpleChatEngine \"Permanent link\")\n\nBases: `BaseChatEngine`\n\nSimple Chat Engine.\n\nHave a conversation with the LLM.\nThis does not make use of a knowledge base.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/simple.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>``` | ```<br>class SimpleChatEngine(BaseChatEngine):<br>    \"\"\"<br>    Simple Chat Engine.<br>    Have a conversation with the LLM.<br>    This does not make use of a knowledge base.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        llm: LLM,<br>        memory: BaseMemory,<br>        prefix_messages: List[ChatMessage],<br>        callback_manager: Optional[CallbackManager] = None,<br>    ) -> None:<br>        self._llm = llm<br>        self._memory = memory<br>        self._prefix_messages = prefix_messages<br>        self.callback_manager = callback_manager or CallbackManager([])<br>    @classmethod<br>    def from_defaults(<br>        cls,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        memory: Optional[BaseMemory] = None,<br>        memory_cls: Type[BaseMemory] = ChatMemoryBuffer,<br>        system_prompt: Optional[str] = None,<br>        prefix_messages: Optional[List[ChatMessage]] = None,<br>        llm: Optional[LLM] = None,<br>        **kwargs: Any,<br>    ) -> \"SimpleChatEngine\":<br>        \"\"\"Initialize a SimpleChatEngine from default parameters.\"\"\"<br>        llm = llm or Settings.llm<br>        chat_history = chat_history or []<br>        memory = memory or memory_cls.from_defaults(chat_history=chat_history, llm=llm)<br>        if system_prompt is not None:<br>            if prefix_messages is not None:<br>                raise ValueError(<br>                    \"Cannot specify both system_prompt and prefix_messages\"<br>                )<br>            prefix_messages = [<br>                ChatMessage(content=system_prompt, role=llm.metadata.system_role)<br>            ]<br>        prefix_messages = prefix_messages or []<br>        return cls(<br>            llm=llm,<br>            memory=memory,<br>            prefix_messages=prefix_messages,<br>            callback_manager=Settings.callback_manager,<br>        )<br>    @trace_method(\"chat\")<br>    def chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> AgentChatResponse:<br>        if chat_history is not None:<br>            self._memory.set(chat_history)<br>        self._memory.put(ChatMessage(content=message, role=\"user\"))<br>        if hasattr(self._memory, \"tokenizer_fn\"):<br>            initial_token_count = len(<br>                self._memory.tokenizer_fn(<br>                    \" \".join([(m.content or \"\") for m in self._prefix_messages])<br>                )<br>            )<br>        else:<br>            initial_token_count = 0<br>        all_messages = self._prefix_messages + self._memory.get(<br>            initial_token_count=initial_token_count<br>        )<br>        chat_response = self._llm.chat(all_messages)<br>        ai_message = chat_response.message<br>        self._memory.put(ai_message)<br>        return AgentChatResponse(response=str(chat_response.message.content))<br>    @trace_method(\"chat\")<br>    def stream_chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> StreamingAgentChatResponse:<br>        if chat_history is not None:<br>            self._memory.set(chat_history)<br>        self._memory.put(ChatMessage(content=message, role=\"user\"))<br>        if hasattr(self._memory, \"tokenizer_fn\"):<br>            initial_token_count = len(<br>                self._memory.tokenizer_fn(<br>                    \" \".join([(m.content or \"\") for m in self._prefix_messages])<br>                )<br>            )<br>        else:<br>            initial_token_count = 0<br>        all_messages = self._prefix_messages + self._memory.get(<br>            initial_token_count=initial_token_count<br>        )<br>        chat_response = StreamingAgentChatResponse(<br>            chat_stream=self._llm.stream_chat(all_messages)<br>        )<br>        thread = Thread(<br>            target=chat_response.write_response_to_history, args=(self._memory,)<br>        )<br>        thread.start()<br>        return chat_response<br>    @trace_method(\"chat\")<br>    async def achat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> AgentChatResponse:<br>        if chat_history is not None:<br>            self._memory.set(chat_history)<br>        self._memory.put(ChatMessage(content=message, role=\"user\"))<br>        if hasattr(self._memory, \"tokenizer_fn\"):<br>            initial_token_count = len(<br>                self._memory.tokenizer_fn(<br>                    \" \".join([(m.content or \"\") for m in self._prefix_messages])<br>                )<br>            )<br>        else:<br>            initial_token_count = 0<br>        all_messages = self._prefix_messages + self._memory.get(<br>            initial_token_count=initial_token_count<br>        )<br>        chat_response = await self._llm.achat(all_messages)<br>        ai_message = chat_response.message<br>        self._memory.put(ai_message)<br>        return AgentChatResponse(response=str(chat_response.message.content))<br>    @trace_method(\"chat\")<br>    async def astream_chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> StreamingAgentChatResponse:<br>        if chat_history is not None:<br>            self._memory.set(chat_history)<br>        self._memory.put(ChatMessage(content=message, role=\"user\"))<br>        if hasattr(self._memory, \"tokenizer_fn\"):<br>            initial_token_count = len(<br>                self._memory.tokenizer_fn(<br>                    \" \".join([(m.content or \"\") for m in self._prefix_messages])<br>                )<br>            )<br>        else:<br>            initial_token_count = 0<br>        all_messages = self._prefix_messages + self._memory.get(<br>            initial_token_count=initial_token_count<br>        )<br>        chat_response = StreamingAgentChatResponse(<br>            achat_stream=await self._llm.astream_chat(all_messages)<br>        )<br>        asyncio.create_task(chat_response.awrite_response_to_history(self._memory))<br>        return chat_response<br>    def reset(self) -> None:<br>        self._memory.reset()<br>    @property<br>    def chat_history(self) -> List[ChatMessage]:<br>        \"\"\"Get chat history.\"\"\"<br>        return self._memory.get_all()<br>``` |\n\n### chat\\_history`property`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/simple/\\#llama_index.core.chat_engine.SimpleChatEngine.chat_history \"Permanent link\")\n\n```\nchat_history: List[ChatMessage]\n\n```\n\nGet chat history.\n\n### from\\_defaults`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/simple/\\#llama_index.core.chat_engine.SimpleChatEngine.from_defaults \"Permanent link\")\n\n```\nfrom_defaults(chat_history: Optional[List[ChatMessage]] = None, memory: Optional[BaseMemory] = None, memory_cls: Type[BaseMemory] = ChatMemoryBuffer, system_prompt: Optional[str] = None, prefix_messages: Optional[List[ChatMessage]] = None, llm: Optional[LLM] = None, **kwargs: Any) -> SimpleChatEngine\n\n```\n\nInitialize a SimpleChatEngine from default parameters.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/simple.py`\n\n|     |     |\n| --- | --- |\n| ```<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>``` | ```<br>@classmethod<br>def from_defaults(<br>    cls,<br>    chat_history: Optional[List[ChatMessage]] = None,<br>    memory: Optional[BaseMemory] = None,<br>    memory_cls: Type[BaseMemory] = ChatMemoryBuffer,<br>    system_prompt: Optional[str] = None,<br>    prefix_messages: Optional[List[ChatMessage]] = None,<br>    llm: Optional[LLM] = None,<br>    **kwargs: Any,<br>) -> \"SimpleChatEngine\":<br>    \"\"\"Initialize a SimpleChatEngine from default parameters.\"\"\"<br>    llm = llm or Settings.llm<br>    chat_history = chat_history or []<br>    memory = memory or memory_cls.from_defaults(chat_history=chat_history, llm=llm)<br>    if system_prompt is not None:<br>        if prefix_messages is not None:<br>            raise ValueError(<br>                \"Cannot specify both system_prompt and prefix_messages\"<br>            )<br>        prefix_messages = [<br>            ChatMessage(content=system_prompt, role=llm.metadata.system_role)<br>        ]<br>    prefix_messages = prefix_messages or []<br>    return cls(<br>        llm=llm,<br>        memory=memory,<br>        prefix_messages=prefix_messages,<br>        callback_manager=Settings.callback_manager,<br>    )<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Simple - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/simple/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/context_relevancy/#llama_index.core.evaluation.ContextRelevancyEvaluator)\n\n# Context relevancy\n\nEvaluation modules.\n\n## ContextRelevancyEvaluator [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/context_relevancy/\\#llama_index.core.evaluation.ContextRelevancyEvaluator \"Permanent link\")\n\nBases: `BaseEvaluator`\n\nContext relevancy evaluator.\n\nEvaluates the relevancy of retrieved contexts to a query.\nThis evaluator considers the query string and retrieved contexts.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `raise_error(Optional[bool])` |  | Whether to raise an error if the response is invalid.<br>Defaults to False. | _required_ |\n| `eval_template(Optional[Union[str,` | `BasePromptTemplate]]` | The template to use for evaluation. | _required_ |\n| `refine_template(Optional[Union[str,` | `BasePromptTemplate]]` | The template to use for refinement. | _required_ |\n\nSource code in `llama-index-core/llama_index/core/evaluation/context_relevancy.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>``` | ```<br>class ContextRelevancyEvaluator(BaseEvaluator):<br>    \"\"\"Context relevancy evaluator.<br>    Evaluates the relevancy of retrieved contexts to a query.<br>    This evaluator considers the query string and retrieved contexts.<br>    Args:<br>        raise_error(Optional[bool]):<br>            Whether to raise an error if the response is invalid.<br>            Defaults to False.<br>        eval_template(Optional[Union[str, BasePromptTemplate]]):<br>            The template to use for evaluation.<br>        refine_template(Optional[Union[str, BasePromptTemplate]]):<br>            The template to use for refinement.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        llm: Optional[LLM] = None,<br>        raise_error: bool = False,<br>        eval_template: str | BasePromptTemplate | None = None,<br>        refine_template: str | BasePromptTemplate | None = None,<br>        score_threshold: float = _DEFAULT_SCORE_THRESHOLD,<br>        parser_function: Callable[<br>            [str], Tuple[Optional[float], Optional[str]]<br>        ] = _default_parser_function,<br>    ) -> None:<br>        \"\"\"Init params.\"\"\"<br>        from llama_index.core import Settings<br>        self._llm = llm or Settings.llm<br>        self._raise_error = raise_error<br>        self._eval_template: BasePromptTemplate<br>        if isinstance(eval_template, str):<br>            self._eval_template = PromptTemplate(eval_template)<br>        else:<br>            self._eval_template = eval_template or DEFAULT_EVAL_TEMPLATE<br>        self._refine_template: BasePromptTemplate<br>        if isinstance(refine_template, str):<br>            self._refine_template = PromptTemplate(refine_template)<br>        else:<br>            self._refine_template = refine_template or DEFAULT_REFINE_TEMPLATE<br>        self.parser_function = parser_function<br>        self.score_threshold = score_threshold<br>    def _get_prompts(self) -> PromptDictType:<br>        \"\"\"Get prompts.\"\"\"<br>        return {<br>            \"eval_template\": self._eval_template,<br>            \"refine_template\": self._refine_template,<br>        }<br>    def _update_prompts(self, prompts: PromptDictType) -> None:<br>        \"\"\"Update prompts.\"\"\"<br>        if \"eval_template\" in prompts:<br>            self._eval_template = prompts[\"eval_template\"]<br>        if \"refine_template\" in prompts:<br>            self._refine_template = prompts[\"refine_template\"]<br>    async def aevaluate(<br>        self,<br>        query: str | None = None,<br>        response: str | None = None,<br>        contexts: Sequence[str] | None = None,<br>        sleep_time_in_seconds: int = 0,<br>        **kwargs: Any,<br>    ) -> EvaluationResult:<br>        \"\"\"Evaluate whether the contexts is relevant to the query.\"\"\"<br>        del kwargs  # Unused<br>        del response  # Unused<br>        if query is None or contexts is None:<br>            raise ValueError(\"Both query and contexts must be provided\")<br>        docs = [Document(text=context) for context in contexts]<br>        index = SummaryIndex.from_documents(docs)<br>        await asyncio.sleep(sleep_time_in_seconds)<br>        query_engine = index.as_query_engine(<br>            llm=self._llm,<br>            text_qa_template=self._eval_template,<br>            refine_template=self._refine_template,<br>        )<br>        response_obj = await query_engine.aquery(query)<br>        raw_response_txt = str(response_obj)<br>        score, reasoning = self.parser_function(raw_response_txt)<br>        invalid_result, invalid_reason = False, None<br>        if score is None and reasoning is None:<br>            if self._raise_error:<br>                raise ValueError(\"The response is invalid\")<br>            invalid_result = True<br>            invalid_reason = \"Unable to parse the output string.\"<br>        if score:<br>            score /= self.score_threshold<br>        return EvaluationResult(<br>            query=query,<br>            contexts=contexts,<br>            score=score,<br>            feedback=raw_response_txt,<br>            invalid_result=invalid_result,<br>            invalid_reason=invalid_reason,<br>        )<br>``` |\n\n### aevaluate`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/context_relevancy/\\#llama_index.core.evaluation.ContextRelevancyEvaluator.aevaluate \"Permanent link\")\n\n```\naevaluate(query: str | None = None, response: str | None = None, contexts: Sequence[str] | None = None, sleep_time_in_seconds: int = 0, **kwargs: Any) -> EvaluationResult\n\n```\n\nEvaluate whether the contexts is relevant to the query.\n\nSource code in `llama-index-core/llama_index/core/evaluation/context_relevancy.py`\n\n|     |     |\n| --- | --- |\n| ```<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>``` | ```<br>async def aevaluate(<br>    self,<br>    query: str | None = None,<br>    response: str | None = None,<br>    contexts: Sequence[str] | None = None,<br>    sleep_time_in_seconds: int = 0,<br>    **kwargs: Any,<br>) -> EvaluationResult:<br>    \"\"\"Evaluate whether the contexts is relevant to the query.\"\"\"<br>    del kwargs  # Unused<br>    del response  # Unused<br>    if query is None or contexts is None:<br>        raise ValueError(\"Both query and contexts must be provided\")<br>    docs = [Document(text=context) for context in contexts]<br>    index = SummaryIndex.from_documents(docs)<br>    await asyncio.sleep(sleep_time_in_seconds)<br>    query_engine = index.as_query_engine(<br>        llm=self._llm,<br>        text_qa_template=self._eval_template,<br>        refine_template=self._refine_template,<br>    )<br>    response_obj = await query_engine.aquery(query)<br>    raw_response_txt = str(response_obj)<br>    score, reasoning = self.parser_function(raw_response_txt)<br>    invalid_result, invalid_reason = False, None<br>    if score is None and reasoning is None:<br>        if self._raise_error:<br>            raise ValueError(\"The response is invalid\")<br>        invalid_result = True<br>        invalid_reason = \"Unable to parse the output string.\"<br>    if score:<br>        score /= self.score_threshold<br>    return EvaluationResult(<br>        query=query,<br>        contexts=contexts,<br>        score=score,<br>        feedback=raw_response_txt,<br>        invalid_result=invalid_result,<br>        invalid_reason=invalid_reason,<br>    )<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Context relevancy - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/evaluation/context_relevancy/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/together/#llama_index.embeddings.together.TogetherEmbedding)\n\n# Together\n\n## TogetherEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/together/\\#llama_index.embeddings.together.TogetherEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-together/llama_index/embeddings/together/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 12<br> 13<br> 14<br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>``` | ```<br>class TogetherEmbedding(BaseEmbedding):<br>    api_base: str = Field(<br>        default=\"https://api.together.xyz/v1\",<br>        description=\"The base URL for the Together API.\",<br>    )<br>    api_key: str = Field(<br>        default=\"\",<br>        description=\"The API key for the Together API. If not set, will attempt to use the TOGETHER_API_KEY environment variable.\",<br>    )<br>    def __init__(<br>        self,<br>        model_name: str,<br>        api_key: Optional[str] = None,<br>        api_base: str = \"https://api.together.xyz/v1\",<br>        **kwargs: Any,<br>    ) -> None:<br>        api_key = api_key or os.environ.get(\"TOGETHER_API_KEY\", None)<br>        super().__init__(<br>            model_name=model_name,<br>            api_key=api_key,<br>            api_base=api_base,<br>            **kwargs,<br>        )<br>    def _generate_embedding(self, text: str, model_api_string: str) -> Embedding:<br>        \"\"\"Generate embeddings from Together API.<br>        Args:<br>            text: str. An input text sentence or document.<br>            model_api_string: str. An API string for a specific embedding model of your choice.<br>        Returns:<br>            embeddings: a list of float numbers. Embeddings correspond to your given text.<br>        \"\"\"<br>        headers = {<br>            \"accept\": \"application/json\",<br>            \"content-type\": \"application/json\",<br>            \"Authorization\": f\"Bearer {self.api_key}\",<br>        }<br>        session = requests.session()<br>        while True:<br>            response = session.post(<br>                self.api_base.strip(\"/\") + \"/embeddings\",<br>                headers=headers,<br>                json={\"input\": text, \"model\": model_api_string},<br>            )<br>            if response.status_code != 200:<br>                if response.status_code == 429:<br>                    \"\"\"Rate limit exceeded, wait for reset\"\"\"<br>                    reset_time = int(response.headers.get(\"X-RateLimit-Reset\", 0))<br>                    if reset_time > 0:<br>                        time.sleep(reset_time)<br>                        continue<br>                    else:<br>                        \"\"\"Rate limit reset time has passed, retry immediately\"\"\"<br>                        continue<br>                \"\"\" Handle other non-200 status codes \"\"\"<br>                raise ValueError(<br>                    f\"Request failed with status code {response.status_code}: {response.text}\"<br>                )<br>            return response.json()[\"data\"][0][\"embedding\"]<br>    async def _agenerate_embedding(self, text: str, model_api_string: str) -> Embedding:<br>        \"\"\"Async generate embeddings from Together API.<br>        Args:<br>            text: str. An input text sentence or document.<br>            model_api_string: str. An API string for a specific embedding model of your choice.<br>        Returns:<br>            embeddings: a list of float numbers. Embeddings correspond to your given text.<br>        \"\"\"<br>        headers = {<br>            \"accept\": \"application/json\",<br>            \"content-type\": \"application/json\",<br>            \"Authorization\": f\"Bearer {self.api_key}\",<br>        }<br>        async with httpx.AsyncClient() as client:<br>            while True:<br>                response = await client.post(<br>                    self.api_base.strip(\"/\") + \"/embeddings\",<br>                    headers=headers,<br>                    json={\"input\": text, \"model\": model_api_string},<br>                )<br>                if response.status_code != 200:<br>                    if response.status_code == 429:<br>                        \"\"\"Rate limit exceeded, wait for reset\"\"\"<br>                        reset_time = int(response.headers.get(\"X-RateLimit-Reset\", 0))<br>                        if reset_time > 0:<br>                            await asyncio.sleep(reset_time)<br>                            continue<br>                        else:<br>                            \"\"\"Rate limit reset time has passed, retry immediately\"\"\"<br>                            continue<br>                    \"\"\" Handle other non-200 status codes\"\"\"<br>                    raise ValueError(<br>                        f\"Request failed with status code {response.status_code}: {response.text}\"<br>                    )<br>                return response.json()[\"data\"][0][\"embedding\"]<br>    def _get_text_embedding(self, text: str) -> Embedding:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._generate_embedding(text, self.model_name)<br>    def _get_query_embedding(self, query: str) -> Embedding:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self._generate_embedding(query, self.model_name)<br>    def _get_text_embeddings(self, texts: List[str]) -> List[Embedding]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        return [self._generate_embedding(text, self.model_name) for text in texts]<br>    async def _aget_text_embedding(self, text: str) -> Embedding:<br>        \"\"\"Async get text embedding.\"\"\"<br>        return await self._agenerate_embedding(text, self.model_name)<br>    async def _aget_query_embedding(self, query: str) -> Embedding:<br>        \"\"\"Async get query embedding.\"\"\"<br>        return await self._agenerate_embedding(query, self.model_name)<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[Embedding]:<br>        \"\"\"Async get text embeddings.\"\"\"<br>        return await asyncio.gather(<br>            *[self._agenerate_embedding(text, self.model_name) for text in texts]<br>        )<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Together - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/together/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/cloudflare_workersai/#llama_index.embeddings.cloudflare_workersai.CloudflareEmbedding)\n\n# Cloudflare workersai\n\n## CloudflareEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/cloudflare_workersai/\\#llama_index.embeddings.cloudflare_workersai.CloudflareEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nCloudflare Workers AI class for generating text embeddings.\n\nThis class allows for the generation of text embeddings using Cloudflare Workers AI with the BAAI general embedding models.\n\nArgs:\naccount\\_id (str): The Cloudflare Account ID.\nauth\\_token (str, Optional): The Cloudflare Auth Token. Alternatively, set up environment variable `CLOUDFLARE_AUTH_TOKEN`.\nmodel (str): The model ID for the embedding service. Cloudflare provides different models for embeddings, check https://developers.cloudflare.com/workers-ai/models/#text-embeddings. Defaults to \"@cf/baai/bge-base-en-v1.5\".\nembed\\_batch\\_size (int): The batch size for embedding generation. Cloudflare's current limit is 100 at max. Defaults to llama\\_index's default.\n\nNote:\nEnsure you have a valid Cloudflare account and have access to the necessary AI services and models. The account ID and authorization token are sensitive details; secure them appropriately.\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-cloudflare-workersai/llama_index/embeddings/cloudflare_workersai/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>``` | ```<br>class CloudflareEmbedding(BaseEmbedding):<br>    \"\"\"<br>    Cloudflare Workers AI class for generating text embeddings.<br>    This class allows for the generation of text embeddings using Cloudflare Workers AI with the BAAI general embedding models.<br>    Args:<br>    account_id (str): The Cloudflare Account ID.<br>    auth_token (str, Optional): The Cloudflare Auth Token. Alternatively, set up environment variable `CLOUDFLARE_AUTH_TOKEN`.<br>    model (str): The model ID for the embedding service. Cloudflare provides different models for embeddings, check https://developers.cloudflare.com/workers-ai/models/#text-embeddings. Defaults to \"@cf/baai/bge-base-en-v1.5\".<br>    embed_batch_size (int): The batch size for embedding generation. Cloudflare's current limit is 100 at max. Defaults to llama_index's default.<br>    Note:<br>    Ensure you have a valid Cloudflare account and have access to the necessary AI services and models. The account ID and authorization token are sensitive details; secure them appropriately.<br>    \"\"\"<br>    account_id: str = Field(default=None, description=\"The Cloudflare Account ID.\")<br>    auth_token: str = Field(default=None, description=\"The Cloudflare Auth Token.\")<br>    model: str = Field(<br>        default=\"@cf/baai/bge-base-en-v1.5\",<br>        description=\"The model to use when calling Cloudflare AI API\",<br>    )<br>    _session: Any = PrivateAttr()<br>    def __init__(<br>        self,<br>        account_id: str,<br>        auth_token: Optional[str] = None,<br>        model: str = \"@cf/baai/bge-base-en-v1.5\",<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>        **kwargs: Any,<br>    ) -> None:<br>        super().__init__(<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            model=model,<br>            **kwargs,<br>        )<br>        self.account_id = account_id<br>        self.auth_token = get_from_param_or_env(<br>            \"auth_token\", auth_token, \"CLOUDFLARE_AUTH_TOKEN\", \"\"<br>        )<br>        self.model = model<br>        self._session = requests.Session()<br>        self._session.headers.update({\"Authorization\": f\"Bearer {self.auth_token}\"})<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"CloudflareEmbedding\"<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self._get_text_embedding(query)<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        return await self._aget_text_embedding(query)<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._get_text_embeddings([text])[0]<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously get text embedding.\"\"\"<br>        result = await self._aget_text_embeddings([text])<br>        return result[0]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        response = self._session.post(<br>            API_URL_TEMPLATE.format(self.account_id, self.model), json={\"text\": texts}<br>        ).json()<br>        if \"result\" not in response:<br>            print(response)<br>            raise RuntimeError(\"Failed to fetch embeddings\")<br>        return response[\"result\"][\"data\"]<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Asynchronously get text embeddings.\"\"\"<br>        import aiohttp<br>        async with aiohttp.ClientSession(trust_env=True) as session:<br>            headers = {<br>                \"Authorization\": f\"Bearer {self.auth_token}\",<br>                \"Accept-Encoding\": \"identity\",<br>            }<br>            async with session.post(<br>                API_URL_TEMPLATE.format(self.account_id, self.model),<br>                json={\"text\": texts},<br>                headers=headers,<br>            ) as response:<br>                resp = await response.json()<br>                if \"result\" not in resp:<br>                    raise RuntimeError(\"Failed to fetch embeddings asynchronously\")<br>                return resp[\"result\"][\"data\"]<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Cloudflare workersai - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/cloudflare_workersai/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/#llama_index.agent.openai.OpenAIAgent)\n\n# Openai\n\n## OpenAIAgent [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/\\#llama_index.agent.openai.OpenAIAgent \"Permanent link\")\n\nBases: `AgentRunner`\n\nOpenAI agent.\n\nSubclasses AgentRunner with a OpenAIAgentWorker.\n\nFor the legacy implementation see:\n\n```\nfrom llama_index..agent.legacy.openai.base import OpenAIAgent\n\n```\n\nSource code in `llama-index-integrations/agent/llama-index-agent-openai/llama_index/agent/openai/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>``` | ````<br>class OpenAIAgent(AgentRunner):<br>    \"\"\"OpenAI agent.<br>    Subclasses AgentRunner with a OpenAIAgentWorker.<br>    For the legacy implementation see:<br>    ```python<br>    from llama_index..agent.legacy.openai.base import OpenAIAgent<br>    ```<br>    \"\"\"<br>    def __init__(<br>        self,<br>        tools: List[BaseTool],<br>        llm: OpenAI,<br>        memory: BaseMemory,<br>        prefix_messages: List[ChatMessage],<br>        verbose: bool = False,<br>        max_function_calls: int = DEFAULT_MAX_FUNCTION_CALLS,<br>        default_tool_choice: str = \"auto\",<br>        callback_manager: Optional[CallbackManager] = None,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        tool_call_parser: Optional[Callable[[OpenAIToolCall], Dict]] = None,<br>    ) -> None:<br>        \"\"\"Init params.\"\"\"<br>        callback_manager = callback_manager or llm.callback_manager<br>        step_engine = OpenAIAgentWorker.from_tools(<br>            tools=tools,<br>            tool_retriever=tool_retriever,<br>            llm=llm,<br>            verbose=verbose,<br>            max_function_calls=max_function_calls,<br>            callback_manager=callback_manager,<br>            prefix_messages=prefix_messages,<br>            tool_call_parser=tool_call_parser,<br>        )<br>        super().__init__(<br>            step_engine,<br>            memory=memory,<br>            llm=llm,<br>            callback_manager=callback_manager,<br>            default_tool_choice=default_tool_choice,<br>        )<br>    @classmethod<br>    def from_tools(<br>        cls,<br>        tools: Optional[List[BaseTool]] = None,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        llm: Optional[LLM] = None,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        memory: Optional[BaseMemory] = None,<br>        memory_cls: Type[BaseMemory] = ChatMemoryBuffer,<br>        verbose: bool = False,<br>        max_function_calls: int = DEFAULT_MAX_FUNCTION_CALLS,<br>        default_tool_choice: str = \"auto\",<br>        callback_manager: Optional[CallbackManager] = None,<br>        system_prompt: Optional[str] = None,<br>        prefix_messages: Optional[List[ChatMessage]] = None,<br>        tool_call_parser: Optional[Callable[[OpenAIToolCall], Dict]] = None,<br>        **kwargs: Any,<br>    ) -> \"OpenAIAgent\":<br>        \"\"\"Create an OpenAIAgent from a list of tools.<br>        Similar to `from_defaults` in other classes, this method will<br>        infer defaults for a variety of parameters, including the LLM,<br>        if they are not specified.<br>        \"\"\"<br>        tools = tools or []<br>        chat_history = chat_history or []<br>        llm = llm or Settings.llm<br>        if not isinstance(llm, OpenAI):<br>            raise ValueError(\"llm must be a OpenAI instance\")<br>        if callback_manager is not None:<br>            llm.callback_manager = callback_manager<br>        memory = memory or memory_cls.from_defaults(chat_history, llm=llm)<br>        if not llm.metadata.is_function_calling_model:<br>            raise ValueError(<br>                f\"Model name {llm.model} does not support function calling API. \"<br>            )<br>        if system_prompt is not None:<br>            if prefix_messages is not None:<br>                raise ValueError(<br>                    \"Cannot specify both system_prompt and prefix_messages\"<br>                )<br>            prefix_messages = [ChatMessage(content=system_prompt, role=\"system\")]<br>        prefix_messages = prefix_messages or []<br>        return cls(<br>            tools=tools,<br>            tool_retriever=tool_retriever,<br>            llm=llm,<br>            memory=memory,<br>            prefix_messages=prefix_messages,<br>            verbose=verbose,<br>            max_function_calls=max_function_calls,<br>            callback_manager=callback_manager,<br>            default_tool_choice=default_tool_choice,<br>            tool_call_parser=tool_call_parser,<br>        )<br>```` |\n\n### from\\_tools`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/\\#llama_index.agent.openai.OpenAIAgent.from_tools \"Permanent link\")\n\n```\nfrom_tools(tools: Optional[List[BaseTool]] = None, tool_retriever: Optional[ObjectRetriever[BaseTool]] = None, llm: Optional[LLM] = None, chat_history: Optional[List[ChatMessage]] = None, memory: Optional[BaseMemory] = None, memory_cls: Type[BaseMemory] = ChatMemoryBuffer, verbose: bool = False, max_function_calls: int = DEFAULT_MAX_FUNCTION_CALLS, default_tool_choice: str = 'auto', callback_manager: Optional[CallbackManager] = None, system_prompt: Optional[str] = None, prefix_messages: Optional[List[ChatMessage]] = None, tool_call_parser: Optional[Callable[[OpenAIToolCall], Dict]] = None, **kwargs: Any) -> OpenAIAgent\n\n```\n\nCreate an OpenAIAgent from a list of tools.\n\nSimilar to `from_defaults` in other classes, this method will\ninfer defaults for a variety of parameters, including the LLM,\nif they are not specified.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-openai/llama_index/agent/openai/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>``` | ```<br>@classmethod<br>def from_tools(<br>    cls,<br>    tools: Optional[List[BaseTool]] = None,<br>    tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>    llm: Optional[LLM] = None,<br>    chat_history: Optional[List[ChatMessage]] = None,<br>    memory: Optional[BaseMemory] = None,<br>    memory_cls: Type[BaseMemory] = ChatMemoryBuffer,<br>    verbose: bool = False,<br>    max_function_calls: int = DEFAULT_MAX_FUNCTION_CALLS,<br>    default_tool_choice: str = \"auto\",<br>    callback_manager: Optional[CallbackManager] = None,<br>    system_prompt: Optional[str] = None,<br>    prefix_messages: Optional[List[ChatMessage]] = None,<br>    tool_call_parser: Optional[Callable[[OpenAIToolCall], Dict]] = None,<br>    **kwargs: Any,<br>) -> \"OpenAIAgent\":<br>    \"\"\"Create an OpenAIAgent from a list of tools.<br>    Similar to `from_defaults` in other classes, this method will<br>    infer defaults for a variety of parameters, including the LLM,<br>    if they are not specified.<br>    \"\"\"<br>    tools = tools or []<br>    chat_history = chat_history or []<br>    llm = llm or Settings.llm<br>    if not isinstance(llm, OpenAI):<br>        raise ValueError(\"llm must be a OpenAI instance\")<br>    if callback_manager is not None:<br>        llm.callback_manager = callback_manager<br>    memory = memory or memory_cls.from_defaults(chat_history, llm=llm)<br>    if not llm.metadata.is_function_calling_model:<br>        raise ValueError(<br>            f\"Model name {llm.model} does not support function calling API. \"<br>        )<br>    if system_prompt is not None:<br>        if prefix_messages is not None:<br>            raise ValueError(<br>                \"Cannot specify both system_prompt and prefix_messages\"<br>            )<br>        prefix_messages = [ChatMessage(content=system_prompt, role=\"system\")]<br>    prefix_messages = prefix_messages or []<br>    return cls(<br>        tools=tools,<br>        tool_retriever=tool_retriever,<br>        llm=llm,<br>        memory=memory,<br>        prefix_messages=prefix_messages,<br>        verbose=verbose,<br>        max_function_calls=max_function_calls,<br>        callback_manager=callback_manager,<br>        default_tool_choice=default_tool_choice,<br>        tool_call_parser=tool_call_parser,<br>    )<br>``` |\n\n## OpenAIAssistantAgent [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/\\#llama_index.agent.openai.OpenAIAssistantAgent \"Permanent link\")\n\nBases: `BaseAgent`\n\nOpenAIAssistant agent.\n\nWrapper around OpenAI assistant API: https://platform.openai.com/docs/assistants/overview\n\nSource code in `llama-index-integrations/agent/llama-index-agent-openai/llama_index/agent/openai/openai_assistant_agent.py`\n\n|     |     |\n| --- | --- |\n| ```<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>472<br>473<br>474<br>475<br>476<br>477<br>478<br>479<br>480<br>481<br>482<br>483<br>484<br>485<br>486<br>487<br>488<br>489<br>490<br>491<br>492<br>493<br>494<br>495<br>496<br>497<br>498<br>499<br>500<br>501<br>502<br>503<br>504<br>505<br>506<br>507<br>508<br>509<br>510<br>511<br>512<br>513<br>514<br>515<br>516<br>517<br>518<br>519<br>520<br>521<br>522<br>523<br>524<br>525<br>526<br>527<br>528<br>529<br>530<br>531<br>532<br>533<br>534<br>535<br>536<br>537<br>538<br>539<br>540<br>541<br>542<br>543<br>544<br>545<br>546<br>547<br>548<br>549<br>550<br>551<br>552<br>553<br>554<br>555<br>556<br>557<br>558<br>559<br>560<br>561<br>562<br>563<br>564<br>565<br>566<br>567<br>568<br>569<br>570<br>571<br>572<br>573<br>574<br>575<br>576<br>577<br>578<br>579<br>580<br>581<br>582<br>583<br>584<br>585<br>586<br>587<br>588<br>589<br>590<br>591<br>592<br>593<br>594<br>``` | ```<br>class OpenAIAssistantAgent(BaseAgent):<br>    \"\"\"OpenAIAssistant agent.<br>    Wrapper around OpenAI assistant API: https://platform.openai.com/docs/assistants/overview<br>    \"\"\"<br>    def __init__(<br>        self,<br>        client: Any,<br>        assistant: Any,<br>        tools: Optional[List[BaseTool]],<br>        callback_manager: Optional[CallbackManager] = None,<br>        thread_id: Optional[str] = None,<br>        instructions_prefix: Optional[str] = None,<br>        run_retrieve_sleep_time: float = 0.1,<br>        file_dict: Dict[str, str] = {},<br>        verbose: bool = False,<br>    ) -> None:<br>        \"\"\"Init params.\"\"\"<br>        from openai import OpenAI<br>        from openai.types.beta.assistant import Assistant<br>        self._client = cast(OpenAI, client)<br>        self._assistant = cast(Assistant, assistant)<br>        self._tools = tools or []<br>        if thread_id is None:<br>            thread = self._client.beta.threads.create()<br>            thread_id = thread.id<br>        self._thread_id = thread_id<br>        self._instructions_prefix = instructions_prefix<br>        self._run_retrieve_sleep_time = run_retrieve_sleep_time<br>        self._verbose = verbose<br>        self.file_dict = file_dict<br>        self.callback_manager = callback_manager or CallbackManager([])<br>    @classmethod<br>    def from_new(<br>        cls,<br>        name: str,<br>        instructions: str,<br>        tools: Optional[List[BaseTool]] = None,<br>        openai_tools: Optional[List[Dict]] = None,<br>        thread_id: Optional[str] = None,<br>        model: str = \"gpt-4-1106-preview\",<br>        instructions_prefix: Optional[str] = None,<br>        run_retrieve_sleep_time: float = 0.1,<br>        files: Optional[List[str]] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        file_ids: Optional[List[str]] = None,<br>        api_key: Optional[str] = None,<br>    ) -> \"OpenAIAssistantAgent\":<br>        \"\"\"From new assistant.<br>        Args:<br>            name: name of assistant<br>            instructions: instructions for assistant<br>            tools: list of tools<br>            openai_tools: list of openai tools<br>            thread_id: thread id<br>            model: model<br>            run_retrieve_sleep_time: run retrieve sleep time<br>            files: files<br>            instructions_prefix: instructions prefix<br>            callback_manager: callback manager<br>            verbose: verbose<br>            file_ids: list of file ids<br>            api_key: OpenAI API key<br>        \"\"\"<br>        from openai import OpenAI<br>        # this is the set of openai tools<br>        # not to be confused with the tools we pass in for function calling<br>        openai_tools = openai_tools or []<br>        tools = tools or []<br>        tool_fns = [t.metadata.to_openai_tool() for t in tools]<br>        all_openai_tools = openai_tools + tool_fns<br>        # initialize client<br>        client = OpenAI(api_key=api_key)<br>        # process files<br>        files = files or []<br>        file_ids = file_ids or []<br>        file_dict = _process_files(client, files)<br>        # TODO: openai's typing is a bit sus<br>        all_openai_tools = cast(List[Any], all_openai_tools)<br>        assistant = client.beta.assistants.create(<br>            name=name,<br>            instructions=instructions,<br>            tools=cast(List[Any], all_openai_tools),<br>            model=model,<br>        )<br>        return cls(<br>            client,<br>            assistant,<br>            tools,<br>            callback_manager=callback_manager,<br>            thread_id=thread_id,<br>            instructions_prefix=instructions_prefix,<br>            file_dict=file_dict,<br>            run_retrieve_sleep_time=run_retrieve_sleep_time,<br>            verbose=verbose,<br>        )<br>    @classmethod<br>    def from_existing(<br>        cls,<br>        assistant_id: str,<br>        tools: Optional[List[BaseTool]] = None,<br>        thread_id: Optional[str] = None,<br>        instructions_prefix: Optional[str] = None,<br>        run_retrieve_sleep_time: float = 0.1,<br>        callback_manager: Optional[CallbackManager] = None,<br>        api_key: Optional[str] = None,<br>        verbose: bool = False,<br>    ) -> \"OpenAIAssistantAgent\":<br>        \"\"\"From existing assistant id.<br>        Args:<br>            assistant_id: id of assistant<br>            tools: list of BaseTools Assistant can use<br>            thread_id: thread id<br>            run_retrieve_sleep_time: run retrieve sleep time<br>            instructions_prefix: instructions prefix<br>            callback_manager: callback manager<br>            api_key: OpenAI API key<br>            verbose: verbose<br>        \"\"\"<br>        from openai import OpenAI<br>        # initialize client<br>        client = OpenAI(api_key=api_key)<br>        # get assistant<br>        assistant = client.beta.assistants.retrieve(assistant_id)<br>        # assistant.tools is incompatible with BaseTools so have to pass from params<br>        return cls(<br>            client,<br>            assistant,<br>            tools=tools,<br>            callback_manager=callback_manager,<br>            thread_id=thread_id,<br>            instructions_prefix=instructions_prefix,<br>            run_retrieve_sleep_time=run_retrieve_sleep_time,<br>            verbose=verbose,<br>        )<br>    @property<br>    def assistant(self) -> Any:<br>        \"\"\"Get assistant.\"\"\"<br>        return self._assistant<br>    @property<br>    def client(self) -> Any:<br>        \"\"\"Get client.\"\"\"<br>        return self._client<br>    @property<br>    def thread_id(self) -> str:<br>        \"\"\"Get thread id.\"\"\"<br>        return self._thread_id<br>    @property<br>    def files_dict(self) -> Dict[str, str]:<br>        \"\"\"Get files dict.\"\"\"<br>        return self.file_dict<br>    @property<br>    def chat_history(self) -> List[ChatMessage]:<br>        raw_messages = self._client.beta.threads.messages.list(<br>            thread_id=self._thread_id, order=\"asc\"<br>        )<br>        return from_openai_thread_messages(list(raw_messages))<br>    def reset(self) -> None:<br>        \"\"\"Delete and create a new thread.\"\"\"<br>        self._client.beta.threads.delete(self._thread_id)<br>        thread = self._client.beta.threads.create()<br>        thread_id = thread.id<br>        self._thread_id = thread_id<br>    def get_tools(self, message: str) -> List[BaseTool]:<br>        \"\"\"Get tools.\"\"\"<br>        return self._tools<br>    def upload_files(self, files: List[str]) -> Dict[str, Any]:<br>        \"\"\"Upload files.\"\"\"<br>        return _process_files(self._client, files)<br>    def add_message(<br>        self,<br>        message: str,<br>        file_ids: Optional[List[str]] = None,<br>        tools: Optional[List[Dict[str, Any]]] = None,<br>    ) -> Any:<br>        \"\"\"Add message to assistant.\"\"\"<br>        attachments = format_attachments(file_ids=file_ids, tools=tools)<br>        return self._client.beta.threads.messages.create(<br>            thread_id=self._thread_id,<br>            role=\"user\",<br>            content=message,<br>            attachments=attachments,<br>        )<br>    def _run_function_calling(self, run: Any) -> List[ToolOutput]:<br>        \"\"\"Run function calling.\"\"\"<br>        tool_calls = run.required_action.submit_tool_outputs.tool_calls<br>        tool_output_dicts = []<br>        tool_output_objs: List[ToolOutput] = []<br>        for tool_call in tool_calls:<br>            fn_obj = tool_call.function<br>            _, tool_output = call_function(self._tools, fn_obj, verbose=self._verbose)<br>            tool_output_dicts.append(<br>                {\"tool_call_id\": tool_call.id, \"output\": str(tool_output)}<br>            )<br>            tool_output_objs.append(tool_output)<br>        # submit tool outputs<br>        # TODO: openai's typing is a bit sus<br>        self._client.beta.threads.runs.submit_tool_outputs(<br>            thread_id=self._thread_id,<br>            run_id=run.id,<br>            tool_outputs=cast(List[Any], tool_output_dicts),<br>        )<br>        return tool_output_objs<br>    async def _arun_function_calling(self, run: Any) -> List[ToolOutput]:<br>        \"\"\"Run function calling.\"\"\"<br>        tool_calls = run.required_action.submit_tool_outputs.tool_calls<br>        tool_output_dicts = []<br>        tool_output_objs: List[ToolOutput] = []<br>        for tool_call in tool_calls:<br>            fn_obj = tool_call.function<br>            _, tool_output = await acall_function(<br>                self._tools, fn_obj, verbose=self._verbose<br>            )<br>            tool_output_dicts.append(<br>                {\"tool_call_id\": tool_call.id, \"output\": str(tool_output)}<br>            )<br>            tool_output_objs.append(tool_output)<br>        # submit tool outputs<br>        self._client.beta.threads.runs.submit_tool_outputs(<br>            thread_id=self._thread_id,<br>            run_id=run.id,<br>            tool_outputs=cast(List[Any], tool_output_dicts),<br>        )<br>        return tool_output_objs<br>    def run_assistant(<br>        self, instructions_prefix: Optional[str] = None<br>    ) -> Tuple[Any, Dict]:<br>        \"\"\"Run assistant.\"\"\"<br>        instructions_prefix = instructions_prefix or self._instructions_prefix<br>        run = self._client.beta.threads.runs.create(<br>            thread_id=self._thread_id,<br>            assistant_id=self._assistant.id,<br>            instructions=instructions_prefix,<br>        )<br>        from openai.types.beta.threads import Run<br>        run = cast(Run, run)<br>        sources = []<br>        while run.status in [\"queued\", \"in_progress\", \"requires_action\"]:<br>            run = self._client.beta.threads.runs.retrieve(<br>                thread_id=self._thread_id, run_id=run.id<br>            )<br>            if run.status == \"requires_action\":<br>                cur_tool_outputs = self._run_function_calling(run)<br>                sources.extend(cur_tool_outputs)<br>            time.sleep(self._run_retrieve_sleep_time)<br>        if run.status == \"failed\":<br>            raise ValueError(<br>                f\"Run failed with status {run.status}.\\n\" f\"Error: {run.last_error}\"<br>            )<br>        return run, {\"sources\": sources}<br>    async def arun_assistant(<br>        self, instructions_prefix: Optional[str] = None<br>    ) -> Tuple[Any, Dict]:<br>        \"\"\"Run assistant.\"\"\"<br>        instructions_prefix = instructions_prefix or self._instructions_prefix<br>        run = self._client.beta.threads.runs.create(<br>            thread_id=self._thread_id,<br>            assistant_id=self._assistant.id,<br>            instructions=instructions_prefix,<br>        )<br>        from openai.types.beta.threads import Run<br>        run = cast(Run, run)<br>        sources = []<br>        while run.status in [\"queued\", \"in_progress\", \"requires_action\"]:<br>            run = self._client.beta.threads.runs.retrieve(<br>                thread_id=self._thread_id, run_id=run.id<br>            )<br>            if run.status == \"requires_action\":<br>                cur_tool_outputs = await self._arun_function_calling(run)<br>                sources.extend(cur_tool_outputs)<br>            await asyncio.sleep(self._run_retrieve_sleep_time)<br>        if run.status == \"failed\":<br>            raise ValueError(<br>                f\"Run failed with status {run.status}.\\n\" f\"Error: {run.last_error}\"<br>            )<br>        return run, {\"sources\": sources}<br>    @property<br>    def latest_message(self) -> ChatMessage:<br>        \"\"\"Get latest message.\"\"\"<br>        raw_messages = self._client.beta.threads.messages.list(<br>            thread_id=self._thread_id, order=\"desc\"<br>        )<br>        messages = from_openai_thread_messages(list(raw_messages))<br>        return messages[0]<br>    def _chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        function_call: Union[str, dict] = \"auto\",<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>    ) -> AGENT_CHAT_RESPONSE_TYPE:<br>        \"\"\"Main chat interface.\"\"\"<br>        # TODO: since chat interface doesn't expose additional kwargs<br>        # we can't pass in file_ids per message<br>        _added_message_obj = self.add_message(message)<br>        _run, metadata = self.run_assistant(<br>            instructions_prefix=self._instructions_prefix,<br>        )<br>        latest_message = self.latest_message<br>        # get most recent message content<br>        return AgentChatResponse(<br>            response=str(latest_message.content),<br>            sources=metadata[\"sources\"],<br>        )<br>    async def _achat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        function_call: Union[str, dict] = \"auto\",<br>        mode: ChatResponseMode = ChatResponseMode.WAIT,<br>    ) -> AGENT_CHAT_RESPONSE_TYPE:<br>        \"\"\"Asynchronous main chat interface.\"\"\"<br>        self.add_message(message)<br>        run, metadata = await self.arun_assistant(<br>            instructions_prefix=self._instructions_prefix,<br>        )<br>        latest_message = self.latest_message<br>        # get most recent message content<br>        return AgentChatResponse(<br>            response=str(latest_message.content),<br>            sources=metadata[\"sources\"],<br>        )<br>    @trace_method(\"chat\")<br>    def chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        function_call: Union[str, dict] = \"auto\",<br>    ) -> AgentChatResponse:<br>        with self.callback_manager.event(<br>            CBEventType.AGENT_STEP,<br>            payload={EventPayload.MESSAGES: [message]},<br>        ) as e:<br>            chat_response = self._chat(<br>                message, chat_history, function_call, mode=ChatResponseMode.WAIT<br>            )<br>            assert isinstance(chat_response, AgentChatResponse)<br>            e.on_end(payload={EventPayload.RESPONSE: chat_response})<br>        return chat_response<br>    @trace_method(\"chat\")<br>    async def achat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        function_call: Union[str, dict] = \"auto\",<br>    ) -> AgentChatResponse:<br>        with self.callback_manager.event(<br>            CBEventType.AGENT_STEP,<br>            payload={EventPayload.MESSAGES: [message]},<br>        ) as e:<br>            chat_response = await self._achat(<br>                message, chat_history, function_call, mode=ChatResponseMode.WAIT<br>            )<br>            assert isinstance(chat_response, AgentChatResponse)<br>            e.on_end(payload={EventPayload.RESPONSE: chat_response})<br>        return chat_response<br>    @trace_method(\"chat\")<br>    def stream_chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        function_call: Union[str, dict] = \"auto\",<br>    ) -> StreamingAgentChatResponse:<br>        raise NotImplementedError(\"stream_chat not implemented\")<br>    @trace_method(\"chat\")<br>    async def astream_chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        function_call: Union[str, dict] = \"auto\",<br>    ) -> StreamingAgentChatResponse:<br>        raise NotImplementedError(\"astream_chat not implemented\")<br>``` |\n\n### assistant`property`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/\\#llama_index.agent.openai.OpenAIAssistantAgent.assistant \"Permanent link\")\n\n```\nassistant: Any\n\n```\n\nGet assistant.\n\n### client`property`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/\\#llama_index.agent.openai.OpenAIAssistantAgent.client \"Permanent link\")\n\n```\nclient: Any\n\n```\n\nGet client.\n\n### thread\\_id`property`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/\\#llama_index.agent.openai.OpenAIAssistantAgent.thread_id \"Permanent link\")\n\n```\nthread_id: str\n\n```\n\nGet thread id.\n\n### files\\_dict`property`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/\\#llama_index.agent.openai.OpenAIAssistantAgent.files_dict \"Permanent link\")\n\n```\nfiles_dict: Dict[str, str]\n\n```\n\nGet files dict.\n\n### latest\\_message`property`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/\\#llama_index.agent.openai.OpenAIAssistantAgent.latest_message \"Permanent link\")\n\n```\nlatest_message: ChatMessage\n\n```\n\nGet latest message.\n\n### from\\_new`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/\\#llama_index.agent.openai.OpenAIAssistantAgent.from_new \"Permanent link\")\n\n```\nfrom_new(name: str, instructions: str, tools: Optional[List[BaseTool]] = None, openai_tools: Optional[List[Dict]] = None, thread_id: Optional[str] = None, model: str = 'gpt-4-1106-preview', instructions_prefix: Optional[str] = None, run_retrieve_sleep_time: float = 0.1, files: Optional[List[str]] = None, callback_manager: Optional[CallbackManager] = None, verbose: bool = False, file_ids: Optional[List[str]] = None, api_key: Optional[str] = None) -> OpenAIAssistantAgent\n\n```\n\nFrom new assistant.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `name` | `str` | name of assistant | _required_ |\n| `instructions` | `str` | instructions for assistant | _required_ |\n| `tools` | `Optional[List[BaseTool]]` | list of tools | `None` |\n| `openai_tools` | `Optional[List[Dict]]` | list of openai tools | `None` |\n| `thread_id` | `Optional[str]` | thread id | `None` |\n| `model` | `str` | model | `'gpt-4-1106-preview'` |\n| `run_retrieve_sleep_time` | `float` | run retrieve sleep time | `0.1` |\n| `files` | `Optional[List[str]]` | files | `None` |\n| `instructions_prefix` | `Optional[str]` | instructions prefix | `None` |\n| `callback_manager` | `Optional[CallbackManager]` | callback manager | `None` |\n| `verbose` | `bool` | verbose | `False` |\n| `file_ids` | `Optional[List[str]]` | list of file ids | `None` |\n| `api_key` | `Optional[str]` | OpenAI API key | `None` |\n\nSource code in `llama-index-integrations/agent/llama-index-agent-openai/llama_index/agent/openai/openai_assistant_agent.py`\n\n|     |     |\n| --- | --- |\n| ```<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>``` | ```<br>@classmethod<br>def from_new(<br>    cls,<br>    name: str,<br>    instructions: str,<br>    tools: Optional[List[BaseTool]] = None,<br>    openai_tools: Optional[List[Dict]] = None,<br>    thread_id: Optional[str] = None,<br>    model: str = \"gpt-4-1106-preview\",<br>    instructions_prefix: Optional[str] = None,<br>    run_retrieve_sleep_time: float = 0.1,<br>    files: Optional[List[str]] = None,<br>    callback_manager: Optional[CallbackManager] = None,<br>    verbose: bool = False,<br>    file_ids: Optional[List[str]] = None,<br>    api_key: Optional[str] = None,<br>) -> \"OpenAIAssistantAgent\":<br>    \"\"\"From new assistant.<br>    Args:<br>        name: name of assistant<br>        instructions: instructions for assistant<br>        tools: list of tools<br>        openai_tools: list of openai tools<br>        thread_id: thread id<br>        model: model<br>        run_retrieve_sleep_time: run retrieve sleep time<br>        files: files<br>        instructions_prefix: instructions prefix<br>        callback_manager: callback manager<br>        verbose: verbose<br>        file_ids: list of file ids<br>        api_key: OpenAI API key<br>    \"\"\"<br>    from openai import OpenAI<br>    # this is the set of openai tools<br>    # not to be confused with the tools we pass in for function calling<br>    openai_tools = openai_tools or []<br>    tools = tools or []<br>    tool_fns = [t.metadata.to_openai_tool() for t in tools]<br>    all_openai_tools = openai_tools + tool_fns<br>    # initialize client<br>    client = OpenAI(api_key=api_key)<br>    # process files<br>    files = files or []<br>    file_ids = file_ids or []<br>    file_dict = _process_files(client, files)<br>    # TODO: openai's typing is a bit sus<br>    all_openai_tools = cast(List[Any], all_openai_tools)<br>    assistant = client.beta.assistants.create(<br>        name=name,<br>        instructions=instructions,<br>        tools=cast(List[Any], all_openai_tools),<br>        model=model,<br>    )<br>    return cls(<br>        client,<br>        assistant,<br>        tools,<br>        callback_manager=callback_manager,<br>        thread_id=thread_id,<br>        instructions_prefix=instructions_prefix,<br>        file_dict=file_dict,<br>        run_retrieve_sleep_time=run_retrieve_sleep_time,<br>        verbose=verbose,<br>    )<br>``` |\n\n### from\\_existing`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/\\#llama_index.agent.openai.OpenAIAssistantAgent.from_existing \"Permanent link\")\n\n```\nfrom_existing(assistant_id: str, tools: Optional[List[BaseTool]] = None, thread_id: Optional[str] = None, instructions_prefix: Optional[str] = None, run_retrieve_sleep_time: float = 0.1, callback_manager: Optional[CallbackManager] = None, api_key: Optional[str] = None, verbose: bool = False) -> OpenAIAssistantAgent\n\n```\n\nFrom existing assistant id.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `assistant_id` | `str` | id of assistant | _required_ |\n| `tools` | `Optional[List[BaseTool]]` | list of BaseTools Assistant can use | `None` |\n| `thread_id` | `Optional[str]` | thread id | `None` |\n| `run_retrieve_sleep_time` | `float` | run retrieve sleep time | `0.1` |\n| `instructions_prefix` | `Optional[str]` | instructions prefix | `None` |\n| `callback_manager` | `Optional[CallbackManager]` | callback manager | `None` |\n| `api_key` | `Optional[str]` | OpenAI API key | `None` |\n| `verbose` | `bool` | verbose | `False` |\n\nSource code in `llama-index-integrations/agent/llama-index-agent-openai/llama_index/agent/openai/openai_assistant_agent.py`\n\n|     |     |\n| --- | --- |\n| ```<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>``` | ```<br>@classmethod<br>def from_existing(<br>    cls,<br>    assistant_id: str,<br>    tools: Optional[List[BaseTool]] = None,<br>    thread_id: Optional[str] = None,<br>    instructions_prefix: Optional[str] = None,<br>    run_retrieve_sleep_time: float = 0.1,<br>    callback_manager: Optional[CallbackManager] = None,<br>    api_key: Optional[str] = None,<br>    verbose: bool = False,<br>) -> \"OpenAIAssistantAgent\":<br>    \"\"\"From existing assistant id.<br>    Args:<br>        assistant_id: id of assistant<br>        tools: list of BaseTools Assistant can use<br>        thread_id: thread id<br>        run_retrieve_sleep_time: run retrieve sleep time<br>        instructions_prefix: instructions prefix<br>        callback_manager: callback manager<br>        api_key: OpenAI API key<br>        verbose: verbose<br>    \"\"\"<br>    from openai import OpenAI<br>    # initialize client<br>    client = OpenAI(api_key=api_key)<br>    # get assistant<br>    assistant = client.beta.assistants.retrieve(assistant_id)<br>    # assistant.tools is incompatible with BaseTools so have to pass from params<br>    return cls(<br>        client,<br>        assistant,<br>        tools=tools,<br>        callback_manager=callback_manager,<br>        thread_id=thread_id,<br>        instructions_prefix=instructions_prefix,<br>        run_retrieve_sleep_time=run_retrieve_sleep_time,<br>        verbose=verbose,<br>    )<br>``` |\n\n### reset [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/\\#llama_index.agent.openai.OpenAIAssistantAgent.reset \"Permanent link\")\n\n```\nreset() -> None\n\n```\n\nDelete and create a new thread.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-openai/llama_index/agent/openai/openai_assistant_agent.py`\n\n|     |     |\n| --- | --- |\n| ```<br>356<br>357<br>358<br>359<br>360<br>361<br>``` | ```<br>def reset(self) -> None:<br>    \"\"\"Delete and create a new thread.\"\"\"<br>    self._client.beta.threads.delete(self._thread_id)<br>    thread = self._client.beta.threads.create()<br>    thread_id = thread.id<br>    self._thread_id = thread_id<br>``` |\n\n### get\\_tools [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/\\#llama_index.agent.openai.OpenAIAssistantAgent.get_tools \"Permanent link\")\n\n```\nget_tools(message: str) -> List[BaseTool]\n\n```\n\nGet tools.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-openai/llama_index/agent/openai/openai_assistant_agent.py`\n\n|     |     |\n| --- | --- |\n| ```<br>363<br>364<br>365<br>``` | ```<br>def get_tools(self, message: str) -> List[BaseTool]:<br>    \"\"\"Get tools.\"\"\"<br>    return self._tools<br>``` |\n\n### upload\\_files [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/\\#llama_index.agent.openai.OpenAIAssistantAgent.upload_files \"Permanent link\")\n\n```\nupload_files(files: List[str]) -> Dict[str, Any]\n\n```\n\nUpload files.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-openai/llama_index/agent/openai/openai_assistant_agent.py`\n\n|     |     |\n| --- | --- |\n| ```<br>367<br>368<br>369<br>``` | ```<br>def upload_files(self, files: List[str]) -> Dict[str, Any]:<br>    \"\"\"Upload files.\"\"\"<br>    return _process_files(self._client, files)<br>``` |\n\n### add\\_message [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/\\#llama_index.agent.openai.OpenAIAssistantAgent.add_message \"Permanent link\")\n\n```\nadd_message(message: str, file_ids: Optional[List[str]] = None, tools: Optional[List[Dict[str, Any]]] = None) -> Any\n\n```\n\nAdd message to assistant.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-openai/llama_index/agent/openai/openai_assistant_agent.py`\n\n|     |     |\n| --- | --- |\n| ```<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>``` | ```<br>def add_message(<br>    self,<br>    message: str,<br>    file_ids: Optional[List[str]] = None,<br>    tools: Optional[List[Dict[str, Any]]] = None,<br>) -> Any:<br>    \"\"\"Add message to assistant.\"\"\"<br>    attachments = format_attachments(file_ids=file_ids, tools=tools)<br>    return self._client.beta.threads.messages.create(<br>        thread_id=self._thread_id,<br>        role=\"user\",<br>        content=message,<br>        attachments=attachments,<br>    )<br>``` |\n\n### run\\_assistant [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/\\#llama_index.agent.openai.OpenAIAssistantAgent.run_assistant \"Permanent link\")\n\n```\nrun_assistant(instructions_prefix: Optional[str] = None) -> Tuple[Any, Dict]\n\n```\n\nRun assistant.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-openai/llama_index/agent/openai/openai_assistant_agent.py`\n\n|     |     |\n| --- | --- |\n| ```<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>``` | ```<br>def run_assistant(<br>    self, instructions_prefix: Optional[str] = None<br>) -> Tuple[Any, Dict]:<br>    \"\"\"Run assistant.\"\"\"<br>    instructions_prefix = instructions_prefix or self._instructions_prefix<br>    run = self._client.beta.threads.runs.create(<br>        thread_id=self._thread_id,<br>        assistant_id=self._assistant.id,<br>        instructions=instructions_prefix,<br>    )<br>    from openai.types.beta.threads import Run<br>    run = cast(Run, run)<br>    sources = []<br>    while run.status in [\"queued\", \"in_progress\", \"requires_action\"]:<br>        run = self._client.beta.threads.runs.retrieve(<br>            thread_id=self._thread_id, run_id=run.id<br>        )<br>        if run.status == \"requires_action\":<br>            cur_tool_outputs = self._run_function_calling(run)<br>            sources.extend(cur_tool_outputs)<br>        time.sleep(self._run_retrieve_sleep_time)<br>    if run.status == \"failed\":<br>        raise ValueError(<br>            f\"Run failed with status {run.status}.\\n\" f\"Error: {run.last_error}\"<br>        )<br>    return run, {\"sources\": sources}<br>``` |\n\n### arun\\_assistant`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/\\#llama_index.agent.openai.OpenAIAssistantAgent.arun_assistant \"Permanent link\")\n\n```\narun_assistant(instructions_prefix: Optional[str] = None) -> Tuple[Any, Dict]\n\n```\n\nRun assistant.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-openai/llama_index/agent/openai/openai_assistant_agent.py`\n\n|     |     |\n| --- | --- |\n| ```<br>462<br>463<br>464<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>472<br>473<br>474<br>475<br>476<br>477<br>478<br>479<br>480<br>481<br>482<br>483<br>484<br>485<br>486<br>487<br>488<br>489<br>490<br>491<br>``` | ```<br>async def arun_assistant(<br>    self, instructions_prefix: Optional[str] = None<br>) -> Tuple[Any, Dict]:<br>    \"\"\"Run assistant.\"\"\"<br>    instructions_prefix = instructions_prefix or self._instructions_prefix<br>    run = self._client.beta.threads.runs.create(<br>        thread_id=self._thread_id,<br>        assistant_id=self._assistant.id,<br>        instructions=instructions_prefix,<br>    )<br>    from openai.types.beta.threads import Run<br>    run = cast(Run, run)<br>    sources = []<br>    while run.status in [\"queued\", \"in_progress\", \"requires_action\"]:<br>        run = self._client.beta.threads.runs.retrieve(<br>            thread_id=self._thread_id, run_id=run.id<br>        )<br>        if run.status == \"requires_action\":<br>            cur_tool_outputs = await self._arun_function_calling(run)<br>            sources.extend(cur_tool_outputs)<br>        await asyncio.sleep(self._run_retrieve_sleep_time)<br>    if run.status == \"failed\":<br>        raise ValueError(<br>            f\"Run failed with status {run.status}.\\n\" f\"Error: {run.last_error}\"<br>        )<br>    return run, {\"sources\": sources}<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Openai - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/agent/openai/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/condense_plus_context/#llama_index.core.chat_engine.CondensePlusContextChatEngine)\n\n# Condense plus context\n\n## CondensePlusContextChatEngine [\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/condense_plus_context/\\#llama_index.core.chat_engine.CondensePlusContextChatEngine \"Permanent link\")\n\nBases: `BaseChatEngine`\n\nCondensed Conversation & Context Chat Engine.\n\nFirst condense a conversation and latest user message to a standalone question\nThen build a context for the standalone question from a retriever,\nThen pass the context along with prompt and user message to LLM to generate a response.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/condense_plus_context.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>``` | ```<br>class CondensePlusContextChatEngine(BaseChatEngine):<br>    \"\"\"<br>    Condensed Conversation & Context Chat Engine.<br>    First condense a conversation and latest user message to a standalone question<br>    Then build a context for the standalone question from a retriever,<br>    Then pass the context along with prompt and user message to LLM to generate a response.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        retriever: BaseRetriever,<br>        llm: LLM,<br>        memory: BaseMemory,<br>        context_prompt: Optional[str] = None,<br>        condense_prompt: Optional[str] = None,<br>        system_prompt: Optional[str] = None,<br>        skip_condense: bool = False,<br>        node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>    ):<br>        self._retriever = retriever<br>        self._llm = llm<br>        self._memory = memory<br>        self._context_prompt_template = (<br>            context_prompt or DEFAULT_CONTEXT_PROMPT_TEMPLATE<br>        )<br>        condense_prompt_str = condense_prompt or DEFAULT_CONDENSE_PROMPT_TEMPLATE<br>        self._condense_prompt_template = PromptTemplate(condense_prompt_str)<br>        self._system_prompt = system_prompt<br>        self._skip_condense = skip_condense<br>        self._node_postprocessors = node_postprocessors or []<br>        self.callback_manager = callback_manager or CallbackManager([])<br>        for node_postprocessor in self._node_postprocessors:<br>            node_postprocessor.callback_manager = self.callback_manager<br>        self._token_counter = TokenCounter()<br>        self._verbose = verbose<br>    @classmethod<br>    def from_defaults(<br>        cls,<br>        retriever: BaseRetriever,<br>        llm: Optional[LLM] = None,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        memory: Optional[BaseMemory] = None,<br>        system_prompt: Optional[str] = None,<br>        context_prompt: Optional[str] = None,<br>        condense_prompt: Optional[str] = None,<br>        skip_condense: bool = False,<br>        node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,<br>        verbose: bool = False,<br>        **kwargs: Any,<br>    ) -> \"CondensePlusContextChatEngine\":<br>        \"\"\"Initialize a CondensePlusContextChatEngine from default parameters.\"\"\"<br>        llm = llm or Settings.llm<br>        chat_history = chat_history or []<br>        memory = memory or ChatMemoryBuffer.from_defaults(<br>            chat_history=chat_history, token_limit=llm.metadata.context_window - 256<br>        )<br>        return cls(<br>            retriever=retriever,<br>            llm=llm,<br>            memory=memory,<br>            context_prompt=context_prompt,<br>            condense_prompt=condense_prompt,<br>            skip_condense=skip_condense,<br>            callback_manager=Settings.callback_manager,<br>            node_postprocessors=node_postprocessors,<br>            system_prompt=system_prompt,<br>            verbose=verbose,<br>        )<br>    def _condense_question(<br>        self, chat_history: List[ChatMessage], latest_message: str<br>    ) -> str:<br>        \"\"\"Condense a conversation history and latest user message to a standalone question.\"\"\"<br>        if self._skip_condense or len(chat_history) == 0:<br>            return latest_message<br>        chat_history_str = messages_to_history_str(chat_history)<br>        logger.debug(chat_history_str)<br>        return self._llm.predict(<br>            self._condense_prompt_template,<br>            question=latest_message,<br>            chat_history=chat_history_str,<br>        )<br>    async def _acondense_question(<br>        self, chat_history: List[ChatMessage], latest_message: str<br>    ) -> str:<br>        \"\"\"Condense a conversation history and latest user message to a standalone question.\"\"\"<br>        if self._skip_condense or len(chat_history) == 0:<br>            return latest_message<br>        chat_history_str = messages_to_history_str(chat_history)<br>        logger.debug(chat_history_str)<br>        return await self._llm.apredict(<br>            self._condense_prompt_template,<br>            question=latest_message,<br>            chat_history=chat_history_str,<br>        )<br>    def _retrieve_context(self, message: str) -> Tuple[str, List[NodeWithScore]]:<br>        \"\"\"Build context for a message from retriever.\"\"\"<br>        nodes = self._retriever.retrieve(message)<br>        for postprocessor in self._node_postprocessors:<br>            nodes = postprocessor.postprocess_nodes(<br>                nodes, query_bundle=QueryBundle(message)<br>            )<br>        context_str = \"\\n\\n\".join(<br>            [n.node.get_content(metadata_mode=MetadataMode.LLM).strip() for n in nodes]<br>        )<br>        return context_str, nodes<br>    async def _aretrieve_context(self, message: str) -> Tuple[str, List[NodeWithScore]]:<br>        \"\"\"Build context for a message from retriever.\"\"\"<br>        nodes = await self._retriever.aretrieve(message)<br>        for postprocessor in self._node_postprocessors:<br>            nodes = postprocessor.postprocess_nodes(<br>                nodes, query_bundle=QueryBundle(message)<br>            )<br>        context_str = \"\\n\\n\".join(<br>            [n.node.get_content(metadata_mode=MetadataMode.LLM).strip() for n in nodes]<br>        )<br>        return context_str, nodes<br>    def _run_c3(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> Tuple[List[ChatMessage], ToolOutput, List[NodeWithScore]]:<br>        if chat_history is not None:<br>            self._memory.set(chat_history)<br>        chat_history = self._memory.get(input=message)<br>        # Condense conversation history and latest message to a standalone question<br>        condensed_question = self._condense_question(chat_history, message)  # type: ignore<br>        logger.info(f\"Condensed question: {condensed_question}\")<br>        if self._verbose:<br>            print(f\"Condensed question: {condensed_question}\")<br>        # Build context for the standalone question from a retriever<br>        context_str, context_nodes = self._retrieve_context(condensed_question)<br>        context_source = ToolOutput(<br>            tool_name=\"retriever\",<br>            content=context_str,<br>            raw_input={\"message\": condensed_question},<br>            raw_output=context_str,<br>        )<br>        logger.debug(f\"Context: {context_str}\")<br>        if self._verbose:<br>            print(f\"Context: {context_str}\")<br>        system_message_content = self._context_prompt_template.format(<br>            context_str=context_str<br>        )<br>        if self._system_prompt:<br>            system_message_content = self._system_prompt + \"\\n\" + system_message_content<br>        system_message = ChatMessage(<br>            content=system_message_content, role=self._llm.metadata.system_role<br>        )<br>        initial_token_count = self._token_counter.estimate_tokens_in_messages(<br>            [system_message]<br>        )<br>        self._memory.put(ChatMessage(content=message, role=MessageRole.USER))<br>        chat_messages = [<br>            system_message,<br>            *self._memory.get(initial_token_count=initial_token_count),<br>        ]<br>        return chat_messages, context_source, context_nodes<br>    async def _arun_c3(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> Tuple[List[ChatMessage], ToolOutput, List[NodeWithScore]]:<br>        if chat_history is not None:<br>            self._memory.set(chat_history)<br>        chat_history = self._memory.get(input=message)<br>        # Condense conversation history and latest message to a standalone question<br>        condensed_question = await self._acondense_question(chat_history, message)  # type: ignore<br>        logger.info(f\"Condensed question: {condensed_question}\")<br>        if self._verbose:<br>            print(f\"Condensed question: {condensed_question}\")<br>        # Build context for the standalone question from a retriever<br>        context_str, context_nodes = await self._aretrieve_context(condensed_question)<br>        context_source = ToolOutput(<br>            tool_name=\"retriever\",<br>            content=context_str,<br>            raw_input={\"message\": condensed_question},<br>            raw_output=context_str,<br>        )<br>        logger.debug(f\"Context: {context_str}\")<br>        if self._verbose:<br>            print(f\"Context: {context_str}\")<br>        system_message_content = self._context_prompt_template.format(<br>            context_str=context_str<br>        )<br>        if self._system_prompt:<br>            system_message_content = self._system_prompt + \"\\n\" + system_message_content<br>        system_message = ChatMessage(<br>            content=system_message_content, role=self._llm.metadata.system_role<br>        )<br>        initial_token_count = self._token_counter.estimate_tokens_in_messages(<br>            [system_message]<br>        )<br>        self._memory.put(ChatMessage(content=message, role=MessageRole.USER))<br>        chat_messages = [<br>            system_message,<br>            *self._memory.get(initial_token_count=initial_token_count),<br>        ]<br>        return chat_messages, context_source, context_nodes<br>    @trace_method(\"chat\")<br>    def chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> AgentChatResponse:<br>        chat_messages, context_source, context_nodes = self._run_c3(<br>            message, chat_history<br>        )<br>        # pass the context, system prompt and user message as chat to LLM to generate a response<br>        chat_response = self._llm.chat(chat_messages)<br>        assistant_message = chat_response.message<br>        self._memory.put(assistant_message)<br>        return AgentChatResponse(<br>            response=str(assistant_message.content),<br>            sources=[context_source],<br>            source_nodes=context_nodes,<br>        )<br>    @trace_method(\"chat\")<br>    def stream_chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> StreamingAgentChatResponse:<br>        chat_messages, context_source, context_nodes = self._run_c3(<br>            message, chat_history<br>        )<br>        # pass the context, system prompt and user message as chat to LLM to generate a response<br>        chat_response = StreamingAgentChatResponse(<br>            chat_stream=self._llm.stream_chat(chat_messages),<br>            sources=[context_source],<br>            source_nodes=context_nodes,<br>        )<br>        thread = Thread(<br>            target=chat_response.write_response_to_history, args=(self._memory,)<br>        )<br>        thread.start()<br>        return chat_response<br>    @trace_method(\"chat\")<br>    async def achat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> AgentChatResponse:<br>        chat_messages, context_source, context_nodes = await self._arun_c3(<br>            message, chat_history<br>        )<br>        # pass the context, system prompt and user message as chat to LLM to generate a response<br>        chat_response = await self._llm.achat(chat_messages)<br>        assistant_message = chat_response.message<br>        self._memory.put(assistant_message)<br>        return AgentChatResponse(<br>            response=str(assistant_message.content),<br>            sources=[context_source],<br>            source_nodes=context_nodes,<br>        )<br>    @trace_method(\"chat\")<br>    async def astream_chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> StreamingAgentChatResponse:<br>        chat_messages, context_source, context_nodes = await self._arun_c3(<br>            message, chat_history<br>        )<br>        # pass the context, system prompt and user message as chat to LLM to generate a response<br>        chat_response = StreamingAgentChatResponse(<br>            achat_stream=await self._llm.astream_chat(chat_messages),<br>            sources=[context_source],<br>            source_nodes=context_nodes,<br>        )<br>        asyncio.create_task(chat_response.awrite_response_to_history(self._memory))<br>        return chat_response<br>    def reset(self) -> None:<br>        # Clear chat history<br>        self._memory.reset()<br>    @property<br>    def chat_history(self) -> List[ChatMessage]:<br>        \"\"\"Get chat history.\"\"\"<br>        return self._memory.get_all()<br>``` |\n\n### chat\\_history`property`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/condense_plus_context/\\#llama_index.core.chat_engine.CondensePlusContextChatEngine.chat_history \"Permanent link\")\n\n```\nchat_history: List[ChatMessage]\n\n```\n\nGet chat history.\n\n### from\\_defaults`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/condense_plus_context/\\#llama_index.core.chat_engine.CondensePlusContextChatEngine.from_defaults \"Permanent link\")\n\n```\nfrom_defaults(retriever: BaseRetriever, llm: Optional[LLM] = None, chat_history: Optional[List[ChatMessage]] = None, memory: Optional[BaseMemory] = None, system_prompt: Optional[str] = None, context_prompt: Optional[str] = None, condense_prompt: Optional[str] = None, skip_condense: bool = False, node_postprocessors: Optional[List[BaseNodePostprocessor]] = None, verbose: bool = False, **kwargs: Any) -> CondensePlusContextChatEngine\n\n```\n\nInitialize a CondensePlusContextChatEngine from default parameters.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/condense_plus_context.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>``` | ```<br>@classmethod<br>def from_defaults(<br>    cls,<br>    retriever: BaseRetriever,<br>    llm: Optional[LLM] = None,<br>    chat_history: Optional[List[ChatMessage]] = None,<br>    memory: Optional[BaseMemory] = None,<br>    system_prompt: Optional[str] = None,<br>    context_prompt: Optional[str] = None,<br>    condense_prompt: Optional[str] = None,<br>    skip_condense: bool = False,<br>    node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,<br>    verbose: bool = False,<br>    **kwargs: Any,<br>) -> \"CondensePlusContextChatEngine\":<br>    \"\"\"Initialize a CondensePlusContextChatEngine from default parameters.\"\"\"<br>    llm = llm or Settings.llm<br>    chat_history = chat_history or []<br>    memory = memory or ChatMemoryBuffer.from_defaults(<br>        chat_history=chat_history, token_limit=llm.metadata.context_window - 256<br>    )<br>    return cls(<br>        retriever=retriever,<br>        llm=llm,<br>        memory=memory,<br>        context_prompt=context_prompt,<br>        condense_prompt=condense_prompt,<br>        skip_condense=skip_condense,<br>        callback_manager=Settings.callback_manager,<br>        node_postprocessors=node_postprocessors,<br>        system_prompt=system_prompt,<br>        verbose=verbose,<br>    )<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Condense plus context - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/condense_plus_context/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/oci_genai/#llama_index.embeddings.oci_genai.OCIGenAIEmbeddings)\n\n# Oci genai\n\n## OCIGenAIEmbeddings [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/oci_genai/\\#llama_index.embeddings.oci_genai.OCIGenAIEmbeddings \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nOCI embedding models.\n\nTo authenticate, the OCI client uses the methods described in\nhttps://docs.oracle.com/en-us/iaas/Content/API/Concepts/sdk\\_authentication\\_methods.htm\n\nThe authentifcation method is passed through auth\\_type and should be one of:\nAPI\\_KEY (default), SECURITY\\_TOKEN, INSTANCE\\_PRINCIPAL, RESOURCE\\_PRINCIPAL\n\nMake sure you have the required policies (profile/roles) to\naccess the OCI Generative AI service. If a specific config profile is used,\nyou must pass the name of the profile (~/.oci/config) through auth\\_profile.\n\nTo use, you must provide the compartment id\nalong with the endpoint url, and model id\nas named parameters to the constructor.\n\nExample\n\n.. code-block:: python\n\n```\nfrom llama_index.embeddings.oci_genai import OCIGenAIEmbeddings\n\nembeddings = OCIGenAIEmbeddings(\n    model_name=\"MY_EMBEDDING_MODEL\",\n    service_endpoint=\"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\",\n    compartment_id=\"MY_OCID\"\n)\n\n```\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-oci-genai/llama_index/embeddings/oci_genai/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>``` | ```<br>class OCIGenAIEmbeddings(BaseEmbedding):<br>    \"\"\"OCI embedding models.<br>    To authenticate, the OCI client uses the methods described in<br>    https://docs.oracle.com/en-us/iaas/Content/API/Concepts/sdk_authentication_methods.htm<br>    The authentifcation method is passed through auth_type and should be one of:<br>    API_KEY (default), SECURITY_TOKEN, INSTANCE_PRINCIPAL, RESOURCE_PRINCIPAL<br>    Make sure you have the required policies (profile/roles) to<br>    access the OCI Generative AI service. If a specific config profile is used,<br>    you must pass the name of the profile (~/.oci/config) through auth_profile.<br>    To use, you must provide the compartment id<br>    along with the endpoint url, and model id<br>    as named parameters to the constructor.<br>    Example:<br>        .. code-block:: python<br>            from llama_index.embeddings.oci_genai import OCIGenAIEmbeddings<br>            embeddings = OCIGenAIEmbeddings(<br>                model_name=\"MY_EMBEDDING_MODEL\",<br>                service_endpoint=\"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\",<br>                compartment_id=\"MY_OCID\"<br>            )<br>    \"\"\"<br>    model_name: str = Field(<br>        description=\"ID or Name of the OCI Generative AI embedding model to use.\"<br>    )<br>    truncate: str = Field(<br>        description=\"Truncate embeddings that are too long from start or end, values START/ END/ NONE\",<br>        default=\"END\",<br>    )<br>    input_type: Optional[str] = Field(<br>        description=\"Model Input type. If not provided, search_document and search_query are used when needed.\",<br>        default=None,<br>    )<br>    service_endpoint: Optional[str] = Field(<br>        description=\"service endpoint url.\",<br>        default=None,<br>    )<br>    compartment_id: Optional[str] = Field(<br>        description=\"OCID of compartment.\",<br>        default=None,<br>    )<br>    auth_type: Optional[str] = Field(<br>        description=\"Authentication type, can be: API_KEY, SECURITY_TOKEN, INSTANCE_PRINCIPAL, RESOURCE_PRINCIPAL. If not specified, API_KEY will be used\",<br>        default=\"API_KEY\",<br>    )<br>    auth_profile: Optional[str] = Field(<br>        description=\"The name of the profile in ~/.oci/config. If not specified , DEFAULT will be used\",<br>        default=\"DEFAULT\",<br>    )<br>    _client: Any = PrivateAttr()<br>    def __init__(<br>        self,<br>        model_name: str,<br>        truncate: str = \"END\",<br>        input_type: Optional[str] = None,<br>        service_endpoint: Optional[str] = None,<br>        compartment_id: Optional[str] = None,<br>        auth_type: Optional[str] = \"API_KEY\",<br>        auth_profile: Optional[str] = \"DEFAULT\",<br>        client: Optional[Any] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>    ):<br>        \"\"\"<br>        Initializes the OCIGenAIEmbeddings class.<br>        Args:<br>            model_name (str): The name or ID of the model to be used for generating embeddings, e.g., \"cohere.embed-english-light-v3.0\".<br>            truncate (str): A string indicating the truncation strategy for long input text. Possible values<br>                            are 'START', 'END', or 'NONE'.<br>            input_type (Optional[str]): An optional string that specifies the type of input provided to the model.<br>                                        This is model-dependent and could be one of the following: \"search_query\",<br>                                        \"search_document\", \"classification\", or \"clustering\".<br>            service_endpoint (str): service endpoint url, e.g., \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\"<br>            compartment_id (str): OCID of the compartment.<br>            auth_type (Optional[str]): Authentication type, can be: API_KEY (default), SECURITY_TOKEN, INSTANCEAL, RESOURCE_PRINCIPAL.<br>                                    If not specified, API_KEY will be used<br>            auth_profile (Optional[str]): The name of the profile in ~/.oci/config. If not specified , DEFAULT will be used<br>            client (Optional[Any]): An optional OCI client object. If not provided, the client will be created using the<br>                                    provided service endpoint and authentifcation method.<br>        \"\"\"<br>        super().__init__(<br>            model_name=model_name,<br>            truncate=truncate,<br>            input_type=input_type,<br>            service_endpoint=service_endpoint,<br>            compartment_id=compartment_id,<br>            auth_type=auth_type,<br>            auth_profile=auth_profile,<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>        )<br>        if client is not None:<br>            self._client = client<br>        else:<br>            try:<br>                import oci<br>                client_kwargs = {<br>                    \"config\": {},<br>                    \"signer\": None,<br>                    \"service_endpoint\": service_endpoint,<br>                    \"retry_strategy\": oci.retry.DEFAULT_RETRY_STRATEGY,<br>                    \"timeout\": (<br>                        10,<br>                        240,<br>                    ),  # default timeout config for OCI Gen AI service<br>                }<br>                if auth_type == OCIAuthType(1).name:<br>                    client_kwargs[\"config\"] = oci.config.from_file(<br>                        profile_name=auth_profile<br>                    )<br>                    client_kwargs.pop(\"signer\", None)<br>                elif auth_type == OCIAuthType(2).name:<br>                    def make_security_token_signer(oci_config):  # type: ignore[no-untyped-def]<br>                        pk = oci.signer.load_private_key_from_file(<br>                            oci_config.get(\"key_file\"), None<br>                        )<br>                        with open(<br>                            oci_config.get(\"security_token_file\"), encoding=\"utf-8\"<br>                        ) as f:<br>                            st_string = f.read()<br>                        return oci.auth.signers.SecurityTokenSigner(st_string, pk)<br>                    client_kwargs[\"config\"] = oci.config.from_file(<br>                        profile_name=auth_profile<br>                    )<br>                    client_kwargs[\"signer\"] = make_security_token_signer(<br>                        oci_config=client_kwargs[\"config\"]<br>                    )<br>                elif auth_type == OCIAuthType(3).name:<br>                    client_kwargs[<br>                        \"signer\"<br>                    ] = oci.auth.signers.InstancePrincipalsSecurityTokenSigner()<br>                elif auth_type == OCIAuthType(4).name:<br>                    client_kwargs[<br>                        \"signer\"<br>                    ] = oci.auth.signers.get_resource_principals_signer()<br>                else:<br>                    raise ValueError(<br>                        f\"Please provide valid value to auth_type, {auth_type} is not valid.\"<br>                    )<br>                self._client = oci.generative_ai_inference.GenerativeAiInferenceClient(<br>                    **client_kwargs<br>                )<br>            except ImportError as ex:<br>                raise ModuleNotFoundError(<br>                    \"Could not import oci python package. \"<br>                    \"Please make sure you have the oci package installed.\"<br>                ) from ex<br>            except Exception as e:<br>                raise ValueError(<br>                    \"\"\"Could not authenticate with OCI client. Please check if ~/.oci/config exists.<br>                    If INSTANCE_PRINCIPAL or RESOURCE_PRINCIPAL is used, please check the specified<br>                    auth_profile and auth_type are valid.\"\"\",<br>                    e,<br>                ) from e<br>    @classmethod<br>    def class_name(self) -> str:<br>        return \"OCIGenAIEmbeddings\"<br>    @staticmethod<br>    def list_supported_models() -> List[str]:<br>        return list(SUPPORTED_MODELS)<br>    def _embed(self, texts: List[str], input_type: str) -> List[List[float]]:<br>        try:<br>            from oci.generative_ai_inference import models<br>        except ImportError as ex:<br>            raise ModuleNotFoundError(<br>                \"Could not import oci python package. \"<br>                \"Please make sure you have the oci package installed.\"<br>            ) from ex<br>        if self.model_name.startswith(CUSTOM_ENDPOINT_PREFIX):<br>            serving_mode = models.DedicatedServingMode(endpoint_id=self.model_name)<br>        else:<br>            serving_mode = models.OnDemandServingMode(model_id=self.model_name)<br>        request = models.EmbedTextDetails(<br>            serving_mode=serving_mode,<br>            compartment_id=self.compartment_id,<br>            input_type=self.input_type or input_type,<br>            truncate=self.truncate,<br>            inputs=texts,<br>        )<br>        response = self._client.embed_text(request)<br>        return response.data.embeddings<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        return self._embed([query], input_type=\"SEARCH_QUERY\")[0]<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        return self._embed([text], input_type=\"SEARCH_DOCUMENT\")[0]<br>    def _get_text_embeddings(self, text: str) -> List[List[float]]:<br>        return self._embed(text, input_type=\"SEARCH_DOCUMENT\")<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        return self._get_text_embedding(text)<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        return self._get_query_embedding(query)<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Oci genai - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/oci_genai/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/mixedbreadai/#llama_index.embeddings.mixedbreadai.MixedbreadAIEmbedding)\n\n# Mixedbreadai\n\n## MixedbreadAIEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/mixedbreadai/\\#llama_index.embeddings.mixedbreadai.MixedbreadAIEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nClass to get embeddings using the mixedbread ai embedding API with models such as 'mixedbread-ai/mxbai-embed-large-v1'.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `api_key` | `Optional[str]` | mixedbread ai API key. Defaults to None. | `None` |\n| `model_name` | `str` | Model for embedding. Defaults to \"mixedbread-ai/mxbai-embed-large-v1\". | `'mixedbread-ai/mxbai-embed-large-v1'` |\n| `encoding_format` | `EncodingFormat` | Encoding format for embeddings. Defaults to EncodingFormat.FLOAT. | `FLOAT` |\n| `truncation_strategy` | `TruncationStrategy` | Truncation strategy. Defaults to TruncationStrategy.START. | `START` |\n| `normalized` | `bool` | Whether to normalize the embeddings. Defaults to True. | `True` |\n| `dimensions` | `Optional[int]` | Number of dimensions for embeddings. Only applicable for models with matryoshka support. | `None` |\n| `prompt` | `Optional[str]` | An optional prompt to provide context to the model. | `None` |\n| `embed_batch_size` | `Optional[int]` | The batch size for embedding calls. Defaults to 128. | `None` |\n| `callback_manager` | `Optional[CallbackManager]` | Manager for handling callbacks. | `None` |\n| `timeout` | `Optional[float]` | Timeout for API calls. | `None` |\n| `max_retries` | `Optional[int]` | Maximum number of retries for API calls. | `None` |\n| `httpx_client` | `Optional[Client]` | Custom HTTPX client. | `None` |\n| `httpx_async_client` | `Optional[AsyncClient]` | Custom asynchronous HTTPX client. | `None` |\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-mixedbreadai/llama_index/embeddings/mixedbreadai/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>``` | ```<br>class MixedbreadAIEmbedding(BaseEmbedding):<br>    \"\"\"<br>    Class to get embeddings using the mixedbread ai embedding API with models such as 'mixedbread-ai/mxbai-embed-large-v1'.<br>    Args:<br>        api_key (Optional[str]): mixedbread ai API key. Defaults to None.<br>        model_name (str): Model for embedding. Defaults to \"mixedbread-ai/mxbai-embed-large-v1\".<br>        encoding_format (EncodingFormat): Encoding format for embeddings. Defaults to EncodingFormat.FLOAT.<br>        truncation_strategy (TruncationStrategy): Truncation strategy. Defaults to TruncationStrategy.START.<br>        normalized (bool): Whether to normalize the embeddings. Defaults to True.<br>        dimensions (Optional[int]): Number of dimensions for embeddings. Only applicable for models with matryoshka support.<br>        prompt (Optional[str]): An optional prompt to provide context to the model.<br>        embed_batch_size (Optional[int]): The batch size for embedding calls. Defaults to 128.<br>        callback_manager (Optional[CallbackManager]): Manager for handling callbacks.<br>        timeout (Optional[float]): Timeout for API calls.<br>        max_retries (Optional[int]): Maximum number of retries for API calls.<br>        httpx_client (Optional[httpx.Client]): Custom HTTPX client.<br>        httpx_async_client (Optional[httpx.AsyncClient]): Custom asynchronous HTTPX client.<br>    \"\"\"<br>    api_key: str = Field(description=\"The mixedbread ai API key.\", min_length=1)<br>    model_name: str = Field(<br>        default=\"mixedbread-ai/mxbai-embed-large-v1\",<br>        description=\"Model to use for embeddings.\",<br>        min_length=1,<br>    )<br>    encoding_format: EncodingFormat = Field(<br>        default=EncodingFormat.FLOAT, description=\"Encoding format for the embeddings.\"<br>    )<br>    truncation_strategy: TruncationStrategy = Field(<br>        default=TruncationStrategy.START,<br>        description=\"Truncation strategy for input text.\",<br>    )<br>    normalized: bool = Field(<br>        default=True, description=\"Whether to normalize the embeddings.\"<br>    )<br>    dimensions: Optional[int] = Field(<br>        default=None,<br>        description=\"Number of dimensions for embeddings. Only applicable for models with matryoshka support.\",<br>        gt=0,<br>    )<br>    prompt: Optional[str] = Field(<br>        default=None,<br>        description=\"An optional prompt to provide context to the model.\",<br>        min_length=1,<br>    )<br>    embed_batch_size: int = Field(<br>        default=128, description=\"The batch size for embedding calls.\", gt=0, lte=256<br>    )<br>    _client: MixedbreadAI = PrivateAttr()<br>    _async_client: AsyncMixedbreadAI = PrivateAttr()<br>    _request_options: Optional[RequestOptions] = PrivateAttr()<br>    def __init__(<br>        self,<br>        api_key: Optional[str] = None,<br>        model_name: str = \"mixedbread-ai/mxbai-embed-large-v1\",<br>        encoding_format: EncodingFormat = EncodingFormat.FLOAT,<br>        truncation_strategy: TruncationStrategy = TruncationStrategy.START,<br>        normalized: bool = True,<br>        dimensions: Optional[int] = None,<br>        prompt: Optional[str] = None,<br>        embed_batch_size: Optional[int] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        timeout: Optional[float] = None,<br>        max_retries: Optional[int] = None,<br>        httpx_client: Optional[httpx.Client] = None,<br>        httpx_async_client: Optional[httpx.AsyncClient] = None,<br>        **kwargs: Any,<br>    ):<br>        if embed_batch_size is None:<br>            embed_batch_size = 128  # Default batch size for mixedbread ai<br>        try:<br>            api_key = api_key or os.environ[\"MXBAI_API_KEY\"]<br>        except KeyError:<br>            raise ValueError(<br>                \"Must pass in mixedbread ai API key or \"<br>                \"specify via MXBAI_API_KEY environment variable \"<br>            )<br>        super().__init__(<br>            api_key=api_key,<br>            model_name=model_name,<br>            encoding_format=encoding_format,<br>            truncation_strategy=truncation_strategy,<br>            normalized=normalized,<br>            dimensions=dimensions,<br>            prompt=prompt,<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            **kwargs,<br>        )<br>        self._client = MixedbreadAI(<br>            api_key=api_key, timeout=timeout, httpx_client=httpx_client<br>        )<br>        self._async_client = AsyncMixedbreadAI(<br>            api_key=api_key, timeout=timeout, httpx_client=httpx_async_client<br>        )<br>        self._request_options = (<br>            RequestOptions(max_retries=max_retries) if max_retries is not None else None<br>        )<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"MixedbreadAIEmbedding\"<br>    def _get_embedding(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"<br>        Get embeddings for a list of texts using the mixedbread ai API.<br>        Args:<br>            texts (List[str]): List of texts to embed.<br>        Returns:<br>            List[List[float]]: List of embeddings.<br>        \"\"\"<br>        response = self._client.embeddings(<br>            model=self.model_name,<br>            input=texts,<br>            encoding_format=self.encoding_format,<br>            normalized=self.normalized,<br>            truncation_strategy=self.truncation_strategy,<br>            dimensions=self.dimensions,<br>            prompt=self.prompt,<br>            request_options=self._request_options,<br>        )<br>        return [item.embedding for item in response.data]<br>    async def _aget_embedding(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"<br>        Asynchronously get embeddings for a list of texts using the mixedbread ai API.<br>        Args:<br>            texts (List[str]): List of texts to embed.<br>        Returns:<br>            List[List[float]]: List of embeddings.<br>        \"\"\"<br>        response = await self._async_client.embeddings(<br>            model=self.model_name,<br>            input=texts,<br>            encoding_format=self.encoding_format,<br>            normalized=self.normalized,<br>            truncation_strategy=self.truncation_strategy,<br>            dimensions=self.dimensions,<br>            prompt=self.prompt,<br>            request_options=self._request_options,<br>        )<br>        return [item.embedding for item in response.data]<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"<br>        Get embedding for a query using the mixedbread ai API.<br>        Args:<br>            query (str): Query text.<br>        Returns:<br>            List[float]: Embedding for the query.<br>        \"\"\"<br>        return self._get_embedding([query])[0]<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"<br>        Asynchronously get embedding for a query using the mixedbread ai API.<br>        Args:<br>            query (str): Query text.<br>        Returns:<br>            List[float]: Embedding for the query.<br>        \"\"\"<br>        r = await self._aget_embedding([query])<br>        return r[0]<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"<br>        Get embedding for a text using the mixedbread ai API.<br>        Args:<br>            text (str): Text to embed.<br>        Returns:<br>            List[float]: Embedding for the text.<br>        \"\"\"<br>        return self._get_embedding([text])[0]<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"<br>        Asynchronously get embedding for a text using the mixedbread ai API.<br>        Args:<br>            text (str): Text to embed.<br>        Returns:<br>            List[float]: Embedding for the text.<br>        \"\"\"<br>        r = await self._aget_embedding([text])<br>        return r[0]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"<br>        Get embeddings for multiple texts using the mixedbread ai API.<br>        Args:<br>            texts (List[str]): List of texts to embed.<br>        Returns:<br>            List[List[float]]: List of embeddings.<br>        \"\"\"<br>        return self._get_embedding(texts)<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"<br>        Asynchronously get embeddings for multiple texts using the mixedbread ai API.<br>        Args:<br>            texts (List[str]): List of texts to embed.<br>        Returns:<br>            List[List[float]]: List of embeddings.<br>        \"\"\"<br>        return await self._aget_embedding(texts)<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Mixedbreadai - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/mixedbreadai/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/jinaai/#llama_index.embeddings.jinaai.JinaEmbedding)\n\n# Jinaai\n\n## JinaEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/jinaai/\\#llama_index.embeddings.jinaai.JinaEmbedding \"Permanent link\")\n\nBases: `MultiModalEmbedding`\n\nJinaAI class for embeddings.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model` | `str` | Model for embedding.<br>Defaults to `jina-embeddings-v2-base-en` | `'jina-embeddings-v2-base-en'` |\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-jinaai/llama_index/embeddings/jinaai/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>``` | ```<br>class JinaEmbedding(MultiModalEmbedding):<br>    \"\"\"<br>    JinaAI class for embeddings.<br>    Args:<br>        model (str): Model for embedding.<br>            Defaults to `jina-embeddings-v2-base-en`<br>    \"\"\"<br>    api_key: Optional[str] = Field(default=None, description=\"The JinaAI API key.\")<br>    model: str = Field(<br>        default=\"jina-embeddings-v2-base-en\",<br>        description=\"The model to use when calling Jina AI API\",<br>    )<br>    _encoding_queries: str = PrivateAttr()<br>    _encoding_documents: str = PrivateAttr()<br>    _api: Any = PrivateAttr()<br>    def __init__(<br>        self,<br>        model: str = \"jina-embeddings-v2-base-en\",<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        api_key: Optional[str] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        encoding_queries: Optional[str] = None,<br>        encoding_documents: Optional[str] = None,<br>        **kwargs: Any,<br>    ) -> None:<br>        super().__init__(<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            model=model,<br>            api_key=api_key,<br>            **kwargs,<br>        )<br>        self._encoding_queries = encoding_queries or \"float\"<br>        self._encoding_documents = encoding_documents or \"float\"<br>        assert (<br>            self._encoding_documents in VALID_ENCODING<br>        ), f\"Encoding Documents parameter {self._encoding_documents} not supported. Please choose one of {VALID_ENCODING}\"<br>        assert (<br>            self._encoding_queries in VALID_ENCODING<br>        ), f\"Encoding Queries parameter {self._encoding_documents} not supported. Please choose one of {VALID_ENCODING}\"<br>        self._api = _JinaAPICaller(model=model, api_key=api_key)<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"JinaAIEmbedding\"<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self._api.get_embeddings(<br>            input=[query], encoding_type=self._encoding_queries<br>        )[0]<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        result = await self._api.aget_embeddings(<br>            input=[query], encoding_type=self._encoding_queries<br>        )<br>        return result[0]<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._get_text_embeddings([text])[0]<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously get text embedding.\"\"\"<br>        result = await self._aget_text_embeddings([text])<br>        return result[0]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        return self._api.get_embeddings(<br>            input=texts, encoding_type=self._encoding_documents<br>        )<br>    async def _aget_text_embeddings(<br>        self,<br>        texts: List[str],<br>    ) -> List[List[float]]:<br>        return await self._api.aget_embeddings(<br>            input=texts, encoding_type=self._encoding_documents<br>        )<br>    def _get_image_embedding(self, img_file_path: ImageType) -> List[float]:<br>        if is_local(img_file_path):<br>            input = [{\"bytes\": get_bytes_str(img_file_path)}]<br>        else:<br>            input = [{\"url\": img_file_path}]<br>        return self._api.get_embeddings(input=input)[0]<br>    async def _aget_image_embedding(self, img_file_path: ImageType) -> List[float]:<br>        if is_local(img_file_path):<br>            input = [{\"bytes\": get_bytes_str(img_file_path)}]<br>        else:<br>            input = [{\"url\": img_file_path}]<br>        return await self._api.aget_embeddings(input=input)[0]<br>    def _get_image_embeddings(<br>        self, img_file_paths: List[ImageType]<br>    ) -> List[List[float]]:<br>        input = []<br>        for img_file_path in img_file_paths:<br>            if is_local(img_file_path):<br>                input.append({\"bytes\": get_bytes_str(img_file_path)})<br>            else:<br>                input.append({\"url\": img_file_path})<br>        return self._api.get_embeddings(input=input)<br>    async def _aget_image_embeddings(<br>        self, img_file_paths: List[ImageType]<br>    ) -> List[List[float]]:<br>        input = []<br>        for img_file_path in img_file_paths:<br>            if is_local(img_file_path):<br>                input.append({\"bytes\": get_bytes_str(img_file_path)})<br>            else:<br>                input.append({\"url\": img_file_path})<br>        return await self._api.aget_embeddings(input=input)<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Jinaai - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/jinaai/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/metrics/#llama_index.core.evaluation.MRR)\n\n# Metrics\n\nEvaluation modules.\n\n## MRR [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/metrics/\\#llama_index.core.evaluation.MRR \"Permanent link\")\n\nBases: `BaseRetrievalMetric`\n\nMRR (Mean Reciprocal Rank) metric with two calculation options.\n\n- The default method calculates the reciprocal rank of the first relevant retrieved document.\n- The more granular method sums the reciprocal ranks of all relevant retrieved documents and divides by the count of relevant documents.\n\n**Attributes:**\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `metric_name` | `str` | The name of the metric. |\n| `use_granular_mrr` | `bool` | Determines whether to use the granular method for calculation. |\n\nSource code in `llama-index-core/llama_index/core/evaluation/retrieval/metrics.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>``` | ```<br>class MRR(BaseRetrievalMetric):<br>    \"\"\"MRR (Mean Reciprocal Rank) metric with two calculation options.<br>    - The default method calculates the reciprocal rank of the first relevant retrieved document.<br>    - The more granular method sums the reciprocal ranks of all relevant retrieved documents and divides by the count of relevant documents.<br>    Attributes:<br>        metric_name (str): The name of the metric.<br>        use_granular_mrr (bool): Determines whether to use the granular method for calculation.<br>    \"\"\"<br>    metric_name: ClassVar[str] = \"mrr\"<br>    use_granular_mrr: bool = False<br>    def compute(<br>        self,<br>        query: Optional[str] = None,<br>        expected_ids: Optional[List[str]] = None,<br>        retrieved_ids: Optional[List[str]] = None,<br>        expected_texts: Optional[List[str]] = None,<br>        retrieved_texts: Optional[List[str]] = None,<br>        **kwargs: Any,<br>    ) -> RetrievalMetricResult:<br>        \"\"\"Compute MRR based on the provided inputs and selected method.<br>        Parameters:<br>            query (Optional[str]): The query string (not used in the current implementation).<br>            expected_ids (Optional[List[str]]): Expected document IDs.<br>            retrieved_ids (Optional[List[str]]): Retrieved document IDs.<br>            expected_texts (Optional[List[str]]): Expected texts (not used in the current implementation).<br>            retrieved_texts (Optional[List[str]]): Retrieved texts (not used in the current implementation).<br>        Raises:<br>            ValueError: If the necessary IDs are not provided.<br>        Returns:<br>            RetrievalMetricResult: The result with the computed MRR score.<br>        \"\"\"<br>        # Checking for the required arguments<br>        if (<br>            retrieved_ids is None<br>            or expected_ids is None<br>            or not retrieved_ids<br>            or not expected_ids<br>        ):<br>            raise ValueError(\"Retrieved ids and expected ids must be provided\")<br>        if self.use_granular_mrr:<br>            # Granular MRR calculation: All relevant retrieved docs have their reciprocal ranks summed and averaged<br>            expected_set = set(expected_ids)<br>            reciprocal_rank_sum = 0.0<br>            relevant_docs_count = 0<br>            for index, doc_id in enumerate(retrieved_ids):<br>                if doc_id in expected_set:<br>                    relevant_docs_count += 1<br>                    reciprocal_rank_sum += 1.0 / (index + 1)<br>            mrr_score = (<br>                reciprocal_rank_sum / relevant_docs_count<br>                if relevant_docs_count > 0<br>                else 0.0<br>            )<br>        else:<br>            # Default MRR calculation: Reciprocal rank of the first relevant document retrieved<br>            for i, id in enumerate(retrieved_ids):<br>                if id in expected_ids:<br>                    return RetrievalMetricResult(score=1.0 / (i + 1))<br>            mrr_score = 0.0<br>        return RetrievalMetricResult(score=mrr_score)<br>``` |\n\n### compute [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/metrics/\\#llama_index.core.evaluation.MRR.compute \"Permanent link\")\n\n```\ncompute(query: Optional[str] = None, expected_ids: Optional[List[str]] = None, retrieved_ids: Optional[List[str]] = None, expected_texts: Optional[List[str]] = None, retrieved_texts: Optional[List[str]] = None, **kwargs: Any) -> RetrievalMetricResult\n\n```\n\nCompute MRR based on the provided inputs and selected method.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `query` | `Optional[str]` | The query string (not used in the current implementation). | `None` |\n| `expected_ids` | `Optional[List[str]]` | Expected document IDs. | `None` |\n| `retrieved_ids` | `Optional[List[str]]` | Retrieved document IDs. | `None` |\n| `expected_texts` | `Optional[List[str]]` | Expected texts (not used in the current implementation). | `None` |\n| `retrieved_texts` | `Optional[List[str]]` | Retrieved texts (not used in the current implementation). | `None` |\n\n**Raises:**\n\n| Type | Description |\n| --- | --- |\n| `ValueError` | If the necessary IDs are not provided. |\n\n**Returns:**\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `RetrievalMetricResult` | `RetrievalMetricResult` | The result with the computed MRR score. |\n\nSource code in `llama-index-core/llama_index/core/evaluation/retrieval/metrics.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>``` | ```<br>def compute(<br>    self,<br>    query: Optional[str] = None,<br>    expected_ids: Optional[List[str]] = None,<br>    retrieved_ids: Optional[List[str]] = None,<br>    expected_texts: Optional[List[str]] = None,<br>    retrieved_texts: Optional[List[str]] = None,<br>    **kwargs: Any,<br>) -> RetrievalMetricResult:<br>    \"\"\"Compute MRR based on the provided inputs and selected method.<br>    Parameters:<br>        query (Optional[str]): The query string (not used in the current implementation).<br>        expected_ids (Optional[List[str]]): Expected document IDs.<br>        retrieved_ids (Optional[List[str]]): Retrieved document IDs.<br>        expected_texts (Optional[List[str]]): Expected texts (not used in the current implementation).<br>        retrieved_texts (Optional[List[str]]): Retrieved texts (not used in the current implementation).<br>    Raises:<br>        ValueError: If the necessary IDs are not provided.<br>    Returns:<br>        RetrievalMetricResult: The result with the computed MRR score.<br>    \"\"\"<br>    # Checking for the required arguments<br>    if (<br>        retrieved_ids is None<br>        or expected_ids is None<br>        or not retrieved_ids<br>        or not expected_ids<br>    ):<br>        raise ValueError(\"Retrieved ids and expected ids must be provided\")<br>    if self.use_granular_mrr:<br>        # Granular MRR calculation: All relevant retrieved docs have their reciprocal ranks summed and averaged<br>        expected_set = set(expected_ids)<br>        reciprocal_rank_sum = 0.0<br>        relevant_docs_count = 0<br>        for index, doc_id in enumerate(retrieved_ids):<br>            if doc_id in expected_set:<br>                relevant_docs_count += 1<br>                reciprocal_rank_sum += 1.0 / (index + 1)<br>        mrr_score = (<br>            reciprocal_rank_sum / relevant_docs_count<br>            if relevant_docs_count > 0<br>            else 0.0<br>        )<br>    else:<br>        # Default MRR calculation: Reciprocal rank of the first relevant document retrieved<br>        for i, id in enumerate(retrieved_ids):<br>            if id in expected_ids:<br>                return RetrievalMetricResult(score=1.0 / (i + 1))<br>        mrr_score = 0.0<br>    return RetrievalMetricResult(score=mrr_score)<br>``` |\n\n## HitRate [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/metrics/\\#llama_index.core.evaluation.HitRate \"Permanent link\")\n\nBases: `BaseRetrievalMetric`\n\nHit rate metric: Compute hit rate with two calculation options.\n\n- The default method checks for a single match between any of the retrieved docs and expected docs.\n- The more granular method checks for all potential matches between retrieved docs and expected docs.\n\n**Attributes:**\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `metric_name` | `str` | The name of the metric. |\n| `use_granular_hit_rate` | `bool` | Determines whether to use the granular method for calculation. |\n\nSource code in `llama-index-core/llama_index/core/evaluation/retrieval/metrics.py`\n\n|     |     |\n| --- | --- |\n| ```<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>``` | ```<br>class HitRate(BaseRetrievalMetric):<br>    \"\"\"Hit rate metric: Compute hit rate with two calculation options.<br>    - The default method checks for a single match between any of the retrieved docs and expected docs.<br>    - The more granular method checks for all potential matches between retrieved docs and expected docs.<br>    Attributes:<br>        metric_name (str): The name of the metric.<br>        use_granular_hit_rate (bool): Determines whether to use the granular method for calculation.<br>    \"\"\"<br>    metric_name: ClassVar[str] = \"hit_rate\"<br>    use_granular_hit_rate: bool = False<br>    def compute(<br>        self,<br>        query: Optional[str] = None,<br>        expected_ids: Optional[List[str]] = None,<br>        retrieved_ids: Optional[List[str]] = None,<br>        expected_texts: Optional[List[str]] = None,<br>        retrieved_texts: Optional[List[str]] = None,<br>        **kwargs: Any,<br>    ) -> RetrievalMetricResult:<br>        \"\"\"Compute metric based on the provided inputs.<br>        Parameters:<br>            query (Optional[str]): The query string (not used in the current implementation).<br>            expected_ids (Optional[List[str]]): Expected document IDs.<br>            retrieved_ids (Optional[List[str]]): Retrieved document IDs.<br>            expected_texts (Optional[List[str]]): Expected texts (not used in the current implementation).<br>            retrieved_texts (Optional[List[str]]): Retrieved texts (not used in the current implementation).<br>        Raises:<br>            ValueError: If the necessary IDs are not provided.<br>        Returns:<br>            RetrievalMetricResult: The result with the computed hit rate score.<br>        \"\"\"<br>        # Checking for the required arguments<br>        if (<br>            retrieved_ids is None<br>            or expected_ids is None<br>            or not retrieved_ids<br>            or not expected_ids<br>        ):<br>            raise ValueError(\"Retrieved ids and expected ids must be provided\")<br>        if self.use_granular_hit_rate:<br>            # Granular HitRate calculation: Calculate all hits and divide by the number of expected docs<br>            expected_set = set(expected_ids)<br>            hits = sum(1 for doc_id in retrieved_ids if doc_id in expected_set)<br>            score = hits / len(expected_ids) if expected_ids else 0.0<br>        else:<br>            # Default HitRate calculation: Check if there is a single hit<br>            is_hit = any(id in expected_ids for id in retrieved_ids)<br>            score = 1.0 if is_hit else 0.0<br>        return RetrievalMetricResult(score=score)<br>``` |\n\n### compute [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/metrics/\\#llama_index.core.evaluation.HitRate.compute \"Permanent link\")\n\n```\ncompute(query: Optional[str] = None, expected_ids: Optional[List[str]] = None, retrieved_ids: Optional[List[str]] = None, expected_texts: Optional[List[str]] = None, retrieved_texts: Optional[List[str]] = None, **kwargs: Any) -> RetrievalMetricResult\n\n```\n\nCompute metric based on the provided inputs.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `query` | `Optional[str]` | The query string (not used in the current implementation). | `None` |\n| `expected_ids` | `Optional[List[str]]` | Expected document IDs. | `None` |\n| `retrieved_ids` | `Optional[List[str]]` | Retrieved document IDs. | `None` |\n| `expected_texts` | `Optional[List[str]]` | Expected texts (not used in the current implementation). | `None` |\n| `retrieved_texts` | `Optional[List[str]]` | Retrieved texts (not used in the current implementation). | `None` |\n\n**Raises:**\n\n| Type | Description |\n| --- | --- |\n| `ValueError` | If the necessary IDs are not provided. |\n\n**Returns:**\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `RetrievalMetricResult` | `RetrievalMetricResult` | The result with the computed hit rate score. |\n\nSource code in `llama-index-core/llama_index/core/evaluation/retrieval/metrics.py`\n\n|     |     |\n| --- | --- |\n| ```<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>``` | ```<br>def compute(<br>    self,<br>    query: Optional[str] = None,<br>    expected_ids: Optional[List[str]] = None,<br>    retrieved_ids: Optional[List[str]] = None,<br>    expected_texts: Optional[List[str]] = None,<br>    retrieved_texts: Optional[List[str]] = None,<br>    **kwargs: Any,<br>) -> RetrievalMetricResult:<br>    \"\"\"Compute metric based on the provided inputs.<br>    Parameters:<br>        query (Optional[str]): The query string (not used in the current implementation).<br>        expected_ids (Optional[List[str]]): Expected document IDs.<br>        retrieved_ids (Optional[List[str]]): Retrieved document IDs.<br>        expected_texts (Optional[List[str]]): Expected texts (not used in the current implementation).<br>        retrieved_texts (Optional[List[str]]): Retrieved texts (not used in the current implementation).<br>    Raises:<br>        ValueError: If the necessary IDs are not provided.<br>    Returns:<br>        RetrievalMetricResult: The result with the computed hit rate score.<br>    \"\"\"<br>    # Checking for the required arguments<br>    if (<br>        retrieved_ids is None<br>        or expected_ids is None<br>        or not retrieved_ids<br>        or not expected_ids<br>    ):<br>        raise ValueError(\"Retrieved ids and expected ids must be provided\")<br>    if self.use_granular_hit_rate:<br>        # Granular HitRate calculation: Calculate all hits and divide by the number of expected docs<br>        expected_set = set(expected_ids)<br>        hits = sum(1 for doc_id in retrieved_ids if doc_id in expected_set)<br>        score = hits / len(expected_ids) if expected_ids else 0.0<br>    else:<br>        # Default HitRate calculation: Check if there is a single hit<br>        is_hit = any(id in expected_ids for id in retrieved_ids)<br>        score = 1.0 if is_hit else 0.0<br>    return RetrievalMetricResult(score=score)<br>``` |\n\n## RetrievalMetricResult [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/metrics/\\#llama_index.core.evaluation.RetrievalMetricResult \"Permanent link\")\n\nBases: `BaseModel`\n\nMetric result.\n\n**Attributes:**\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `score` | `float` | Score for the metric |\n| `metadata` | `Dict[str, Any]` | Metadata for the metric result |\n\nSource code in `llama-index-core/llama_index/core/evaluation/retrieval/metrics_base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>``` | ```<br>class RetrievalMetricResult(BaseModel):<br>    \"\"\"Metric result.<br>    Attributes:<br>        score (float): Score for the metric<br>        metadata (Dict[str, Any]): Metadata for the metric result<br>    \"\"\"<br>    score: float = Field(..., description=\"Score for the metric\")<br>    metadata: Dict[str, Any] = Field(<br>        default_factory=dict, description=\"Metadata for the metric result\"<br>    )<br>    def __str__(self) -> str:<br>        \"\"\"String representation.\"\"\"<br>        return f\"Score: {self.score}\\nMetadata: {self.metadata}\"<br>    def __float__(self) -> float:<br>        \"\"\"Float representation.\"\"\"<br>        return self.score<br>``` |\n\n## resolve\\_metrics [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/metrics/\\#llama_index.core.evaluation.resolve_metrics \"Permanent link\")\n\n```\nresolve_metrics(metrics: List[str]) -> List[Type[BaseRetrievalMetric]]\n\n```\n\nResolve metrics from list of metric names.\n\nSource code in `llama-index-core/llama_index/core/evaluation/retrieval/metrics.py`\n\n|     |     |\n| --- | --- |\n| ```<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>``` | ```<br>def resolve_metrics(metrics: List[str]) -> List[Type[BaseRetrievalMetric]]:<br>    \"\"\"Resolve metrics from list of metric names.\"\"\"<br>    for metric in metrics:<br>        if metric not in METRIC_REGISTRY:<br>            raise ValueError(f\"Invalid metric name: {metric}\")<br>    return [METRIC_REGISTRY[metric] for metric in metrics]<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Metrics - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/evaluation/metrics/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/huggingface_optimum_intel/#llama_index.embeddings.huggingface_optimum_intel.IntelEmbedding)\n\n# Huggingface optimum intel\n\n## IntelEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/huggingface_optimum_intel/\\#llama_index.embeddings.huggingface_optimum_intel.IntelEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-huggingface-optimum-intel/llama_index/embeddings/huggingface_optimum_intel/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 14<br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>``` | ```<br>class IntelEmbedding(BaseEmbedding):<br>    folder_name: str = Field(description=\"Folder name to load from.\")<br>    max_length: int = Field(description=\"Maximum length of input.\")<br>    pooling: str = Field(description=\"Pooling strategy. One of ['cls', 'mean'].\")<br>    normalize: str = Field(default=True, description=\"Normalize embeddings or not.\")<br>    query_instruction: Optional[str] = Field(<br>        description=\"Instruction to prepend to query text.\"<br>    )<br>    text_instruction: Optional[str] = Field(<br>        description=\"Instruction to prepend to text.\"<br>    )<br>    cache_folder: Optional[str] = Field(<br>        description=\"Cache folder for huggingface files.\"<br>    )<br>    _model: Any = PrivateAttr()<br>    _tokenizer: Any = PrivateAttr()<br>    _device: Any = PrivateAttr()<br>    def __init__(<br>        self,<br>        folder_name: str,<br>        pooling: str = \"cls\",<br>        max_length: Optional[int] = None,<br>        normalize: bool = True,<br>        query_instruction: Optional[str] = None,<br>        text_instruction: Optional[str] = None,<br>        model: Optional[Any] = None,<br>        tokenizer: Optional[Any] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>        device: Optional[str] = None,<br>    ):<br>        try:<br>            from optimum.intel import IPEXModel<br>        except ImportError:<br>            raise ImportError(<br>                \"Optimum-Intel requires the following dependencies; please install with \"<br>                \"`pip install optimum[exporters] \"<br>                \"optimum-intel neural-compressor intel_extension_for_pytorch`\"<br>            )<br>        model = model or IPEXModel.from_pretrained(folder_name)<br>        tokenizer = tokenizer or AutoTokenizer.from_pretrained(folder_name)<br>        device = device or infer_torch_device()<br>        if max_length is None:<br>            try:<br>                max_length = int(model.config.max_position_embeddings)<br>            except Exception:<br>                raise ValueError(<br>                    \"Unable to find max_length from model config. \"<br>                    \"Please provide max_length.\"<br>                )<br>            try:<br>                max_length = min(max_length, int(tokenizer.model_max_length))<br>            except Exception as exc:<br>                print(f\"An error occurred while retrieving tokenizer max length: {exc}\")<br>        if pooling not in [\"cls\", \"mean\"]:<br>            raise ValueError(f\"Pooling {pooling} not supported.\")<br>        super().__init__(<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            folder_name=folder_name,<br>            max_length=max_length,<br>            pooling=pooling,<br>            normalize=normalize,<br>            query_instruction=query_instruction,<br>            text_instruction=text_instruction,<br>        )<br>        self._model = model<br>        self._tokenizer = tokenizer<br>        self._device = device<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"IntelEmbedding\"<br>    def _mean_pooling(self, model_output: Any, attention_mask: Any) -> Any:<br>        \"\"\"Mean Pooling - Take attention mask into account for correct averaging.\"\"\"<br>        import torch<br>        # First element of model_output contains all token embeddings<br>        token_embeddings = model_output[0]<br>        input_mask_expanded = (<br>            attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()<br>        )<br>        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(<br>            input_mask_expanded.sum(1), min=1e-9<br>        )<br>    def _cls_pooling(self, model_output: list) -> Any:<br>        \"\"\"Use the CLS token as the pooling token.\"\"\"<br>        if isinstance(model_output, dict):<br>            token_embeddings = model_output[\"last_hidden_state\"]<br>        else:<br>            token_embeddings = model_output[0]<br>        return token_embeddings[:, 0]<br>    def _embed(self, sentences: List[str]) -> List[List[float]]:<br>        \"\"\"Embed sentences.\"\"\"<br>        encoded_input = self._tokenizer(<br>            sentences,<br>            padding=True,<br>            max_length=self.max_length,<br>            truncation=True,<br>            return_tensors=\"pt\",<br>        )<br>        import torch<br>        with torch.inference_mode(), torch.cpu.amp.autocast():<br>            model_output = self._model(**encoded_input)<br>        if self.pooling == \"cls\":<br>            embeddings = self._cls_pooling(model_output)<br>        else:<br>            embeddings = self._mean_pooling(<br>                model_output, encoded_input[\"attention_mask\"].to(self._device)<br>            )<br>        if self.normalize:<br>            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)<br>        return embeddings.tolist()<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        query = format_query(query, self.model_name, self.query_instruction)<br>        return self._embed([query])[0]<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding async.\"\"\"<br>        return self._get_query_embedding(query)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding async.\"\"\"<br>        return self._get_text_embedding(text)<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        text = format_text(text, self.model_name, self.text_instruction)<br>        return self._embed([text])[0]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        texts = [<br>            format_text(text, self.model_name, self.text_instruction) for text in texts<br>        ]<br>        return self._embed(texts)<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Huggingface optimum intel - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/huggingface_optimum_intel/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/gemini/#llama_index.embeddings.gemini.GeminiEmbedding)\n\n# Gemini\n\n## GeminiEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/gemini/\\#llama_index.embeddings.gemini.GeminiEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nGoogle Gemini embeddings.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model_name` | `str` | Model for embedding.<br>Defaults to \"models/embedding-001\". | `'models/embedding-001'` |\n| `api_key` | `Optional[str]` | API key to access the model. Defaults to None. | `None` |\n| `api_base` | `Optional[str]` | API base to access the model. Defaults to Official Base. | `None` |\n| `transport` | `Optional[str]` | Transport to access the model. | `None` |\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-gemini/llama_index/embeddings/gemini/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>``` | ```<br>class GeminiEmbedding(BaseEmbedding):<br>    \"\"\"Google Gemini embeddings.<br>    Args:<br>        model_name (str): Model for embedding.<br>            Defaults to \"models/embedding-001\".<br>        api_key (Optional[str]): API key to access the model. Defaults to None.<br>        api_base (Optional[str]): API base to access the model. Defaults to Official Base.<br>        transport (Optional[str]): Transport to access the model.<br>    \"\"\"<br>    _model: Any = PrivateAttr()<br>    title: Optional[str] = Field(<br>        default=\"\",<br>        description=\"Title is only applicable for retrieval_document tasks, and is used to represent a document title. For other tasks, title is invalid.\",<br>    )<br>    task_type: Optional[str] = Field(<br>        default=\"retrieval_document\",<br>        description=\"The task for embedding model.\",<br>    )<br>    api_key: Optional[str] = Field(<br>        default=None,<br>        description=\"API key to access the model. Defaults to None.\",<br>    )<br>    def __init__(<br>        self,<br>        model_name: str = \"models/embedding-001\",<br>        task_type: Optional[str] = \"retrieval_document\",<br>        api_key: Optional[str] = None,<br>        api_base: Optional[str] = None,<br>        transport: Optional[str] = None,<br>        title: Optional[str] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>        **kwargs: Any,<br>    ):<br>        # API keys are optional. The API can be authorised via OAuth (detected<br>        # environmentally) or by the GOOGLE_API_KEY environment variable.<br>        config_params: Dict[str, Any] = {<br>            \"api_key\": api_key or os.getenv(\"GOOGLE_API_KEY\"),<br>        }<br>        if api_base:<br>            config_params[\"client_options\"] = {\"api_endpoint\": api_base}<br>        if transport:<br>            config_params[\"transport\"] = transport<br>        # transport: A string, one of: [`rest`, `grpc`, `grpc_asyncio`].<br>        super().__init__(<br>            api_key=api_key,<br>            model_name=model_name,<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            title=title,<br>            task_type=task_type,<br>            **kwargs,<br>        )<br>        gemini.configure(**config_params)<br>        self._model = gemini<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"GeminiEmbedding\"<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self._model.embed_content(<br>            model=self.model_name,<br>            content=query,<br>            title=self.title,<br>            task_type=self.task_type,<br>        )[\"embedding\"]<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._model.embed_content(<br>            model=self.model_name,<br>            content=text,<br>            title=self.title,<br>            task_type=self.task_type,<br>        )[\"embedding\"]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        return [<br>            self._model.embed_content(<br>                model=self.model_name,<br>                content=text,<br>                title=self.title,<br>                task_type=self.task_type,<br>            )[\"embedding\"]<br>            for text in texts<br>        ]<br>    ### Async methods ###<br>    # need to wait async calls from Gemini side to be implemented.<br>    # Issue: https://github.com/google/generative-ai-python/issues/125<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        return self._get_query_embedding(query)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously get text embedding.\"\"\"<br>        return self._get_text_embedding(text)<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Asynchronously get text embeddings.\"\"\"<br>        return self._get_text_embeddings(texts)<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Gemini - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/gemini/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/semantic_similarity/#llama_index.core.evaluation.SemanticSimilarityEvaluator)\n\n# Semantic similarity\n\nEvaluation modules.\n\n## SemanticSimilarityEvaluator [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/semantic_similarity/\\#llama_index.core.evaluation.SemanticSimilarityEvaluator \"Permanent link\")\n\nBases: `BaseEvaluator`\n\nEmbedding similarity evaluator.\n\nEvaluate the quality of a question answering system by\ncomparing the similarity between embeddings of the generated answer\nand the reference answer.\n\nInspired by this paper:\n\\- Semantic Answer Similarity for Evaluating Question Answering Models\nhttps://arxiv.org/pdf/2108.06130.pdf\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `similarity_threshold` | `float` | Embedding similarity threshold for \"passing\".<br>Defaults to 0.8. | `0.8` |\n\nSource code in `llama-index-core/llama_index/core/evaluation/semantic_similarity.py`\n\n|     |     |\n| --- | --- |\n| ```<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>``` | ```<br>class SemanticSimilarityEvaluator(BaseEvaluator):<br>    \"\"\"Embedding similarity evaluator.<br>    Evaluate the quality of a question answering system by<br>    comparing the similarity between embeddings of the generated answer<br>    and the reference answer.<br>    Inspired by this paper:<br>    - Semantic Answer Similarity for Evaluating Question Answering Models<br>        https://arxiv.org/pdf/2108.06130.pdf<br>    Args:<br>        similarity_threshold (float): Embedding similarity threshold for \"passing\".<br>            Defaults to 0.8.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        embed_model: Optional[BaseEmbedding] = None,<br>        similarity_fn: Optional[Callable[..., float]] = None,<br>        similarity_mode: Optional[SimilarityMode] = None,<br>        similarity_threshold: float = 0.8,<br>    ) -> None:<br>        self._embed_model = embed_model or Settings.embed_model<br>        if similarity_fn is None:<br>            similarity_mode = similarity_mode or SimilarityMode.DEFAULT<br>            self._similarity_fn = lambda x, y: similarity(x, y, mode=similarity_mode)<br>        else:<br>            if similarity_mode is not None:<br>                raise ValueError(<br>                    \"Cannot specify both similarity_fn and similarity_mode\"<br>                )<br>            self._similarity_fn = similarity_fn<br>        self._similarity_threshold = similarity_threshold<br>    def _get_prompts(self) -> PromptDictType:<br>        \"\"\"Get prompts.\"\"\"<br>        return {}<br>    def _update_prompts(self, prompts: PromptDictType) -> None:<br>        \"\"\"Update prompts.\"\"\"<br>    async def aevaluate(<br>        self,<br>        query: Optional[str] = None,<br>        response: Optional[str] = None,<br>        contexts: Optional[Sequence[str]] = None,<br>        reference: Optional[str] = None,<br>        **kwargs: Any,<br>    ) -> EvaluationResult:<br>        del query, contexts, kwargs  # Unused<br>        if response is None or reference is None:<br>            raise ValueError(\"Must specify both response and reference\")<br>        response_embedding = await self._embed_model.aget_text_embedding(response)<br>        reference_embedding = await self._embed_model.aget_text_embedding(reference)<br>        similarity_score = self._similarity_fn(response_embedding, reference_embedding)<br>        passing = similarity_score >= self._similarity_threshold<br>        return EvaluationResult(<br>            score=similarity_score,<br>            passing=passing,<br>            feedback=f\"Similarity score: {similarity_score}\",<br>        )<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Semantic similarity - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/evaluation/semantic_similarity/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/llamafile/#llama_index.embeddings.llamafile.LlamafileEmbedding)\n\n# Llamafile\n\n## LlamafileEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/llamafile/\\#llama_index.embeddings.llamafile.LlamafileEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nClass for llamafile embeddings.\n\nllamafile lets you distribute and run large language models with a\nsingle file.\n\nTo get started, see: https://github.com/Mozilla-Ocho/llamafile\n\nTo use this class, you will need to first:\n\n1. Download a llamafile.\n2. Make the downloaded file executable: `chmod +x path/to/model.llamafile`\n3. Start the llamafile in server mode with embeddings enabled:\n\n`./path/to/model.llamafile --server --nobrowser --embedding`\n\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-llamafile/llama_index/embeddings/llamafile/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 13<br> 14<br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>``` | ```<br>class LlamafileEmbedding(BaseEmbedding):<br>    \"\"\"Class for llamafile embeddings.<br>    llamafile lets you distribute and run large language models with a<br>    single file.<br>    To get started, see: https://github.com/Mozilla-Ocho/llamafile<br>    To use this class, you will need to first:<br>    1. Download a llamafile.<br>    2. Make the downloaded file executable: `chmod +x path/to/model.llamafile`<br>    3. Start the llamafile in server mode with embeddings enabled:<br>        `./path/to/model.llamafile --server --nobrowser --embedding`<br>    \"\"\"<br>    base_url: str = Field(<br>        description=\"base url of the llamafile server\", default=\"http://localhost:8080\"<br>    )<br>    request_timeout: float = Field(<br>        default=DEFAULT_REQUEST_TIMEOUT,<br>        description=\"The timeout for making http request to llamafile API server\",<br>    )<br>    def __init__(<br>        self,<br>        base_url: str = \"http://localhost:8080\",<br>        callback_manager: Optional[CallbackManager] = None,<br>        **kwargs,<br>    ) -> None:<br>        super().__init__(<br>            base_url=base_url,<br>            callback_manager=callback_manager or CallbackManager([]),<br>            **kwargs,<br>        )<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"LlamafileEmbedding\"<br>    def _get_query_embedding(self, query: str) -> Embedding:<br>        return self._get_text_embedding(query)<br>    async def _aget_query_embedding(self, query: str) -> Embedding:<br>        return await self._aget_text_embedding(query)<br>    def _get_text_embedding(self, text: str) -> Embedding:<br>        \"\"\"<br>        Embed the input text synchronously.<br>        \"\"\"<br>        request_body = {<br>            \"content\": text,<br>        }<br>        with httpx.Client(timeout=Timeout(self.request_timeout)) as client:<br>            response = client.post(<br>                url=f\"{self.base_url}/embedding\",<br>                headers={\"Content-Type\": \"application/json\"},<br>                json=request_body,<br>            )<br>            response.encoding = \"utf-8\"<br>            response.raise_for_status()<br>            return response.json()[\"embedding\"]<br>    async def _aget_text_embedding(self, text: str) -> Embedding:<br>        \"\"\"<br>        Embed the input text asynchronously.<br>        \"\"\"<br>        request_body = {<br>            \"content\": text,<br>        }<br>        async with httpx.AsyncClient(timeout=Timeout(self.request_timeout)) as client:<br>            response = await client.post(<br>                url=f\"{self.base_url}/embedding\",<br>                headers={\"Content-Type\": \"application/json\"},<br>                json=request_body,<br>            )<br>            response.encoding = \"utf-8\"<br>            response.raise_for_status()<br>            return response.json()[\"embedding\"]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[Embedding]:<br>        \"\"\"<br>        Embed the input texts synchronously.<br>        \"\"\"<br>        request_body = {<br>            \"content\": texts,<br>        }<br>        with httpx.Client(timeout=Timeout(self.request_timeout)) as client:<br>            response = client.post(<br>                url=f\"{self.base_url}/embedding\",<br>                headers={\"Content-Type\": \"application/json\"},<br>                json=request_body,<br>            )<br>            response.encoding = \"utf-8\"<br>            response.raise_for_status()<br>            return [output[\"embedding\"] for output in response.json()[\"results\"]]<br>    async def _aget_text_embeddings(self, texts: List[str]) -> Embedding:<br>        \"\"\"<br>        Embed the input text asynchronously.<br>        \"\"\"<br>        request_body = {<br>            \"content\": texts,<br>        }<br>        async with httpx.AsyncClient(timeout=Timeout(self.request_timeout)) as client:<br>            response = await client.post(<br>                url=f\"{self.base_url}/embedding\",<br>                headers={\"Content-Type\": \"application/json\"},<br>                json=request_body,<br>            )<br>            response.encoding = \"utf-8\"<br>            response.raise_for_status()<br>            return [output[\"embedding\"] for output in response.json()[\"results\"]]<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Llamafile - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/llamafile/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/query_response/#llama_index.core.evaluation.QueryResponseEvaluator)\n\n# Query response\n\nEvaluation modules.\n\n## QueryResponseEvaluator`module-attribute`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/query_response/\\#llama_index.core.evaluation.QueryResponseEvaluator \"Permanent link\")\n\n```\nQueryResponseEvaluator = RelevancyEvaluator\n\n```\n\n## RelevancyEvaluator [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/query_response/\\#llama_index.core.evaluation.RelevancyEvaluator \"Permanent link\")\n\nBases: `BaseEvaluator`\n\nRelenvancy evaluator.\n\nEvaluates the relevancy of retrieved contexts and response to a query.\nThis evaluator considers the query string, retrieved contexts, and response string.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `raise_error(Optional[bool])` |  | Whether to raise an error if the response is invalid.<br>Defaults to False. | _required_ |\n| `eval_template(Optional[Union[str,` | `BasePromptTemplate]]` | The template to use for evaluation. | _required_ |\n| `refine_template(Optional[Union[str,` | `BasePromptTemplate]]` | The template to use for refinement. | _required_ |\n\nSource code in `llama-index-core/llama_index/core/evaluation/relevancy.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>``` | ```<br>class RelevancyEvaluator(BaseEvaluator):<br>    \"\"\"Relenvancy evaluator.<br>    Evaluates the relevancy of retrieved contexts and response to a query.<br>    This evaluator considers the query string, retrieved contexts, and response string.<br>    Args:<br>        raise_error(Optional[bool]):<br>            Whether to raise an error if the response is invalid.<br>            Defaults to False.<br>        eval_template(Optional[Union[str, BasePromptTemplate]]):<br>            The template to use for evaluation.<br>        refine_template(Optional[Union[str, BasePromptTemplate]]):<br>            The template to use for refinement.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        llm: Optional[LLM] = None,<br>        raise_error: bool = False,<br>        eval_template: Optional[Union[str, BasePromptTemplate]] = None,<br>        refine_template: Optional[Union[str, BasePromptTemplate]] = None,<br>    ) -> None:<br>        \"\"\"Init params.\"\"\"<br>        self._llm = llm or Settings.llm<br>        self._raise_error = raise_error<br>        self._eval_template: BasePromptTemplate<br>        if isinstance(eval_template, str):<br>            self._eval_template = PromptTemplate(eval_template)<br>        else:<br>            self._eval_template = eval_template or DEFAULT_EVAL_TEMPLATE<br>        self._refine_template: BasePromptTemplate<br>        if isinstance(refine_template, str):<br>            self._refine_template = PromptTemplate(refine_template)<br>        else:<br>            self._refine_template = refine_template or DEFAULT_REFINE_TEMPLATE<br>    def _get_prompts(self) -> PromptDictType:<br>        \"\"\"Get prompts.\"\"\"<br>        return {<br>            \"eval_template\": self._eval_template,<br>            \"refine_template\": self._refine_template,<br>        }<br>    def _update_prompts(self, prompts: PromptDictType) -> None:<br>        \"\"\"Update prompts.\"\"\"<br>        if \"eval_template\" in prompts:<br>            self._eval_template = prompts[\"eval_template\"]<br>        if \"refine_template\" in prompts:<br>            self._refine_template = prompts[\"refine_template\"]<br>    async def aevaluate(<br>        self,<br>        query: str | None = None,<br>        response: str | None = None,<br>        contexts: Sequence[str] | None = None,<br>        sleep_time_in_seconds: int = 0,<br>        **kwargs: Any,<br>    ) -> EvaluationResult:<br>        \"\"\"Evaluate whether the contexts and response are relevant to the query.\"\"\"<br>        del kwargs  # Unused<br>        if query is None or contexts is None or response is None:<br>            raise ValueError(\"query, contexts, and response must be provided\")<br>        docs = [Document(text=context) for context in contexts]<br>        index = SummaryIndex.from_documents(docs)<br>        query_response = f\"Question: {query}\\nResponse: {response}\"<br>        await asyncio.sleep(sleep_time_in_seconds)<br>        query_engine = index.as_query_engine(<br>            llm=self._llm,<br>            text_qa_template=self._eval_template,<br>            refine_template=self._refine_template,<br>        )<br>        response_obj = await query_engine.aquery(query_response)<br>        raw_response_txt = str(response_obj)<br>        if \"yes\" in raw_response_txt.lower():<br>            passing = True<br>        else:<br>            if self._raise_error:<br>                raise ValueError(\"The response is invalid\")<br>            passing = False<br>        return EvaluationResult(<br>            query=query,<br>            response=response,<br>            passing=passing,<br>            score=1.0 if passing else 0.0,<br>            feedback=raw_response_txt,<br>            contexts=contexts,<br>        )<br>``` |\n\n### aevaluate`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/query_response/\\#llama_index.core.evaluation.RelevancyEvaluator.aevaluate \"Permanent link\")\n\n```\naevaluate(query: str | None = None, response: str | None = None, contexts: Sequence[str] | None = None, sleep_time_in_seconds: int = 0, **kwargs: Any) -> EvaluationResult\n\n```\n\nEvaluate whether the contexts and response are relevant to the query.\n\nSource code in `llama-index-core/llama_index/core/evaluation/relevancy.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>``` | ```<br>async def aevaluate(<br>    self,<br>    query: str | None = None,<br>    response: str | None = None,<br>    contexts: Sequence[str] | None = None,<br>    sleep_time_in_seconds: int = 0,<br>    **kwargs: Any,<br>) -> EvaluationResult:<br>    \"\"\"Evaluate whether the contexts and response are relevant to the query.\"\"\"<br>    del kwargs  # Unused<br>    if query is None or contexts is None or response is None:<br>        raise ValueError(\"query, contexts, and response must be provided\")<br>    docs = [Document(text=context) for context in contexts]<br>    index = SummaryIndex.from_documents(docs)<br>    query_response = f\"Question: {query}\\nResponse: {response}\"<br>    await asyncio.sleep(sleep_time_in_seconds)<br>    query_engine = index.as_query_engine(<br>        llm=self._llm,<br>        text_qa_template=self._eval_template,<br>        refine_template=self._refine_template,<br>    )<br>    response_obj = await query_engine.aquery(query_response)<br>    raw_response_txt = str(response_obj)<br>    if \"yes\" in raw_response_txt.lower():<br>        passing = True<br>    else:<br>        if self._raise_error:<br>            raise ValueError(\"The response is invalid\")<br>        passing = False<br>    return EvaluationResult(<br>        query=query,<br>        response=response,<br>        passing=passing,<br>        score=1.0 if passing else 0.0,<br>        feedback=raw_response_txt,<br>        contexts=contexts,<br>    )<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Query response - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/evaluation/query_response/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/huggingface_openvino/#llama_index.embeddings.huggingface_openvino.OpenVINOEmbedding)\n\n# Huggingface openvino\n\n## OpenVINOEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/huggingface_openvino/\\#llama_index.embeddings.huggingface_openvino.OpenVINOEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-huggingface-openvino/llama_index/embeddings/huggingface_openvino/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>``` | ```<br>class OpenVINOEmbedding(BaseEmbedding):<br>    model_id_or_path: str = Field(description=\"Huggingface model id or local path.\")<br>    max_length: int = Field(description=\"Maximum length of input.\")<br>    pooling: str = Field(description=\"Pooling strategy. One of ['cls', 'mean'].\")<br>    normalize: bool = Field(default=True, description=\"Normalize embeddings or not.\")<br>    query_instruction: Optional[str] = Field(<br>        description=\"Instruction to prepend to query text.\"<br>    )<br>    text_instruction: Optional[str] = Field(<br>        description=\"Instruction to prepend to text.\"<br>    )<br>    cache_folder: Optional[str] = Field(<br>        description=\"Cache folder for huggingface files.\", default=None<br>    )<br>    _model: Any = PrivateAttr()<br>    _tokenizer: Any = PrivateAttr()<br>    _device: Any = PrivateAttr()<br>    def __init__(<br>        self,<br>        model_id_or_path: str = \"BAAI/bge-m3\",<br>        pooling: str = \"cls\",<br>        max_length: Optional[int] = None,<br>        normalize: bool = True,<br>        query_instruction: Optional[str] = None,<br>        text_instruction: Optional[str] = None,<br>        model: Optional[Any] = None,<br>        tokenizer: Optional[Any] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>        model_kwargs: Dict[str, Any] = {},<br>        device: Optional[str] = \"auto\",<br>    ):<br>        try:<br>            from huggingface_hub import HfApi<br>        except ImportError as e:<br>            raise ValueError(<br>                \"Could not import huggingface_hub python package. \"<br>                \"Please install it with: \"<br>                \"`pip install -U huggingface_hub`.\"<br>            ) from e<br>        def require_model_export(<br>            model_id: str, revision: Any = None, subfolder: Any = None<br>        ) -> bool:<br>            model_dir = Path(model_id)<br>            if subfolder is not None:<br>                model_dir = model_dir / subfolder<br>            if model_dir.is_dir():<br>                return (<br>                    not (model_dir / \"openvino_model.xml\").exists()<br>                    or not (model_dir / \"openvino_model.bin\").exists()<br>                )<br>            hf_api = HfApi()<br>            try:<br>                model_info = hf_api.model_info(model_id, revision=revision or \"main\")<br>                normalized_subfolder = (<br>                    None if subfolder is None else Path(subfolder).as_posix()<br>                )<br>                model_files = [<br>                    file.rfilename<br>                    for file in model_info.siblings<br>                    if normalized_subfolder is None<br>                    or file.rfilename.startswith(normalized_subfolder)<br>                ]<br>                ov_model_path = (<br>                    \"openvino_model.xml\"<br>                    if subfolder is None<br>                    else f\"{normalized_subfolder}/openvino_model.xml\"<br>                )<br>                return (<br>                    ov_model_path not in model_files<br>                    or ov_model_path.replace(\".xml\", \".bin\") not in model_files<br>                )<br>            except Exception:<br>                return True<br>        if require_model_export(model_id_or_path):<br>            # use remote model<br>            model = model or OVModelForFeatureExtraction.from_pretrained(<br>                model_id_or_path, export=True, device=device, **model_kwargs<br>            )<br>        else:<br>            # use local model<br>            model = model or OVModelForFeatureExtraction.from_pretrained(<br>                model_id_or_path, device=device, **model_kwargs<br>            )<br>        tokenizer = tokenizer or AutoTokenizer.from_pretrained(model_id_or_path)<br>        if max_length is None:<br>            try:<br>                max_length = int(model.config.max_position_embeddings)<br>            except Exception:<br>                raise ValueError(<br>                    \"Unable to find max_length from model config. \"<br>                    \"Please provide max_length.\"<br>                )<br>            try:<br>                max_length = min(max_length, int(tokenizer.model_max_length))<br>            except Exception as exc:<br>                print(f\"An error occurred while retrieving tokenizer max length: {exc}\")<br>        if pooling not in [\"cls\", \"mean\"]:<br>            raise ValueError(f\"Pooling {pooling} not supported.\")<br>        super().__init__(<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager or CallbackManager([]),<br>            model_id_or_path=model_id_or_path,<br>            max_length=max_length,<br>            pooling=pooling,<br>            normalize=normalize,<br>            query_instruction=query_instruction,<br>            text_instruction=text_instruction,<br>        )<br>        self._device = device<br>        self._model = model<br>        self._tokenizer = tokenizer<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"OpenVINOEmbedding\"<br>    @staticmethod<br>    def create_and_save_openvino_model(<br>        model_name_or_path: str,<br>        output_path: str,<br>        export_kwargs: Optional[dict] = None,<br>    ) -> None:<br>        try:<br>            from optimum.intel.openvino import OVModelForFeatureExtraction<br>            from transformers import AutoTokenizer<br>        except ImportError:<br>            raise ImportError(<br>                \"OptimumEmbedding requires transformers to be installed.\\n\"<br>                \"Please install transformers with \"<br>                \"`pip install transformers optimum[openvino]`.\"<br>            )<br>        export_kwargs = export_kwargs or {}<br>        model = OVModelForFeatureExtraction.from_pretrained(<br>            model_name_or_path, export=True, compile=False, **export_kwargs<br>        )<br>        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)<br>        model.save_pretrained(output_path)<br>        tokenizer.save_pretrained(output_path)<br>        print(<br>            f\"Saved OpenVINO model to {output_path}. Use it with \"<br>            f\"`embed_model = OpenVINOEmbedding(folder_name='{output_path}')`.\"<br>        )<br>    def _mean_pooling(self, model_output: Any, attention_mask: Any) -> Any:<br>        \"\"\"Mean Pooling - Take attention mask into account for correct averaging.\"\"\"<br>        import torch<br>        # First element of model_output contains all token embeddings<br>        token_embeddings = model_output[0]<br>        input_mask_expanded = (<br>            attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()<br>        )<br>        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(<br>            input_mask_expanded.sum(1), min=1e-9<br>        )<br>    def _cls_pooling(self, model_output: list) -> Any:<br>        \"\"\"Use the CLS token as the pooling token.\"\"\"<br>        return model_output[0][:, 0]<br>    def _embed(self, sentences: List[str]) -> List[List[float]]:<br>        \"\"\"Embed sentences.\"\"\"<br>        length = self._model.request.inputs[0].get_partial_shape()[1]<br>        if length.is_dynamic:<br>            encoded_input = self._tokenizer(<br>                sentences,<br>                padding=True,<br>                max_length=self.max_length,<br>                truncation=True,<br>                return_tensors=\"pt\",<br>            )<br>        else:<br>            encoded_input = self._tokenizer(<br>                sentences,<br>                padding=\"max_length\",<br>                max_length=length.get_length(),<br>                truncation=True,<br>                return_tensors=\"pt\",<br>            )<br>        model_output = self._model(**encoded_input)<br>        if self.pooling == \"cls\":<br>            embeddings = self._cls_pooling(model_output)<br>        else:<br>            embeddings = self._mean_pooling(<br>                model_output, encoded_input[\"attention_mask\"]<br>            )<br>        if self.normalize:<br>            import torch<br>            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)<br>        return embeddings.tolist()<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        query = format_query(query, self.model_name, self.query_instruction)<br>        return self._embed([query])[0]<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding async.\"\"\"<br>        return self._get_query_embedding(query)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding async.\"\"\"<br>        return self._get_text_embedding(text)<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        text = format_text(text, self.model_name, self.text_instruction)<br>        return self._embed([text])[0]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        texts = [<br>            format_text(text, self.model_name, self.text_instruction) for text in texts<br>        ]<br>        return self._embed(texts)<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Huggingface openvino - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/huggingface_openvino/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/yandexgpt/#llama_index.embeddings.yandexgpt.YandexGPTEmbedding)\n\n# Yandexgpt\n\n## YandexGPTEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/yandexgpt/\\#llama_index.embeddings.yandexgpt.YandexGPTEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nA class representation for generating embeddings using the Yandex Cloud API.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `api_key` | `Optional[str]` | An API key for Yandex Cloud. | `None` |\n| `model_name` | `str` | The name of the model to be used for generating embeddings.<br>The class ensures that this model is supported. Defaults to \"general:embedding\". | `'general:embedding'` |\n| `embed_batch_size` | `int` | The batch size for embedding. Defaults to DEFAULT\\_EMBED\\_BATCH\\_SIZE. | `DEFAULT_EMBED_BATCH_SIZE` |\n| `callback_manager` | `Optional[CallbackManager]` | Callback manager for hooks. | `None` |\n\nExample\n\n. code-block:: python\n\n```\nfrom llama_index.embeddings.yandexgpt import YandexGPTEmbedding\n\nembeddings = YandexGPTEmbedding(\n    api_key=\"your-api-key\",\n    folder_id=\"your-folder-id\",\n)\n\n```\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-yandexgpt/llama_index/embeddings/yandexgpt/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>``` | ```<br>class YandexGPTEmbedding(BaseEmbedding):<br>    \"\"\"<br>    A class representation for generating embeddings using the Yandex Cloud API.<br>    Args:<br>      api_key (Optional[str]): An API key for Yandex Cloud.<br>      model_name (str): The name of the model to be used for generating embeddings.<br>                         The class ensures that this model is supported. Defaults to \"general:embedding\".<br>      embed_batch_size (int): The batch size for embedding. Defaults to DEFAULT_EMBED_BATCH_SIZE.<br>      callback_manager (Optional[CallbackManager]): Callback manager for hooks.<br>    Example:<br>        . code-block:: python<br>            from llama_index.embeddings.yandexgpt import YandexGPTEmbedding<br>            embeddings = YandexGPTEmbedding(<br>                api_key=\"your-api-key\",<br>                folder_id=\"your-folder-id\",<br>            )<br>    \"\"\"<br>    api_key: str = Field(description=\"The YandexGPT API key.\")<br>    folder_id: str = Field(description=\"The folder id for YandexGPT API.\")<br>    retries: int = 6<br>    sleep_interval: float = 0.1<br>    def __init__(<br>        self,<br>        api_key: Optional[str] = None,<br>        folder_id: Optional[str] = None,<br>        model_name: str = \"general:embedding\",<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>        **kwargs: Any,<br>    ) -> None:<br>        if not api_key:<br>            raise ValueError(<br>                \"You must provide an API key or IAM token to use YandexGPT. \"<br>                \"You can either pass it in as an argument or set it `YANDEXGPT_API_KEY`.\"<br>            )<br>        if not folder_id:<br>            raise ValueError(<br>                \"You must provide catalog_id to use YandexGPT. \"<br>                \"You can either pass it in as an argument or set it `YANDEXGPT_CATALOG_ID`.\"<br>            )<br>        api_key = get_from_param_or_env(\"api_key\", api_key, \"YANDEXGPT_KEY\")<br>        folder_id = get_from_param_or_env(<br>            \"folder_id\", folder_id, \"YANDEXGPT_CATALOG_ID\"<br>        )<br>        super().__init__(<br>            model_name=model_name,<br>            api_key=api_key,<br>            folder_id=folder_id,<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            **kwargs,<br>        )<br>    def _getModelUri(self, is_document: bool = False) -> str:<br>        \"\"\"Construct the model URI based on whether the text is a document or a query.\"\"\"<br>        return f\"emb://{self.folder_id}/text-search-{'doc' if is_document else 'query'}/latest\"<br>    @classmethod<br>    def class_name(cls) -> str:<br>        \"\"\"Return the class name.\"\"\"<br>        return \"YandexGPTEmbedding\"<br>    def _embed(self, text: str, is_document: bool = False) -> List[float]:<br>        \"\"\"<br>        Embeds text using the YandexGPT Cloud API synchronously.<br>        Args:<br>          text: The text to embed.<br>          is_document: Whether the text is a document (True) or a query (False).<br>        Returns:<br>          A list of floats representing the embedding.<br>        Raises:<br>          YException: If an error occurs during embedding.<br>        \"\"\"<br>        payload = {\"modelUri\": self._getModelUri(is_document), \"text\": text}<br>        header = {<br>            \"Content-Type\": \"application/json\",<br>            \"Authorization\": f\"Api-Key {self.api_key}\",<br>            \"x-data-logging-enabled\": \"false\",<br>        }<br>        try:<br>            for attempt in Retrying(<br>                stop=stop_after_attempt(self.retries),<br>                wait=wait_fixed(self.sleep_interval),<br>            ):<br>                with attempt:<br>                    response = requests.post(<br>                        DEFAULT_YANDEXGPT_API_BASE, json=payload, headers=header<br>                    )<br>                    response = response.json()<br>                    if \"embedding\" in response:<br>                        return response[\"embedding\"]<br>                    raise YException(f\"No embedding found, result returned: {response}\")<br>        except RetryError:<br>            raise YException(<br>                f\"Error computing embeddings after {self.retries} retries. Result returned:\\n{response}\"<br>            )<br>    async def _aembed(self, text: str, is_document: bool = False) -> List[float]:<br>        \"\"\"<br>        Embeds text using the YandexGPT Cloud API asynchronously.<br>        Args:<br>          text: The text to embed.<br>          is_document: Whether the text is a document (True) or a query (False).<br>        Returns:<br>          A list of floats representing the embedding.<br>        Raises:<br>          YException: If an error occurs during embedding.<br>        \"\"\"<br>        payload = {\"modelUri\": self._getModelUri(is_document), \"text\": text}<br>        header = {<br>            \"Content-Type\": \"application/json\",<br>            \"Authorization\": f\"Api-Key {self.api_key}\",<br>            \"x-data-logging-enabled\": \"false\",<br>        }<br>        try:<br>            for attempt in Retrying(<br>                stop=stop_after_attempt(self.retries),<br>                wait=wait_fixed(self.sleep_interval),<br>            ):<br>                with attempt:<br>                    async with aiohttp.ClientSession() as session:<br>                        async with session.post(<br>                            DEFAULT_YANDEXGPT_API_BASE, json=payload, headers=header<br>                        ) as response:<br>                            result = await response.json()<br>                            if \"embedding\" in result:<br>                                return result[\"embedding\"]<br>                            raise YException(<br>                                f\"No embedding found, result returned: {result}\"<br>                            )<br>        except RetryError:<br>            raise YException(<br>                f\"Error computing embeddings after {self.retries} retries. Result returned:\\n{result}\"<br>            )<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding sync.\"\"\"<br>        return self._embed(text, is_document=True)<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get list of texts embeddings sync.\"\"\"<br>        embeddings = []<br>        for text in texts:<br>            embeddings.append(self._embed(text, is_document=True))<br>            time.sleep(self.sleep_interval)<br>        return embeddings<br>    def _get_query_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get query embedding sync.\"\"\"<br>        return self._embed(text, is_document=False)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get query text async.\"\"\"<br>        return await self._aembed(text, is_document=True)<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get list of texts embeddings async.\"\"\"<br>        embeddings = []<br>        for text in texts:<br>            embeddings.append(await self._aembed(text, is_document=True))<br>            await asyncio.sleep(self.sleep_interval)<br>        return embeddings<br>    async def _aget_query_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get query embedding async.\"\"\"<br>        return await self._aembed(text, is_document=False)<br>``` |\n\n### class\\_name`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/yandexgpt/\\#llama_index.embeddings.yandexgpt.YandexGPTEmbedding.class_name \"Permanent link\")\n\n```\nclass_name() -> str\n\n```\n\nReturn the class name.\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-yandexgpt/llama_index/embeddings/yandexgpt/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>90<br>91<br>92<br>93<br>``` | ```<br>@classmethod<br>def class_name(cls) -> str:<br>    \"\"\"Return the class name.\"\"\"<br>    return \"YandexGPTEmbedding\"<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Yandexgpt - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/yandexgpt/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "\n\n# Nomic\n\n## NomicEmbedding [\\#](\\#llama_index.embeddings.nomic.NomicEmbedding \"Permanent link\")\n\nBases: `MultiModalEmbedding`\n\nNomicEmbedding uses the Nomic API to generate embeddings.\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-nomic/llama_index/embeddings/nomic/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>``` | ```<br>class NomicEmbedding(MultiModalEmbedding):<br>    \"\"\"NomicEmbedding uses the Nomic API to generate embeddings.\"\"\"<br>    query_task_type: Optional[NomicTaskType] = Field(<br>        description=\"Task type for queries\",<br>    )<br>    document_task_type: Optional[NomicTaskType] = Field(<br>        description=\"Task type for documents\",<br>    )<br>    dimensionality: Optional[int] = Field(<br>        description=\"Embedding dimension, for use with Matryoshka-capable models\",<br>    )<br>    model_name: str = Field(description=\"Embedding model name\")<br>    vision_model_name: Optional[str] = Field(<br>        description=\"Vision model name for multimodal embeddings\",<br>    )<br>    inference_mode: NomicInferenceMode = Field(<br>        description=\"Whether to generate embeddings locally\",<br>    )<br>    device: Optional[str] = Field(description=\"Device to use for local embeddings\")<br>    def __init__(<br>        self,<br>        model_name: str = \"nomic-embed-text-v1\",<br>        vision_model_name: Optional[str] = \"nomic-embed-vision-v1\",<br>        embed_batch_size: int = 32,<br>        api_key: Optional[str] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        query_task_type: Optional[str] = \"search_query\",<br>        document_task_type: Optional[str] = \"search_document\",<br>        dimensionality: Optional[int] = 768,<br>        inference_mode: str = \"remote\",<br>        device: Optional[str] = None,<br>    ):<br>        if api_key is not None:<br>            nomic.login(api_key)<br>        super().__init__(<br>            model_name=model_name,<br>            vision_model_name=vision_model_name,<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            query_task_type=query_task_type,<br>            document_task_type=document_task_type,<br>            dimensionality=dimensionality,<br>            inference_mode=inference_mode,<br>            device=device,<br>        )<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"NomicEmbedding\"<br>    def load_images(self, image_paths: List[ImageType]) -> List[Image.Image]:<br>        \"\"\"Load images from the specified paths.\"\"\"<br>        return [Image.open(image_path).convert(\"RGB\") for image_path in image_paths]<br>    def _embed_text(<br>        self, texts: List[str], task_type: Optional[str] = None<br>    ) -> List[List[float]]:<br>        result = nomic.embed.text(<br>            texts,<br>            model=self.model_name,<br>            task_type=task_type,<br>            dimensionality=self.dimensionality,<br>            inference_mode=self.inference_mode,<br>            device=self.device,<br>        )<br>        return result[\"embeddings\"]<br>    def _embed_image(self, images_paths: List[ImageType]) -> List[List[float]]:<br>        images = self.load_images(images_paths)<br>        result = nomic.embed.image(images, model=self.vision_model_name)<br>        return result[\"embeddings\"]<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        return self._embed_text([query], task_type=self.query_task_type)[0]<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        self._warn_async()<br>        return self._get_query_embedding(query)<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        return self._embed_text([text], task_type=self.document_task_type)[0]<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        self._warn_async()<br>        return self._get_text_embedding(text)<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        return self._embed_text(texts, task_type=self.document_task_type)<br>    def _get_image_embedding(self, image: ImageType) -> List[float]:<br>        return self._embed_image([image])[0]<br>    async def _aget_image_embedding(self, image: ImageType) -> List[float]:<br>        self._warn_async()<br>        return self._get_image_embedding(image)<br>    def _get_image_embeddings(self, images: List[ImageType]) -> List[List[float]]:<br>        return self._embed_image(images)<br>    def _warn_async(self) -> None:<br>        warnings.warn(<br>            f\"{self.class_name()} does not implement async embeddings, falling back to sync method.\",<br>        )<br>``` |\n\n### load\\_images [\\#](\\#llama_index.embeddings.nomic.NomicEmbedding.load_images \"Permanent link\")\n\n```\nload_images(image_paths: List[ImageType]) -> List[Image]\n\n```\n\nLoad images from the specified paths.\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-nomic/llama_index/embeddings/nomic/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>88<br>89<br>90<br>``` | ```<br>def load_images(self, image_paths: List[ImageType]) -> List[Image.Image]:<br>    \"\"\"Load images from the specified paths.\"\"\"<br>    return [Image.open(image_path).convert(\"RGB\") for image_path in image_paths]<br>``` |\n\nBack to top",
      "metadata": {
        "title": "Nomic - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/nomic/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/extractors/entity/#llama_index.extractors.entity.EntityExtractor)\n\n# Entity\n\n## EntityExtractor [\\#](https://docs.llamaindex.ai/en/stable/api_reference/extractors/entity/\\#llama_index.extractors.entity.EntityExtractor \"Permanent link\")\n\nBases: `BaseExtractor`\n\nEntity extractor. Extracts `entities` into a metadata field using a default model\n`tomaarsen/span-marker-mbert-base-multinerd` and the SpanMarker library.\n\nInstall SpanMarker with `pip install span-marker`.\n\nSource code in `llama-index-integrations/extractors/llama-index-extractors-entity/llama_index/extractors/entity/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>``` | ```<br>class EntityExtractor(BaseExtractor):<br>    \"\"\"<br>    Entity extractor. Extracts `entities` into a metadata field using a default model<br>    `tomaarsen/span-marker-mbert-base-multinerd` and the SpanMarker library.<br>    Install SpanMarker with `pip install span-marker`.<br>    \"\"\"<br>    model_name: str = Field(<br>        default=DEFAULT_ENTITY_MODEL,<br>        description=\"The model name of the SpanMarker model to use.\",<br>    )<br>    prediction_threshold: float = Field(<br>        default=0.5,<br>        description=\"The confidence threshold for accepting predictions.\",<br>        gte=0.0,<br>        lte=1.0,<br>    )<br>    span_joiner: str = Field(<br>        default=\" \", description=\"The separator between entity names.\"<br>    )<br>    label_entities: bool = Field(<br>        default=False, description=\"Include entity class labels or not.\"<br>    )<br>    device: Optional[str] = Field(<br>        default=None, description=\"Device to run model on, i.e. 'cuda', 'cpu'\"<br>    )<br>    entity_map: Dict[str, str] = Field(<br>        default_factory=dict,<br>        description=\"Mapping of entity class names to usable names.\",<br>    )<br>    _tokenizer: Callable = PrivateAttr()<br>    _model: Any = PrivateAttr()<br>    def __init__(<br>        self,<br>        model_name: str = DEFAULT_ENTITY_MODEL,<br>        prediction_threshold: float = 0.5,<br>        span_joiner: str = \" \",<br>        label_entities: bool = False,<br>        device: Optional[str] = None,<br>        entity_map: Optional[Dict[str, str]] = None,<br>        tokenizer: Optional[Callable[[str], List[str]]] = None,<br>        **kwargs: Any,<br>    ):<br>        \"\"\"<br>        Entity extractor for extracting entities from text and inserting<br>        into node metadata.<br>        Args:<br>            model_name (str):<br>                Name of the SpanMarker model to use.<br>            prediction_threshold (float):<br>                Minimum prediction threshold for entities. Defaults to 0.5.<br>            span_joiner (str):<br>                String to join spans with. Defaults to \" \".<br>            label_entities (bool):<br>                Whether to label entities with their type. Setting to true can be<br>                slightly error prone, but can be useful for downstream tasks.<br>                Defaults to False.<br>            device (Optional[str]):<br>                Device to use for SpanMarker model, i.e. \"cpu\" or \"cuda\".<br>                Loads onto \"cpu\" by default.<br>            entity_map (Optional[Dict[str, str]]):<br>                Mapping from entity class name to label.<br>            tokenizer (Optional[Callable[[str], List[str]]]):<br>                Tokenizer to use for splitting text into words.<br>                Defaults to NLTK word_tokenize.<br>        \"\"\"<br>        base_entity_map = DEFAULT_ENTITY_MAP<br>        if entity_map is not None:<br>            base_entity_map.update(entity_map)<br>        super().__init__(<br>            model_name=model_name,<br>            prediction_threshold=prediction_threshold,<br>            span_joiner=span_joiner,<br>            label_entities=label_entities,<br>            device=device,<br>            entity_map=base_entity_map,<br>            **kwargs,<br>        )<br>        self._model = SpanMarkerModel.from_pretrained(model_name)<br>        if device is not None:<br>            self._model = self._model.to(device)<br>        self._tokenizer = tokenizer or word_tokenize<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"EntityExtractor\"<br>    async def aextract(self, nodes: Sequence[BaseNode]) -> List[Dict]:<br>        # Extract node-level entity metadata<br>        metadata_list: List[Dict] = [{} for _ in nodes]<br>        metadata_queue: Iterable[int] = get_tqdm_iterable(<br>            range(len(nodes)), self.show_progress, \"Extracting entities\"<br>        )<br>        for i in metadata_queue:<br>            metadata = metadata_list[i]<br>            node_text = nodes[i].get_content(metadata_mode=self.metadata_mode)<br>            words = self._tokenizer(node_text)<br>            spans = self._model.predict(words)<br>            for span in spans:<br>                if span[\"score\"] > self.prediction_threshold:<br>                    ent_label = self.entity_map.get(span[\"label\"], span[\"label\"])<br>                    metadata_label = ent_label if self.label_entities else \"entities\"<br>                    if metadata_label not in metadata:<br>                        metadata[metadata_label] = set()<br>                    metadata[metadata_label].add(self.span_joiner.join(span[\"span\"]))<br>        # convert metadata from set to list<br>        for metadata in metadata_list:<br>            for key, val in metadata.items():<br>                metadata[key] = list(val)<br>        return metadata_list<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Entity - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/extractors/entity/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "\n\n# Xinference\n\n## XinferenceEmbedding [\\#](\\#llama_index.embeddings.xinference.XinferenceEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nClass for Xinference embeddings.\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-xinference/llama_index/embeddings/xinference/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>  9<br> 10<br> 11<br> 12<br> 13<br> 14<br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>``` | ```<br>class XinferenceEmbedding(BaseEmbedding):<br>    \"\"\"Class for Xinference embeddings.\"\"\"<br>    model_uid: str = Field(<br>        default=\"unknown\",<br>        description=\"The Xinference model uid to use.\",<br>    )<br>    base_url: str = Field(<br>        default=\"http://localhost:9997\",<br>        description=\"The Xinference base url to use.\",<br>    )<br>    timeout: float = Field(<br>        default=60.0,<br>        description=\"Timeout in seconds for the request.\",<br>    )<br>    def __init__(<br>        self,<br>        model_uid: str,<br>        base_url: str = \"http://localhost:9997\",<br>        timeout: float = 60.0,<br>        **kwargs: Any,<br>    ) -> None:<br>        super().__init__(<br>            model_uid=model_uid,<br>            base_url=base_url,<br>            timeout=timeout,<br>            **kwargs,<br>        )<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"XinferenceEmbedding\"<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self.get_general_text_embedding(query)<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        return await self.aget_general_text_embedding(query)<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self.get_general_text_embedding(text)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously get text embedding.\"\"\"<br>        return await self.aget_general_text_embedding(text)<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        embeddings_list: List[List[float]] = []<br>        for text in texts:<br>            embeddings = self.get_general_text_embedding(text)<br>            embeddings_list.append(embeddings)<br>        return embeddings_list<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Asynchronously get text embeddings.\"\"\"<br>        return await asyncio.gather(<br>            *[self.aget_general_text_embedding(text) for text in texts]<br>        )<br>    def get_general_text_embedding(self, prompt: str) -> List[float]:<br>        \"\"\"Get Xinference embeddings.\"\"\"<br>        headers = {\"Content-Type\": \"application/json\"}<br>        json_data = {\"input\": prompt, \"model\": self.model_uid}<br>        response = requests.post(<br>            url=f\"{self.base_url}/v1/embeddings\",<br>            headers=headers,<br>            json=json_data,<br>            timeout=self.timeout,<br>        )<br>        response.encoding = \"utf-8\"<br>        if response.status_code != 200:<br>            raise Exception(<br>                f\"Xinference call failed with status code {response.status_code}.\"<br>                f\"Details: {response.text}\"<br>            )<br>        return response.json()[\"data\"][0][\"embedding\"]<br>    async def aget_general_text_embedding(self, prompt: str) -> List[float]:<br>        \"\"\"Asynchronously get Xinference embeddings.\"\"\"<br>        headers = {\"Content-Type\": \"application/json\"}<br>        json_data = {\"input\": prompt, \"model\": self.model_uid}<br>        async with aiohttp.ClientSession() as session:<br>            async with session.post(<br>                url=f\"{self.base_url}/v1/embeddings\",<br>                headers=headers,<br>                json=json_data,<br>                timeout=self.timeout,<br>            ) as response:<br>                if response.status != 200:<br>                    raise Exception(<br>                        f\"Xinference call failed with status code {response.status}.\"<br>                    )<br>                data = await response.json()<br>                return data[\"data\"][0][\"embedding\"]<br>``` |\n\n### get\\_general\\_text\\_embedding [\\#](\\#llama_index.embeddings.xinference.XinferenceEmbedding.get_general_text_embedding \"Permanent link\")\n\n```\nget_general_text_embedding(prompt: str) -> List[float]\n\n```\n\nGet Xinference embeddings.\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-xinference/llama_index/embeddings/xinference/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>89<br>``` | ```<br>def get_general_text_embedding(self, prompt: str) -> List[float]:<br>    \"\"\"Get Xinference embeddings.\"\"\"<br>    headers = {\"Content-Type\": \"application/json\"}<br>    json_data = {\"input\": prompt, \"model\": self.model_uid}<br>    response = requests.post(<br>        url=f\"{self.base_url}/v1/embeddings\",<br>        headers=headers,<br>        json=json_data,<br>        timeout=self.timeout,<br>    )<br>    response.encoding = \"utf-8\"<br>    if response.status_code != 200:<br>        raise Exception(<br>            f\"Xinference call failed with status code {response.status_code}.\"<br>            f\"Details: {response.text}\"<br>        )<br>    return response.json()[\"data\"][0][\"embedding\"]<br>``` |\n\n### aget\\_general\\_text\\_embedding`async`[\\#](\\#llama_index.embeddings.xinference.XinferenceEmbedding.aget_general_text_embedding \"Permanent link\")\n\n```\naget_general_text_embedding(prompt: str) -> List[float]\n\n```\n\nAsynchronously get Xinference embeddings.\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-xinference/llama_index/embeddings/xinference/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>``` | ```<br>async def aget_general_text_embedding(self, prompt: str) -> List[float]:<br>    \"\"\"Asynchronously get Xinference embeddings.\"\"\"<br>    headers = {\"Content-Type\": \"application/json\"}<br>    json_data = {\"input\": prompt, \"model\": self.model_uid}<br>    async with aiohttp.ClientSession() as session:<br>        async with session.post(<br>            url=f\"{self.base_url}/v1/embeddings\",<br>            headers=headers,<br>            json=json_data,<br>            timeout=self.timeout,<br>        ) as response:<br>            if response.status != 200:<br>                raise Exception(<br>                    f\"Xinference call failed with status code {response.status}.\"<br>                )<br>            data = await response.json()<br>            return data[\"data\"][0][\"embedding\"]<br>``` |\n\nBack to top",
      "metadata": {
        "title": "Xinference - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/xinference/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/langfuse/#llama_index.callbacks.langfuse.langfuse_callback_handler)\n\n# Langfuse\n\n## langfuse\\_callback\\_handler [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/langfuse/\\#llama_index.callbacks.langfuse.langfuse_callback_handler \"Permanent link\")\n\n```\nlangfuse_callback_handler(**eval_params: Any) -> BaseCallbackHandler\n\n```\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-langfuse/llama_index/callbacks/langfuse/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 8<br> 9<br>10<br>11<br>``` | ```<br>def langfuse_callback_handler(**eval_params: Any) -> BaseCallbackHandler:<br>    return LlamaIndexCallbackHandler(<br>        **eval_params, sdk_integration=\"llama-index_set-global-handler\"<br>    )<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Langfuse - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/langfuse/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "\n\n# Ipex llm\n\n## IpexLLMEmbedding [\\#](\\#llama_index.embeddings.ipex_llm.IpexLLMEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-ipex-llm/llama_index/embeddings/ipex_llm/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>``` | ```<br>class IpexLLMEmbedding(BaseEmbedding):<br>    max_length: int = Field(<br>        default=DEFAULT_HUGGINGFACE_LENGTH, description=\"Maximum length of input.\", gt=0<br>    )<br>    normalize: bool = Field(default=True, description=\"Normalize embeddings or not.\")<br>    query_instruction: Optional[str] = Field(<br>        description=\"Instruction to prepend to query text.\"<br>    )<br>    text_instruction: Optional[str] = Field(<br>        description=\"Instruction to prepend to text.\"<br>    )<br>    cache_folder: Optional[str] = Field(<br>        description=\"Cache folder for Hugging Face files.\"<br>    )<br>    _model: Any = PrivateAttr()<br>    _device: str = PrivateAttr()<br>    def __init__(<br>        self,<br>        model_name: str = DEFAULT_HUGGINGFACE_EMBEDDING_MODEL,<br>        max_length: Optional[int] = None,<br>        query_instruction: Optional[str] = None,<br>        text_instruction: Optional[str] = None,<br>        normalize: bool = True,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        cache_folder: Optional[str] = None,<br>        trust_remote_code: bool = False,<br>        device: str = \"cpu\",<br>        callback_manager: Optional[CallbackManager] = None,<br>        **model_kwargs,<br>    ):<br>        if device not in [\"cpu\", \"xpu\"] and not device.startswith(\"xpu:\"):<br>            raise ValueError(<br>                \"IpexLLMEmbedding currently only supports device to be 'cpu', 'xpu', \"<br>                f\"or 'xpu:<device_id>', but you have: {device}.\"<br>            )<br>        device = device<br>        cache_folder = cache_folder or get_cache_dir()<br>        if model_name is None:<br>            raise ValueError(\"The `model_name` argument must be provided.\")<br>        if not is_listed_model(model_name, BGE_MODELS):<br>            bge_model_list_str = \", \".join(BGE_MODELS)<br>            logger.warning(<br>                \"IpexLLMEmbedding currently only provides optimization for \"<br>                f\"Hugging Face BGE models, which are: {bge_model_list_str}\"<br>            )<br>        model = SentenceTransformer(<br>            model_name,<br>            device=device,<br>            cache_folder=cache_folder,<br>            trust_remote_code=trust_remote_code,<br>            prompts={<br>                \"query\": query_instruction<br>                or get_query_instruct_for_model_name(model_name),<br>                \"text\": text_instruction<br>                or get_text_instruct_for_model_name(model_name),<br>            },<br>            **model_kwargs,<br>        )<br>        # Apply ipex-llm optimizations<br>        model = _optimize_pre(self._model)<br>        model = _optimize_post(self._model)<br>        if device == \"xpu\":<br>            # TODO: apply `ipex_llm.optimize_model`<br>            model = model.half().to(device)<br>        if max_length:<br>            model.max_seq_length = max_length<br>        else:<br>            max_length = model.max_seq_length<br>        super().__init__(<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            model_name=model_name,<br>            max_length=max_length,<br>            normalize=normalize,<br>            query_instruction=query_instruction,<br>            text_instruction=text_instruction,<br>        )<br>        self._model = model<br>        self._device = device<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"IpexLLMEmbedding\"<br>    def _embed(<br>        self,<br>        sentences: List[str],<br>        prompt_name: Optional[str] = None,<br>    ) -> List[List[float]]:<br>        \"\"\"Embed sentences.\"\"\"<br>        return self._model.encode(<br>            sentences,<br>            batch_size=self.embed_batch_size,<br>            prompt_name=prompt_name,<br>            normalize_embeddings=self.normalize,<br>        ).tolist()<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self._embed(query, prompt_name=\"query\")<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding async.\"\"\"<br>        return self._get_query_embedding(query)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding async.\"\"\"<br>        return self._get_text_embedding(text)<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._embed(text, prompt_name=\"text\")<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        return self._embed(texts, prompt_name=\"text\")<br>``` |\n\nBack to top",
      "metadata": {
        "title": "Ipex llm - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/ipex_llm/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "\n\n# Promptlayer\n\n## PromptLayerHandler [\\#](\\#llama_index.callbacks.promptlayer.PromptLayerHandler \"Permanent link\")\n\nBases: `BaseCallbackHandler`\n\nCallback handler for sending to promptlayer.com.\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-promptlayer/llama_index/callbacks/promptlayer/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 13<br> 14<br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>``` | ```<br>class PromptLayerHandler(BaseCallbackHandler):<br>    \"\"\"Callback handler for sending to promptlayer.com.\"\"\"<br>    pl_tags: Optional[List[str]]<br>    return_pl_id: bool = False<br>    def __init__(self, pl_tags: List[str] = [], return_pl_id: bool = False) -> None:<br>        try:<br>            from promptlayer.utils import get_api_key, promptlayer_api_request<br>            self._promptlayer_api_request = promptlayer_api_request<br>            self._promptlayer_api_key = get_api_key()<br>        except ImportError:<br>            raise ImportError(<br>                \"Please install PromptLAyer with `pip install promptlayer`\"<br>            )<br>        self.pl_tags = pl_tags<br>        self.return_pl_id = return_pl_id<br>        super().__init__(event_starts_to_ignore=[], event_ends_to_ignore=[])<br>    def start_trace(self, trace_id: Optional[str] = None) -> None:<br>        return<br>    def end_trace(<br>        self,<br>        trace_id: Optional[str] = None,<br>        trace_map: Optional[Dict[str, List[str]]] = None,<br>    ) -> None:<br>        return<br>    event_map: Dict[str, Dict[str, Any]] = {}<br>    def add_event(self, event_id: str, **kwargs: Any) -> None:<br>        self.event_map[event_id] = {<br>            \"kwargs\": kwargs,<br>            \"request_start_time\": datetime.datetime.now().timestamp(),<br>        }<br>    def get_event(<br>        self,<br>        event_id: str,<br>    ) -> Dict[str, Any]:<br>        return self.event_map[event_id] or {}<br>    def on_event_start(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        parent_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> str:<br>        if event_type == CBEventType.LLM and payload is not None:<br>            self.add_event(<br>                event_id=event_id, **payload.get(EventPayload.SERIALIZED, {})<br>            )<br>        return event_id<br>    def on_event_end(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> None:<br>        if event_type != CBEventType.LLM or payload is None:<br>            return<br>        request_end_time = datetime.datetime.now().timestamp()<br>        prompt = str(payload.get(EventPayload.PROMPT))<br>        completion = payload.get(EventPayload.COMPLETION)<br>        response = payload.get(EventPayload.RESPONSE)<br>        function_name = PROMPT_LAYER_CHAT_FUNCTION_NAME<br>        event_data = self.get_event(event_id=event_id)<br>        resp: Union[str, Dict]<br>        extra_args = {}<br>        resp = None<br>        if response:<br>            messages = cast(List[ChatMessage], payload.get(EventPayload.MESSAGES, []))<br>            resp = response.message.dict()<br>            assert isinstance(resp, dict)<br>            usage_dict: Dict[str, int] = {}<br>            try:<br>                usage = response.raw.get(\"usage\", None)  # type: ignore<br>                if isinstance(usage, dict):<br>                    usage_dict = {<br>                        \"prompt_tokens\": usage.get(\"prompt_tokens\", 0),<br>                        \"completion_tokens\": usage.get(\"completion_tokens\", 0),<br>                        \"total_tokens\": usage.get(\"total_tokens\", 0),<br>                    }<br>                elif isinstance(usage, BaseModel):<br>                    usage_dict = usage.dict()<br>            except Exception:<br>                pass<br>            extra_args = {<br>                \"messages\": [message.dict() for message in messages],<br>                \"usage\": usage_dict,<br>            }<br>            ## promptlayer needs tool_calls toplevel.<br>            if \"tool_calls\" in response.message.additional_kwargs:<br>                resp[\"tool_calls\"] = [<br>                    tool_call.dict()<br>                    for tool_call in resp[\"additional_kwargs\"][\"tool_calls\"]<br>                ]<br>                del resp[\"additional_kwargs\"][\"tool_calls\"]<br>        if completion:<br>            function_name = PROMPT_LAYER_COMPLETION_FUNCTION_NAME<br>            resp = str(completion)<br>        if resp:<br>            _pl_request_id = self._promptlayer_api_request(<br>                function_name,<br>                \"openai\",<br>                [prompt],<br>                {<br>                    **extra_args,<br>                    **event_data[\"kwargs\"],<br>                },<br>                self.pl_tags,<br>                [resp],<br>                event_data[\"request_start_time\"],<br>                request_end_time,<br>                self._promptlayer_api_key,<br>                return_pl_id=self.return_pl_id,<br>            )<br>``` |\n\nBack to top",
      "metadata": {
        "title": "Promptlayer - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/promptlayer/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/elasticsearch/#llama_index.embeddings.elasticsearch.ElasticsearchEmbedding)\n\n# Elasticsearch\n\n## ElasticsearchEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/elasticsearch/\\#llama_index.embeddings.elasticsearch.ElasticsearchEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nElasticsearch embedding models.\n\nThis class provides an interface to generate embeddings using a model deployed\nin an Elasticsearch cluster. It requires an Elasticsearch connection object\nand the model\\_id of the model deployed in the cluster.\n\nIn Elasticsearch you need to have an embedding model loaded and deployed.\n\\- https://www.elastic.co\n/guide/en/elasticsearch/reference/current/infer-trained-model.html\n\\- https://www.elastic.co\n/guide/en/machine-learning/current/ml-nlp-deploy-models.html\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-elasticsearch/llama_index/embeddings/elasticsearch/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 10<br> 11<br> 12<br> 13<br> 14<br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>``` | ```<br>class ElasticsearchEmbedding(BaseEmbedding):<br>    \"\"\"Elasticsearch embedding models.<br>    This class provides an interface to generate embeddings using a model deployed<br>    in an Elasticsearch cluster. It requires an Elasticsearch connection object<br>    and the model_id of the model deployed in the cluster.<br>    In Elasticsearch you need to have an embedding model loaded and deployed.<br>    - https://www.elastic.co<br>        /guide/en/elasticsearch/reference/current/infer-trained-model.html<br>    - https://www.elastic.co<br>        /guide/en/machine-learning/current/ml-nlp-deploy-models.html<br>    \"\"\"  #<br>    _client: Any = PrivateAttr()<br>    model_id: str<br>    input_field: str<br>    def class_name(self) -> str:<br>        return \"ElasticsearchEmbedding\"<br>    def __init__(<br>        self,<br>        client: Any,<br>        model_id: str,<br>        input_field: str = \"text_field\",<br>        **kwargs: Any,<br>    ):<br>        super().__init__(model_id=model_id, input_field=input_field, **kwargs)<br>        self._client = client<br>    @classmethod<br>    def from_es_connection(<br>        cls,<br>        model_id: str,<br>        es_connection: Any,<br>        input_field: str = \"text_field\",<br>    ) -> BaseEmbedding:<br>        \"\"\"<br>        Instantiate embeddings from an existing Elasticsearch connection.<br>        This method provides a way to create an instance of the ElasticsearchEmbedding<br>        class using an existing Elasticsearch connection. The connection object is used<br>        to create an MlClient, which is then used to initialize the<br>        ElasticsearchEmbedding instance.<br>        Args:<br>        model_id (str): The model_id of the model deployed in the Elasticsearch cluster.<br>        es_connection (elasticsearch.Elasticsearch): An existing Elasticsearch<br>            connection object.<br>        input_field (str, optional): The name of the key for the input text field<br>            in the document. Defaults to 'text_field'.<br>        Returns:<br>        ElasticsearchEmbedding: An instance of the ElasticsearchEmbedding class.<br>        Example:<br>            .. code-block:: python<br>                from elasticsearch import Elasticsearch<br>                from llama_index.embeddings.elasticsearch import ElasticsearchEmbedding<br>                # Define the model ID and input field name (if different from default)<br>                model_id = \"your_model_id\"<br>                # Optional, only if different from 'text_field'<br>                input_field = \"your_input_field\"<br>                # Create Elasticsearch connection<br>                es_connection = Elasticsearch(hosts=[\"localhost:9200\"], basic_auth=(\"user\", \"password\"))<br>                # Instantiate ElasticsearchEmbedding using the existing connection<br>                embeddings = ElasticsearchEmbedding.from_es_connection(<br>                    model_id,<br>                    es_connection,<br>                    input_field=input_field,<br>                )<br>        \"\"\"<br>        client = MlClient(es_connection)<br>        return cls(client, model_id, input_field=input_field)<br>    @classmethod<br>    def from_credentials(<br>        cls,<br>        model_id: str,<br>        es_url: str,<br>        es_username: str,<br>        es_password: str,<br>        input_field: str = \"text_field\",<br>    ) -> BaseEmbedding:<br>        \"\"\"Instantiate embeddings from Elasticsearch credentials.<br>        Args:<br>            model_id (str): The model_id of the model deployed in the Elasticsearch<br>                cluster.<br>            input_field (str): The name of the key for the input text field in the<br>                document. Defaults to 'text_field'.<br>            es_url: (str): The Elasticsearch url to connect to.<br>            es_username: (str): Elasticsearch username.<br>            es_password: (str): Elasticsearch password.<br>        Example:<br>            .. code-block:: python<br>                from llama_index.embeddings.bedrock import ElasticsearchEmbedding<br>                # Define the model ID and input field name (if different from default)<br>                model_id = \"your_model_id\"<br>                # Optional, only if different from 'text_field'<br>                input_field = \"your_input_field\"<br>                embeddings = ElasticsearchEmbedding.from_credentials(<br>                    model_id,<br>                    input_field=input_field,<br>                    es_url=\"foo\",<br>                    es_username=\"bar\",<br>                    es_password=\"baz\",<br>                )<br>        \"\"\"<br>        es_connection = Elasticsearch(<br>            hosts=[es_url],<br>            basic_auth=(es_username, es_password),<br>        )<br>        client = MlClient(es_connection)<br>        return cls(client, model_id, input_field=input_field)<br>    def _get_embedding(self, text: str) -> List[float]:<br>        \"\"\"<br>        Generate an embedding for a single query text.<br>        Args:<br>            text (str): The query text to generate an embedding for.<br>        Returns:<br>            List[float]: The embedding for the input query text.<br>        \"\"\"<br>        response = self._client.infer_trained_model(<br>            model_id=self.model_id,<br>            docs=[{self.input_field: text}],<br>        )<br>        return response[\"inference_results\"][0][\"predicted_value\"]<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        return self._get_embedding(text)<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        return self._get_embedding(query)<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        return self._get_query_embedding(query)<br>``` |\n\n### from\\_es\\_connection`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/elasticsearch/\\#llama_index.embeddings.elasticsearch.ElasticsearchEmbedding.from_es_connection \"Permanent link\")\n\n```\nfrom_es_connection(model_id: str, es_connection: Any, input_field: str = 'text_field') -> BaseEmbedding\n\n```\n\nInstantiate embeddings from an existing Elasticsearch connection.\n\nThis method provides a way to create an instance of the ElasticsearchEmbedding\nclass using an existing Elasticsearch connection. The connection object is used\nto create an MlClient, which is then used to initialize the\nElasticsearchEmbedding instance.\n\nArgs:\nmodel\\_id (str): The model\\_id of the model deployed in the Elasticsearch cluster.\nes\\_connection (elasticsearch.Elasticsearch): An existing Elasticsearch\nconnection object.\ninput\\_field (str, optional): The name of the key for the input text field\nin the document. Defaults to 'text\\_field'.\n\nReturns:\nElasticsearchEmbedding: An instance of the ElasticsearchEmbedding class.\n\nExample\n\n.. code-block:: python\n\n```\nfrom elasticsearch import Elasticsearch\n\nfrom llama_index.embeddings.elasticsearch import ElasticsearchEmbedding\n\n# Define the model ID and input field name (if different from default)\nmodel_id = \"your_model_id\"\n# Optional, only if different from 'text_field'\ninput_field = \"your_input_field\"\n\n# Create Elasticsearch connection\nes_connection = Elasticsearch(hosts=[\"localhost:9200\"], basic_auth=(\"user\", \"password\"))\n\n# Instantiate ElasticsearchEmbedding using the existing connection\nembeddings = ElasticsearchEmbedding.from_es_connection(\n    model_id,\n    es_connection,\n    input_field=input_field,\n)\n\n```\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-elasticsearch/llama_index/embeddings/elasticsearch/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>89<br>``` | ```<br>@classmethod<br>def from_es_connection(<br>    cls,<br>    model_id: str,<br>    es_connection: Any,<br>    input_field: str = \"text_field\",<br>) -> BaseEmbedding:<br>    \"\"\"<br>    Instantiate embeddings from an existing Elasticsearch connection.<br>    This method provides a way to create an instance of the ElasticsearchEmbedding<br>    class using an existing Elasticsearch connection. The connection object is used<br>    to create an MlClient, which is then used to initialize the<br>    ElasticsearchEmbedding instance.<br>    Args:<br>    model_id (str): The model_id of the model deployed in the Elasticsearch cluster.<br>    es_connection (elasticsearch.Elasticsearch): An existing Elasticsearch<br>        connection object.<br>    input_field (str, optional): The name of the key for the input text field<br>        in the document. Defaults to 'text_field'.<br>    Returns:<br>    ElasticsearchEmbedding: An instance of the ElasticsearchEmbedding class.<br>    Example:<br>        .. code-block:: python<br>            from elasticsearch import Elasticsearch<br>            from llama_index.embeddings.elasticsearch import ElasticsearchEmbedding<br>            # Define the model ID and input field name (if different from default)<br>            model_id = \"your_model_id\"<br>            # Optional, only if different from 'text_field'<br>            input_field = \"your_input_field\"<br>            # Create Elasticsearch connection<br>            es_connection = Elasticsearch(hosts=[\"localhost:9200\"], basic_auth=(\"user\", \"password\"))<br>            # Instantiate ElasticsearchEmbedding using the existing connection<br>            embeddings = ElasticsearchEmbedding.from_es_connection(<br>                model_id,<br>                es_connection,<br>                input_field=input_field,<br>            )<br>    \"\"\"<br>    client = MlClient(es_connection)<br>    return cls(client, model_id, input_field=input_field)<br>``` |\n\n### from\\_credentials`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/elasticsearch/\\#llama_index.embeddings.elasticsearch.ElasticsearchEmbedding.from_credentials \"Permanent link\")\n\n```\nfrom_credentials(model_id: str, es_url: str, es_username: str, es_password: str, input_field: str = 'text_field') -> BaseEmbedding\n\n```\n\nInstantiate embeddings from Elasticsearch credentials.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model_id` | `str` | The model\\_id of the model deployed in the Elasticsearch<br>cluster. | _required_ |\n| `input_field` | `str` | The name of the key for the input text field in the<br>document. Defaults to 'text\\_field'. | `'text_field'` |\n| `es_url` | `str` | (str): The Elasticsearch url to connect to. | _required_ |\n| `es_username` | `str` | (str): Elasticsearch username. | _required_ |\n| `es_password` | `str` | (str): Elasticsearch password. | _required_ |\n\nExample\n\n.. code-block:: python\n\n```\nfrom llama_index.embeddings.bedrock import ElasticsearchEmbedding\n\n# Define the model ID and input field name (if different from default)\nmodel_id = \"your_model_id\"\n# Optional, only if different from 'text_field'\ninput_field = \"your_input_field\"\n\nembeddings = ElasticsearchEmbedding.from_credentials(\n    model_id,\n    input_field=input_field,\n    es_url=\"foo\",\n    es_username=\"bar\",\n    es_password=\"baz\",\n)\n\n```\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-elasticsearch/llama_index/embeddings/elasticsearch/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>``` | ```<br>@classmethod<br>def from_credentials(<br>    cls,<br>    model_id: str,<br>    es_url: str,<br>    es_username: str,<br>    es_password: str,<br>    input_field: str = \"text_field\",<br>) -> BaseEmbedding:<br>    \"\"\"Instantiate embeddings from Elasticsearch credentials.<br>    Args:<br>        model_id (str): The model_id of the model deployed in the Elasticsearch<br>            cluster.<br>        input_field (str): The name of the key for the input text field in the<br>            document. Defaults to 'text_field'.<br>        es_url: (str): The Elasticsearch url to connect to.<br>        es_username: (str): Elasticsearch username.<br>        es_password: (str): Elasticsearch password.<br>    Example:<br>        .. code-block:: python<br>            from llama_index.embeddings.bedrock import ElasticsearchEmbedding<br>            # Define the model ID and input field name (if different from default)<br>            model_id = \"your_model_id\"<br>            # Optional, only if different from 'text_field'<br>            input_field = \"your_input_field\"<br>            embeddings = ElasticsearchEmbedding.from_credentials(<br>                model_id,<br>                input_field=input_field,<br>                es_url=\"foo\",<br>                es_username=\"bar\",<br>                es_password=\"baz\",<br>            )<br>    \"\"\"<br>    es_connection = Elasticsearch(<br>        hosts=[es_url],<br>        basic_auth=(es_username, es_password),<br>    )<br>    client = MlClient(es_connection)<br>    return cls(client, model_id, input_field=input_field)<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Elasticsearch - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/elasticsearch/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/adapter/#llama_index.embeddings.adapter.LinearAdapterEmbeddingModel)\n\n# Adapter\n\n## LinearAdapterEmbeddingModel`module-attribute`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/adapter/\\#llama_index.embeddings.adapter.LinearAdapterEmbeddingModel \"Permanent link\")\n\n```\nLinearAdapterEmbeddingModel = AdapterEmbeddingModel\n\n```\n\n## AdapterEmbeddingModel [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/adapter/\\#llama_index.embeddings.adapter.AdapterEmbeddingModel \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nAdapter for any embedding model.\n\nThis is a wrapper around any embedding model that adds an adapter layer on top of it.\nThis is useful for finetuning an embedding model on a downstream task.\nThe embedding model can be any model - it does not need to expose gradients.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `base_embed_model` | `BaseEmbedding` | Base embedding model. | _required_ |\n| `adapter_path` | `str` | Path to adapter. | _required_ |\n| `adapter_cls` | `Optional[Type[Any]]` | Adapter class. Defaults to None, in which case a linear adapter is used. | `None` |\n| `transform_query` | `bool` | Whether to transform query embeddings. Defaults to True. | `True` |\n| `device` | `Optional[str]` | Device to use. Defaults to None. | `None` |\n| `embed_batch_size` | `int` | Batch size for embedding. Defaults to 10. | `DEFAULT_EMBED_BATCH_SIZE` |\n| `callback_manager` | `Optional[CallbackManager]` | Callback manager. Defaults to None. | `None` |\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-adapter/llama_index/embeddings/adapter/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>``` | ```<br>class AdapterEmbeddingModel(BaseEmbedding):<br>    \"\"\"Adapter for any embedding model.<br>    This is a wrapper around any embedding model that adds an adapter layer \\<br>        on top of it.<br>    This is useful for finetuning an embedding model on a downstream task.<br>    The embedding model can be any model - it does not need to expose gradients.<br>    Args:<br>        base_embed_model (BaseEmbedding): Base embedding model.<br>        adapter_path (str): Path to adapter.<br>        adapter_cls (Optional[Type[Any]]): Adapter class. Defaults to None, in which \\<br>            case a linear adapter is used.<br>        transform_query (bool): Whether to transform query embeddings. Defaults to True.<br>        device (Optional[str]): Device to use. Defaults to None.<br>        embed_batch_size (int): Batch size for embedding. Defaults to 10.<br>        callback_manager (Optional[CallbackManager]): Callback manager. \\<br>            Defaults to None.<br>    \"\"\"<br>    _base_embed_model: BaseEmbedding = PrivateAttr()<br>    _adapter: Any = PrivateAttr()<br>    _transform_query: bool = PrivateAttr()<br>    _device: Optional[str] = PrivateAttr()<br>    _target_device: Any = PrivateAttr()<br>    def __init__(<br>        self,<br>        base_embed_model: BaseEmbedding,<br>        adapter_path: str,<br>        adapter_cls: Optional[Type[Any]] = None,<br>        transform_query: bool = True,<br>        device: Optional[str] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>    ) -> None:<br>        \"\"\"Init params.\"\"\"<br>        import torch<br>        from llama_index.embeddings.adapter.utils import BaseAdapter, LinearLayer<br>        super().__init__(<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            model_name=f\"Adapter for {base_embed_model.model_name}\",<br>        )<br>        if device is None:<br>            device = infer_torch_device()<br>            logger.info(f\"Use pytorch device: {device}\")<br>        self._target_device = torch.device(device)<br>        self._base_embed_model = base_embed_model<br>        if adapter_cls is None:<br>            adapter_cls = LinearLayer<br>        else:<br>            adapter_cls = cast(Type[BaseAdapter], adapter_cls)<br>        adapter = adapter_cls.load(adapter_path)<br>        self._adapter = cast(BaseAdapter, adapter)<br>        self._adapter.to(self._target_device)<br>        self._transform_query = transform_query<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"AdapterEmbeddingModel\"<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        import torch<br>        query_embedding = self._base_embed_model._get_query_embedding(query)<br>        if self._transform_query:<br>            query_embedding_t = torch.tensor(query_embedding).to(self._target_device)<br>            query_embedding_t = self._adapter.forward(query_embedding_t)<br>            query_embedding = query_embedding_t.tolist()<br>        return query_embedding<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        import torch<br>        query_embedding = await self._base_embed_model._aget_query_embedding(query)<br>        if self._transform_query:<br>            query_embedding_t = torch.tensor(query_embedding).to(self._target_device)<br>            query_embedding_t = self._adapter.forward(query_embedding_t)<br>            query_embedding = query_embedding_t.tolist()<br>        return query_embedding<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        return self._base_embed_model._get_text_embedding(text)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        return await self._base_embed_model._aget_text_embedding(text)<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Adapter - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/adapter/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "# Agentops\n\nBack to top",
      "metadata": {
        "title": "Agentops - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/agentops/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "\n\n# Llama debug\n\n## LlamaDebugHandler [\\#](\\#llama_index.core.callbacks.llama_debug.LlamaDebugHandler \"Permanent link\")\n\nBases: `PythonicallyPrintingBaseHandler`\n\nCallback handler that keeps track of debug info.\n\nNOTE: this is a beta feature. The usage within our codebase, and the interface\nmay change.\n\nThis handler simply keeps track of event starts/ends, separated by event types.\nYou can use this callback handler to keep track of and debug events.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `event_starts_to_ignore` | `Optional[List[CBEventType]]` | list of event types to<br>ignore when tracking event starts. | `None` |\n| `event_ends_to_ignore` | `Optional[List[CBEventType]]` | list of event types to<br>ignore when tracking event ends. | `None` |\n\nSource code in `llama-index-core/llama_index/core/callbacks/llama_debug.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>``` | ```<br>class LlamaDebugHandler(PythonicallyPrintingBaseHandler):<br>    \"\"\"Callback handler that keeps track of debug info.<br>    NOTE: this is a beta feature. The usage within our codebase, and the interface<br>    may change.<br>    This handler simply keeps track of event starts/ends, separated by event types.<br>    You can use this callback handler to keep track of and debug events.<br>    Args:<br>        event_starts_to_ignore (Optional[List[CBEventType]]): list of event types to<br>            ignore when tracking event starts.<br>        event_ends_to_ignore (Optional[List[CBEventType]]): list of event types to<br>            ignore when tracking event ends.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        event_starts_to_ignore: Optional[List[CBEventType]] = None,<br>        event_ends_to_ignore: Optional[List[CBEventType]] = None,<br>        print_trace_on_end: bool = True,<br>        logger: Optional[logging.Logger] = None,<br>    ) -> None:<br>        \"\"\"Initialize the llama debug handler.\"\"\"<br>        self._event_pairs_by_type: Dict[CBEventType, List[CBEvent]] = defaultdict(list)<br>        self._event_pairs_by_id: Dict[str, List[CBEvent]] = defaultdict(list)<br>        self._sequential_events: List[CBEvent] = []<br>        self._cur_trace_id: Optional[str] = None<br>        self._trace_map: Dict[str, List[str]] = defaultdict(list)<br>        self.print_trace_on_end = print_trace_on_end<br>        event_starts_to_ignore = (<br>            event_starts_to_ignore if event_starts_to_ignore else []<br>        )<br>        event_ends_to_ignore = event_ends_to_ignore if event_ends_to_ignore else []<br>        super().__init__(<br>            event_starts_to_ignore=event_starts_to_ignore,<br>            event_ends_to_ignore=event_ends_to_ignore,<br>            logger=logger,<br>        )<br>    def on_event_start(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        parent_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> str:<br>        \"\"\"Store event start data by event type.<br>        Args:<br>            event_type (CBEventType): event type to store.<br>            payload (Optional[Dict[str, Any]]): payload to store.<br>            event_id (str): event id to store.<br>            parent_id (str): parent event id.<br>        \"\"\"<br>        event = CBEvent(event_type, payload=payload, id_=event_id)<br>        self._event_pairs_by_type[event.event_type].append(event)<br>        self._event_pairs_by_id[event.id_].append(event)<br>        self._sequential_events.append(event)<br>        return event.id_<br>    def on_event_end(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> None:<br>        \"\"\"Store event end data by event type.<br>        Args:<br>            event_type (CBEventType): event type to store.<br>            payload (Optional[Dict[str, Any]]): payload to store.<br>            event_id (str): event id to store.<br>        \"\"\"<br>        event = CBEvent(event_type, payload=payload, id_=event_id)<br>        self._event_pairs_by_type[event.event_type].append(event)<br>        self._event_pairs_by_id[event.id_].append(event)<br>        self._sequential_events.append(event)<br>        self._trace_map = defaultdict(list)<br>    def get_events(self, event_type: Optional[CBEventType] = None) -> List[CBEvent]:<br>        \"\"\"Get all events for a specific event type.\"\"\"<br>        if event_type is not None:<br>            return self._event_pairs_by_type[event_type]<br>        return self._sequential_events<br>    def _get_event_pairs(self, events: List[CBEvent]) -> List[List[CBEvent]]:<br>        \"\"\"Helper function to pair events according to their ID.\"\"\"<br>        event_pairs: Dict[str, List[CBEvent]] = defaultdict(list)<br>        for event in events:<br>            event_pairs[event.id_].append(event)<br>        return sorted(<br>            event_pairs.values(),<br>            key=lambda x: datetime.strptime(x[0].time, TIMESTAMP_FORMAT),<br>        )<br>    def _get_time_stats_from_event_pairs(<br>        self, event_pairs: List[List[CBEvent]]<br>    ) -> EventStats:<br>        \"\"\"Calculate time-based stats for a set of event pairs.\"\"\"<br>        total_secs = 0.0<br>        for event_pair in event_pairs:<br>            start_time = datetime.strptime(event_pair[0].time, TIMESTAMP_FORMAT)<br>            end_time = datetime.strptime(event_pair[-1].time, TIMESTAMP_FORMAT)<br>            total_secs += (end_time - start_time).total_seconds()<br>        return EventStats(<br>            total_secs=total_secs,<br>            average_secs=total_secs / len(event_pairs),<br>            total_count=len(event_pairs),<br>        )<br>    def get_event_pairs(<br>        self, event_type: Optional[CBEventType] = None<br>    ) -> List[List[CBEvent]]:<br>        \"\"\"Pair events by ID, either all events or a specific type.\"\"\"<br>        if event_type is not None:<br>            return self._get_event_pairs(self._event_pairs_by_type[event_type])<br>        return self._get_event_pairs(self._sequential_events)<br>    def get_llm_inputs_outputs(self) -> List[List[CBEvent]]:<br>        \"\"\"Get the exact LLM inputs and outputs.\"\"\"<br>        return self._get_event_pairs(self._event_pairs_by_type[CBEventType.LLM])<br>    def get_event_time_info(<br>        self, event_type: Optional[CBEventType] = None<br>    ) -> EventStats:<br>        event_pairs = self.get_event_pairs(event_type)<br>        return self._get_time_stats_from_event_pairs(event_pairs)<br>    def flush_event_logs(self) -> None:<br>        \"\"\"Clear all events from memory.\"\"\"<br>        self._event_pairs_by_type = defaultdict(list)<br>        self._event_pairs_by_id = defaultdict(list)<br>        self._sequential_events = []<br>    def start_trace(self, trace_id: Optional[str] = None) -> None:<br>        \"\"\"Launch a trace.\"\"\"<br>        self._trace_map = defaultdict(list)<br>        self._cur_trace_id = trace_id<br>    def end_trace(<br>        self,<br>        trace_id: Optional[str] = None,<br>        trace_map: Optional[Dict[str, List[str]]] = None,<br>    ) -> None:<br>        \"\"\"Shutdown the current trace.\"\"\"<br>        self._trace_map = trace_map or defaultdict(list)<br>        if self.print_trace_on_end:<br>            self.print_trace_map()<br>    def _print_trace_map(self, cur_event_id: str, level: int = 0) -> None:<br>        \"\"\"Recursively print trace map to terminal for debugging.\"\"\"<br>        event_pair = self._event_pairs_by_id[cur_event_id]<br>        if event_pair:<br>            time_stats = self._get_time_stats_from_event_pairs([event_pair])<br>            indent = \" \" * level * 2<br>            self._print(<br>                f\"{indent}|_{event_pair[0].event_type} -> {time_stats.total_secs} seconds\",<br>            )<br>        child_event_ids = self._trace_map[cur_event_id]<br>        for child_event_id in child_event_ids:<br>            self._print_trace_map(child_event_id, level=level + 1)<br>    def print_trace_map(self) -> None:<br>        \"\"\"Print simple trace map to terminal for debugging of the most recent trace.\"\"\"<br>        self._print(\"*\" * 10)<br>        self._print(f\"Trace: {self._cur_trace_id}\")<br>        self._print_trace_map(BASE_TRACE_EVENT, level=1)<br>        self._print(\"*\" * 10)<br>    @property<br>    def event_pairs_by_type(self) -> Dict[CBEventType, List[CBEvent]]:<br>        return self._event_pairs_by_type<br>    @property<br>    def events_pairs_by_id(self) -> Dict[str, List[CBEvent]]:<br>        return self._event_pairs_by_id<br>    @property<br>    def sequential_events(self) -> List[CBEvent]:<br>        return self._sequential_events<br>``` |\n\n### on\\_event\\_start [\\#](\\#llama_index.core.callbacks.llama_debug.LlamaDebugHandler.on_event_start \"Permanent link\")\n\n```\non_event_start(event_type: CBEventType, payload: Optional[Dict[str, Any]] = None, event_id: str = '', parent_id: str = '', **kwargs: Any) -> str\n\n```\n\nStore event start data by event type.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `event_type` | `CBEventType` | event type to store. | _required_ |\n| `payload` | `Optional[Dict[str, Any]]` | payload to store. | `None` |\n| `event_id` | `str` | event id to store. | `''` |\n| `parent_id` | `str` | parent event id. | `''` |\n\nSource code in `llama-index-core/llama_index/core/callbacks/llama_debug.py`\n\n|     |     |\n| --- | --- |\n| ```<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>``` | ```<br>def on_event_start(<br>    self,<br>    event_type: CBEventType,<br>    payload: Optional[Dict[str, Any]] = None,<br>    event_id: str = \"\",<br>    parent_id: str = \"\",<br>    **kwargs: Any,<br>) -> str:<br>    \"\"\"Store event start data by event type.<br>    Args:<br>        event_type (CBEventType): event type to store.<br>        payload (Optional[Dict[str, Any]]): payload to store.<br>        event_id (str): event id to store.<br>        parent_id (str): parent event id.<br>    \"\"\"<br>    event = CBEvent(event_type, payload=payload, id_=event_id)<br>    self._event_pairs_by_type[event.event_type].append(event)<br>    self._event_pairs_by_id[event.id_].append(event)<br>    self._sequential_events.append(event)<br>    return event.id_<br>``` |\n\n### on\\_event\\_end [\\#](\\#llama_index.core.callbacks.llama_debug.LlamaDebugHandler.on_event_end \"Permanent link\")\n\n```\non_event_end(event_type: CBEventType, payload: Optional[Dict[str, Any]] = None, event_id: str = '', **kwargs: Any) -> None\n\n```\n\nStore event end data by event type.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `event_type` | `CBEventType` | event type to store. | _required_ |\n| `payload` | `Optional[Dict[str, Any]]` | payload to store. | `None` |\n| `event_id` | `str` | event id to store. | `''` |\n\nSource code in `llama-index-core/llama_index/core/callbacks/llama_debug.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>``` | ```<br>def on_event_end(<br>    self,<br>    event_type: CBEventType,<br>    payload: Optional[Dict[str, Any]] = None,<br>    event_id: str = \"\",<br>    **kwargs: Any,<br>) -> None:<br>    \"\"\"Store event end data by event type.<br>    Args:<br>        event_type (CBEventType): event type to store.<br>        payload (Optional[Dict[str, Any]]): payload to store.<br>        event_id (str): event id to store.<br>    \"\"\"<br>    event = CBEvent(event_type, payload=payload, id_=event_id)<br>    self._event_pairs_by_type[event.event_type].append(event)<br>    self._event_pairs_by_id[event.id_].append(event)<br>    self._sequential_events.append(event)<br>    self._trace_map = defaultdict(list)<br>``` |\n\n### get\\_events [\\#](\\#llama_index.core.callbacks.llama_debug.LlamaDebugHandler.get_events \"Permanent link\")\n\n```\nget_events(event_type: Optional[CBEventType] = None) -> List[CBEvent]\n\n```\n\nGet all events for a specific event type.\n\nSource code in `llama-index-core/llama_index/core/callbacks/llama_debug.py`\n\n|     |     |\n| --- | --- |\n| ```<br>102<br>103<br>104<br>105<br>106<br>107<br>``` | ```<br>def get_events(self, event_type: Optional[CBEventType] = None) -> List[CBEvent]:<br>    \"\"\"Get all events for a specific event type.\"\"\"<br>    if event_type is not None:<br>        return self._event_pairs_by_type[event_type]<br>    return self._sequential_events<br>``` |\n\n### get\\_event\\_pairs [\\#](\\#llama_index.core.callbacks.llama_debug.LlamaDebugHandler.get_event_pairs \"Permanent link\")\n\n```\nget_event_pairs(event_type: Optional[CBEventType] = None) -> List[List[CBEvent]]\n\n```\n\nPair events by ID, either all events or a specific type.\n\nSource code in `llama-index-core/llama_index/core/callbacks/llama_debug.py`\n\n|     |     |\n| --- | --- |\n| ```<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>``` | ```<br>def get_event_pairs(<br>    self, event_type: Optional[CBEventType] = None<br>) -> List[List[CBEvent]]:<br>    \"\"\"Pair events by ID, either all events or a specific type.\"\"\"<br>    if event_type is not None:<br>        return self._get_event_pairs(self._event_pairs_by_type[event_type])<br>    return self._get_event_pairs(self._sequential_events)<br>``` |\n\n### get\\_llm\\_inputs\\_outputs [\\#](\\#llama_index.core.callbacks.llama_debug.LlamaDebugHandler.get_llm_inputs_outputs \"Permanent link\")\n\n```\nget_llm_inputs_outputs() -> List[List[CBEvent]]\n\n```\n\nGet the exact LLM inputs and outputs.\n\nSource code in `llama-index-core/llama_index/core/callbacks/llama_debug.py`\n\n|     |     |\n| --- | --- |\n| ```<br>145<br>146<br>147<br>``` | ```<br>def get_llm_inputs_outputs(self) -> List[List[CBEvent]]:<br>    \"\"\"Get the exact LLM inputs and outputs.\"\"\"<br>    return self._get_event_pairs(self._event_pairs_by_type[CBEventType.LLM])<br>``` |\n\n### flush\\_event\\_logs [\\#](\\#llama_index.core.callbacks.llama_debug.LlamaDebugHandler.flush_event_logs \"Permanent link\")\n\n```\nflush_event_logs() -> None\n\n```\n\nClear all events from memory.\n\nSource code in `llama-index-core/llama_index/core/callbacks/llama_debug.py`\n\n|     |     |\n| --- | --- |\n| ```<br>155<br>156<br>157<br>158<br>159<br>``` | ```<br>def flush_event_logs(self) -> None:<br>    \"\"\"Clear all events from memory.\"\"\"<br>    self._event_pairs_by_type = defaultdict(list)<br>    self._event_pairs_by_id = defaultdict(list)<br>    self._sequential_events = []<br>``` |\n\n### start\\_trace [\\#](\\#llama_index.core.callbacks.llama_debug.LlamaDebugHandler.start_trace \"Permanent link\")\n\n```\nstart_trace(trace_id: Optional[str] = None) -> None\n\n```\n\nLaunch a trace.\n\nSource code in `llama-index-core/llama_index/core/callbacks/llama_debug.py`\n\n|     |     |\n| --- | --- |\n| ```<br>161<br>162<br>163<br>164<br>``` | ```<br>def start_trace(self, trace_id: Optional[str] = None) -> None:<br>    \"\"\"Launch a trace.\"\"\"<br>    self._trace_map = defaultdict(list)<br>    self._cur_trace_id = trace_id<br>``` |\n\n### end\\_trace [\\#](\\#llama_index.core.callbacks.llama_debug.LlamaDebugHandler.end_trace \"Permanent link\")\n\n```\nend_trace(trace_id: Optional[str] = None, trace_map: Optional[Dict[str, List[str]]] = None) -> None\n\n```\n\nShutdown the current trace.\n\nSource code in `llama-index-core/llama_index/core/callbacks/llama_debug.py`\n\n|     |     |\n| --- | --- |\n| ```<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>``` | ```<br>def end_trace(<br>    self,<br>    trace_id: Optional[str] = None,<br>    trace_map: Optional[Dict[str, List[str]]] = None,<br>) -> None:<br>    \"\"\"Shutdown the current trace.\"\"\"<br>    self._trace_map = trace_map or defaultdict(list)<br>    if self.print_trace_on_end:<br>        self.print_trace_map()<br>``` |\n\n### print\\_trace\\_map [\\#](\\#llama_index.core.callbacks.llama_debug.LlamaDebugHandler.print_trace_map \"Permanent link\")\n\n```\nprint_trace_map() -> None\n\n```\n\nPrint simple trace map to terminal for debugging of the most recent trace.\n\nSource code in `llama-index-core/llama_index/core/callbacks/llama_debug.py`\n\n|     |     |\n| --- | --- |\n| ```<br>190<br>191<br>192<br>193<br>194<br>195<br>``` | ```<br>def print_trace_map(self) -> None:<br>    \"\"\"Print simple trace map to terminal for debugging of the most recent trace.\"\"\"<br>    self._print(\"*\" * 10)<br>    self._print(f\"Trace: {self._cur_trace_id}\")<br>    self._print_trace_map(BASE_TRACE_EVENT, level=1)<br>    self._print(\"*\" * 10)<br>``` |\n\nBack to top",
      "metadata": {
        "title": "Llama debug - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/llama_debug/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/voyageai/#llama_index.embeddings.voyageai.VoyageEmbedding)\n\n# Voyageai\n\n## VoyageEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/voyageai/\\#llama_index.embeddings.voyageai.VoyageEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nClass for Voyage embeddings.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model_name` | `str` | Model for embedding.<br>Defaults to \"voyage-01\". | _required_ |\n| `voyage_api_key` | `Optional[str]` | Voyage API key. Defaults to None.<br>You can either specify the key here or store it as an environment variable. | `None` |\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-voyageai/llama_index/embeddings/voyageai/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>``` | ```<br>class VoyageEmbedding(BaseEmbedding):<br>    \"\"\"Class for Voyage embeddings.<br>    Args:<br>        model_name (str): Model for embedding.<br>            Defaults to \"voyage-01\".<br>        voyage_api_key (Optional[str]): Voyage API key. Defaults to None.<br>            You can either specify the key here or store it as an environment variable.<br>    \"\"\"<br>    _client: voyageai.Client = PrivateAttr(None)<br>    _aclient: voyageai.client_async.AsyncClient = PrivateAttr()<br>    truncation: Optional[bool] = None<br>    def __init__(<br>        self,<br>        model_name: str,<br>        voyage_api_key: Optional[str] = None,<br>        embed_batch_size: Optional[int] = None,<br>        truncation: Optional[bool] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        **kwargs: Any,<br>    ):<br>        if model_name == \"voyage-01\":<br>            logger.warning(<br>                \"voyage-01 is not the latest model by Voyage AI. Please note that `model_name` \"<br>                \"will be a required argument in the future. We recommend setting it explicitly. Please see \"<br>                \"https://docs.voyageai.com/docs/embeddings for the latest models offered by Voyage AI.\"<br>            )<br>        if embed_batch_size is None:<br>            embed_batch_size = 72 if model_name in [\"voyage-2\", \"voyage-02\"] else 7<br>        super().__init__(<br>            model_name=model_name,<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            **kwargs,<br>        )<br>        self._client = voyageai.Client(api_key=voyage_api_key)<br>        self._aclient = voyageai.AsyncClient(api_key=voyage_api_key)<br>        self.truncation = truncation<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"VoyageEmbedding\"<br>    def _get_embedding(self, texts: List[str], input_type: str) -> List[List[float]]:<br>        return self._client.embed(<br>            texts,<br>            model=self.model_name,<br>            input_type=input_type,<br>            truncation=self.truncation,<br>        ).embeddings<br>    async def _aget_embedding(<br>        self, texts: List[str], input_type: str<br>    ) -> List[List[float]]:<br>        r = await self._aclient.embed(<br>            texts,<br>            model=self.model_name,<br>            input_type=input_type,<br>            truncation=self.truncation,<br>        )<br>        return r.embeddings<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self._get_embedding([query], input_type=\"query\")[0]<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        r = await self._aget_embedding([query], input_type=\"query\")<br>        return r[0]<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._get_embedding([text], input_type=\"document\")[0]<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously get text embedding.\"\"\"<br>        r = await self._aget_embedding([text], input_type=\"document\")<br>        return r[0]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        return self._get_embedding(texts, input_type=\"document\")<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Asynchronously get text embeddings.\"\"\"<br>        return await self._aget_embedding(texts, input_type=\"document\")<br>    def get_general_text_embedding(<br>        self, text: str, input_type: Optional[str] = None<br>    ) -> List[float]:<br>        \"\"\"Get general text embedding with input_type.\"\"\"<br>        return self._get_embedding([text], input_type=input_type)[0]<br>    async def aget_general_text_embedding(<br>        self, text: str, input_type: Optional[str] = None<br>    ) -> List[float]:<br>        \"\"\"Asynchronously get general text embedding with input_type.\"\"\"<br>        r = await self._aget_embedding([text], input_type=input_type)<br>        return r[0]<br>``` |\n\n### get\\_general\\_text\\_embedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/voyageai/\\#llama_index.embeddings.voyageai.VoyageEmbedding.get_general_text_embedding \"Permanent link\")\n\n```\nget_general_text_embedding(text: str, input_type: Optional[str] = None) -> List[float]\n\n```\n\nGet general text embedding with input\\_type.\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-voyageai/llama_index/embeddings/voyageai/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>109<br>110<br>111<br>112<br>113<br>``` | ```<br>def get_general_text_embedding(<br>    self, text: str, input_type: Optional[str] = None<br>) -> List[float]:<br>    \"\"\"Get general text embedding with input_type.\"\"\"<br>    return self._get_embedding([text], input_type=input_type)[0]<br>``` |\n\n### aget\\_general\\_text\\_embedding`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/voyageai/\\#llama_index.embeddings.voyageai.VoyageEmbedding.aget_general_text_embedding \"Permanent link\")\n\n```\naget_general_text_embedding(text: str, input_type: Optional[str] = None) -> List[float]\n\n```\n\nAsynchronously get general text embedding with input\\_type.\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-voyageai/llama_index/embeddings/voyageai/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>115<br>116<br>117<br>118<br>119<br>120<br>``` | ```<br>async def aget_general_text_embedding(<br>    self, text: str, input_type: Optional[str] = None<br>) -> List[float]:<br>    \"\"\"Asynchronously get general text embedding with input_type.\"\"\"<br>    r = await self._aget_embedding([text], input_type=input_type)<br>    return r[0]<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Voyageai - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/voyageai/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/cohere/#llama_index.embeddings.cohere.CohereEmbedding)\n\n# Cohere\n\n## CohereEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/cohere/\\#llama_index.embeddings.cohere.CohereEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nCohereEmbedding uses the Cohere API to generate embeddings for text.\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-cohere/llama_index/embeddings/cohere/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>``` | ```<br>class CohereEmbedding(BaseEmbedding):<br>    \"\"\"CohereEmbedding uses the Cohere API to generate embeddings for text.\"\"\"<br>    # Instance variables initialized via Pydantic's mechanism<br>    api_key: str = Field(description=\"The Cohere API key.\")<br>    truncate: str = Field(description=\"Truncation type - START/ END/ NONE\")<br>    input_type: Optional[str] = Field(<br>        description=\"Model Input type. If not provided, search_document and search_query are used when needed.\"<br>    )<br>    embedding_type: str = Field(<br>        description=\"Embedding type. If not provided float embedding_type is used when needed.\"<br>    )<br>    _client: cohere.Client = PrivateAttr()<br>    _async_client: cohere.AsyncClient = PrivateAttr()<br>    _base_url: Optional[str] = PrivateAttr()<br>    _timeout: Optional[float] = PrivateAttr()<br>    _httpx_client: Optional[httpx.Client] = PrivateAttr()<br>    _httpx_async_client: Optional[httpx.AsyncClient] = PrivateAttr()<br>    def __init__(<br>        self,<br>        # deprecated<br>        cohere_api_key: Optional[str] = None,<br>        api_key: Optional[str] = None,<br>        model_name: str = \"embed-english-v3.0\",<br>        truncate: str = \"END\",<br>        input_type: Optional[str] = None,<br>        embedding_type: str = \"float\",<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>        base_url: Optional[str] = None,<br>        timeout: Optional[float] = None,<br>        httpx_client: Optional[httpx.Client] = None,<br>        httpx_async_client: Optional[httpx.AsyncClient] = None,<br>        num_workers: Optional[int] = None,<br>        **kwargs: Any,<br>    ):<br>        \"\"\"<br>        A class representation for generating embeddings using the Cohere API.<br>        Args:<br>            truncate (str): A string indicating the truncation strategy to be applied to input text. Possible values<br>                        are 'START', 'END', or 'NONE'.<br>            input_type (Optional[str]): An optional string that specifies the type of input provided to the model.<br>                                    This is model-dependent and could be one of the following: 'search_query',<br>                                    'search_document', 'classification', or 'clustering'.<br>            model_name (str): The name of the model to be used for generating embeddings. The class ensures that<br>                          this model is supported and that the input type provided is compatible with the model.<br>        \"\"\"<br>        # Validate model_name and input_type<br>        if model_name not in VALID_MODEL_INPUT_TYPES:<br>            raise ValueError(f\"{model_name} is not a valid model name\")<br>        if input_type not in VALID_MODEL_INPUT_TYPES[model_name]:<br>            raise ValueError(<br>                f\"{input_type} is not a valid input type for the provided model.\"<br>            )<br>        if embedding_type not in VALID_MODEL_EMBEDDING_TYPES[model_name]:<br>            raise ValueError(<br>                f\"{embedding_type} is not a embedding type for the provided model.\"<br>            )<br>        if truncate not in VALID_TRUNCATE_OPTIONS:<br>            raise ValueError(f\"truncate must be one of {VALID_TRUNCATE_OPTIONS}\")<br>        super().__init__(<br>            api_key=api_key or cohere_api_key,<br>            model_name=model_name,<br>            input_type=input_type,<br>            embedding_type=embedding_type,<br>            truncate=truncate,<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            num_workers=num_workers,<br>            **kwargs,<br>        )<br>        self._client = None<br>        self._async_client = None<br>        self._base_url = base_url<br>        self._timeout = timeout<br>        self._httpx_client = httpx_client<br>        self._httpx_async_client = httpx_async_client<br>    def _get_client(self) -> cohere.Client:<br>        if self._client is None:<br>            self._client = cohere.Client(<br>                api_key=self.api_key,<br>                client_name=\"llama_index\",<br>                base_url=self._base_url,<br>                timeout=self._timeout,<br>                httpx_client=self._httpx_client,<br>            )<br>        return self._client<br>    def _get_async_client(self) -> cohere.AsyncClient:<br>        if self._async_client is None:<br>            self._async_client = cohere.AsyncClient(<br>                api_key=self.api_key,<br>                client_name=\"llama_index\",<br>                base_url=self._base_url,<br>                timeout=self._timeout,<br>                httpx_client=self._httpx_async_client,<br>            )<br>        return self._async_client<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"CohereEmbedding\"<br>    def _embed(self, texts: List[str], input_type: str) -> List[List[float]]:<br>        \"\"\"Embed sentences using Cohere.\"\"\"<br>        client = self._get_client()<br>        if self.model_name in V3_MODELS:<br>            result = client.embed(<br>                texts=texts,<br>                input_type=self.input_type or input_type,<br>                embedding_types=[self.embedding_type],<br>                model=self.model_name,<br>                truncate=self.truncate,<br>            ).embeddings<br>        else:<br>            result = client.embed(<br>                texts=texts,<br>                model=self.model_name,<br>                embedding_types=[self.embedding_type],<br>                truncate=self.truncate,<br>            ).embeddings<br>        return getattr(result, self.embedding_type, None)<br>    async def _aembed(self, texts: List[str], input_type: str) -> List[List[float]]:<br>        \"\"\"Embed sentences using Cohere.\"\"\"<br>        async_client = self._get_async_client()<br>        if self.model_name in V3_MODELS:<br>            result = (<br>                await async_client.embed(<br>                    texts=texts,<br>                    input_type=self.input_type or input_type,<br>                    embedding_types=[self.embedding_type],<br>                    model=self.model_name,<br>                    truncate=self.truncate,<br>                )<br>            ).embeddings<br>        else:<br>            result = (<br>                await async_client.embed(<br>                    texts=texts,<br>                    model=self.model_name,<br>                    embedding_types=[self.embedding_type],<br>                    truncate=self.truncate,<br>                )<br>            ).embeddings<br>        return getattr(result, self.embedding_type, None)<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding. For query embeddings, input_type='search_query'.\"\"\"<br>        return self._embed([query], input_type=\"search_query\")[0]<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding async. For query embeddings, input_type='search_query'.\"\"\"<br>        return (await self._aembed([query], input_type=\"search_query\"))[0]<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._embed([text], input_type=\"search_document\")[0]<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding async.\"\"\"<br>        return (await self._aembed([text], input_type=\"search_document\"))[0]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        return self._embed(texts, input_type=\"search_document\")<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        return await self._aembed(texts, input_type=\"search_document\")<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Cohere - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/cohere/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/response/#llama_index.core.evaluation.ResponseEvaluator)\n\n# Response\n\nEvaluation modules.\n\n## ResponseEvaluator`module-attribute`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/response/\\#llama_index.core.evaluation.ResponseEvaluator \"Permanent link\")\n\n```\nResponseEvaluator = FaithfulnessEvaluator\n\n```\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Response - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/evaluation/response/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "\n\n# React\n\n## ReActAgent [\\#](\\#llama_index.core.agent.react.ReActAgent \"Permanent link\")\n\nBases: `AgentRunner`\n\nReAct agent.\n\nSubclasses AgentRunner with a ReActAgentWorker.\n\nFor the legacy implementation see:\n\n```\nfrom llama_index.core.agent.legacy.react.base import ReActAgent\n\n```\n\nSource code in `llama-index-core/llama_index/core/agent/react/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>``` | ````<br>class ReActAgent(AgentRunner):<br>    \"\"\"ReAct agent.<br>    Subclasses AgentRunner with a ReActAgentWorker.<br>    For the legacy implementation see:<br>    ```python<br>    from llama_index.core.agent.legacy.react.base import ReActAgent<br>    ```<br>    \"\"\"<br>    def __init__(<br>        self,<br>        tools: Sequence[BaseTool],<br>        llm: LLM,<br>        memory: BaseMemory,<br>        max_iterations: int = 10,<br>        react_chat_formatter: Optional[ReActChatFormatter] = None,<br>        output_parser: Optional[ReActOutputParser] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        context: Optional[str] = None,<br>        handle_reasoning_failure_fn: Optional[<br>            Callable[[CallbackManager, Exception], ToolOutput]<br>        ] = None,<br>    ) -> None:<br>        \"\"\"Init params.\"\"\"<br>        callback_manager = callback_manager or llm.callback_manager<br>        if context and react_chat_formatter:<br>            raise ValueError(\"Cannot provide both context and react_chat_formatter\")<br>        if context:<br>            react_chat_formatter = ReActChatFormatter.from_context(context)<br>        step_engine = ReActAgentWorker.from_tools(<br>            tools=tools,<br>            tool_retriever=tool_retriever,<br>            llm=llm,<br>            max_iterations=max_iterations,<br>            react_chat_formatter=react_chat_formatter,<br>            output_parser=output_parser,<br>            callback_manager=callback_manager,<br>            verbose=verbose,<br>            handle_reasoning_failure_fn=handle_reasoning_failure_fn,<br>        )<br>        super().__init__(<br>            step_engine,<br>            memory=memory,<br>            llm=llm,<br>            callback_manager=callback_manager,<br>            verbose=verbose,<br>        )<br>    @classmethod<br>    def from_tools(<br>        cls,<br>        tools: Optional[List[BaseTool]] = None,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        llm: Optional[LLM] = None,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        memory: Optional[BaseMemory] = None,<br>        memory_cls: Type[BaseMemory] = ChatMemoryBuffer,<br>        max_iterations: int = 10,<br>        react_chat_formatter: Optional[ReActChatFormatter] = None,<br>        output_parser: Optional[ReActOutputParser] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        context: Optional[str] = None,<br>        handle_reasoning_failure_fn: Optional[<br>            Callable[[CallbackManager, Exception], ToolOutput]<br>        ] = None,<br>        **kwargs: Any,<br>    ) -> \"ReActAgent\":<br>        \"\"\"Convenience constructor method from set of BaseTools (Optional).<br>        NOTE: kwargs should have been exhausted by this point. In other words<br>        the various upstream components such as BaseSynthesizer (response synthesizer)<br>        or BaseRetriever should have picked up off their respective kwargs in their<br>        constructions.<br>        If `handle_reasoning_failure_fn` is provided, when LLM fails to follow the response templates specified in<br>        the System Prompt, this function will be called. This function should provide to the Agent, so that the Agent<br>        can have a second chance to fix its mistakes.<br>        To handle the exception yourself, you can provide a function that raises the `Exception`.<br>        Note: If you modified any response template in the System Prompt, you should override the method<br>        `_extract_reasoning_step` in `ReActAgentWorker`.<br>        Returns:<br>            ReActAgent<br>        \"\"\"<br>        llm = llm or Settings.llm<br>        if callback_manager is not None:<br>            llm.callback_manager = callback_manager<br>        memory = memory or memory_cls.from_defaults(<br>            chat_history=chat_history or [], llm=llm<br>        )<br>        return cls(<br>            tools=tools or [],<br>            tool_retriever=tool_retriever,<br>            llm=llm,<br>            memory=memory,<br>            max_iterations=max_iterations,<br>            react_chat_formatter=react_chat_formatter,<br>            output_parser=output_parser,<br>            callback_manager=callback_manager,<br>            verbose=verbose,<br>            context=context,<br>            handle_reasoning_failure_fn=handle_reasoning_failure_fn,<br>        )<br>    def _get_prompt_modules(self) -> PromptMixinType:<br>        \"\"\"Get prompt modules.\"\"\"<br>        return {\"agent_worker\": self.agent_worker}<br>```` |\n\n### from\\_tools`classmethod`[\\#](\\#llama_index.core.agent.react.ReActAgent.from_tools \"Permanent link\")\n\n```\nfrom_tools(tools: Optional[List[BaseTool]] = None, tool_retriever: Optional[ObjectRetriever[BaseTool]] = None, llm: Optional[LLM] = None, chat_history: Optional[List[ChatMessage]] = None, memory: Optional[BaseMemory] = None, memory_cls: Type[BaseMemory] = ChatMemoryBuffer, max_iterations: int = 10, react_chat_formatter: Optional[ReActChatFormatter] = None, output_parser: Optional[ReActOutputParser] = None, callback_manager: Optional[CallbackManager] = None, verbose: bool = False, context: Optional[str] = None, handle_reasoning_failure_fn: Optional[Callable[[CallbackManager, Exception], ToolOutput]] = None, **kwargs: Any) -> ReActAgent\n\n```\n\nConvenience constructor method from set of BaseTools (Optional).\n\nNOTE: kwargs should have been exhausted by this point. In other words\nthe various upstream components such as BaseSynthesizer (response synthesizer)\nor BaseRetriever should have picked up off their respective kwargs in their\nconstructions.\n\nIf `handle_reasoning_failure_fn` is provided, when LLM fails to follow the response templates specified in\nthe System Prompt, this function will be called. This function should provide to the Agent, so that the Agent\ncan have a second chance to fix its mistakes.\nTo handle the exception yourself, you can provide a function that raises the `Exception`.\n\nNote: If you modified any response template in the System Prompt, you should override the method\n`_extract_reasoning_step` in `ReActAgentWorker`.\n\n**Returns:**\n\n| Type | Description |\n| --- | --- |\n| `ReActAgent` | ReActAgent |\n\nSource code in `llama-index-core/llama_index/core/agent/react/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>``` | ```<br>@classmethod<br>def from_tools(<br>    cls,<br>    tools: Optional[List[BaseTool]] = None,<br>    tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>    llm: Optional[LLM] = None,<br>    chat_history: Optional[List[ChatMessage]] = None,<br>    memory: Optional[BaseMemory] = None,<br>    memory_cls: Type[BaseMemory] = ChatMemoryBuffer,<br>    max_iterations: int = 10,<br>    react_chat_formatter: Optional[ReActChatFormatter] = None,<br>    output_parser: Optional[ReActOutputParser] = None,<br>    callback_manager: Optional[CallbackManager] = None,<br>    verbose: bool = False,<br>    context: Optional[str] = None,<br>    handle_reasoning_failure_fn: Optional[<br>        Callable[[CallbackManager, Exception], ToolOutput]<br>    ] = None,<br>    **kwargs: Any,<br>) -> \"ReActAgent\":<br>    \"\"\"Convenience constructor method from set of BaseTools (Optional).<br>    NOTE: kwargs should have been exhausted by this point. In other words<br>    the various upstream components such as BaseSynthesizer (response synthesizer)<br>    or BaseRetriever should have picked up off their respective kwargs in their<br>    constructions.<br>    If `handle_reasoning_failure_fn` is provided, when LLM fails to follow the response templates specified in<br>    the System Prompt, this function will be called. This function should provide to the Agent, so that the Agent<br>    can have a second chance to fix its mistakes.<br>    To handle the exception yourself, you can provide a function that raises the `Exception`.<br>    Note: If you modified any response template in the System Prompt, you should override the method<br>    `_extract_reasoning_step` in `ReActAgentWorker`.<br>    Returns:<br>        ReActAgent<br>    \"\"\"<br>    llm = llm or Settings.llm<br>    if callback_manager is not None:<br>        llm.callback_manager = callback_manager<br>    memory = memory or memory_cls.from_defaults(<br>        chat_history=chat_history or [], llm=llm<br>    )<br>    return cls(<br>        tools=tools or [],<br>        tool_retriever=tool_retriever,<br>        llm=llm,<br>        memory=memory,<br>        max_iterations=max_iterations,<br>        react_chat_formatter=react_chat_formatter,<br>        output_parser=output_parser,<br>        callback_manager=callback_manager,<br>        verbose=verbose,<br>        context=context,<br>        handle_reasoning_failure_fn=handle_reasoning_failure_fn,<br>    )<br>``` |\n\n## ReActAgentWorker [\\#](\\#llama_index.core.agent.react.ReActAgentWorker \"Permanent link\")\n\nBases: `BaseAgentWorker`\n\nOpenAI Agent worker.\n\nSource code in `llama-index-core/llama_index/core/agent/react/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>472<br>473<br>474<br>475<br>476<br>477<br>478<br>479<br>480<br>481<br>482<br>483<br>484<br>485<br>486<br>487<br>488<br>489<br>490<br>491<br>492<br>493<br>494<br>495<br>496<br>497<br>498<br>499<br>500<br>501<br>502<br>503<br>504<br>505<br>506<br>507<br>508<br>509<br>510<br>511<br>512<br>513<br>514<br>515<br>516<br>517<br>518<br>519<br>520<br>521<br>522<br>523<br>524<br>525<br>526<br>527<br>528<br>529<br>530<br>531<br>532<br>533<br>534<br>535<br>536<br>537<br>538<br>539<br>540<br>541<br>542<br>543<br>544<br>545<br>546<br>547<br>548<br>549<br>550<br>551<br>552<br>553<br>554<br>555<br>556<br>557<br>558<br>559<br>560<br>561<br>562<br>563<br>564<br>565<br>566<br>567<br>568<br>569<br>570<br>571<br>572<br>573<br>574<br>575<br>576<br>577<br>578<br>579<br>580<br>581<br>582<br>583<br>584<br>585<br>586<br>587<br>588<br>589<br>590<br>591<br>592<br>593<br>594<br>595<br>596<br>597<br>598<br>599<br>600<br>601<br>602<br>603<br>604<br>605<br>606<br>607<br>608<br>609<br>610<br>611<br>612<br>613<br>614<br>615<br>616<br>617<br>618<br>619<br>620<br>621<br>622<br>623<br>624<br>625<br>626<br>627<br>628<br>629<br>630<br>631<br>632<br>633<br>634<br>635<br>636<br>637<br>638<br>639<br>640<br>641<br>642<br>643<br>644<br>645<br>646<br>647<br>648<br>649<br>650<br>651<br>652<br>653<br>654<br>655<br>656<br>657<br>658<br>659<br>660<br>661<br>662<br>663<br>664<br>665<br>666<br>667<br>668<br>669<br>670<br>671<br>672<br>673<br>674<br>675<br>676<br>677<br>678<br>679<br>680<br>681<br>682<br>683<br>684<br>685<br>686<br>687<br>688<br>689<br>690<br>691<br>692<br>693<br>694<br>695<br>696<br>697<br>698<br>699<br>700<br>701<br>702<br>703<br>704<br>705<br>706<br>707<br>708<br>709<br>710<br>711<br>712<br>713<br>714<br>715<br>716<br>717<br>718<br>719<br>720<br>721<br>722<br>723<br>724<br>725<br>726<br>727<br>728<br>729<br>730<br>731<br>732<br>733<br>734<br>735<br>736<br>737<br>738<br>739<br>740<br>741<br>742<br>743<br>744<br>745<br>746<br>747<br>748<br>749<br>750<br>751<br>752<br>753<br>754<br>755<br>756<br>757<br>758<br>759<br>760<br>761<br>762<br>763<br>764<br>765<br>766<br>767<br>768<br>769<br>770<br>771<br>772<br>773<br>774<br>775<br>776<br>777<br>778<br>779<br>780<br>781<br>782<br>783<br>784<br>785<br>786<br>787<br>788<br>789<br>790<br>791<br>792<br>793<br>794<br>795<br>796<br>797<br>798<br>799<br>800<br>801<br>802<br>803<br>804<br>805<br>806<br>807<br>808<br>809<br>810<br>811<br>812<br>813<br>814<br>815<br>816<br>817<br>818<br>819<br>820<br>821<br>822<br>823<br>824<br>825<br>826<br>827<br>828<br>829<br>830<br>``` | ```<br>class ReActAgentWorker(BaseAgentWorker):<br>    \"\"\"OpenAI Agent worker.\"\"\"<br>    def __init__(<br>        self,<br>        tools: Sequence[BaseTool],<br>        llm: LLM,<br>        max_iterations: int = 10,<br>        react_chat_formatter: Optional[ReActChatFormatter] = None,<br>        output_parser: Optional[ReActOutputParser] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        handle_reasoning_failure_fn: Optional[<br>            Callable[[CallbackManager, Exception], ToolOutput]<br>        ] = None,<br>    ) -> None:<br>        self._llm = llm<br>        self.callback_manager = callback_manager or llm.callback_manager<br>        self._max_iterations = max_iterations<br>        self._react_chat_formatter = react_chat_formatter or ReActChatFormatter()<br>        self._output_parser = output_parser or ReActOutputParser()<br>        self._verbose = verbose<br>        self._handle_reasoning_failure_fn = (<br>            handle_reasoning_failure_fn<br>            or tell_llm_about_failure_in_extract_reasoning_step<br>        )<br>        if len(tools) > 0 and tool_retriever is not None:<br>            raise ValueError(\"Cannot specify both tools and tool_retriever\")<br>        elif len(tools) > 0:<br>            self._get_tools = lambda _: tools<br>        elif tool_retriever is not None:<br>            tool_retriever_c = cast(ObjectRetriever[BaseTool], tool_retriever)<br>            self._get_tools = lambda message: tool_retriever_c.retrieve(message)<br>        else:<br>            self._get_tools = lambda _: []<br>    @classmethod<br>    def from_tools(<br>        cls,<br>        tools: Optional[Sequence[BaseTool]] = None,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        llm: Optional[LLM] = None,<br>        max_iterations: int = 10,<br>        react_chat_formatter: Optional[ReActChatFormatter] = None,<br>        output_parser: Optional[ReActOutputParser] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        handle_reasoning_failure_fn: Optional[<br>            Callable[[CallbackManager, Exception], ToolOutput]<br>        ] = None,<br>        **kwargs: Any,<br>    ) -> \"ReActAgentWorker\":<br>        \"\"\"Convenience constructor method from set of BaseTools (Optional).<br>        NOTE: kwargs should have been exhausted by this point. In other words<br>        the various upstream components such as BaseSynthesizer (response synthesizer)<br>        or BaseRetriever should have picked up off their respective kwargs in their<br>        constructions.<br>        Returns:<br>            ReActAgentWorker<br>        \"\"\"<br>        llm = llm or Settings.llm<br>        if callback_manager is not None:<br>            llm.callback_manager = callback_manager<br>        return cls(<br>            tools=tools or [],<br>            tool_retriever=tool_retriever,<br>            llm=llm,<br>            max_iterations=max_iterations,<br>            react_chat_formatter=react_chat_formatter,<br>            output_parser=output_parser,<br>            callback_manager=callback_manager,<br>            verbose=verbose,<br>            handle_reasoning_failure_fn=handle_reasoning_failure_fn,<br>        )<br>    def _get_prompts(self) -> PromptDictType:<br>        \"\"\"Get prompts.\"\"\"<br>        # TODO: the ReAct formatter does not explicitly specify PromptTemplate<br>        # objects, but wrap it in this to obey the interface<br>        sys_header = self._react_chat_formatter.system_header<br>        return {\"system_prompt\": PromptTemplate(sys_header)}<br>    def _update_prompts(self, prompts: PromptDictType) -> None:<br>        \"\"\"Update prompts.\"\"\"<br>        if \"system_prompt\" in prompts:<br>            sys_prompt = cast(PromptTemplate, prompts[\"system_prompt\"])<br>            self._react_chat_formatter.system_header = sys_prompt.template<br>    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>        \"\"\"Initialize step from task.\"\"\"<br>        sources: List[ToolOutput] = []<br>        current_reasoning: List[BaseReasoningStep] = []<br>        # temporary memory for new messages<br>        new_memory = ChatMemoryBuffer.from_defaults()<br>        # initialize task state<br>        task_state = {<br>            \"sources\": sources,<br>            \"current_reasoning\": current_reasoning,<br>            \"new_memory\": new_memory,<br>        }<br>        task.extra_state.update(task_state)<br>        return TaskStep(<br>            task_id=task.task_id,<br>            step_id=str(uuid.uuid4()),<br>            input=task.input,<br>            step_state={\"is_first\": True},<br>        )<br>    def get_tools(self, input: str) -> List[AsyncBaseTool]:<br>        \"\"\"Get tools.\"\"\"<br>        return [adapt_to_async_tool(t) for t in self._get_tools(input)]<br>    def _extract_reasoning_step(<br>        self, output: ChatResponse, is_streaming: bool = False<br>    ) -> Tuple[str, List[BaseReasoningStep], bool]:<br>        \"\"\"<br>        Extracts the reasoning step from the given output.<br>        This method parses the message content from the output,<br>        extracts the reasoning step, and determines whether the processing is<br>        complete. It also performs validation checks on the output and<br>        handles possible errors.<br>        \"\"\"<br>        if output.message.content is None:<br>            raise ValueError(\"Got empty message.\")<br>        message_content = output.message.content<br>        current_reasoning = []<br>        try:<br>            reasoning_step = self._output_parser.parse(message_content, is_streaming)<br>        except BaseException as exc:<br>            raise ValueError(f\"Could not parse output: {message_content}\") from exc<br>        if self._verbose:<br>            print_text(f\"{reasoning_step.get_content()}\\n\", color=\"pink\")<br>        current_reasoning.append(reasoning_step)<br>        if reasoning_step.is_done:<br>            return message_content, current_reasoning, True<br>        reasoning_step = cast(ActionReasoningStep, reasoning_step)<br>        if not isinstance(reasoning_step, ActionReasoningStep):<br>            raise ValueError(f\"Expected ActionReasoningStep, got {reasoning_step}\")<br>        return message_content, current_reasoning, False<br>    def _process_actions(<br>        self,<br>        task: Task,<br>        tools: Sequence[AsyncBaseTool],<br>        output: ChatResponse,<br>        is_streaming: bool = False,<br>    ) -> Tuple[List[BaseReasoningStep], bool]:<br>        tools_dict: Dict[str, AsyncBaseTool] = {<br>            tool.metadata.get_name(): tool for tool in tools<br>        }<br>        tool = None<br>        try:<br>            _, current_reasoning, is_done = self._extract_reasoning_step(<br>                output, is_streaming<br>            )<br>        except ValueError as exp:<br>            current_reasoning = []<br>            tool_output = self._handle_reasoning_failure_fn(self.callback_manager, exp)<br>        else:<br>            if is_done:<br>                return current_reasoning, True<br>            # call tool with input<br>            reasoning_step = cast(ActionReasoningStep, current_reasoning[-1])<br>            if reasoning_step.action in tools_dict:<br>                tool = tools_dict[reasoning_step.action]<br>                with self.callback_manager.event(<br>                    CBEventType.FUNCTION_CALL,<br>                    payload={<br>                        EventPayload.FUNCTION_CALL: reasoning_step.action_input,<br>                        EventPayload.TOOL: tool.metadata,<br>                    },<br>                ) as event:<br>                    try:<br>                        dispatcher.event(<br>                            AgentToolCallEvent(<br>                                arguments=json.dumps({**reasoning_step.action_input}),<br>                                tool=tool.metadata,<br>                            )<br>                        )<br>                        tool_output = tool.call(**reasoning_step.action_input)<br>                    except Exception as e:<br>                        tool_output = ToolOutput(<br>                            content=f\"Error: {e!s}\",<br>                            tool_name=tool.metadata.name,<br>                            raw_input={\"kwargs\": reasoning_step.action_input},<br>                            raw_output=e,<br>                            is_error=True,<br>                        )<br>                    event.on_end(<br>                        payload={EventPayload.FUNCTION_OUTPUT: str(tool_output)}<br>                    )<br>            else:<br>                tool_output = self._handle_nonexistent_tool_name(reasoning_step)<br>        task.extra_state[\"sources\"].append(tool_output)<br>        observation_step = ObservationReasoningStep(<br>            observation=str(tool_output),<br>            return_direct=(<br>                tool.metadata.return_direct and not tool_output.is_error<br>                if tool<br>                else False<br>            ),<br>        )<br>        current_reasoning.append(observation_step)<br>        if self._verbose:<br>            print_text(f\"{observation_step.get_content()}\\n\", color=\"blue\")<br>        return (<br>            current_reasoning,<br>            tool.metadata.return_direct and not tool_output.is_error if tool else False,<br>        )<br>    async def _aprocess_actions(<br>        self,<br>        task: Task,<br>        tools: Sequence[AsyncBaseTool],<br>        output: ChatResponse,<br>        is_streaming: bool = False,<br>    ) -> Tuple[List[BaseReasoningStep], bool]:<br>        tools_dict = {tool.metadata.name: tool for tool in tools}<br>        tool = None<br>        try:<br>            _, current_reasoning, is_done = self._extract_reasoning_step(<br>                output, is_streaming<br>            )<br>        except ValueError as exp:<br>            current_reasoning = []<br>            tool_output = self._handle_reasoning_failure_fn(self.callback_manager, exp)<br>        else:<br>            if is_done:<br>                return current_reasoning, True<br>            # call tool with input<br>            reasoning_step = cast(ActionReasoningStep, current_reasoning[-1])<br>            if reasoning_step.action in tools_dict:<br>                tool = tools_dict[reasoning_step.action]<br>                with self.callback_manager.event(<br>                    CBEventType.FUNCTION_CALL,<br>                    payload={<br>                        EventPayload.FUNCTION_CALL: reasoning_step.action_input,<br>                        EventPayload.TOOL: tool.metadata,<br>                    },<br>                ) as event:<br>                    try:<br>                        dispatcher.event(<br>                            AgentToolCallEvent(<br>                                arguments=json.dumps({**reasoning_step.action_input}),<br>                                tool=tool.metadata,<br>                            )<br>                        )<br>                        tool_output = await tool.acall(**reasoning_step.action_input)<br>                    except Exception as e:<br>                        tool_output = ToolOutput(<br>                            content=f\"Error: {e!s}\",<br>                            tool_name=tool.metadata.name,<br>                            raw_input={\"kwargs\": reasoning_step.action_input},<br>                            raw_output=e,<br>                            is_error=True,<br>                        )<br>                    event.on_end(<br>                        payload={EventPayload.FUNCTION_OUTPUT: str(tool_output)}<br>                    )<br>            else:<br>                tool_output = self._handle_nonexistent_tool_name(reasoning_step)<br>        task.extra_state[\"sources\"].append(tool_output)<br>        observation_step = ObservationReasoningStep(<br>            observation=str(tool_output),<br>            return_direct=(<br>                tool.metadata.return_direct and not tool_output.is_error<br>                if tool<br>                else False<br>            ),<br>        )<br>        current_reasoning.append(observation_step)<br>        if self._verbose:<br>            print_text(f\"{observation_step.get_content()}\\n\", color=\"blue\")<br>        return (<br>            current_reasoning,<br>            tool.metadata.return_direct and not tool_output.is_error if tool else False,<br>        )<br>    def _handle_nonexistent_tool_name(<br>        self, reasoning_step: ActionReasoningStep<br>    ) -> ToolOutput:<br>        # We still emit a `tool_output` object to the task, so that the LLM can know<br>        # it has hallucinated in the next reasoning step.<br>        with self.callback_manager.event(<br>            CBEventType.FUNCTION_CALL,<br>            payload={<br>                EventPayload.FUNCTION_CALL: reasoning_step.action_input,<br>            },<br>        ) as event:<br>            # TODO(L10N): This should be localized.<br>            content = f\"Error: No such tool named `{reasoning_step.action}`.\"<br>            tool_output = ToolOutput(<br>                content=content,<br>                tool_name=reasoning_step.action,<br>                raw_input={\"kwargs\": reasoning_step.action_input},<br>                raw_output=content,<br>                is_error=True,<br>            )<br>            event.on_end(payload={EventPayload.FUNCTION_OUTPUT: str(tool_output)})<br>        return tool_output<br>    def _get_response(<br>        self,<br>        current_reasoning: List[BaseReasoningStep],<br>        sources: List[ToolOutput],<br>    ) -> AgentChatResponse:<br>        \"\"\"Get response from reasoning steps.\"\"\"<br>        if len(current_reasoning) == 0:<br>            raise ValueError(\"No reasoning steps were taken.\")<br>        elif len(current_reasoning) == self._max_iterations:<br>            raise ValueError(\"Reached max iterations.\")<br>        if isinstance(current_reasoning[-1], ResponseReasoningStep):<br>            response_step = cast(ResponseReasoningStep, current_reasoning[-1])<br>            response_str = response_step.response<br>        elif (<br>            isinstance(current_reasoning[-1], ObservationReasoningStep)<br>            and current_reasoning[-1].return_direct<br>        ):<br>            response_str = current_reasoning[-1].observation<br>        else:<br>            response_str = current_reasoning[-1].get_content()<br>        # TODO: add sources from reasoning steps<br>        return AgentChatResponse(response=response_str, sources=sources)<br>    def _get_task_step_response(<br>        self, agent_response: AGENT_CHAT_RESPONSE_TYPE, step: TaskStep, is_done: bool<br>    ) -> TaskStepOutput:<br>        \"\"\"Get task step response.\"\"\"<br>        if is_done:<br>            new_steps = []<br>        else:<br>            new_steps = [<br>                step.get_next_step(<br>                    step_id=str(uuid.uuid4()),<br>                    # NOTE: input is unused<br>                    input=None,<br>                )<br>            ]<br>        return TaskStepOutput(<br>            output=agent_response,<br>            task_step=step,<br>            is_last=is_done,<br>            next_steps=new_steps,<br>        )<br>    def _infer_stream_chunk_is_final(<br>        self, chunk: ChatResponse, missed_chunks_storage: list<br>    ) -> bool:<br>        \"\"\"Infers if a chunk from a live stream is the start of the final<br>        reasoning step. (i.e., and should eventually become<br>        ResponseReasoningStep \u2014 not part of this function's logic tho.).<br>        Args:<br>            chunk (ChatResponse): the current chunk stream to check<br>            missed_chunks_storage (list): list to store missed chunks<br>        Returns:<br>            bool: Boolean on whether the chunk is the start of the final response<br>        \"\"\"<br>        latest_content = chunk.message.content<br>        if latest_content:<br>            # doesn't follow thought-action format<br>            # keep first chunks<br>            if len(latest_content) < len(\"Thought\"):<br>                missed_chunks_storage.append(chunk)<br>            elif not latest_content.startswith(\"Thought\"):<br>                return True<br>            elif \"Answer: \" in latest_content:<br>                missed_chunks_storage.clear()<br>                return True<br>        return False<br>    def _add_back_chunk_to_stream(<br>        self,<br>        chunks: List[ChatResponse],<br>        chat_stream: Generator[ChatResponse, None, None],<br>    ) -> Generator[ChatResponse, None, None]:<br>        \"\"\"Helper method for adding back initial chunk stream of final response<br>        back to the rest of the chat_stream.<br>        Args:<br>            chunks List[ChatResponse]: the chunks to add back to the beginning of the<br>                                    chat_stream.<br>        Return:<br>            Generator[ChatResponse, None, None]: the updated chat_stream<br>        \"\"\"<br>        def gen() -> Generator[ChatResponse, None, None]:<br>            yield from chunks<br>            yield from chat_stream<br>        return gen()<br>    async def _async_add_back_chunk_to_stream(<br>        self,<br>        chunks: List[ChatResponse],<br>        chat_stream: AsyncGenerator[ChatResponse, None],<br>    ) -> AsyncGenerator[ChatResponse, None]:<br>        \"\"\"Helper method for adding back initial chunk stream of final response<br>        back to the rest of the chat_stream.<br>        NOTE: this itself is not an async function.<br>        Args:<br>            chunks List[ChatResponse]: the chunks to add back to the beginning of the<br>                                    chat_stream.<br>        Return:<br>            AsyncGenerator[ChatResponse, None]: the updated async chat_stream<br>        \"\"\"<br>        for chunk in chunks:<br>            yield chunk<br>        async for item in chat_stream:<br>            yield item<br>    def _run_step(<br>        self,<br>        step: TaskStep,<br>        task: Task,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        if step.input is not None:<br>            add_user_step_to_reasoning(<br>                step,<br>                task.extra_state[\"new_memory\"],<br>                task.extra_state[\"current_reasoning\"],<br>                verbose=self._verbose,<br>            )<br>        # TODO: see if we want to do step-based inputs<br>        tools = self.get_tools(task.input)<br>        input_chat = self._react_chat_formatter.format(<br>            tools,<br>            chat_history=task.memory.get(input=task.input)<br>            + task.extra_state[\"new_memory\"].get_all(),<br>            current_reasoning=task.extra_state[\"current_reasoning\"],<br>        )<br>        # send prompt<br>        chat_response = self._llm.chat(input_chat)<br>        # given react prompt outputs, call tools or return response<br>        reasoning_steps, is_done = self._process_actions(<br>            task, tools, output=chat_response<br>        )<br>        task.extra_state[\"current_reasoning\"].extend(reasoning_steps)<br>        agent_response = self._get_response(<br>            task.extra_state[\"current_reasoning\"], task.extra_state[\"sources\"]<br>        )<br>        if is_done:<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(content=agent_response.response, role=MessageRole.ASSISTANT)<br>            )<br>        return self._get_task_step_response(agent_response, step, is_done)<br>    async def _arun_step(<br>        self,<br>        step: TaskStep,<br>        task: Task,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        if step.input is not None:<br>            add_user_step_to_reasoning(<br>                step,<br>                task.extra_state[\"new_memory\"],<br>                task.extra_state[\"current_reasoning\"],<br>                verbose=self._verbose,<br>            )<br>        # TODO: see if we want to do step-based inputs<br>        tools = self.get_tools(task.input)<br>        input_chat = self._react_chat_formatter.format(<br>            tools,<br>            chat_history=task.memory.get(input=task.input)<br>            + task.extra_state[\"new_memory\"].get_all(),<br>            current_reasoning=task.extra_state[\"current_reasoning\"],<br>        )<br>        # send prompt<br>        chat_response = await self._llm.achat(input_chat)<br>        # given react prompt outputs, call tools or return response<br>        reasoning_steps, is_done = await self._aprocess_actions(<br>            task, tools, output=chat_response<br>        )<br>        task.extra_state[\"current_reasoning\"].extend(reasoning_steps)<br>        agent_response = self._get_response(<br>            task.extra_state[\"current_reasoning\"], task.extra_state[\"sources\"]<br>        )<br>        if is_done:<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(content=agent_response.response, role=MessageRole.ASSISTANT)<br>            )<br>        return self._get_task_step_response(agent_response, step, is_done)<br>    def _run_step_stream(<br>        self,<br>        step: TaskStep,<br>        task: Task,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        if step.input is not None:<br>            add_user_step_to_reasoning(<br>                step,<br>                task.extra_state[\"new_memory\"],<br>                task.extra_state[\"current_reasoning\"],<br>                verbose=self._verbose,<br>            )<br>        # TODO: see if we want to do step-based inputs<br>        tools = self.get_tools(task.input)<br>        input_chat = self._react_chat_formatter.format(<br>            tools,<br>            chat_history=task.memory.get(input=task.input)<br>            + task.extra_state[\"new_memory\"].get_all(),<br>            current_reasoning=task.extra_state[\"current_reasoning\"],<br>        )<br>        chat_stream = self._llm.stream_chat(input_chat)<br>        # iterate over stream, break out if is final answer after the \"Answer: \"<br>        full_response = ChatResponse(<br>            message=ChatMessage(content=None, role=\"assistant\")<br>        )<br>        missed_chunks_storage: List[ChatResponse] = []<br>        is_done = False<br>        for latest_chunk in chat_stream:<br>            full_response = latest_chunk<br>            is_done = self._infer_stream_chunk_is_final(<br>                latest_chunk, missed_chunks_storage<br>            )<br>            if is_done:<br>                break<br>        non_streaming_agent_response = None<br>        agent_response_stream = None<br>        if not is_done:<br>            # given react prompt outputs, call tools or return response<br>            reasoning_steps, is_done = self._process_actions(<br>                task, tools=tools, output=full_response, is_streaming=True<br>            )<br>            task.extra_state[\"current_reasoning\"].extend(reasoning_steps)<br>            # use _get_response to return intermediate response<br>            non_streaming_agent_response = self._get_response(<br>                task.extra_state[\"current_reasoning\"], task.extra_state[\"sources\"]<br>            )<br>            if is_done:<br>                non_streaming_agent_response.is_dummy_stream = True<br>                task.extra_state[\"new_memory\"].put(<br>                    ChatMessage(<br>                        content=non_streaming_agent_response.response,<br>                        role=MessageRole.ASSISTANT,<br>                    )<br>                )<br>        else:<br>            # Get the response in a separate thread so we can yield the response<br>            response_stream = self._add_back_chunk_to_stream(<br>                chunks=[*missed_chunks_storage, latest_chunk], chat_stream=chat_stream<br>            )<br>            agent_response_stream = StreamingAgentChatResponse(<br>                chat_stream=response_stream,<br>                sources=task.extra_state[\"sources\"],<br>            )<br>            thread = Thread(<br>                target=agent_response_stream.write_response_to_history,<br>                args=(task.extra_state[\"new_memory\"],),<br>                kwargs={\"on_stream_end_fn\": partial(self.finalize_task, task)},<br>            )<br>            thread.start()<br>        response = agent_response_stream or non_streaming_agent_response<br>        assert response is not None<br>        return self._get_task_step_response(response, step, is_done)<br>    async def _arun_step_stream(<br>        self,<br>        step: TaskStep,<br>        task: Task,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        if step.input is not None:<br>            add_user_step_to_reasoning(<br>                step,<br>                task.extra_state[\"new_memory\"],<br>                task.extra_state[\"current_reasoning\"],<br>                verbose=self._verbose,<br>            )<br>        # TODO: see if we want to do step-based inputs<br>        tools = self.get_tools(task.input)<br>        input_chat = self._react_chat_formatter.format(<br>            tools,<br>            chat_history=task.memory.get(input=task.input)<br>            + task.extra_state[\"new_memory\"].get_all(),<br>            current_reasoning=task.extra_state[\"current_reasoning\"],<br>        )<br>        chat_stream = await self._llm.astream_chat(input_chat)<br>        # iterate over stream, break out if is final answer after the \"Answer: \"<br>        full_response = ChatResponse(<br>            message=ChatMessage(content=None, role=\"assistant\")<br>        )<br>        missed_chunks_storage: List[ChatResponse] = []<br>        is_done = False<br>        async for latest_chunk in chat_stream:<br>            full_response = latest_chunk<br>            is_done = self._infer_stream_chunk_is_final(<br>                latest_chunk, missed_chunks_storage<br>            )<br>            if is_done:<br>                break<br>        non_streaming_agent_response = None<br>        agent_response_stream = None<br>        if not is_done:<br>            # given react prompt outputs, call tools or return response<br>            reasoning_steps, is_done = await self._aprocess_actions(<br>                task, tools=tools, output=full_response, is_streaming=True<br>            )<br>            task.extra_state[\"current_reasoning\"].extend(reasoning_steps)<br>            # use _get_response to return intermediate response<br>            non_streaming_agent_response = self._get_response(<br>                task.extra_state[\"current_reasoning\"], task.extra_state[\"sources\"]<br>            )<br>            if is_done:<br>                non_streaming_agent_response.is_dummy_stream = True<br>                task.extra_state[\"new_memory\"].put(<br>                    ChatMessage(<br>                        content=non_streaming_agent_response.response,<br>                        role=MessageRole.ASSISTANT,<br>                    )<br>                )<br>        else:<br>            # Get the response in a separate thread so we can yield the response<br>            response_stream = self._async_add_back_chunk_to_stream(<br>                chunks=[*missed_chunks_storage, latest_chunk], chat_stream=chat_stream<br>            )<br>            agent_response_stream = StreamingAgentChatResponse(<br>                achat_stream=response_stream,<br>                sources=task.extra_state[\"sources\"],<br>            )<br>            # create task to write chat response to history<br>            asyncio.create_task(<br>                agent_response_stream.awrite_response_to_history(<br>                    task.extra_state[\"new_memory\"],<br>                    on_stream_end_fn=partial(self.finalize_task, task),<br>                )<br>            )<br>            # wait until response writing is done<br>            agent_response_stream._ensure_async_setup()<br>            assert agent_response_stream.is_function_false_event is not None<br>            await agent_response_stream.is_function_false_event.wait()<br>        response = agent_response_stream or non_streaming_agent_response<br>        assert response is not None<br>        return self._get_task_step_response(response, step, is_done)<br>    @trace_method(\"run_step\")<br>    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        return self._run_step(step, task)<br>    @trace_method(\"run_step\")<br>    async def arun_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        return await self._arun_step(step, task)<br>    @trace_method(\"run_step\")<br>    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        # TODO: figure out if we need a different type for TaskStepOutput<br>        return self._run_step_stream(step, task)<br>    @trace_method(\"run_step\")<br>    async def astream_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async stream).\"\"\"<br>        return await self._arun_step_stream(step, task)<br>    def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>        \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>        # add new messages to memory<br>        task.memory.set(<br>            task.memory.get_all() + task.extra_state[\"new_memory\"].get_all()<br>        )<br>        # reset new memory<br>        task.extra_state[\"new_memory\"].reset()<br>    def set_callback_manager(self, callback_manager: CallbackManager) -> None:<br>        \"\"\"Set callback manager.\"\"\"<br>        # TODO: make this abstractmethod (right now will break some agent impls)<br>        self.callback_manager = callback_manager<br>``` |\n\n### from\\_tools`classmethod`[\\#](\\#llama_index.core.agent.react.ReActAgentWorker.from_tools \"Permanent link\")\n\n```\nfrom_tools(tools: Optional[Sequence[BaseTool]] = None, tool_retriever: Optional[ObjectRetriever[BaseTool]] = None, llm: Optional[LLM] = None, max_iterations: int = 10, react_chat_formatter: Optional[ReActChatFormatter] = None, output_parser: Optional[ReActOutputParser] = None, callback_manager: Optional[CallbackManager] = None, verbose: bool = False, handle_reasoning_failure_fn: Optional[Callable[[CallbackManager, Exception], ToolOutput]] = None, **kwargs: Any) -> ReActAgentWorker\n\n```\n\nConvenience constructor method from set of BaseTools (Optional).\n\nNOTE: kwargs should have been exhausted by this point. In other words\nthe various upstream components such as BaseSynthesizer (response synthesizer)\nor BaseRetriever should have picked up off their respective kwargs in their\nconstructions.\n\n**Returns:**\n\n| Type | Description |\n| --- | --- |\n| `ReActAgentWorker` | ReActAgentWorker |\n\nSource code in `llama-index-core/llama_index/core/agent/react/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>``` | ```<br>@classmethod<br>def from_tools(<br>    cls,<br>    tools: Optional[Sequence[BaseTool]] = None,<br>    tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>    llm: Optional[LLM] = None,<br>    max_iterations: int = 10,<br>    react_chat_formatter: Optional[ReActChatFormatter] = None,<br>    output_parser: Optional[ReActOutputParser] = None,<br>    callback_manager: Optional[CallbackManager] = None,<br>    verbose: bool = False,<br>    handle_reasoning_failure_fn: Optional[<br>        Callable[[CallbackManager, Exception], ToolOutput]<br>    ] = None,<br>    **kwargs: Any,<br>) -> \"ReActAgentWorker\":<br>    \"\"\"Convenience constructor method from set of BaseTools (Optional).<br>    NOTE: kwargs should have been exhausted by this point. In other words<br>    the various upstream components such as BaseSynthesizer (response synthesizer)<br>    or BaseRetriever should have picked up off their respective kwargs in their<br>    constructions.<br>    Returns:<br>        ReActAgentWorker<br>    \"\"\"<br>    llm = llm or Settings.llm<br>    if callback_manager is not None:<br>        llm.callback_manager = callback_manager<br>    return cls(<br>        tools=tools or [],<br>        tool_retriever=tool_retriever,<br>        llm=llm,<br>        max_iterations=max_iterations,<br>        react_chat_formatter=react_chat_formatter,<br>        output_parser=output_parser,<br>        callback_manager=callback_manager,<br>        verbose=verbose,<br>        handle_reasoning_failure_fn=handle_reasoning_failure_fn,<br>    )<br>``` |\n\n### initialize\\_step [\\#](\\#llama_index.core.agent.react.ReActAgentWorker.initialize_step \"Permanent link\")\n\n```\ninitialize_step(task: Task, **kwargs: Any) -> TaskStep\n\n```\n\nInitialize step from task.\n\nSource code in `llama-index-core/llama_index/core/agent/react/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>``` | ```<br>def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>    \"\"\"Initialize step from task.\"\"\"<br>    sources: List[ToolOutput] = []<br>    current_reasoning: List[BaseReasoningStep] = []<br>    # temporary memory for new messages<br>    new_memory = ChatMemoryBuffer.from_defaults()<br>    # initialize task state<br>    task_state = {<br>        \"sources\": sources,<br>        \"current_reasoning\": current_reasoning,<br>        \"new_memory\": new_memory,<br>    }<br>    task.extra_state.update(task_state)<br>    return TaskStep(<br>        task_id=task.task_id,<br>        step_id=str(uuid.uuid4()),<br>        input=task.input,<br>        step_state={\"is_first\": True},<br>    )<br>``` |\n\n### get\\_tools [\\#](\\#llama_index.core.agent.react.ReActAgentWorker.get_tools \"Permanent link\")\n\n```\nget_tools(input: str) -> List[AsyncBaseTool]\n\n```\n\nGet tools.\n\nSource code in `llama-index-core/llama_index/core/agent/react/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>222<br>223<br>224<br>``` | ```<br>def get_tools(self, input: str) -> List[AsyncBaseTool]:<br>    \"\"\"Get tools.\"\"\"<br>    return [adapt_to_async_tool(t) for t in self._get_tools(input)]<br>``` |\n\n### run\\_step [\\#](\\#llama_index.core.agent.react.ReActAgentWorker.run_step \"Permanent link\")\n\n```\nrun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step.\n\nSource code in `llama-index-core/llama_index/core/agent/react/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>793<br>794<br>795<br>796<br>``` | ```<br>@trace_method(\"run_step\")<br>def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step.\"\"\"<br>    return self._run_step(step, task)<br>``` |\n\n### arun\\_step`async`[\\#](\\#llama_index.core.agent.react.ReActAgentWorker.arun_step \"Permanent link\")\n\n```\narun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async).\n\nSource code in `llama-index-core/llama_index/core/agent/react/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>798<br>799<br>800<br>801<br>802<br>803<br>``` | ```<br>@trace_method(\"run_step\")<br>async def arun_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async).\"\"\"<br>    return await self._arun_step(step, task)<br>``` |\n\n### stream\\_step [\\#](\\#llama_index.core.agent.react.ReActAgentWorker.stream_step \"Permanent link\")\n\n```\nstream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (stream).\n\nSource code in `llama-index-core/llama_index/core/agent/react/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>805<br>806<br>807<br>808<br>809<br>``` | ```<br>@trace_method(\"run_step\")<br>def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step (stream).\"\"\"<br>    # TODO: figure out if we need a different type for TaskStepOutput<br>    return self._run_step_stream(step, task)<br>``` |\n\n### astream\\_step`async`[\\#](\\#llama_index.core.agent.react.ReActAgentWorker.astream_step \"Permanent link\")\n\n```\nastream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async stream).\n\nSource code in `llama-index-core/llama_index/core/agent/react/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>811<br>812<br>813<br>814<br>815<br>816<br>``` | ```<br>@trace_method(\"run_step\")<br>async def astream_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async stream).\"\"\"<br>    return await self._arun_step_stream(step, task)<br>``` |\n\n### finalize\\_task [\\#](\\#llama_index.core.agent.react.ReActAgentWorker.finalize_task \"Permanent link\")\n\n```\nfinalize_task(task: Task, **kwargs: Any) -> None\n\n```\n\nFinalize task, after all the steps are completed.\n\nSource code in `llama-index-core/llama_index/core/agent/react/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>818<br>819<br>820<br>821<br>822<br>823<br>824<br>825<br>``` | ```<br>def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>    \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>    # add new messages to memory<br>    task.memory.set(<br>        task.memory.get_all() + task.extra_state[\"new_memory\"].get_all()<br>    )<br>    # reset new memory<br>    task.extra_state[\"new_memory\"].reset()<br>``` |\n\n### set\\_callback\\_manager [\\#](\\#llama_index.core.agent.react.ReActAgentWorker.set_callback_manager \"Permanent link\")\n\n```\nset_callback_manager(callback_manager: CallbackManager) -> None\n\n```\n\nSet callback manager.\n\nSource code in `llama-index-core/llama_index/core/agent/react/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>827<br>828<br>829<br>830<br>``` | ```<br>def set_callback_manager(self, callback_manager: CallbackManager) -> None:<br>    \"\"\"Set callback manager.\"\"\"<br>    # TODO: make this abstractmethod (right now will break some agent impls)<br>    self.callback_manager = callback_manager<br>``` |\n\n## ReActChatFormatter [\\#](\\#llama_index.core.agent.react.ReActChatFormatter \"Permanent link\")\n\nBases: `BaseAgentChatFormatter`\n\nReAct chat formatter.\n\nSource code in `llama-index-core/llama_index/core/agent/react/formatter.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>``` | ```<br>class ReActChatFormatter(BaseAgentChatFormatter):<br>    \"\"\"ReAct chat formatter.\"\"\"<br>    system_header: str = REACT_CHAT_SYSTEM_HEADER  # default<br>    context: str = \"\"  # not needed w/ default<br>    def format(<br>        self,<br>        tools: Sequence[BaseTool],<br>        chat_history: List[ChatMessage],<br>        current_reasoning: Optional[List[BaseReasoningStep]] = None,<br>    ) -> List[ChatMessage]:<br>        \"\"\"Format chat history into list of ChatMessage.\"\"\"<br>        current_reasoning = current_reasoning or []<br>        format_args = {<br>            \"tool_desc\": \"\\n\".join(get_react_tool_descriptions(tools)),<br>            \"tool_names\": \", \".join([tool.metadata.get_name() for tool in tools]),<br>        }<br>        if self.context:<br>            format_args[\"context\"] = self.context<br>        fmt_sys_header = self.system_header.format(**format_args)<br>        # format reasoning history as alternating user and assistant messages<br>        # where the assistant messages are thoughts and actions and the user<br>        # messages are observations<br>        reasoning_history = []<br>        for reasoning_step in current_reasoning:<br>            if isinstance(reasoning_step, ObservationReasoningStep):<br>                message = ChatMessage(<br>                    role=MessageRole.USER,<br>                    content=reasoning_step.get_content(),<br>                )<br>            else:<br>                message = ChatMessage(<br>                    role=MessageRole.ASSISTANT,<br>                    content=reasoning_step.get_content(),<br>                )<br>            reasoning_history.append(message)<br>        return [<br>            ChatMessage(role=MessageRole.SYSTEM, content=fmt_sys_header),<br>            *chat_history,<br>            *reasoning_history,<br>        ]<br>    @classmethod<br>    def from_defaults(<br>        cls,<br>        system_header: Optional[str] = None,<br>        context: Optional[str] = None,<br>    ) -> \"ReActChatFormatter\":<br>        \"\"\"Create ReActChatFormatter from defaults.\"\"\"<br>        if not system_header:<br>            system_header = (<br>                REACT_CHAT_SYSTEM_HEADER<br>                if not context<br>                else CONTEXT_REACT_CHAT_SYSTEM_HEADER<br>            )<br>        return ReActChatFormatter(<br>            system_header=system_header,<br>            context=context or \"\",<br>        )<br>    @classmethod<br>    def from_context(cls, context: str) -> \"ReActChatFormatter\":<br>        \"\"\"Create ReActChatFormatter from context.<br>        NOTE: deprecated<br>        \"\"\"<br>        logger.warning(<br>            \"ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\"<br>        )<br>        return ReActChatFormatter.from_defaults(<br>            system_header=CONTEXT_REACT_CHAT_SYSTEM_HEADER, context=context<br>        )<br>``` |\n\n### format [\\#](\\#llama_index.core.agent.react.ReActChatFormatter.format \"Permanent link\")\n\n```\nformat(tools: Sequence[BaseTool], chat_history: List[ChatMessage], current_reasoning: Optional[List[BaseReasoningStep]] = None) -> List[ChatMessage]\n\n```\n\nFormat chat history into list of ChatMessage.\n\nSource code in `llama-index-core/llama_index/core/agent/react/formatter.py`\n\n|     |     |\n| --- | --- |\n| ```<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>89<br>90<br>91<br>92<br>93<br>94<br>95<br>96<br>``` | ```<br>def format(<br>    self,<br>    tools: Sequence[BaseTool],<br>    chat_history: List[ChatMessage],<br>    current_reasoning: Optional[List[BaseReasoningStep]] = None,<br>) -> List[ChatMessage]:<br>    \"\"\"Format chat history into list of ChatMessage.\"\"\"<br>    current_reasoning = current_reasoning or []<br>    format_args = {<br>        \"tool_desc\": \"\\n\".join(get_react_tool_descriptions(tools)),<br>        \"tool_names\": \", \".join([tool.metadata.get_name() for tool in tools]),<br>    }<br>    if self.context:<br>        format_args[\"context\"] = self.context<br>    fmt_sys_header = self.system_header.format(**format_args)<br>    # format reasoning history as alternating user and assistant messages<br>    # where the assistant messages are thoughts and actions and the user<br>    # messages are observations<br>    reasoning_history = []<br>    for reasoning_step in current_reasoning:<br>        if isinstance(reasoning_step, ObservationReasoningStep):<br>            message = ChatMessage(<br>                role=MessageRole.USER,<br>                content=reasoning_step.get_content(),<br>            )<br>        else:<br>            message = ChatMessage(<br>                role=MessageRole.ASSISTANT,<br>                content=reasoning_step.get_content(),<br>            )<br>        reasoning_history.append(message)<br>    return [<br>        ChatMessage(role=MessageRole.SYSTEM, content=fmt_sys_header),<br>        *chat_history,<br>        *reasoning_history,<br>    ]<br>``` |\n\n### from\\_defaults`classmethod`[\\#](\\#llama_index.core.agent.react.ReActChatFormatter.from_defaults \"Permanent link\")\n\n```\nfrom_defaults(system_header: Optional[str] = None, context: Optional[str] = None) -> ReActChatFormatter\n\n```\n\nCreate ReActChatFormatter from defaults.\n\nSource code in `llama-index-core/llama_index/core/agent/react/formatter.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>``` | ```<br>@classmethod<br>def from_defaults(<br>    cls,<br>    system_header: Optional[str] = None,<br>    context: Optional[str] = None,<br>) -> \"ReActChatFormatter\":<br>    \"\"\"Create ReActChatFormatter from defaults.\"\"\"<br>    if not system_header:<br>        system_header = (<br>            REACT_CHAT_SYSTEM_HEADER<br>            if not context<br>            else CONTEXT_REACT_CHAT_SYSTEM_HEADER<br>        )<br>    return ReActChatFormatter(<br>        system_header=system_header,<br>        context=context or \"\",<br>    )<br>``` |\n\n### from\\_context`classmethod`[\\#](\\#llama_index.core.agent.react.ReActChatFormatter.from_context \"Permanent link\")\n\n```\nfrom_context(context: str) -> ReActChatFormatter\n\n```\n\nCreate ReActChatFormatter from context.\n\nNOTE: deprecated\n\nSource code in `llama-index-core/llama_index/core/agent/react/formatter.py`\n\n|     |     |\n| --- | --- |\n| ```<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>``` | ```<br>@classmethod<br>def from_context(cls, context: str) -> \"ReActChatFormatter\":<br>    \"\"\"Create ReActChatFormatter from context.<br>    NOTE: deprecated<br>    \"\"\"<br>    logger.warning(<br>        \"ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\"<br>    )<br>    return ReActChatFormatter.from_defaults(<br>        system_header=CONTEXT_REACT_CHAT_SYSTEM_HEADER, context=context<br>    )<br>``` |\n\nBack to top",
      "metadata": {
        "title": "React - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/agent/react/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/#core-callback-classes)\n\n# Core Callback Classes [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#core-callback-classes \"Permanent link\")\n\n## CallbackManager [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.base.CallbackManager \"Permanent link\")\n\nBases: `BaseCallbackHandler`, `ABC`\n\nCallback manager that handles callbacks for events within LlamaIndex.\n\nThe callback manager provides a way to call handlers on event starts/ends.\n\nAdditionally, the callback manager traces the current stack of events.\nIt does this by using a few key attributes.\n\\- trace\\_stack - The current stack of events that have not ended yet.\nWhen an event ends, it's removed from the stack.\nSince this is a contextvar, it is unique to each\nthread/task.\n\\- trace\\_map - A mapping of event ids to their children events.\nOn the start of events, the bottom of the trace stack\nis used as the current parent event for the trace map.\n\\- trace\\_id - A simple name for the current trace, usually denoting the\nentrypoint (query, index\\_construction, insert, etc.)\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `handlers` | `List[BaseCallbackHandler]` | list of handlers to use. | `None` |\n\nUsage\n\nwith callback\\_manager.event(CBEventType.QUERY) as event:\nevent.on\\_start(payload={key, val})\n...\nevent.on\\_end(payload={key, val})\n\nSource code in `llama-index-core/llama_index/core/callbacks/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>``` | ```<br>class CallbackManager(BaseCallbackHandler, ABC):<br>    \"\"\"<br>    Callback manager that handles callbacks for events within LlamaIndex.<br>    The callback manager provides a way to call handlers on event starts/ends.<br>    Additionally, the callback manager traces the current stack of events.<br>    It does this by using a few key attributes.<br>    - trace_stack - The current stack of events that have not ended yet.<br>                    When an event ends, it's removed from the stack.<br>                    Since this is a contextvar, it is unique to each<br>                    thread/task.<br>    - trace_map - A mapping of event ids to their children events.<br>                  On the start of events, the bottom of the trace stack<br>                  is used as the current parent event for the trace map.<br>    - trace_id - A simple name for the current trace, usually denoting the<br>                 entrypoint (query, index_construction, insert, etc.)<br>    Args:<br>        handlers (List[BaseCallbackHandler]): list of handlers to use.<br>    Usage:<br>        with callback_manager.event(CBEventType.QUERY) as event:<br>            event.on_start(payload={key, val})<br>            ...<br>            event.on_end(payload={key, val})<br>    \"\"\"<br>    def __init__(self, handlers: Optional[List[BaseCallbackHandler]] = None):<br>        \"\"\"Initialize the manager with a list of handlers.\"\"\"<br>        from llama_index.core import global_handler<br>        handlers = handlers or []<br>        # add eval handlers based on global defaults<br>        if global_handler is not None:<br>            new_handler = global_handler<br>            # go through existing handlers, check if any are same type as new handler<br>            # if so, error<br>            for existing_handler in handlers:<br>                if isinstance(existing_handler, type(new_handler)):<br>                    raise ValueError(<br>                        \"Cannot add two handlers of the same type \"<br>                        f\"{type(new_handler)} to the callback manager.\"<br>                    )<br>            handlers.append(new_handler)<br>        # if we passed in no handlers, use the global default<br>        if len(handlers) == 0:<br>            from llama_index.core.settings import Settings<br>            # hidden var access to prevent recursion in getter<br>            cb_manager = Settings._callback_manager<br>            if cb_manager is not None:<br>                handlers = cb_manager.handlers<br>        self.handlers: List[BaseCallbackHandler] = handlers<br>        self._trace_map: Dict[str, List[str]] = defaultdict(list)<br>    def on_event_start(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: Optional[str] = None,<br>        parent_id: Optional[str] = None,<br>        **kwargs: Any,<br>    ) -> str:<br>        \"\"\"Run handlers when an event starts and return id of event.\"\"\"<br>        event_id = event_id or str(uuid.uuid4())<br>        # if no trace is running, start a default trace<br>        try:<br>            parent_id = parent_id or global_stack_trace.get()[-1]<br>        except IndexError:<br>            self.start_trace(\"llama-index\")<br>            parent_id = global_stack_trace.get()[-1]<br>        parent_id = cast(str, parent_id)<br>        self._trace_map[parent_id].append(event_id)<br>        for handler in self.handlers:<br>            if event_type not in handler.event_starts_to_ignore:<br>                handler.on_event_start(<br>                    event_type,<br>                    payload,<br>                    event_id=event_id,<br>                    parent_id=parent_id,<br>                    **kwargs,<br>                )<br>        if event_type not in LEAF_EVENTS:<br>            # copy the stack trace to prevent conflicts with threads/coroutines<br>            current_trace_stack = global_stack_trace.get().copy()<br>            current_trace_stack.append(event_id)<br>            global_stack_trace.set(current_trace_stack)<br>        return event_id<br>    def on_event_end(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: Optional[str] = None,<br>        **kwargs: Any,<br>    ) -> None:<br>        \"\"\"Run handlers when an event ends.\"\"\"<br>        event_id = event_id or str(uuid.uuid4())<br>        for handler in self.handlers:<br>            if event_type not in handler.event_ends_to_ignore:<br>                handler.on_event_end(event_type, payload, event_id=event_id, **kwargs)<br>        if event_type not in LEAF_EVENTS:<br>            # copy the stack trace to prevent conflicts with threads/coroutines<br>            current_trace_stack = global_stack_trace.get().copy()<br>            current_trace_stack.pop()<br>            global_stack_trace.set(current_trace_stack)<br>    def add_handler(self, handler: BaseCallbackHandler) -> None:<br>        \"\"\"Add a handler to the callback manager.\"\"\"<br>        self.handlers.append(handler)<br>    def remove_handler(self, handler: BaseCallbackHandler) -> None:<br>        \"\"\"Remove a handler from the callback manager.\"\"\"<br>        self.handlers.remove(handler)<br>    def set_handlers(self, handlers: List[BaseCallbackHandler]) -> None:<br>        \"\"\"Set handlers as the only handlers on the callback manager.\"\"\"<br>        self.handlers = handlers<br>    @contextmanager<br>    def event(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: Optional[str] = None,<br>    ) -> Generator[\"EventContext\", None, None]:<br>        \"\"\"Context manager for lanching and shutdown of events.<br>        Handles sending on_evnt_start and on_event_end to handlers for specified event.<br>        Usage:<br>            with callback_manager.event(CBEventType.QUERY, payload={key, val}) as event:<br>                ...<br>                event.on_end(payload={key, val})  # optional<br>        \"\"\"<br>        # create event context wrapper<br>        event = EventContext(self, event_type, event_id=event_id)<br>        event.on_start(payload=payload)<br>        payload = None<br>        try:<br>            yield event<br>        except Exception as e:<br>            # data already logged to trace?<br>            if not hasattr(e, \"event_added\"):<br>                payload = {EventPayload.EXCEPTION: e}<br>                e.event_added = True  # type: ignore<br>                if not event.finished:<br>                    event.on_end(payload=payload)<br>            raise<br>        finally:<br>            # ensure event is ended<br>            if not event.finished:<br>                event.on_end(payload=payload)<br>    @contextmanager<br>    def as_trace(self, trace_id: str) -> Generator[None, None, None]:<br>        \"\"\"Context manager tracer for lanching and shutdown of traces.\"\"\"<br>        self.start_trace(trace_id=trace_id)<br>        try:<br>            yield<br>        except Exception as e:<br>            # event already added to trace?<br>            if not hasattr(e, \"event_added\"):<br>                self.on_event_start(<br>                    CBEventType.EXCEPTION, payload={EventPayload.EXCEPTION: e}<br>                )<br>                e.event_added = True  # type: ignore<br>            raise<br>        finally:<br>            # ensure trace is ended<br>            self.end_trace(trace_id=trace_id)<br>    def start_trace(self, trace_id: Optional[str] = None) -> None:<br>        \"\"\"Run when an overall trace is launched.\"\"\"<br>        current_trace_stack_ids = global_stack_trace_ids.get().copy()<br>        if trace_id is not None:<br>            if len(current_trace_stack_ids) == 0:<br>                self._reset_trace_events()<br>                for handler in self.handlers:<br>                    handler.start_trace(trace_id=trace_id)<br>                current_trace_stack_ids = [trace_id]<br>            else:<br>                current_trace_stack_ids.append(trace_id)<br>        global_stack_trace_ids.set(current_trace_stack_ids)<br>    def end_trace(<br>        self,<br>        trace_id: Optional[str] = None,<br>        trace_map: Optional[Dict[str, List[str]]] = None,<br>    ) -> None:<br>        \"\"\"Run when an overall trace is exited.\"\"\"<br>        current_trace_stack_ids = global_stack_trace_ids.get().copy()<br>        if trace_id is not None and len(current_trace_stack_ids) > 0:<br>            current_trace_stack_ids.pop()<br>            if len(current_trace_stack_ids) == 0:<br>                for handler in self.handlers:<br>                    handler.end_trace(trace_id=trace_id, trace_map=self._trace_map)<br>                current_trace_stack_ids = []<br>        global_stack_trace_ids.set(current_trace_stack_ids)<br>    def _reset_trace_events(self) -> None:<br>        \"\"\"Helper function to reset the current trace.\"\"\"<br>        self._trace_map = defaultdict(list)<br>        global_stack_trace.set([BASE_TRACE_EVENT])<br>    @property<br>    def trace_map(self) -> Dict[str, List[str]]:<br>        return self._trace_map<br>    @classmethod<br>    def __get_pydantic_core_schema__(<br>        cls, source: Type[Any], handler: GetCoreSchemaHandler<br>    ) -> CoreSchema:<br>        return core_schema.any_schema()<br>    @classmethod<br>    def __get_pydantic_json_schema__(<br>        cls, core_schema: CoreSchema, handler: GetJsonSchemaHandler<br>    ) -> Dict[str, Any]:<br>        json_schema = handler(core_schema)<br>        return handler.resolve_ref_schema(json_schema)<br>``` |\n\n### on\\_event\\_start [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.base.CallbackManager.on_event_start \"Permanent link\")\n\n```\non_event_start(event_type: CBEventType, payload: Optional[Dict[str, Any]] = None, event_id: Optional[str] = None, parent_id: Optional[str] = None, **kwargs: Any) -> str\n\n```\n\nRun handlers when an event starts and return id of event.\n\nSource code in `llama-index-core/llama_index/core/callbacks/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>``` | ```<br>def on_event_start(<br>    self,<br>    event_type: CBEventType,<br>    payload: Optional[Dict[str, Any]] = None,<br>    event_id: Optional[str] = None,<br>    parent_id: Optional[str] = None,<br>    **kwargs: Any,<br>) -> str:<br>    \"\"\"Run handlers when an event starts and return id of event.\"\"\"<br>    event_id = event_id or str(uuid.uuid4())<br>    # if no trace is running, start a default trace<br>    try:<br>        parent_id = parent_id or global_stack_trace.get()[-1]<br>    except IndexError:<br>        self.start_trace(\"llama-index\")<br>        parent_id = global_stack_trace.get()[-1]<br>    parent_id = cast(str, parent_id)<br>    self._trace_map[parent_id].append(event_id)<br>    for handler in self.handlers:<br>        if event_type not in handler.event_starts_to_ignore:<br>            handler.on_event_start(<br>                event_type,<br>                payload,<br>                event_id=event_id,<br>                parent_id=parent_id,<br>                **kwargs,<br>            )<br>    if event_type not in LEAF_EVENTS:<br>        # copy the stack trace to prevent conflicts with threads/coroutines<br>        current_trace_stack = global_stack_trace.get().copy()<br>        current_trace_stack.append(event_id)<br>        global_stack_trace.set(current_trace_stack)<br>    return event_id<br>``` |\n\n### on\\_event\\_end [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.base.CallbackManager.on_event_end \"Permanent link\")\n\n```\non_event_end(event_type: CBEventType, payload: Optional[Dict[str, Any]] = None, event_id: Optional[str] = None, **kwargs: Any) -> None\n\n```\n\nRun handlers when an event ends.\n\nSource code in `llama-index-core/llama_index/core/callbacks/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>``` | ```<br>def on_event_end(<br>    self,<br>    event_type: CBEventType,<br>    payload: Optional[Dict[str, Any]] = None,<br>    event_id: Optional[str] = None,<br>    **kwargs: Any,<br>) -> None:<br>    \"\"\"Run handlers when an event ends.\"\"\"<br>    event_id = event_id or str(uuid.uuid4())<br>    for handler in self.handlers:<br>        if event_type not in handler.event_ends_to_ignore:<br>            handler.on_event_end(event_type, payload, event_id=event_id, **kwargs)<br>    if event_type not in LEAF_EVENTS:<br>        # copy the stack trace to prevent conflicts with threads/coroutines<br>        current_trace_stack = global_stack_trace.get().copy()<br>        current_trace_stack.pop()<br>        global_stack_trace.set(current_trace_stack)<br>``` |\n\n### add\\_handler [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.base.CallbackManager.add_handler \"Permanent link\")\n\n```\nadd_handler(handler: BaseCallbackHandler) -> None\n\n```\n\nAdd a handler to the callback manager.\n\nSource code in `llama-index-core/llama_index/core/callbacks/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>144<br>145<br>146<br>``` | ```<br>def add_handler(self, handler: BaseCallbackHandler) -> None:<br>    \"\"\"Add a handler to the callback manager.\"\"\"<br>    self.handlers.append(handler)<br>``` |\n\n### remove\\_handler [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.base.CallbackManager.remove_handler \"Permanent link\")\n\n```\nremove_handler(handler: BaseCallbackHandler) -> None\n\n```\n\nRemove a handler from the callback manager.\n\nSource code in `llama-index-core/llama_index/core/callbacks/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>148<br>149<br>150<br>``` | ```<br>def remove_handler(self, handler: BaseCallbackHandler) -> None:<br>    \"\"\"Remove a handler from the callback manager.\"\"\"<br>    self.handlers.remove(handler)<br>``` |\n\n### set\\_handlers [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.base.CallbackManager.set_handlers \"Permanent link\")\n\n```\nset_handlers(handlers: List[BaseCallbackHandler]) -> None\n\n```\n\nSet handlers as the only handlers on the callback manager.\n\nSource code in `llama-index-core/llama_index/core/callbacks/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>152<br>153<br>154<br>``` | ```<br>def set_handlers(self, handlers: List[BaseCallbackHandler]) -> None:<br>    \"\"\"Set handlers as the only handlers on the callback manager.\"\"\"<br>    self.handlers = handlers<br>``` |\n\n### event [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.base.CallbackManager.event \"Permanent link\")\n\n```\nevent(event_type: CBEventType, payload: Optional[Dict[str, Any]] = None, event_id: Optional[str] = None) -> Generator[EventContext, None, None]\n\n```\n\nContext manager for lanching and shutdown of events.\n\nHandles sending on\\_evnt\\_start and on\\_event\\_end to handlers for specified event.\n\nUsage\n\nwith callback\\_manager.event(CBEventType.QUERY, payload={key, val}) as event:\n...\nevent.on\\_end(payload={key, val}) # optional\n\nSource code in `llama-index-core/llama_index/core/callbacks/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>``` | ```<br>@contextmanager<br>def event(<br>    self,<br>    event_type: CBEventType,<br>    payload: Optional[Dict[str, Any]] = None,<br>    event_id: Optional[str] = None,<br>) -> Generator[\"EventContext\", None, None]:<br>    \"\"\"Context manager for lanching and shutdown of events.<br>    Handles sending on_evnt_start and on_event_end to handlers for specified event.<br>    Usage:<br>        with callback_manager.event(CBEventType.QUERY, payload={key, val}) as event:<br>            ...<br>            event.on_end(payload={key, val})  # optional<br>    \"\"\"<br>    # create event context wrapper<br>    event = EventContext(self, event_type, event_id=event_id)<br>    event.on_start(payload=payload)<br>    payload = None<br>    try:<br>        yield event<br>    except Exception as e:<br>        # data already logged to trace?<br>        if not hasattr(e, \"event_added\"):<br>            payload = {EventPayload.EXCEPTION: e}<br>            e.event_added = True  # type: ignore<br>            if not event.finished:<br>                event.on_end(payload=payload)<br>        raise<br>    finally:<br>        # ensure event is ended<br>        if not event.finished:<br>            event.on_end(payload=payload)<br>``` |\n\n### as\\_trace [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.base.CallbackManager.as_trace \"Permanent link\")\n\n```\nas_trace(trace_id: str) -> Generator[None, None, None]\n\n```\n\nContext manager tracer for lanching and shutdown of traces.\n\nSource code in `llama-index-core/llama_index/core/callbacks/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>``` | ```<br>@contextmanager<br>def as_trace(self, trace_id: str) -> Generator[None, None, None]:<br>    \"\"\"Context manager tracer for lanching and shutdown of traces.\"\"\"<br>    self.start_trace(trace_id=trace_id)<br>    try:<br>        yield<br>    except Exception as e:<br>        # event already added to trace?<br>        if not hasattr(e, \"event_added\"):<br>            self.on_event_start(<br>                CBEventType.EXCEPTION, payload={EventPayload.EXCEPTION: e}<br>            )<br>            e.event_added = True  # type: ignore<br>        raise<br>    finally:<br>        # ensure trace is ended<br>        self.end_trace(trace_id=trace_id)<br>``` |\n\n### start\\_trace [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.base.CallbackManager.start_trace \"Permanent link\")\n\n```\nstart_trace(trace_id: Optional[str] = None) -> None\n\n```\n\nRun when an overall trace is launched.\n\nSource code in `llama-index-core/llama_index/core/callbacks/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>``` | ```<br>def start_trace(self, trace_id: Optional[str] = None) -> None:<br>    \"\"\"Run when an overall trace is launched.\"\"\"<br>    current_trace_stack_ids = global_stack_trace_ids.get().copy()<br>    if trace_id is not None:<br>        if len(current_trace_stack_ids) == 0:<br>            self._reset_trace_events()<br>            for handler in self.handlers:<br>                handler.start_trace(trace_id=trace_id)<br>            current_trace_stack_ids = [trace_id]<br>        else:<br>            current_trace_stack_ids.append(trace_id)<br>    global_stack_trace_ids.set(current_trace_stack_ids)<br>``` |\n\n### end\\_trace [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.base.CallbackManager.end_trace \"Permanent link\")\n\n```\nend_trace(trace_id: Optional[str] = None, trace_map: Optional[Dict[str, List[str]]] = None) -> None\n\n```\n\nRun when an overall trace is exited.\n\nSource code in `llama-index-core/llama_index/core/callbacks/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>``` | ```<br>def end_trace(<br>    self,<br>    trace_id: Optional[str] = None,<br>    trace_map: Optional[Dict[str, List[str]]] = None,<br>) -> None:<br>    \"\"\"Run when an overall trace is exited.\"\"\"<br>    current_trace_stack_ids = global_stack_trace_ids.get().copy()<br>    if trace_id is not None and len(current_trace_stack_ids) > 0:<br>        current_trace_stack_ids.pop()<br>        if len(current_trace_stack_ids) == 0:<br>            for handler in self.handlers:<br>                handler.end_trace(trace_id=trace_id, trace_map=self._trace_map)<br>            current_trace_stack_ids = []<br>    global_stack_trace_ids.set(current_trace_stack_ids)<br>``` |\n\n## BaseCallbackHandler [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.base_handler.BaseCallbackHandler \"Permanent link\")\n\nBases: `ABC`\n\nBase callback handler that can be used to track event starts and ends.\n\nSource code in `llama-index-core/llama_index/core/callbacks/base_handler.py`\n\n|     |     |\n| --- | --- |\n| ```<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>``` | ```<br>class BaseCallbackHandler(ABC):<br>    \"\"\"Base callback handler that can be used to track event starts and ends.\"\"\"<br>    def __init__(<br>        self,<br>        event_starts_to_ignore: List[CBEventType],<br>        event_ends_to_ignore: List[CBEventType],<br>    ) -> None:<br>        \"\"\"Initialize the base callback handler.\"\"\"<br>        self.event_starts_to_ignore = tuple(event_starts_to_ignore)<br>        self.event_ends_to_ignore = tuple(event_ends_to_ignore)<br>    @abstractmethod<br>    def on_event_start(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        parent_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> str:<br>        \"\"\"Run when an event starts and return id of event.\"\"\"<br>    @abstractmethod<br>    def on_event_end(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> None:<br>        \"\"\"Run when an event ends.\"\"\"<br>    @abstractmethod<br>    def start_trace(self, trace_id: Optional[str] = None) -> None:<br>        \"\"\"Run when an overall trace is launched.\"\"\"<br>    @abstractmethod<br>    def end_trace(<br>        self,<br>        trace_id: Optional[str] = None,<br>        trace_map: Optional[Dict[str, List[str]]] = None,<br>    ) -> None:<br>        \"\"\"Run when an overall trace is exited.\"\"\"<br>``` |\n\n### on\\_event\\_start`abstractmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.base_handler.BaseCallbackHandler.on_event_start \"Permanent link\")\n\n```\non_event_start(event_type: CBEventType, payload: Optional[Dict[str, Any]] = None, event_id: str = '', parent_id: str = '', **kwargs: Any) -> str\n\n```\n\nRun when an event starts and return id of event.\n\nSource code in `llama-index-core/llama_index/core/callbacks/base_handler.py`\n\n|     |     |\n| --- | --- |\n| ```<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>``` | ```<br>@abstractmethod<br>def on_event_start(<br>    self,<br>    event_type: CBEventType,<br>    payload: Optional[Dict[str, Any]] = None,<br>    event_id: str = \"\",<br>    parent_id: str = \"\",<br>    **kwargs: Any,<br>) -> str:<br>    \"\"\"Run when an event starts and return id of event.\"\"\"<br>``` |\n\n### on\\_event\\_end`abstractmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.base_handler.BaseCallbackHandler.on_event_end \"Permanent link\")\n\n```\non_event_end(event_type: CBEventType, payload: Optional[Dict[str, Any]] = None, event_id: str = '', **kwargs: Any) -> None\n\n```\n\nRun when an event ends.\n\nSource code in `llama-index-core/llama_index/core/callbacks/base_handler.py`\n\n|     |     |\n| --- | --- |\n| ```<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>``` | ```<br>@abstractmethod<br>def on_event_end(<br>    self,<br>    event_type: CBEventType,<br>    payload: Optional[Dict[str, Any]] = None,<br>    event_id: str = \"\",<br>    **kwargs: Any,<br>) -> None:<br>    \"\"\"Run when an event ends.\"\"\"<br>``` |\n\n### start\\_trace`abstractmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.base_handler.BaseCallbackHandler.start_trace \"Permanent link\")\n\n```\nstart_trace(trace_id: Optional[str] = None) -> None\n\n```\n\nRun when an overall trace is launched.\n\nSource code in `llama-index-core/llama_index/core/callbacks/base_handler.py`\n\n|     |     |\n| --- | --- |\n| ```<br>45<br>46<br>47<br>``` | ```<br>@abstractmethod<br>def start_trace(self, trace_id: Optional[str] = None) -> None:<br>    \"\"\"Run when an overall trace is launched.\"\"\"<br>``` |\n\n### end\\_trace`abstractmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.base_handler.BaseCallbackHandler.end_trace \"Permanent link\")\n\n```\nend_trace(trace_id: Optional[str] = None, trace_map: Optional[Dict[str, List[str]]] = None) -> None\n\n```\n\nRun when an overall trace is exited.\n\nSource code in `llama-index-core/llama_index/core/callbacks/base_handler.py`\n\n|     |     |\n| --- | --- |\n| ```<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>``` | ```<br>@abstractmethod<br>def end_trace(<br>    self,<br>    trace_id: Optional[str] = None,<br>    trace_map: Optional[Dict[str, List[str]]] = None,<br>) -> None:<br>    \"\"\"Run when an overall trace is exited.\"\"\"<br>``` |\n\nBase schema for callback managers.\n\n## CBEvent`dataclass`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.schema.CBEvent \"Permanent link\")\n\nGeneric class to store event information.\n\nSource code in `llama-index-core/llama_index/core/callbacks/schema.py`\n\n|     |     |\n| --- | --- |\n| ```<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>89<br>``` | ```<br>@dataclass<br>class CBEvent:<br>    \"\"\"Generic class to store event information.\"\"\"<br>    event_type: CBEventType<br>    payload: Optional[Dict[str, Any]] = None<br>    time: str = \"\"<br>    id_: str = \"\"<br>    def __post_init__(self) -> None:<br>        \"\"\"Init time and id if needed.\"\"\"<br>        if not self.time:<br>            self.time = datetime.now().strftime(TIMESTAMP_FORMAT)<br>        if not self.id_:<br>            self.id = str(uuid.uuid4())<br>``` |\n\n## CBEventType [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.schema.CBEventType \"Permanent link\")\n\nBases: `str`, `Enum`\n\nCallback manager event types.\n\n**Attributes:**\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `CHUNKING` |  | Logs for the before and after of text splitting. |\n| `NODE_PARSING` |  | Logs for the documents and the nodes that they are parsed into. |\n| `EMBEDDING` |  | Logs for the number of texts embedded. |\n| `LLM` |  | Logs for the template and response of LLM calls. |\n| `QUERY` |  | Keeps track of the start and end of each query. |\n| `RETRIEVE` |  | Logs for the nodes retrieved for a query. |\n| `SYNTHESIZE` |  | Logs for the result for synthesize calls. |\n| `TREE` |  | Logs for the summary and level of summaries generated. |\n| `SUB_QUESTION` |  | Logs for a generated sub question and answer. |\n\nSource code in `llama-index-core/llama_index/core/callbacks/schema.py`\n\n|     |     |\n| --- | --- |\n| ```<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>``` | ```<br>class CBEventType(str, Enum):<br>    \"\"\"Callback manager event types.<br>    Attributes:<br>        CHUNKING: Logs for the before and after of text splitting.<br>        NODE_PARSING: Logs for the documents and the nodes that they are parsed into.<br>        EMBEDDING: Logs for the number of texts embedded.<br>        LLM: Logs for the template and response of LLM calls.<br>        QUERY: Keeps track of the start and end of each query.<br>        RETRIEVE: Logs for the nodes retrieved for a query.<br>        SYNTHESIZE: Logs for the result for synthesize calls.<br>        TREE: Logs for the summary and level of summaries generated.<br>        SUB_QUESTION: Logs for a generated sub question and answer.<br>    \"\"\"<br>    CHUNKING = \"chunking\"<br>    NODE_PARSING = \"node_parsing\"<br>    EMBEDDING = \"embedding\"<br>    LLM = \"llm\"<br>    QUERY = \"query\"<br>    RETRIEVE = \"retrieve\"<br>    SYNTHESIZE = \"synthesize\"<br>    TREE = \"tree\"<br>    SUB_QUESTION = \"sub_question\"<br>    TEMPLATING = \"templating\"<br>    FUNCTION_CALL = \"function_call\"<br>    RERANKING = \"reranking\"<br>    EXCEPTION = \"exception\"<br>    AGENT_STEP = \"agent_step\"<br>``` |\n\n## EventPayload [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/\\#llama_index.core.callbacks.schema.EventPayload \"Permanent link\")\n\nBases: `str`, `Enum`\n\nSource code in `llama-index-core/llama_index/core/callbacks/schema.py`\n\n|     |     |\n| --- | --- |\n| ```<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>``` | ```<br>class EventPayload(str, Enum):<br>    DOCUMENTS = \"documents\"  # list of documents before parsing<br>    CHUNKS = \"chunks\"  # list of text chunks<br>    NODES = \"nodes\"  # list of nodes<br>    PROMPT = \"formatted_prompt\"  # formatted prompt sent to LLM<br>    MESSAGES = \"messages\"  # list of messages sent to LLM<br>    COMPLETION = \"completion\"  # completion from LLM<br>    RESPONSE = \"response\"  # message response from LLM<br>    QUERY_STR = \"query_str\"  # query used for query engine<br>    SUB_QUESTION = \"sub_question\"  # a sub question & answer + sources<br>    EMBEDDINGS = \"embeddings\"  # list of embeddings<br>    TOP_K = \"top_k\"  # top k nodes retrieved<br>    ADDITIONAL_KWARGS = \"additional_kwargs\"  # additional kwargs for event call<br>    SERIALIZED = \"serialized\"  # serialized object for event caller<br>    FUNCTION_CALL = \"function_call\"  # function call for the LLM<br>    FUNCTION_OUTPUT = \"function_call_response\"  # function call output<br>    TOOL = \"tool\"  # tool used in LLM call<br>    MODEL_NAME = \"model_name\"  # model name used in an event<br>    TEMPLATE = \"template\"  # template used in LLM call<br>    TEMPLATE_VARS = \"template_vars\"  # template variables used in LLM call<br>    SYSTEM_PROMPT = \"system_prompt\"  # system prompt used in LLM call<br>    QUERY_WRAPPER_PROMPT = \"query_wrapper_prompt\"  # query wrapper prompt used in LLM<br>    EXCEPTION = \"exception\"  # exception raised in an event<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Core Callback Classes - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/agent/coa/#llama_index.agent.coa.CoAAgentWorker)\n\n# Coa\n\n## CoAAgentWorker [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/coa/\\#llama_index.agent.coa.CoAAgentWorker \"Permanent link\")\n\nBases: `BaseAgentWorker`\n\nChain-of-abstraction Agent Worker.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-coa/llama_index/agent/coa/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>``` | ```<br>class CoAAgentWorker(BaseAgentWorker):<br>    \"\"\"Chain-of-abstraction Agent Worker.\"\"\"<br>    def __init__(<br>        self,<br>        llm: LLM,<br>        reasoning_prompt_template: str,<br>        refine_reasoning_prompt_template: str,<br>        output_parser: BaseOutputParser,<br>        tools: Optional[Sequence[BaseTool]] = None,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>    ) -> None:<br>        self.llm = llm<br>        self.callback_manager = callback_manager or llm.callback_manager<br>        if tools is None and tool_retriever is None:<br>            raise ValueError(\"Either tools or tool_retriever must be provided.\")<br>        self.tools = tools<br>        self.tool_retriever = tool_retriever<br>        self.reasoning_prompt_template = reasoning_prompt_template<br>        self.refine_reasoning_prompt_template = refine_reasoning_prompt_template<br>        self.output_parser = output_parser<br>        self.verbose = verbose<br>    @classmethod<br>    def from_tools(<br>        cls,<br>        tools: Optional[Sequence[BaseTool]] = None,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        llm: Optional[LLM] = None,<br>        reasoning_prompt_template: Optional[str] = None,<br>        refine_reasoning_prompt_template: Optional[str] = None,<br>        output_parser: Optional[BaseOutputParser] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        **kwargs: Any,<br>    ) -> \"CoAAgentWorker\":<br>        \"\"\"Convenience constructor method from set of of BaseTools (Optional).<br>        Returns:<br>            LLMCompilerAgentWorker: the LLMCompilerAgentWorker instance<br>        \"\"\"<br>        llm = llm or Settings.llm<br>        if callback_manager is not None:<br>            llm.callback_manager = callback_manager<br>        reasoning_prompt_template = (<br>            reasoning_prompt_template or REASONING_PROMPT_TEMPALTE<br>        )<br>        refine_reasoning_prompt_template = (<br>            refine_reasoning_prompt_template or REFINE_REASONING_PROMPT_TEMPALTE<br>        )<br>        output_parser = output_parser or ChainOfAbstractionParser(verbose=verbose)<br>        return cls(<br>            llm,<br>            reasoning_prompt_template,<br>            refine_reasoning_prompt_template,<br>            output_parser,<br>            tools=tools,<br>            tool_retriever=tool_retriever,<br>            callback_manager=callback_manager,<br>            verbose=verbose,<br>        )<br>    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>        \"\"\"Initialize step from task.\"\"\"<br>        sources: List[ToolOutput] = []<br>        # temporary memory for new messages<br>        new_memory = ChatMemoryBuffer.from_defaults()<br>        # put current history in new memory<br>        messages = task.memory.get(input=task.input)<br>        for message in messages:<br>            new_memory.put(message)<br>        # initialize task state<br>        task_state = {<br>            \"sources\": sources,<br>            \"new_memory\": new_memory,<br>        }<br>        task.extra_state.update(task_state)<br>        return TaskStep(<br>            task_id=task.task_id,<br>            step_id=str(uuid.uuid4()),<br>            input=task.input,<br>            step_state={\"prev_reasoning\": \"\"},<br>        )<br>    def get_tools(self, query_str: str) -> List[AsyncBaseTool]:<br>        \"\"\"Get tools.\"\"\"<br>        if self.tool_retriever:<br>            tools = self.tool_retriever.retrieve(query_str)<br>        else:<br>            tools = self.tools<br>        return [adapt_to_async_tool(t) for t in tools]<br>    async def _arun_step(<br>        self,<br>        step: TaskStep,<br>        task: Task,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        tools = self.get_tools(task.input)<br>        tools_by_name = {tool.metadata.name: tool for tool in tools}<br>        tools_strs = []<br>        for tool in tools:<br>            if isinstance(tool, FunctionTool):<br>                description = tool.metadata.description<br>                # remove function def, we will make our own<br>                if \"def \" in description:<br>                    description = \"\\n\".join(description.split(\"\\n\")[1:])<br>            else:<br>                description = tool.metadata.description<br>            tool_str = json_schema_to_python(<br>                tool.metadata.fn_schema_str, tool.metadata.name, description=description<br>            )<br>            tools_strs.append(tool_str)<br>        prev_reasoning = step.step_state.get(\"prev_reasoning\", \"\")<br>        # show available functions if first step<br>        if self.verbose and not prev_reasoning:<br>            print(f\"==== Available Parsed Functions ====\")<br>            for tool_str in tools_strs:<br>                print(tool_str)<br>        if not prev_reasoning:<br>            # get the reasoning prompt<br>            reasoning_prompt = self.reasoning_prompt_template.format(<br>                functions=\"\\n\".join(tools_strs), question=step.input<br>            )<br>        else:<br>            # get the refine reasoning prompt<br>            reasoning_prompt = self.refine_reasoning_prompt_template.format(<br>                question=step.input, prev_reasoning=prev_reasoning<br>            )<br>        messages = task.extra_state[\"new_memory\"].get()<br>        reasoning_message = ChatMessage(role=\"user\", content=reasoning_prompt)<br>        messages.append(reasoning_message)<br>        # run the reasoning prompt<br>        response = await self.llm.achat(messages)<br>        # print the chain of abstraction if first step<br>        if self.verbose and not prev_reasoning:<br>            print(f\"==== Generated Chain of Abstraction ====\")<br>            print(str(response.message.content))<br>        # parse the output, run functions<br>        parsed_response, tool_sources = await self.output_parser.aparse(<br>            response.message.content, tools_by_name<br>        )<br>        if len(tool_sources) == 0 or prev_reasoning:<br>            is_done = True<br>            new_steps = []<br>            # only add to memory when we are done<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(role=\"user\", content=task.input)<br>            )<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(role=\"assistant\", content=parsed_response)<br>            )<br>        else:<br>            is_done = False<br>            new_steps = [<br>                TaskStep(<br>                    task_id=task.task_id,<br>                    step_id=str(uuid.uuid4()),<br>                    input=task.input,<br>                    step_state={<br>                        \"prev_reasoning\": parsed_response,<br>                    },<br>                )<br>            ]<br>        agent_response = AgentChatResponse(<br>            response=parsed_response, sources=tool_sources<br>        )<br>        return TaskStepOutput(<br>            output=agent_response,<br>            task_step=step,<br>            is_last=is_done,<br>            next_steps=new_steps,<br>        )<br>    @trace_method(\"run_step\")<br>    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        return asyncio.run(self.arun_step(step=step, task=task, **kwargs))<br>    @trace_method(\"run_step\")<br>    async def arun_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        return await self._arun_step(step, task)<br>    @trace_method(\"run_step\")<br>    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        # Streaming isn't really possible, because we need the full response to know if we are done<br>        raise NotImplementedError<br>    @trace_method(\"run_step\")<br>    async def astream_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async stream).\"\"\"<br>        # Streaming isn't really possible, because we need the full response to know if we are done<br>        raise NotImplementedError<br>    def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>        \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>        # add new messages to memory<br>        task.memory.put_messages(task.extra_state[\"new_memory\"].get_all())<br>        # reset new memory<br>        task.extra_state[\"new_memory\"].reset()<br>``` |\n\n### from\\_tools`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/coa/\\#llama_index.agent.coa.CoAAgentWorker.from_tools \"Permanent link\")\n\n```\nfrom_tools(tools: Optional[Sequence[BaseTool]] = None, tool_retriever: Optional[ObjectRetriever[BaseTool]] = None, llm: Optional[LLM] = None, reasoning_prompt_template: Optional[str] = None, refine_reasoning_prompt_template: Optional[str] = None, output_parser: Optional[BaseOutputParser] = None, callback_manager: Optional[CallbackManager] = None, verbose: bool = False, **kwargs: Any) -> CoAAgentWorker\n\n```\n\nConvenience constructor method from set of of BaseTools (Optional).\n\n**Returns:**\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `LLMCompilerAgentWorker` | `CoAAgentWorker` | the LLMCompilerAgentWorker instance |\n\nSource code in `llama-index-integrations/agent/llama-index-agent-coa/llama_index/agent/coa/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>``` | ```<br>@classmethod<br>def from_tools(<br>    cls,<br>    tools: Optional[Sequence[BaseTool]] = None,<br>    tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>    llm: Optional[LLM] = None,<br>    reasoning_prompt_template: Optional[str] = None,<br>    refine_reasoning_prompt_template: Optional[str] = None,<br>    output_parser: Optional[BaseOutputParser] = None,<br>    callback_manager: Optional[CallbackManager] = None,<br>    verbose: bool = False,<br>    **kwargs: Any,<br>) -> \"CoAAgentWorker\":<br>    \"\"\"Convenience constructor method from set of of BaseTools (Optional).<br>    Returns:<br>        LLMCompilerAgentWorker: the LLMCompilerAgentWorker instance<br>    \"\"\"<br>    llm = llm or Settings.llm<br>    if callback_manager is not None:<br>        llm.callback_manager = callback_manager<br>    reasoning_prompt_template = (<br>        reasoning_prompt_template or REASONING_PROMPT_TEMPALTE<br>    )<br>    refine_reasoning_prompt_template = (<br>        refine_reasoning_prompt_template or REFINE_REASONING_PROMPT_TEMPALTE<br>    )<br>    output_parser = output_parser or ChainOfAbstractionParser(verbose=verbose)<br>    return cls(<br>        llm,<br>        reasoning_prompt_template,<br>        refine_reasoning_prompt_template,<br>        output_parser,<br>        tools=tools,<br>        tool_retriever=tool_retriever,<br>        callback_manager=callback_manager,<br>        verbose=verbose,<br>    )<br>``` |\n\n### initialize\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/coa/\\#llama_index.agent.coa.CoAAgentWorker.initialize_step \"Permanent link\")\n\n```\ninitialize_step(task: Task, **kwargs: Any) -> TaskStep\n\n```\n\nInitialize step from task.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-coa/llama_index/agent/coa/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>``` | ```<br>def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>    \"\"\"Initialize step from task.\"\"\"<br>    sources: List[ToolOutput] = []<br>    # temporary memory for new messages<br>    new_memory = ChatMemoryBuffer.from_defaults()<br>    # put current history in new memory<br>    messages = task.memory.get(input=task.input)<br>    for message in messages:<br>        new_memory.put(message)<br>    # initialize task state<br>    task_state = {<br>        \"sources\": sources,<br>        \"new_memory\": new_memory,<br>    }<br>    task.extra_state.update(task_state)<br>    return TaskStep(<br>        task_id=task.task_id,<br>        step_id=str(uuid.uuid4()),<br>        input=task.input,<br>        step_state={\"prev_reasoning\": \"\"},<br>    )<br>``` |\n\n### get\\_tools [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/coa/\\#llama_index.agent.coa.CoAAgentWorker.get_tools \"Permanent link\")\n\n```\nget_tools(query_str: str) -> List[AsyncBaseTool]\n\n```\n\nGet tools.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-coa/llama_index/agent/coa/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>``` | ```<br>def get_tools(self, query_str: str) -> List[AsyncBaseTool]:<br>    \"\"\"Get tools.\"\"\"<br>    if self.tool_retriever:<br>        tools = self.tool_retriever.retrieve(query_str)<br>    else:<br>        tools = self.tools<br>    return [adapt_to_async_tool(t) for t in tools]<br>``` |\n\n### run\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/coa/\\#llama_index.agent.coa.CoAAgentWorker.run_step \"Permanent link\")\n\n```\nrun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-coa/llama_index/agent/coa/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>244<br>245<br>246<br>247<br>``` | ```<br>@trace_method(\"run_step\")<br>def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step.\"\"\"<br>    return asyncio.run(self.arun_step(step=step, task=task, **kwargs))<br>``` |\n\n### arun\\_step`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/coa/\\#llama_index.agent.coa.CoAAgentWorker.arun_step \"Permanent link\")\n\n```\narun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async).\n\nSource code in `llama-index-integrations/agent/llama-index-agent-coa/llama_index/agent/coa/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>249<br>250<br>251<br>252<br>253<br>254<br>``` | ```<br>@trace_method(\"run_step\")<br>async def arun_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async).\"\"\"<br>    return await self._arun_step(step, task)<br>``` |\n\n### stream\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/coa/\\#llama_index.agent.coa.CoAAgentWorker.stream_step \"Permanent link\")\n\n```\nstream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (stream).\n\nSource code in `llama-index-integrations/agent/llama-index-agent-coa/llama_index/agent/coa/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>256<br>257<br>258<br>259<br>260<br>``` | ```<br>@trace_method(\"run_step\")<br>def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step (stream).\"\"\"<br>    # Streaming isn't really possible, because we need the full response to know if we are done<br>    raise NotImplementedError<br>``` |\n\n### astream\\_step`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/coa/\\#llama_index.agent.coa.CoAAgentWorker.astream_step \"Permanent link\")\n\n```\nastream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async stream).\n\nSource code in `llama-index-integrations/agent/llama-index-agent-coa/llama_index/agent/coa/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>``` | ```<br>@trace_method(\"run_step\")<br>async def astream_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async stream).\"\"\"<br>    # Streaming isn't really possible, because we need the full response to know if we are done<br>    raise NotImplementedError<br>``` |\n\n### finalize\\_task [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/coa/\\#llama_index.agent.coa.CoAAgentWorker.finalize_task \"Permanent link\")\n\n```\nfinalize_task(task: Task, **kwargs: Any) -> None\n\n```\n\nFinalize task, after all the steps are completed.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-coa/llama_index/agent/coa/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>270<br>271<br>272<br>273<br>274<br>275<br>``` | ```<br>def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>    \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>    # add new messages to memory<br>    task.memory.put_messages(task.extra_state[\"new_memory\"].get_all())<br>    # reset new memory<br>    task.extra_state[\"new_memory\"].reset()<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Coa - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/agent/coa/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "\n\n# Guideline\n\nEvaluation modules.\n\n## GuidelineEvaluator [\\#](\\#llama_index.core.evaluation.GuidelineEvaluator \"Permanent link\")\n\nBases: `BaseEvaluator`\n\nGuideline evaluator.\n\nEvaluates whether a query and response pair passes the given guidelines.\n\nThis evaluator only considers the query string and the response string.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `guidelines(Optional[str])` |  | User-added guidelines to use for evaluation.<br>Defaults to None, which uses the default guidelines. | _required_ |\n| `eval_template(Optional[Union[str,` | `BasePromptTemplate]] ` | The template to use for evaluation. | _required_ |\n\nSource code in `llama-index-core/llama_index/core/evaluation/guideline.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>``` | ```<br>class GuidelineEvaluator(BaseEvaluator):<br>    \"\"\"Guideline evaluator.<br>    Evaluates whether a query and response pair passes the given guidelines.<br>    This evaluator only considers the query string and the response string.<br>    Args:<br>        guidelines(Optional[str]): User-added guidelines to use for evaluation.<br>            Defaults to None, which uses the default guidelines.<br>        eval_template(Optional[Union[str, BasePromptTemplate]] ):<br>            The template to use for evaluation.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        llm: Optional[LLM] = None,<br>        guidelines: Optional[str] = None,<br>        eval_template: Optional[Union[str, BasePromptTemplate]] = None,<br>        output_parser: Optional[PydanticOutputParser] = None,<br>    ) -> None:<br>        self._llm = llm or Settings.llm<br>        self._guidelines = guidelines or DEFAULT_GUIDELINES<br>        self._eval_template: BasePromptTemplate<br>        if isinstance(eval_template, str):<br>            self._eval_template = PromptTemplate(eval_template)<br>        else:<br>            self._eval_template = eval_template or DEFAULT_EVAL_TEMPLATE<br>        self._output_parser = output_parser or PydanticOutputParser(<br>            output_cls=EvaluationData<br>        )<br>        self._eval_template.output_parser = self._output_parser<br>    def _get_prompts(self) -> PromptDictType:<br>        \"\"\"Get prompts.\"\"\"<br>        return {<br>            \"eval_template\": self._eval_template,<br>        }<br>    def _update_prompts(self, prompts: PromptDictType) -> None:<br>        \"\"\"Update prompts.\"\"\"<br>        if \"eval_template\" in prompts:<br>            self._eval_template = prompts[\"eval_template\"]<br>    async def aevaluate(<br>        self,<br>        query: Optional[str] = None,<br>        response: Optional[str] = None,<br>        contexts: Optional[Sequence[str]] = None,<br>        sleep_time_in_seconds: int = 0,<br>        **kwargs: Any,<br>    ) -> EvaluationResult:<br>        \"\"\"Evaluate whether the query and response pair passes the guidelines.\"\"\"<br>        del contexts  # Unused<br>        del kwargs  # Unused<br>        if query is None or response is None:<br>            raise ValueError(\"query and response must be provided\")<br>        logger.debug(\"prompt: %s\", self._eval_template)<br>        logger.debug(\"query: %s\", query)<br>        logger.debug(\"response: %s\", response)<br>        logger.debug(\"guidelines: %s\", self._guidelines)<br>        await asyncio.sleep(sleep_time_in_seconds)<br>        eval_response = await self._llm.apredict(<br>            self._eval_template,<br>            query=query,<br>            response=response,<br>            guidelines=self._guidelines,<br>        )<br>        eval_data = self._output_parser.parse(eval_response)<br>        eval_data = cast(EvaluationData, eval_data)<br>        return EvaluationResult(<br>            query=query,<br>            response=response,<br>            passing=eval_data.passing,<br>            score=1.0 if eval_data.passing else 0.0,<br>            feedback=eval_data.feedback,<br>        )<br>``` |\n\n### aevaluate`async`[\\#](\\#llama_index.core.evaluation.GuidelineEvaluator.aevaluate \"Permanent link\")\n\n```\naevaluate(query: Optional[str] = None, response: Optional[str] = None, contexts: Optional[Sequence[str]] = None, sleep_time_in_seconds: int = 0, **kwargs: Any) -> EvaluationResult\n\n```\n\nEvaluate whether the query and response pair passes the guidelines.\n\nSource code in `llama-index-core/llama_index/core/evaluation/guideline.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>``` | ```<br>async def aevaluate(<br>    self,<br>    query: Optional[str] = None,<br>    response: Optional[str] = None,<br>    contexts: Optional[Sequence[str]] = None,<br>    sleep_time_in_seconds: int = 0,<br>    **kwargs: Any,<br>) -> EvaluationResult:<br>    \"\"\"Evaluate whether the query and response pair passes the guidelines.\"\"\"<br>    del contexts  # Unused<br>    del kwargs  # Unused<br>    if query is None or response is None:<br>        raise ValueError(\"query and response must be provided\")<br>    logger.debug(\"prompt: %s\", self._eval_template)<br>    logger.debug(\"query: %s\", query)<br>    logger.debug(\"response: %s\", response)<br>    logger.debug(\"guidelines: %s\", self._guidelines)<br>    await asyncio.sleep(sleep_time_in_seconds)<br>    eval_response = await self._llm.apredict(<br>        self._eval_template,<br>        query=query,<br>        response=response,<br>        guidelines=self._guidelines,<br>    )<br>    eval_data = self._output_parser.parse(eval_response)<br>    eval_data = cast(EvaluationData, eval_data)<br>    return EvaluationResult(<br>        query=query,<br>        response=response,<br>        passing=eval_data.passing,<br>        score=1.0 if eval_data.passing else 0.0,<br>        feedback=eval_data.feedback,<br>    )<br>``` |\n\nBack to top",
      "metadata": {
        "title": "Guideline - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/evaluation/guideline/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "\n\n# Answer relevancy\n\nEvaluation modules.\n\n## AnswerRelevancyEvaluator [\\#](\\#llama_index.core.evaluation.AnswerRelevancyEvaluator \"Permanent link\")\n\nBases: `BaseEvaluator`\n\nAnswer relevancy evaluator.\n\nEvaluates the relevancy of response to a query.\nThis evaluator considers the query string and response string.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `raise_error(Optional[bool])` |  | Whether to raise an error if the response is invalid.<br>Defaults to False. | _required_ |\n| `eval_template(Optional[Union[str,` | `BasePromptTemplate]]` | The template to use for evaluation. | _required_ |\n| `refine_template(Optional[Union[str,` | `BasePromptTemplate]]` | The template to use for refinement. | _required_ |\n\nSource code in `llama-index-core/llama_index/core/evaluation/answer_relevancy.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>``` | ```<br>class AnswerRelevancyEvaluator(BaseEvaluator):<br>    \"\"\"Answer relevancy evaluator.<br>    Evaluates the relevancy of response to a query.<br>    This evaluator considers the query string and response string.<br>    Args:<br>        raise_error(Optional[bool]):<br>            Whether to raise an error if the response is invalid.<br>            Defaults to False.<br>        eval_template(Optional[Union[str, BasePromptTemplate]]):<br>            The template to use for evaluation.<br>        refine_template(Optional[Union[str, BasePromptTemplate]]):<br>            The template to use for refinement.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        llm: Optional[LLM] = None,<br>        raise_error: bool = False,<br>        eval_template: str | BasePromptTemplate | None = None,<br>        score_threshold: float = _DEFAULT_SCORE_THRESHOLD,<br>        parser_function: Callable[<br>            [str], Tuple[Optional[float], Optional[str]]<br>        ] = _default_parser_function,<br>    ) -> None:<br>        \"\"\"Init params.\"\"\"<br>        self._llm = llm or Settings.llm<br>        self._raise_error = raise_error<br>        self._eval_template: BasePromptTemplate<br>        if isinstance(eval_template, str):<br>            self._eval_template = PromptTemplate(eval_template)<br>        else:<br>            self._eval_template = eval_template or DEFAULT_EVAL_TEMPLATE<br>        self.parser_function = parser_function<br>        self.score_threshold = score_threshold<br>    def _get_prompts(self) -> PromptDictType:<br>        \"\"\"Get prompts.\"\"\"<br>        return {<br>            \"eval_template\": self._eval_template,<br>            \"refine_template\": self._refine_template,<br>        }<br>    def _update_prompts(self, prompts: PromptDictType) -> None:<br>        \"\"\"Update prompts.\"\"\"<br>        if \"eval_template\" in prompts:<br>            self._eval_template = prompts[\"eval_template\"]<br>        if \"refine_template\" in prompts:<br>            self._refine_template = prompts[\"refine_template\"]<br>    async def aevaluate(<br>        self,<br>        query: str | None = None,<br>        response: str | None = None,<br>        contexts: Sequence[str] | None = None,<br>        sleep_time_in_seconds: int = 0,<br>        **kwargs: Any,<br>    ) -> EvaluationResult:<br>        \"\"\"Evaluate whether the response is relevant to the query.\"\"\"<br>        del kwargs  # Unused<br>        del contexts  # Unused<br>        if query is None or response is None:<br>            raise ValueError(\"query and response must be provided\")<br>        await asyncio.sleep(sleep_time_in_seconds)<br>        eval_response = await self._llm.apredict(<br>            prompt=self._eval_template,<br>            query=query,<br>            response=response,<br>        )<br>        score, reasoning = self.parser_function(eval_response)<br>        invalid_result, invalid_reason = False, None<br>        if score is None and reasoning is None:<br>            if self._raise_error:<br>                raise ValueError(\"The response is invalid\")<br>            invalid_result = True<br>            invalid_reason = \"Unable to parse the output string.\"<br>        if score:<br>            score /= self.score_threshold<br>        return EvaluationResult(<br>            query=query,<br>            response=response,<br>            score=score,<br>            feedback=eval_response,<br>            invalid_result=invalid_result,<br>            invalid_reason=invalid_reason,<br>        )<br>``` |\n\n### aevaluate`async`[\\#](\\#llama_index.core.evaluation.AnswerRelevancyEvaluator.aevaluate \"Permanent link\")\n\n```\naevaluate(query: str | None = None, response: str | None = None, contexts: Sequence[str] | None = None, sleep_time_in_seconds: int = 0, **kwargs: Any) -> EvaluationResult\n\n```\n\nEvaluate whether the response is relevant to the query.\n\nSource code in `llama-index-core/llama_index/core/evaluation/answer_relevancy.py`\n\n|     |     |\n| --- | --- |\n| ```<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>``` | ```<br>async def aevaluate(<br>    self,<br>    query: str | None = None,<br>    response: str | None = None,<br>    contexts: Sequence[str] | None = None,<br>    sleep_time_in_seconds: int = 0,<br>    **kwargs: Any,<br>) -> EvaluationResult:<br>    \"\"\"Evaluate whether the response is relevant to the query.\"\"\"<br>    del kwargs  # Unused<br>    del contexts  # Unused<br>    if query is None or response is None:<br>        raise ValueError(\"query and response must be provided\")<br>    await asyncio.sleep(sleep_time_in_seconds)<br>    eval_response = await self._llm.apredict(<br>        prompt=self._eval_template,<br>        query=query,<br>        response=response,<br>    )<br>    score, reasoning = self.parser_function(eval_response)<br>    invalid_result, invalid_reason = False, None<br>    if score is None and reasoning is None:<br>        if self._raise_error:<br>            raise ValueError(\"The response is invalid\")<br>        invalid_result = True<br>        invalid_reason = \"Unable to parse the output string.\"<br>    if score:<br>        score /= self.score_threshold<br>    return EvaluationResult(<br>        query=query,<br>        response=response,<br>        score=score,<br>        feedback=eval_response,<br>        invalid_result=invalid_result,<br>        invalid_reason=invalid_reason,<br>    )<br>``` |\n\nBack to top",
      "metadata": {
        "title": "Answer relevancy - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/evaluation/answer_relevancy/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "\n\n# Introspective\n\n## IntrospectiveAgentWorker [\\#](\\#llama_index.agent.introspective.IntrospectiveAgentWorker \"Permanent link\")\n\nBases: `BaseAgentWorker`\n\nIntrospective Agent Worker.\n\nThis agent worker implements the Reflection AI agentic pattern. It does\nso by merely delegating the work to two other agents in a purely\ndeterministic fashion.\n\nThe task this agent performs (again via delegation) is to generate a response\nto a query and perform reflection and correction on the response. This\nagent delegates the task to (optionally) first a `main_agent_worker` that\ngenerates the initial response to the query. This initial response is then\npassed to the `reflective_agent_worker` to perform the reflection and\ncorrection of the initial response. Optionally, the `main_agent_worker`\ncan be skipped if none is provided. In this case, the users input query\nwill be assumed to contain the original response that needs to go thru\nreflection and correction.\n\n**Attributes:**\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `reflective_agent_worker` | `BaseAgentWorker` | Reflective agent responsible for<br>performing reflection and correction of the initial response. |\n| `main_agent_worker` | `Optional[BaseAgentWorker]` | Main agent responsible<br>for generating an initial response to the user query. Defaults to None.<br>If None, the user input is assumed as the initial response. |\n| `verbose` | `bool` | Whether execution should be verbose. Defaults to False. |\n| `callback_manager` | `Optional[CallbackManager]` | Callback manager. Defaults to None. |\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>``` | ```<br>class IntrospectiveAgentWorker(BaseAgentWorker):<br>    \"\"\"Introspective Agent Worker.<br>    This agent worker implements the Reflection AI agentic pattern. It does<br>    so by merely delegating the work to two other agents in a purely<br>    deterministic fashion.<br>    The task this agent performs (again via delegation) is to generate a response<br>    to a query and perform reflection and correction on the response. This<br>    agent delegates the task to (optionally) first a `main_agent_worker` that<br>    generates the initial response to the query. This initial response is then<br>    passed to the `reflective_agent_worker` to perform the reflection and<br>    correction of the initial response. Optionally, the `main_agent_worker`<br>    can be skipped if none is provided. In this case, the users input query<br>    will be assumed to contain the original response that needs to go thru<br>    reflection and correction.<br>    Attributes:<br>        reflective_agent_worker (BaseAgentWorker): Reflective agent responsible for<br>            performing reflection and correction of the initial response.<br>        main_agent_worker (Optional[BaseAgentWorker], optional): Main agent responsible<br>            for generating an initial response to the user query. Defaults to None.<br>            If None, the user input is assumed as the initial response.<br>        verbose (bool, optional): Whether execution should be verbose. Defaults to False.<br>        callback_manager (Optional[CallbackManager], optional): Callback manager. Defaults to None.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        reflective_agent_worker: BaseAgentWorker,<br>        main_agent_worker: Optional[BaseAgentWorker] = None,<br>        verbose: bool = False,<br>        callback_manager: Optional[CallbackManager] = None,<br>    ) -> None:<br>        \"\"\"Init params.\"\"\"<br>        self._verbose = verbose<br>        self._main_agent_worker = main_agent_worker<br>        self._reflective_agent_worker = reflective_agent_worker<br>        self.callback_manager = callback_manager or CallbackManager([])<br>    @classmethod<br>    def from_defaults(<br>        cls,<br>        reflective_agent_worker: BaseAgentWorker,<br>        main_agent_worker: Optional[BaseAgentWorker] = None,<br>        verbose: bool = False,<br>        callback_manager: Optional[CallbackManager] = None,<br>        **kwargs: Any,<br>    ) -> \"IntrospectiveAgentWorker\":<br>        \"\"\"Create an IntrospectiveAgentWorker from args.<br>        Similar to `from_defaults` in other classes, this method will<br>        infer defaults for a variety of parameters, including the LLM,<br>        if they are not specified.<br>        \"\"\"<br>        return cls(<br>            main_agent_worker=main_agent_worker,<br>            reflective_agent_worker=reflective_agent_worker,<br>            verbose=verbose,<br>            callback_manager=callback_manager,<br>            **kwargs,<br>        )<br>    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>        \"\"\"Initialize step from task.\"\"\"<br>        # temporary memory for new messages<br>        main_memory = ChatMemoryBuffer.from_defaults()<br>        reflective_memory = ChatMemoryBuffer.from_defaults()<br>        # put current history in new memory<br>        messages = task.memory.get()<br>        for message in messages:<br>            main_memory.put(message)<br>        # initialize task state<br>        task_state = {<br>            \"main\": {<br>                \"memory\": main_memory,<br>                \"sources\": [],<br>            },<br>            \"reflection\": {\"memory\": reflective_memory, \"sources\": []},<br>        }<br>        task.extra_state.update(task_state)<br>        return TaskStep(<br>            task_id=task.task_id,<br>            step_id=str(uuid.uuid4()),<br>            input=task.input,<br>        )<br>    def get_all_messages(self, task: Task) -> List[ChatMessage]:<br>        return (<br>            +task.memory.get()<br>            + task.extra_state[\"main\"][\"memory\"].get_all()<br>            + task.extra_state[\"reflection\"][\"memory\"].get_all()<br>        )<br>    @trace_method(\"run_step\")<br>    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        # run main agent<br>        if self._main_agent_worker is not None:<br>            main_agent_messages = task.extra_state[\"main\"][\"memory\"].get()<br>            main_agent = self._main_agent_worker.as_agent(<br>                chat_history=main_agent_messages<br>            )<br>            main_agent_response = main_agent.chat(task.input)<br>            original_response = main_agent_response.response<br>            task.extra_state[\"main\"][\"sources\"] = main_agent_response.sources<br>            task.extra_state[\"main\"][\"memory\"] = main_agent.memory<br>        else:<br>            pass<br>        # run reflective agent<br>        reflective_agent_messages = task.extra_state[\"main\"][\"memory\"].get()<br>        reflective_agent = self._reflective_agent_worker.as_agent(<br>            chat_history=reflective_agent_messages<br>        )<br>        # NOTE: atm you *need* to pass an input string to `chat`, even if the memory is already<br>        # preloaded. Input will be concatenated on top of chat history from memory<br>        # which will be used to generate the response.<br>        # TODO: make agent interface more flexible<br>        reflective_agent_response = reflective_agent.chat(original_response)<br>        task.extra_state[\"reflection\"][\"sources\"] = reflective_agent_response.sources<br>        task.extra_state[\"reflection\"][\"memory\"] = reflective_agent.memory<br>        agent_response = AgentChatResponse(<br>            response=str(reflective_agent_response.response),<br>            sources=task.extra_state[\"main\"][\"sources\"]<br>            + task.extra_state[\"reflection\"][\"sources\"],<br>        )<br>        return TaskStepOutput(<br>            output=agent_response,<br>            task_step=step,<br>            is_last=True,<br>            next_steps=[],<br>        )<br>    @trace_method(\"run_step\")<br>    async def arun_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        # run main agent if one is supplied otherwise assume user input<br>        # is the original response to be reflected on and subsequently corrected<br>        if self._main_agent_worker is not None:<br>            main_agent_messages = task.extra_state[\"main\"][\"memory\"].get()<br>            main_agent = self._main_agent_worker.as_agent(<br>                chat_history=main_agent_messages, verbose=self._verbose<br>            )<br>            main_agent_response = await main_agent.achat(task.input)<br>            original_response = main_agent_response.response<br>            task.extra_state[\"main\"][\"sources\"] = main_agent_response.sources<br>            task.extra_state[\"main\"][\"memory\"] = main_agent.memory<br>        else:<br>            add_user_step_to_memory(<br>                step, task.extra_state[\"main\"][\"memory\"], verbose=self._verbose<br>            )<br>            original_response = step.input<br>            # fictitious agent's initial response (to get reflection/correction cycle started)<br>            task.extra_state[\"main\"][\"memory\"].put(<br>                ChatMessage(content=original_response, role=\"assistant\")<br>            )<br>        # run reflective agent<br>        reflective_agent_messages = task.extra_state[\"main\"][\"memory\"].get()<br>        reflective_agent = self._reflective_agent_worker.as_agent(<br>            chat_history=reflective_agent_messages, verbose=self._verbose<br>        )<br>        reflective_agent_response = await reflective_agent.achat(original_response)<br>        task.extra_state[\"reflection\"][\"sources\"] = reflective_agent_response.sources<br>        task.extra_state[\"reflection\"][\"memory\"] = reflective_agent.memory<br>        agent_response = AgentChatResponse(<br>            response=str(reflective_agent_response.response),<br>            sources=task.extra_state[\"main\"][\"sources\"]<br>            + task.extra_state[\"reflection\"][\"sources\"],<br>        )<br>        return TaskStepOutput(<br>            output=agent_response,<br>            task_step=step,<br>            is_last=True,<br>            next_steps=[],<br>        )<br>    @trace_method(\"run_step\")<br>    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        raise NotImplementedError(\"Stream not supported for introspective agent\")<br>    @trace_method(\"run_step\")<br>    async def astream_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async stream).\"\"\"<br>        raise NotImplementedError(\"Stream not supported for introspective agent\")<br>    def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>        \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>        # add new messages to memory<br>        main_memory = task.extra_state[\"main\"][<br>            \"memory\"<br>        ].get_all()  # contains initial response as final message<br>        final_corrected_message = task.extra_state[\"reflection\"][\"memory\"].get_all()[-1]<br>        # swap main workers response with the reflected/corrected one<br>        finalized_task_memory = main_memory[:-1] + [final_corrected_message]<br>        task.memory.set(finalized_task_memory)<br>``` |\n\n### from\\_defaults`classmethod`[\\#](\\#llama_index.agent.introspective.IntrospectiveAgentWorker.from_defaults \"Permanent link\")\n\n```\nfrom_defaults(reflective_agent_worker: BaseAgentWorker, main_agent_worker: Optional[BaseAgentWorker] = None, verbose: bool = False, callback_manager: Optional[CallbackManager] = None, **kwargs: Any) -> IntrospectiveAgentWorker\n\n```\n\nCreate an IntrospectiveAgentWorker from args.\n\nSimilar to `from_defaults` in other classes, this method will\ninfer defaults for a variety of parameters, including the LLM,\nif they are not specified.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>``` | ```<br>@classmethod<br>def from_defaults(<br>    cls,<br>    reflective_agent_worker: BaseAgentWorker,<br>    main_agent_worker: Optional[BaseAgentWorker] = None,<br>    verbose: bool = False,<br>    callback_manager: Optional[CallbackManager] = None,<br>    **kwargs: Any,<br>) -> \"IntrospectiveAgentWorker\":<br>    \"\"\"Create an IntrospectiveAgentWorker from args.<br>    Similar to `from_defaults` in other classes, this method will<br>    infer defaults for a variety of parameters, including the LLM,<br>    if they are not specified.<br>    \"\"\"<br>    return cls(<br>        main_agent_worker=main_agent_worker,<br>        reflective_agent_worker=reflective_agent_worker,<br>        verbose=verbose,<br>        callback_manager=callback_manager,<br>        **kwargs,<br>    )<br>``` |\n\n### initialize\\_step [\\#](\\#llama_index.agent.introspective.IntrospectiveAgentWorker.initialize_step \"Permanent link\")\n\n```\ninitialize_step(task: Task, **kwargs: Any) -> TaskStep\n\n```\n\nInitialize step from task.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>``` | ```<br>def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>    \"\"\"Initialize step from task.\"\"\"<br>    # temporary memory for new messages<br>    main_memory = ChatMemoryBuffer.from_defaults()<br>    reflective_memory = ChatMemoryBuffer.from_defaults()<br>    # put current history in new memory<br>    messages = task.memory.get()<br>    for message in messages:<br>        main_memory.put(message)<br>    # initialize task state<br>    task_state = {<br>        \"main\": {<br>            \"memory\": main_memory,<br>            \"sources\": [],<br>        },<br>        \"reflection\": {\"memory\": reflective_memory, \"sources\": []},<br>    }<br>    task.extra_state.update(task_state)<br>    return TaskStep(<br>        task_id=task.task_id,<br>        step_id=str(uuid.uuid4()),<br>        input=task.input,<br>    )<br>``` |\n\n### run\\_step [\\#](\\#llama_index.agent.introspective.IntrospectiveAgentWorker.run_step \"Permanent link\")\n\n```\nrun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>``` | ```<br>@trace_method(\"run_step\")<br>def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step.\"\"\"<br>    # run main agent<br>    if self._main_agent_worker is not None:<br>        main_agent_messages = task.extra_state[\"main\"][\"memory\"].get()<br>        main_agent = self._main_agent_worker.as_agent(<br>            chat_history=main_agent_messages<br>        )<br>        main_agent_response = main_agent.chat(task.input)<br>        original_response = main_agent_response.response<br>        task.extra_state[\"main\"][\"sources\"] = main_agent_response.sources<br>        task.extra_state[\"main\"][\"memory\"] = main_agent.memory<br>    else:<br>        pass<br>    # run reflective agent<br>    reflective_agent_messages = task.extra_state[\"main\"][\"memory\"].get()<br>    reflective_agent = self._reflective_agent_worker.as_agent(<br>        chat_history=reflective_agent_messages<br>    )<br>    # NOTE: atm you *need* to pass an input string to `chat`, even if the memory is already<br>    # preloaded. Input will be concatenated on top of chat history from memory<br>    # which will be used to generate the response.<br>    # TODO: make agent interface more flexible<br>    reflective_agent_response = reflective_agent.chat(original_response)<br>    task.extra_state[\"reflection\"][\"sources\"] = reflective_agent_response.sources<br>    task.extra_state[\"reflection\"][\"memory\"] = reflective_agent.memory<br>    agent_response = AgentChatResponse(<br>        response=str(reflective_agent_response.response),<br>        sources=task.extra_state[\"main\"][\"sources\"]<br>        + task.extra_state[\"reflection\"][\"sources\"],<br>    )<br>    return TaskStepOutput(<br>        output=agent_response,<br>        task_step=step,<br>        is_last=True,<br>        next_steps=[],<br>    )<br>``` |\n\n### arun\\_step`async`[\\#](\\#llama_index.agent.introspective.IntrospectiveAgentWorker.arun_step \"Permanent link\")\n\n```\narun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async).\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>``` | ```<br>@trace_method(\"run_step\")<br>async def arun_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async).\"\"\"<br>    # run main agent if one is supplied otherwise assume user input<br>    # is the original response to be reflected on and subsequently corrected<br>    if self._main_agent_worker is not None:<br>        main_agent_messages = task.extra_state[\"main\"][\"memory\"].get()<br>        main_agent = self._main_agent_worker.as_agent(<br>            chat_history=main_agent_messages, verbose=self._verbose<br>        )<br>        main_agent_response = await main_agent.achat(task.input)<br>        original_response = main_agent_response.response<br>        task.extra_state[\"main\"][\"sources\"] = main_agent_response.sources<br>        task.extra_state[\"main\"][\"memory\"] = main_agent.memory<br>    else:<br>        add_user_step_to_memory(<br>            step, task.extra_state[\"main\"][\"memory\"], verbose=self._verbose<br>        )<br>        original_response = step.input<br>        # fictitious agent's initial response (to get reflection/correction cycle started)<br>        task.extra_state[\"main\"][\"memory\"].put(<br>            ChatMessage(content=original_response, role=\"assistant\")<br>        )<br>    # run reflective agent<br>    reflective_agent_messages = task.extra_state[\"main\"][\"memory\"].get()<br>    reflective_agent = self._reflective_agent_worker.as_agent(<br>        chat_history=reflective_agent_messages, verbose=self._verbose<br>    )<br>    reflective_agent_response = await reflective_agent.achat(original_response)<br>    task.extra_state[\"reflection\"][\"sources\"] = reflective_agent_response.sources<br>    task.extra_state[\"reflection\"][\"memory\"] = reflective_agent.memory<br>    agent_response = AgentChatResponse(<br>        response=str(reflective_agent_response.response),<br>        sources=task.extra_state[\"main\"][\"sources\"]<br>        + task.extra_state[\"reflection\"][\"sources\"],<br>    )<br>    return TaskStepOutput(<br>        output=agent_response,<br>        task_step=step,<br>        is_last=True,<br>        next_steps=[],<br>    )<br>``` |\n\n### stream\\_step [\\#](\\#llama_index.agent.introspective.IntrospectiveAgentWorker.stream_step \"Permanent link\")\n\n```\nstream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (stream).\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>214<br>215<br>216<br>217<br>``` | ```<br>@trace_method(\"run_step\")<br>def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step (stream).\"\"\"<br>    raise NotImplementedError(\"Stream not supported for introspective agent\")<br>``` |\n\n### astream\\_step`async`[\\#](\\#llama_index.agent.introspective.IntrospectiveAgentWorker.astream_step \"Permanent link\")\n\n```\nastream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async stream).\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>219<br>220<br>221<br>222<br>223<br>224<br>``` | ```<br>@trace_method(\"run_step\")<br>async def astream_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async stream).\"\"\"<br>    raise NotImplementedError(\"Stream not supported for introspective agent\")<br>``` |\n\n### finalize\\_task [\\#](\\#llama_index.agent.introspective.IntrospectiveAgentWorker.finalize_task \"Permanent link\")\n\n```\nfinalize_task(task: Task, **kwargs: Any) -> None\n\n```\n\nFinalize task, after all the steps are completed.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>``` | ```<br>def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>    \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>    # add new messages to memory<br>    main_memory = task.extra_state[\"main\"][<br>        \"memory\"<br>    ].get_all()  # contains initial response as final message<br>    final_corrected_message = task.extra_state[\"reflection\"][\"memory\"].get_all()[-1]<br>    # swap main workers response with the reflected/corrected one<br>    finalized_task_memory = main_memory[:-1] + [final_corrected_message]<br>    task.memory.set(finalized_task_memory)<br>``` |\n\n## SelfReflectionAgentWorker [\\#](\\#llama_index.agent.introspective.SelfReflectionAgentWorker \"Permanent link\")\n\nBases: `BaseModel`, `BaseAgentWorker`\n\nSelf Reflection Agent Worker.\n\nThis agent performs a reflection without any tools on a given response\nand subsequently performs correction. It should be noted that this reflection\nimplementation has been inspired by two works:\n\n1. Reflexion: Language Agents with Verbal Reinforcement Learning, by Shinn et al. (2023)\n    (https://arxiv.org/pdf/2303.11366.pdf)\n2. CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing, by Gou et al. (2024)\n    (https://arxiv.org/pdf/2305.11738.pdf)\n\nThis agent performs cycles of reflection and correction on an initial response\nuntil a satisfactory correction has been generated or a max number of cycles\nhas been reached. To perform reflection, this agent utilizes a user-specified\nLLM along with a PydanticProgram (thru structured\\_predict) to generate a structured\noutput that contains an LLM generated reflection of the current response. After reflection,\nthe same user-specified LLM is used again but this time with another PydanticProgram\nto generate a structured output that contains an LLM generated corrected\nversion of the current response against the priorly generated reflection.\n\nAttr\n\nmax\\_iterations (int, optional): The max number of reflection & correction.\nDefaults to DEFAULT\\_MAX\\_ITERATIONS.\ncallback\\_manager (Optional\\[CallbackManager\\], optional): Callback manager.\nDefaults to None.\nllm (Optional\\[LLM\\], optional): The LLM used to perform reflection and correction.\nMust be an OpenAI LLM at this time. Defaults to None.\nverbose (bool, optional): Whether execution should be verbose. Defaults to False.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/reflective/self_reflection.py`\n\n|     |     |\n| --- | --- |\n| ```<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>472<br>473<br>``` | ```<br>class SelfReflectionAgentWorker(BaseModel, BaseAgentWorker):<br>    \"\"\"Self Reflection Agent Worker.<br>    This agent performs a reflection without any tools on a given response<br>    and subsequently performs correction. It should be noted that this reflection<br>    implementation has been inspired by two works:<br>    1. Reflexion: Language Agents with Verbal Reinforcement Learning, by Shinn et al. (2023)<br>        (https://arxiv.org/pdf/2303.11366.pdf)<br>    2. CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing, by Gou et al. (2024)<br>       (https://arxiv.org/pdf/2305.11738.pdf)<br>    This agent performs cycles of reflection and correction on an initial response<br>    until a satisfactory correction has been generated or a max number of cycles<br>    has been reached. To perform reflection, this agent utilizes a user-specified<br>    LLM along with a PydanticProgram (thru structured_predict) to generate a structured<br>    output that contains an LLM generated reflection of the current response. After reflection,<br>    the same user-specified LLM is used again but this time with another PydanticProgram<br>    to generate a structured output that contains an LLM generated corrected<br>    version of the current response against the priorly generated reflection.<br>    Attr:<br>        max_iterations (int, optional): The max number of reflection & correction.<br>            Defaults to DEFAULT_MAX_ITERATIONS.<br>        callback_manager (Optional[CallbackManager], optional): Callback manager.<br>            Defaults to None.<br>        llm (Optional[LLM], optional): The LLM used to perform reflection and correction.<br>            Must be an OpenAI LLM at this time. Defaults to None.<br>        verbose (bool, optional): Whether execution should be verbose. Defaults to False.<br>    \"\"\"<br>    callback_manager: CallbackManager = Field(default=CallbackManager([]))<br>    max_iterations: int = Field(default=DEFAULT_MAX_ITERATIONS)<br>    _llm: LLM = PrivateAttr()<br>    _verbose: bool = PrivateAttr()<br>    class Config:<br>        arbitrary_types_allowed = True<br>    def __init__(<br>        self,<br>        max_iterations: int = DEFAULT_MAX_ITERATIONS,<br>        callback_manager: Optional[CallbackManager] = None,<br>        llm: Optional[LLM] = None,<br>        verbose: bool = False,<br>        **kwargs: Any,<br>    ) -> None:<br>        \"\"\"__init__.\"\"\"<br>        super().__init__(<br>            callback_manager=callback_manager or CallbackManager([]),<br>            max_iterations=max_iterations,<br>            **kwargs,<br>        )<br>        self._llm = llm<br>        self._verbose = verbose<br>    @classmethod<br>    def from_defaults(<br>        cls,<br>        llm: Optional[LLM] = None,<br>        max_iterations: int = DEFAULT_MAX_ITERATIONS,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        **kwargs: Any,<br>    ) -> \"SelfReflectionAgentWorker\":<br>        \"\"\"Convenience constructor.\"\"\"<br>        if llm is None:<br>            try:<br>                from llama_index.llms.openai import OpenAI<br>            except ImportError:<br>                raise ImportError(<br>                    \"Missing OpenAI LLMs. Please run `pip install llama-index-llms-openai`.\"<br>                )<br>            llm = OpenAI(model=\"gpt-4-turbo-preview\", temperature=0)<br>        return cls(<br>            llm=llm,<br>            max_iterations=max_iterations,<br>            callback_manager=callback_manager,<br>            verbose=verbose,<br>            **kwargs,<br>        )<br>    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>        \"\"\"Initialize step from task.\"\"\"<br>        # temporary memory for new messages<br>        new_memory = ChatMemoryBuffer.from_defaults()<br>        # put current history in new memory<br>        messages = task.memory.get()<br>        for message in messages:<br>            new_memory.put(message)<br>        # inject new input into memory<br>        new_memory.put(ChatMessage(content=task.input, role=MessageRole.USER))<br>        # initialize task state<br>        task_state = {<br>            \"new_memory\": new_memory,<br>            \"sources\": [],<br>        }<br>        task.extra_state.update(task_state)<br>        return TaskStep(<br>            task_id=task.task_id,<br>            step_id=str(uuid.uuid4()),<br>            input=task.input,<br>            step_state={\"count\": 0},<br>        )<br>    def _remove_correction_str_prefix(self, correct_msg: str) -> str:<br>        \"\"\"Helper function to format correction message for final response.\"\"\"<br>        return correct_msg.replace(CORRECT_RESPONSE_PREFIX, \"\")<br>    @dispatcher.span<br>    def _reflect(<br>        self, chat_history: List[ChatMessage]<br>    ) -> Tuple[Reflection, ChatMessage]:<br>        \"\"\"Reflect on the trajectory.\"\"\"<br>        reflection = self._llm.structured_predict(<br>            Reflection,<br>            PromptTemplate(REFLECTION_PROMPT_TEMPLATE),<br>            chat_history=messages_to_prompt(chat_history),<br>        )<br>        if self._verbose:<br>            print(f\"> Reflection: {reflection.model_dump()}\")<br>        # end state: return user message<br>        reflection_output_str = (<br>            f\"Is Done: {reflection.is_done}\\nCritique: {reflection.feedback}\"<br>        )<br>        critique = REFLECTION_RESPONSE_TEMPLATE.format(<br>            reflection_output=reflection_output_str<br>        )<br>        return reflection, ChatMessage.from_str(critique, role=\"user\")<br>    @dispatcher.span<br>    def _correct(self, input_str: str, critique: str) -> ChatMessage:<br>        correction = self._llm.structured_predict(<br>            Correction,<br>            PromptTemplate(CORRECT_PROMPT_TEMPLATE),<br>            input_str=input_str,<br>            feedback=critique,<br>        )<br>        correct_response_str = CORRECT_RESPONSE_FSTRING.format(<br>            correction=correction.correction<br>        )<br>        if self._verbose:<br>            print(f\"Correction: {correction.correction}\", flush=True)<br>        return ChatMessage.from_str(correct_response_str, role=\"assistant\")<br>    @dispatcher.span<br>    @trace_method(\"run_step\")<br>    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        state = step.step_state<br>        state[\"count\"] += 1<br>        # new_memory should at the very least contain the user input<br>        messages = task.extra_state[\"new_memory\"].get()<br>        prev_correct_str = messages[-1].content<br>        prev_correct_str_without_prefix = self._remove_correction_str_prefix(<br>            prev_correct_str<br>        )<br>        # reflect phase<br>        reflection, reflection_msg = self._reflect(chat_history=messages)<br>        is_done = reflection.is_done<br>        critique_msg = ChatMessage(role=MessageRole.USER, content=reflection_msg)<br>        task.extra_state[\"new_memory\"].put(critique_msg)<br>        # correction phase<br>        if is_done:<br>            # no correction to be made prev correction is sufficient<br>            agent_response = AgentChatResponse(<br>                response=prev_correct_str_without_prefix,<br>                sources=task.extra_state[\"sources\"],<br>            )<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(<br>                    role=MessageRole.ASSISTANT,<br>                    content=prev_correct_str_without_prefix,<br>                )<br>            )<br>            new_steps = []<br>        else:<br>            # generate a new correction<br>            correct_msg = self._correct(<br>                input_str=prev_correct_str_without_prefix,<br>                critique=reflection_msg.content,<br>            )<br>            correct_str_without_prefix = self._remove_correction_str_prefix(<br>                correct_msg.content<br>            )<br>            if self.max_iterations == state[\"count\"]:<br>                # this will be the last iteration<br>                task.extra_state[\"new_memory\"].put(<br>                    ChatMessage(<br>                        role=MessageRole.ASSISTANT,<br>                        content=correct_str_without_prefix,<br>                    )<br>                )<br>                agent_response = AgentChatResponse(response=correct_str_without_prefix)<br>                new_steps = []<br>            else:<br>                # another round of reflection/correction will take place<br>                task.extra_state[\"new_memory\"].put(correct_msg)<br>                agent_response = AgentChatResponse(response=str(correct_msg))<br>                new_steps = [<br>                    step.get_next_step(<br>                        step_id=str(uuid.uuid4()),<br>                        # NOTE: input is unused<br>                        input=None,<br>                        step_state=state,<br>                    )<br>                ]<br>        return TaskStepOutput(<br>            output=agent_response,<br>            task_step=step,<br>            is_last=is_done | (self.max_iterations == state[\"count\"]),<br>            next_steps=new_steps,<br>        )<br>    # Async methods<br>    @dispatcher.span<br>    async def _areflect(<br>        self, chat_history: List[ChatMessage]<br>    ) -> Tuple[Reflection, ChatMessage]:<br>        \"\"\"Reflect on the trajectory.\"\"\"<br>        reflection = await self._llm.astructured_predict(<br>            Reflection,<br>            PromptTemplate(REFLECTION_PROMPT_TEMPLATE),<br>            chat_history=messages_to_prompt(chat_history),<br>        )<br>        if self._verbose:<br>            print(f\"> Reflection: {reflection.model_dump()}\")<br>        # end state: return user message<br>        reflection_output_str = (<br>            f\"Is Done: {reflection.is_done}\\nCritique: {reflection.feedback}\"<br>        )<br>        critique = REFLECTION_RESPONSE_TEMPLATE.format(<br>            reflection_output=reflection_output_str<br>        )<br>        return reflection, ChatMessage.from_str(critique, role=\"user\")<br>    @dispatcher.span<br>    async def _acorrect(self, input_str: str, critique: str) -> ChatMessage:<br>        correction = await self._llm.astructured_predict(<br>            Correction,<br>            PromptTemplate(CORRECT_PROMPT_TEMPLATE),<br>            input_str=input_str,<br>            feedback=critique,<br>        )<br>        correct_response_str = CORRECT_RESPONSE_FSTRING.format(<br>            correction=correction.correction<br>        )<br>        if self._verbose:<br>            print(f\"Correction: {correction.correction}\", flush=True)<br>        return ChatMessage.from_str(correct_response_str, role=\"assistant\")<br>    @dispatcher.span<br>    @trace_method(\"run_step\")<br>    async def arun_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        state = step.step_state<br>        state[\"count\"] += 1<br>        messages = task.extra_state[\"new_memory\"].get()<br>        prev_correct_str = messages[-1].content<br>        prev_correct_str_without_prefix = self._remove_correction_str_prefix(<br>            prev_correct_str<br>        )<br>        # reflect<br>        reflection, reflection_msg = await self._areflect(chat_history=messages)<br>        is_done = reflection.is_done<br>        critique_msg = ChatMessage(role=MessageRole.USER, content=reflection_msg)<br>        task.extra_state[\"new_memory\"].put(critique_msg)<br>        # correction phase<br>        if is_done:<br>            # no correction to be made prev correction is sufficient<br>            agent_response = AgentChatResponse(<br>                response=prev_correct_str_without_prefix,<br>                sources=task.extra_state[\"sources\"],<br>            )<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(<br>                    role=MessageRole.ASSISTANT,<br>                    content=prev_correct_str_without_prefix,<br>                )<br>            )<br>            new_steps = []<br>        else:<br>            # generate a new correction<br>            correct_msg = await self._acorrect(<br>                input_str=prev_correct_str_without_prefix,<br>                critique=reflection_msg.content,<br>            )<br>            correct_str_without_prefix = self._remove_correction_str_prefix(<br>                correct_msg.content<br>            )<br>            if self.max_iterations == state[\"count\"]:<br>                # this will be the last iteration<br>                task.extra_state[\"new_memory\"].put(<br>                    ChatMessage(<br>                        role=MessageRole.ASSISTANT,<br>                        content=correct_str_without_prefix,<br>                    )<br>                )<br>                agent_response = AgentChatResponse(response=correct_str_without_prefix)<br>                new_steps = []<br>            else:<br>                # another round of reflection/correction will take place<br>                task.extra_state[\"new_memory\"].put(correct_msg)<br>                agent_response = AgentChatResponse(response=str(correct_msg))<br>                new_steps = [<br>                    step.get_next_step(<br>                        step_id=str(uuid.uuid4()),<br>                        # NOTE: input is unused<br>                        input=None,<br>                        step_state=state,<br>                    )<br>                ]<br>        return TaskStepOutput(<br>            output=agent_response,<br>            task_step=step,<br>            is_last=is_done | (self.max_iterations == state[\"count\"]),<br>            next_steps=new_steps,<br>        )<br>    # Stream methods<br>    @dispatcher.span<br>    @trace_method(\"run_step\")<br>    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        raise NotImplementedError(\"Stream not supported for self reflection agent\")<br>    @dispatcher.span<br>    @trace_method(\"run_step\")<br>    async def astream_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async stream).\"\"\"<br>        raise NotImplementedError(\"Stream not supported for self reflection agent\")<br>    def get_all_messages(self, task: Task) -> List[ChatMessage]:<br>        return (<br>            self.prefix_messages<br>            + task.memory.get()<br>            + task.extra_state[\"new_memory\"].get_all()<br>        )<br>    def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>        \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>        # add new messages to memory<br>        task.memory.set(task.extra_state[\"new_memory\"].get_all())<br>        # reset new memory<br>        task.extra_state[\"new_memory\"].reset()<br>``` |\n\n### from\\_defaults`classmethod`[\\#](\\#llama_index.agent.introspective.SelfReflectionAgentWorker.from_defaults \"Permanent link\")\n\n```\nfrom_defaults(llm: Optional[LLM] = None, max_iterations: int = DEFAULT_MAX_ITERATIONS, callback_manager: Optional[CallbackManager] = None, verbose: bool = False, **kwargs: Any) -> SelfReflectionAgentWorker\n\n```\n\nConvenience constructor.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/reflective/self_reflection.py`\n\n|     |     |\n| --- | --- |\n| ```<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>``` | ```<br>@classmethod<br>def from_defaults(<br>    cls,<br>    llm: Optional[LLM] = None,<br>    max_iterations: int = DEFAULT_MAX_ITERATIONS,<br>    callback_manager: Optional[CallbackManager] = None,<br>    verbose: bool = False,<br>    **kwargs: Any,<br>) -> \"SelfReflectionAgentWorker\":<br>    \"\"\"Convenience constructor.\"\"\"<br>    if llm is None:<br>        try:<br>            from llama_index.llms.openai import OpenAI<br>        except ImportError:<br>            raise ImportError(<br>                \"Missing OpenAI LLMs. Please run `pip install llama-index-llms-openai`.\"<br>            )<br>        llm = OpenAI(model=\"gpt-4-turbo-preview\", temperature=0)<br>    return cls(<br>        llm=llm,<br>        max_iterations=max_iterations,<br>        callback_manager=callback_manager,<br>        verbose=verbose,<br>        **kwargs,<br>    )<br>``` |\n\n### initialize\\_step [\\#](\\#llama_index.agent.introspective.SelfReflectionAgentWorker.initialize_step \"Permanent link\")\n\n```\ninitialize_step(task: Task, **kwargs: Any) -> TaskStep\n\n```\n\nInitialize step from task.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/reflective/self_reflection.py`\n\n|     |     |\n| --- | --- |\n| ```<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>``` | ```<br>def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>    \"\"\"Initialize step from task.\"\"\"<br>    # temporary memory for new messages<br>    new_memory = ChatMemoryBuffer.from_defaults()<br>    # put current history in new memory<br>    messages = task.memory.get()<br>    for message in messages:<br>        new_memory.put(message)<br>    # inject new input into memory<br>    new_memory.put(ChatMessage(content=task.input, role=MessageRole.USER))<br>    # initialize task state<br>    task_state = {<br>        \"new_memory\": new_memory,<br>        \"sources\": [],<br>    }<br>    task.extra_state.update(task_state)<br>    return TaskStep(<br>        task_id=task.task_id,<br>        step_id=str(uuid.uuid4()),<br>        input=task.input,<br>        step_state={\"count\": 0},<br>    )<br>``` |\n\n### run\\_step [\\#](\\#llama_index.agent.introspective.SelfReflectionAgentWorker.run_step \"Permanent link\")\n\n```\nrun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/reflective/self_reflection.py`\n\n|     |     |\n| --- | --- |\n| ```<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>``` | ```<br>@dispatcher.span<br>@trace_method(\"run_step\")<br>def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step.\"\"\"<br>    state = step.step_state<br>    state[\"count\"] += 1<br>    # new_memory should at the very least contain the user input<br>    messages = task.extra_state[\"new_memory\"].get()<br>    prev_correct_str = messages[-1].content<br>    prev_correct_str_without_prefix = self._remove_correction_str_prefix(<br>        prev_correct_str<br>    )<br>    # reflect phase<br>    reflection, reflection_msg = self._reflect(chat_history=messages)<br>    is_done = reflection.is_done<br>    critique_msg = ChatMessage(role=MessageRole.USER, content=reflection_msg)<br>    task.extra_state[\"new_memory\"].put(critique_msg)<br>    # correction phase<br>    if is_done:<br>        # no correction to be made prev correction is sufficient<br>        agent_response = AgentChatResponse(<br>            response=prev_correct_str_without_prefix,<br>            sources=task.extra_state[\"sources\"],<br>        )<br>        task.extra_state[\"new_memory\"].put(<br>            ChatMessage(<br>                role=MessageRole.ASSISTANT,<br>                content=prev_correct_str_without_prefix,<br>            )<br>        )<br>        new_steps = []<br>    else:<br>        # generate a new correction<br>        correct_msg = self._correct(<br>            input_str=prev_correct_str_without_prefix,<br>            critique=reflection_msg.content,<br>        )<br>        correct_str_without_prefix = self._remove_correction_str_prefix(<br>            correct_msg.content<br>        )<br>        if self.max_iterations == state[\"count\"]:<br>            # this will be the last iteration<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(<br>                    role=MessageRole.ASSISTANT,<br>                    content=correct_str_without_prefix,<br>                )<br>            )<br>            agent_response = AgentChatResponse(response=correct_str_without_prefix)<br>            new_steps = []<br>        else:<br>            # another round of reflection/correction will take place<br>            task.extra_state[\"new_memory\"].put(correct_msg)<br>            agent_response = AgentChatResponse(response=str(correct_msg))<br>            new_steps = [<br>                step.get_next_step(<br>                    step_id=str(uuid.uuid4()),<br>                    # NOTE: input is unused<br>                    input=None,<br>                    step_state=state,<br>                )<br>            ]<br>    return TaskStepOutput(<br>        output=agent_response,<br>        task_step=step,<br>        is_last=is_done | (self.max_iterations == state[\"count\"]),<br>        next_steps=new_steps,<br>    )<br>``` |\n\n### arun\\_step`async`[\\#](\\#llama_index.agent.introspective.SelfReflectionAgentWorker.arun_step \"Permanent link\")\n\n```\narun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async).\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/reflective/self_reflection.py`\n\n|     |     |\n| --- | --- |\n| ```<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>``` | ```<br>@dispatcher.span<br>@trace_method(\"run_step\")<br>async def arun_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async).\"\"\"<br>    state = step.step_state<br>    state[\"count\"] += 1<br>    messages = task.extra_state[\"new_memory\"].get()<br>    prev_correct_str = messages[-1].content<br>    prev_correct_str_without_prefix = self._remove_correction_str_prefix(<br>        prev_correct_str<br>    )<br>    # reflect<br>    reflection, reflection_msg = await self._areflect(chat_history=messages)<br>    is_done = reflection.is_done<br>    critique_msg = ChatMessage(role=MessageRole.USER, content=reflection_msg)<br>    task.extra_state[\"new_memory\"].put(critique_msg)<br>    # correction phase<br>    if is_done:<br>        # no correction to be made prev correction is sufficient<br>        agent_response = AgentChatResponse(<br>            response=prev_correct_str_without_prefix,<br>            sources=task.extra_state[\"sources\"],<br>        )<br>        task.extra_state[\"new_memory\"].put(<br>            ChatMessage(<br>                role=MessageRole.ASSISTANT,<br>                content=prev_correct_str_without_prefix,<br>            )<br>        )<br>        new_steps = []<br>    else:<br>        # generate a new correction<br>        correct_msg = await self._acorrect(<br>            input_str=prev_correct_str_without_prefix,<br>            critique=reflection_msg.content,<br>        )<br>        correct_str_without_prefix = self._remove_correction_str_prefix(<br>            correct_msg.content<br>        )<br>        if self.max_iterations == state[\"count\"]:<br>            # this will be the last iteration<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(<br>                    role=MessageRole.ASSISTANT,<br>                    content=correct_str_without_prefix,<br>                )<br>            )<br>            agent_response = AgentChatResponse(response=correct_str_without_prefix)<br>            new_steps = []<br>        else:<br>            # another round of reflection/correction will take place<br>            task.extra_state[\"new_memory\"].put(correct_msg)<br>            agent_response = AgentChatResponse(response=str(correct_msg))<br>            new_steps = [<br>                step.get_next_step(<br>                    step_id=str(uuid.uuid4()),<br>                    # NOTE: input is unused<br>                    input=None,<br>                    step_state=state,<br>                )<br>            ]<br>    return TaskStepOutput(<br>        output=agent_response,<br>        task_step=step,<br>        is_last=is_done | (self.max_iterations == state[\"count\"]),<br>        next_steps=new_steps,<br>    )<br>``` |\n\n### stream\\_step [\\#](\\#llama_index.agent.introspective.SelfReflectionAgentWorker.stream_step \"Permanent link\")\n\n```\nstream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (stream).\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/reflective/self_reflection.py`\n\n|     |     |\n| --- | --- |\n| ```<br>447<br>448<br>449<br>450<br>451<br>``` | ```<br>@dispatcher.span<br>@trace_method(\"run_step\")<br>def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step (stream).\"\"\"<br>    raise NotImplementedError(\"Stream not supported for self reflection agent\")<br>``` |\n\n### astream\\_step`async`[\\#](\\#llama_index.agent.introspective.SelfReflectionAgentWorker.astream_step \"Permanent link\")\n\n```\nastream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async stream).\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/reflective/self_reflection.py`\n\n|     |     |\n| --- | --- |\n| ```<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>``` | ```<br>@dispatcher.span<br>@trace_method(\"run_step\")<br>async def astream_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async stream).\"\"\"<br>    raise NotImplementedError(\"Stream not supported for self reflection agent\")<br>``` |\n\n### finalize\\_task [\\#](\\#llama_index.agent.introspective.SelfReflectionAgentWorker.finalize_task \"Permanent link\")\n\n```\nfinalize_task(task: Task, **kwargs: Any) -> None\n\n```\n\nFinalize task, after all the steps are completed.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/reflective/self_reflection.py`\n\n|     |     |\n| --- | --- |\n| ```<br>468<br>469<br>470<br>471<br>472<br>473<br>``` | ```<br>def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>    \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>    # add new messages to memory<br>    task.memory.set(task.extra_state[\"new_memory\"].get_all())<br>    # reset new memory<br>    task.extra_state[\"new_memory\"].reset()<br>``` |\n\n## ToolInteractiveReflectionAgentWorker [\\#](\\#llama_index.agent.introspective.ToolInteractiveReflectionAgentWorker \"Permanent link\")\n\nBases: `BaseModel`, `BaseAgentWorker`\n\nTool-Interactive Reflection Agent Worker.\n\nThis agent worker implements the CRITIC reflection framework introduced\nby Gou, Zhibin, et al. (2024) ICLR. (source: https://arxiv.org/pdf/2305.11738)\n\nCRITIC stands for `Correcting with tool-interactive critiquing`. It works\nby performing a reflection on a response to a task/query using external tools\n(e.g., fact checking using a Google search tool) and subsequently using\nthe critique to generate a corrected response. It cycles thru tool-interactive\nreflection and correction until a specific stopping criteria has been met\nor a max number of iterations has been reached.\n\nThis agent delegates the critique subtask to a user-supplied `critique_agent_worker`\nthat is of `FunctionCallingAgentWorker` type i.e. it uses tools to perform\ntasks. For correction, it uses a user-specified `correction_llm` with a\nPydanticProgram (determined dynamically with llm.structured\\_predict)\nin order to produce a structured output, namely `Correction` that\ncontains the correction generated by the `correction_llm`.\n\n**Attributes:**\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `critique_agent_worker` | `FunctionCallingAgentWorker` | Critique agent responsible<br>for performing the critique reflection. |\n| `critique_template` | `str` | The template containing instructions for how the<br>Critique agent should perform the reflection. |\n| `max_iterations` | `int` | The max number of reflection & correction<br>cycles permitted. Defaults to DEFAULT\\_MAX\\_ITERATIONS = 5. |\n| `stopping_callable` | `Optional[StoppingCallable]` | An optional stopping<br>condition that operates over the critique reflection string and returns<br>a boolean to determine if the latest correction is sufficient. Defaults to None. |\n| `correction_llm` | `Optional[LLM]` | The LLM used for producing corrected<br>responses against a critique or reflection. Defaults to None. |\n| `callback_manager` | `Optional[CallbackManager]` | Callback manager. Defaults to None. |\n| `verbose` | `bool` | Whether execution should be verbose. Defaults to False. |\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/reflective/tool_interactive_reflection.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>``` | ```<br>class ToolInteractiveReflectionAgentWorker(BaseModel, BaseAgentWorker):<br>    \"\"\"Tool-Interactive Reflection Agent Worker.<br>    This agent worker implements the CRITIC reflection framework introduced<br>    by Gou, Zhibin, et al. (2024) ICLR. (source: https://arxiv.org/pdf/2305.11738)<br>    CRITIC stands for `Correcting with tool-interactive critiquing`. It works<br>    by performing a reflection on a response to a task/query using external tools<br>    (e.g., fact checking using a Google search tool) and subsequently using<br>    the critique to generate a corrected response. It cycles thru tool-interactive<br>    reflection and correction until a specific stopping criteria has been met<br>    or a max number of iterations has been reached.<br>    This agent delegates the critique subtask to a user-supplied `critique_agent_worker`<br>    that is of `FunctionCallingAgentWorker` type i.e. it uses tools to perform<br>    tasks. For correction, it uses a user-specified `correction_llm` with a<br>    PydanticProgram (determined dynamically with llm.structured_predict)<br>    in order to produce a structured output, namely `Correction` that<br>    contains the correction generated by the `correction_llm`.<br>    Attributes:<br>        critique_agent_worker (FunctionCallingAgentWorker): Critique agent responsible<br>            for performing the critique reflection.<br>        critique_template (str): The template containing instructions for how the<br>            Critique agent should perform the reflection.<br>        max_iterations (int, optional): The max number of reflection & correction<br>            cycles permitted. Defaults to DEFAULT_MAX_ITERATIONS = 5.<br>        stopping_callable (Optional[StoppingCallable], optional): An optional stopping<br>            condition that operates over the critique reflection string and returns<br>            a boolean to determine if the latest correction is sufficient. Defaults to None.<br>        correction_llm (Optional[LLM], optional): The LLM used for producing corrected<br>            responses against a critique or reflection. Defaults to None.<br>        callback_manager (Optional[CallbackManager], optional): Callback manager. Defaults to None.<br>        verbose (bool, optional): Whether execution should be verbose. Defaults to False.<br>    \"\"\"<br>    callback_manager: CallbackManager = Field(default=CallbackManager([]))<br>    max_iterations: int = Field(default=DEFAULT_MAX_ITERATIONS)<br>    stopping_callable: Optional[StoppingCallable] = Field(<br>        default=None,<br>        description=\"Optional function that operates on critique string to see if no more corrections are needed.\",<br>    )<br>    _critique_agent_worker: FunctionCallingAgentWorker = PrivateAttr()<br>    _critique_template: str = PrivateAttr()<br>    _correction_llm: LLM = PrivateAttr()<br>    _verbose: bool = PrivateAttr()<br>    class Config:<br>        arbitrary_types_allowed = True<br>    def __init__(<br>        self,<br>        critique_agent_worker: FunctionCallingAgentWorker,<br>        critique_template: str,<br>        max_iterations: int = DEFAULT_MAX_ITERATIONS,<br>        stopping_callable: Optional[StoppingCallable] = None,<br>        correction_llm: Optional[LLM] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        **kwargs: Any,<br>    ) -> None:<br>        \"\"\"__init__.\"\"\"<br>        super().__init__(<br>            callback_manager=callback_manager,<br>            max_iterations=max_iterations,<br>            stopping_callable=stopping_callable,<br>            **kwargs,<br>        )<br>        self._critique_agent_worker = critique_agent_worker<br>        self._critique_template = critique_template<br>        self._verbose = verbose<br>        self._correction_llm = correction_llm<br>    @classmethod<br>    def from_defaults(<br>        cls,<br>        critique_agent_worker: FunctionCallingAgentWorker,<br>        critique_template: str,<br>        correction_llm: Optional[LLM] = None,<br>        max_iterations: int = DEFAULT_MAX_ITERATIONS,<br>        stopping_callable: Optional[StoppingCallable] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        **kwargs: Any,<br>    ) -> \"ToolInteractiveReflectionAgentWorker\":<br>        \"\"\"Convenience constructor method from set of of BaseTools (Optional).\"\"\"<br>        if correction_llm is None:<br>            try:<br>                from llama_index.llms.openai import OpenAI<br>            except ImportError:<br>                raise ImportError(<br>                    \"Missing OpenAI LLMs. Please run `pip install llama-index-llms-openai`.\"<br>                )<br>            correction_llm = OpenAI(model=\"gpt-4-turbo-preview\", temperature=0)<br>        return cls(<br>            critique_agent_worker=critique_agent_worker,<br>            critique_template=critique_template,<br>            correction_llm=correction_llm,<br>            max_iterations=max_iterations,<br>            stopping_callable=stopping_callable,<br>            callback_manager=callback_manager or CallbackManager([]),<br>            verbose=verbose,<br>            **kwargs,<br>        )<br>    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>        \"\"\"Initialize step from task.\"\"\"<br>        # temporary memory for new messages<br>        new_memory = ChatMemoryBuffer.from_defaults()<br>        # put current history in new memory<br>        messages = task.memory.get()<br>        for message in messages:<br>            new_memory.put(message)<br>        # inject new input into memory<br>        new_memory.put(ChatMessage(content=task.input, role=MessageRole.USER))<br>        # initialize task state<br>        task_state = {<br>            \"new_memory\": new_memory,<br>            \"sources\": [],<br>        }<br>        task.extra_state.update(task_state)<br>        return TaskStep(<br>            task_id=task.task_id,<br>            step_id=str(uuid.uuid4()),<br>            input=task.input,<br>            step_state={\"count\": 0},<br>        )<br>    def _remove_correction_str_prefix(self, correct_msg: str) -> str:<br>        \"\"\"Helper function to format correction message for final response.\"\"\"<br>        return correct_msg.replace(CORRECT_RESPONSE_PREFIX, \"\")<br>    @dispatcher.span<br>    def _critique(self, input_str: str) -> AgentChatResponse:<br>        agent = self._critique_agent_worker.as_agent(verbose=self._verbose)<br>        critique = agent.chat(self._critique_template.format(input_str=input_str))<br>        if self._verbose:<br>            print(f\"Critique: {critique.response}\", flush=True)<br>        return critique<br>    @dispatcher.span<br>    def _correct(self, input_str: str, critique: str) -> ChatMessage:<br>        correction = self._correction_llm.structured_predict(<br>            Correction,<br>            PromptTemplate(CORRECT_PROMPT_TEMPLATE),<br>            input_str=input_str,<br>            critique=critique,<br>        )<br>        correct_response_str = CORRECT_RESPONSE_FSTRING.format(<br>            correction=correction.correction<br>        )<br>        if self._verbose:<br>            print(f\"Correction: {correction.correction}\", flush=True)<br>        return ChatMessage.from_str(correct_response_str, role=\"assistant\")<br>    @dispatcher.span<br>    @trace_method(\"run_step\")<br>    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        state = step.step_state<br>        state[\"count\"] += 1<br>        messages = task.extra_state[\"new_memory\"].get()<br>        prev_correct_str = messages[-1].content<br>        prev_correct_str_without_prefix = self._remove_correction_str_prefix(<br>            prev_correct_str<br>        )<br>        # critique phase<br>        critique_response = self._critique(input_str=prev_correct_str_without_prefix)<br>        task.extra_state[\"sources\"].extend(critique_response.sources)<br>        is_done = False<br>        if self.stopping_callable:<br>            is_done = self.stopping_callable(critique_str=critique_response.response)<br>        critique_msg = ChatMessage(<br>            role=MessageRole.USER, content=critique_response.response<br>        )<br>        task.extra_state[\"new_memory\"].put(critique_msg)<br>        # correction phase<br>        if is_done:<br>            # no correction to be made prev correction is sufficient<br>            agent_response = AgentChatResponse(<br>                response=prev_correct_str_without_prefix,<br>                sources=task.extra_state[\"sources\"],<br>            )<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(<br>                    role=MessageRole.ASSISTANT, content=prev_correct_str_without_prefix<br>                )<br>            )<br>            new_steps = []<br>        else:<br>            # generate a new correction<br>            correct_msg = self._correct(<br>                input_str=prev_correct_str_without_prefix,<br>                critique=critique_response.response,<br>            )<br>            correct_str_without_prefix = self._remove_correction_str_prefix(<br>                correct_msg.content<br>            )<br>            # reached max iterations, no further reflection/correction cycles<br>            if self.max_iterations == state[\"count\"]:<br>                task.extra_state[\"new_memory\"].put(<br>                    ChatMessage(<br>                        role=MessageRole.ASSISTANT, content=correct_str_without_prefix<br>                    )<br>                )<br>                agent_response = AgentChatResponse(response=correct_str_without_prefix)<br>                new_steps = []<br>            else:<br>                # another round of reflection/correction will take place<br>                task.extra_state[\"new_memory\"].put(correct_msg)<br>                agent_response = AgentChatResponse(<br>                    response=str(correct_msg), sources=critique_response.sources<br>                )<br>                new_steps = [<br>                    step.get_next_step(<br>                        step_id=str(uuid.uuid4()),<br>                        # NOTE: input is unused<br>                        input=None,<br>                        step_state=state,<br>                    )<br>                ]<br>        return TaskStepOutput(<br>            output=agent_response,<br>            task_step=step,<br>            is_last=is_done | (self.max_iterations == state[\"count\"]),<br>            next_steps=new_steps,<br>        )<br>    # Async Methods<br>    @dispatcher.span<br>    async def _acritique(self, input_str: str) -> AgentChatResponse:<br>        agent = self._critique_agent_worker.as_agent(verbose=self._verbose)<br>        critique = await agent.achat(<br>            self._critique_template.format(input_str=input_str)<br>        )<br>        if self._verbose:<br>            print(f\"Critique: {critique.response}\", flush=True)<br>        return critique<br>    @dispatcher.span<br>    async def _acorrect(self, input_str: str, critique: str) -> ChatMessage:<br>        correction = await self._correction_llm.astructured_predict(<br>            Correction,<br>            PromptTemplate(CORRECT_PROMPT_TEMPLATE),<br>            input_str=input_str,<br>            critique=critique,<br>        )<br>        correct_response_str = CORRECT_RESPONSE_FSTRING.format(<br>            correction=correction.correction<br>        )<br>        if self._verbose:<br>            print(f\"Correction: {correction.correction}\", flush=True)<br>        return ChatMessage.from_str(correct_response_str, role=\"assistant\")<br>    @dispatcher.span<br>    @trace_method(\"run_step\")<br>    async def arun_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        state = step.step_state<br>        state[\"count\"] += 1<br>        messages = task.extra_state[\"new_memory\"].get()<br>        prev_correct_str = messages[-1].content<br>        prev_correct_str_without_prefix = self._remove_correction_str_prefix(<br>            prev_correct_str<br>        )<br>        # critique phase<br>        critique_response = await self._acritique(<br>            input_str=prev_correct_str_without_prefix<br>        )<br>        task.extra_state[\"sources\"].extend(critique_response.sources)<br>        is_done = False<br>        if self.stopping_callable:<br>            is_done = self.stopping_callable(critique_str=critique_response.response)<br>        critique_msg = ChatMessage(<br>            role=MessageRole.USER, content=critique_response.response<br>        )<br>        task.extra_state[\"new_memory\"].put(critique_msg)<br>        # correction phase<br>        if is_done:<br>            # no correction to be made prev correction is sufficient<br>            agent_response = AgentChatResponse(<br>                response=prev_correct_str_without_prefix,<br>                sources=task.extra_state[\"sources\"],<br>            )<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(<br>                    role=MessageRole.ASSISTANT, content=prev_correct_str_without_prefix<br>                )<br>            )<br>            new_steps = []<br>        else:<br>            # generate a new correction<br>            correct_msg = await self._acorrect(<br>                input_str=prev_correct_str_without_prefix,<br>                critique=critique_response.response,<br>            )<br>            correct_str_without_prefix = self._remove_correction_str_prefix(<br>                correct_msg.content<br>            )<br>            # reached max iterations, no further reflection/correction cycles<br>            if self.max_iterations == state[\"count\"]:<br>                task.extra_state[\"new_memory\"].put(<br>                    ChatMessage(<br>                        role=MessageRole.ASSISTANT, content=correct_str_without_prefix<br>                    )<br>                )<br>                agent_response = AgentChatResponse(response=correct_str_without_prefix)<br>                new_steps = []<br>            else:<br>                # another round of reflection/correction will take place<br>                task.extra_state[\"new_memory\"].put(correct_msg)<br>                agent_response = AgentChatResponse(<br>                    response=str(correct_msg), sources=critique_response.sources<br>                )<br>                new_steps = [<br>                    step.get_next_step(<br>                        step_id=str(uuid.uuid4()),<br>                        # NOTE: input is unused<br>                        input=None,<br>                        step_state=state,<br>                    )<br>                ]<br>        return TaskStepOutput(<br>            output=agent_response,<br>            task_step=step,<br>            is_last=is_done | (self.max_iterations == state[\"count\"]),<br>            next_steps=new_steps,<br>        )<br>    # Steam methods<br>    @dispatcher.span<br>    @trace_method(\"run_step\")<br>    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        raise NotImplementedError(<br>            \"Stream not supported for tool-interactive reflection agent\"<br>        )<br>    @dispatcher.span<br>    @trace_method(\"run_step\")<br>    async def astream_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async stream).\"\"\"<br>        raise NotImplementedError(<br>            \"Stream not supported for tool-interactive reflection agent\"<br>        )<br>    def get_all_messages(self, task: Task) -> List[ChatMessage]:<br>        return (<br>            self.prefix_messages<br>            + task.memory.get()<br>            + task.extra_state[\"new_memory\"].get_all()<br>        )<br>    def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>        \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>        # add new messages to memory<br>        task.memory.set(task.extra_state[\"new_memory\"].get_all())<br>        # reset new memory<br>        task.extra_state[\"new_memory\"].reset()<br>``` |\n\n### from\\_defaults`classmethod`[\\#](\\#llama_index.agent.introspective.ToolInteractiveReflectionAgentWorker.from_defaults \"Permanent link\")\n\n```\nfrom_defaults(critique_agent_worker: FunctionCallingAgentWorker, critique_template: str, correction_llm: Optional[LLM] = None, max_iterations: int = DEFAULT_MAX_ITERATIONS, stopping_callable: Optional[StoppingCallable] = None, callback_manager: Optional[CallbackManager] = None, verbose: bool = False, **kwargs: Any) -> ToolInteractiveReflectionAgentWorker\n\n```\n\nConvenience constructor method from set of of BaseTools (Optional).\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/reflective/tool_interactive_reflection.py`\n\n|     |     |\n| --- | --- |\n| ```<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>``` | ```<br>@classmethod<br>def from_defaults(<br>    cls,<br>    critique_agent_worker: FunctionCallingAgentWorker,<br>    critique_template: str,<br>    correction_llm: Optional[LLM] = None,<br>    max_iterations: int = DEFAULT_MAX_ITERATIONS,<br>    stopping_callable: Optional[StoppingCallable] = None,<br>    callback_manager: Optional[CallbackManager] = None,<br>    verbose: bool = False,<br>    **kwargs: Any,<br>) -> \"ToolInteractiveReflectionAgentWorker\":<br>    \"\"\"Convenience constructor method from set of of BaseTools (Optional).\"\"\"<br>    if correction_llm is None:<br>        try:<br>            from llama_index.llms.openai import OpenAI<br>        except ImportError:<br>            raise ImportError(<br>                \"Missing OpenAI LLMs. Please run `pip install llama-index-llms-openai`.\"<br>            )<br>        correction_llm = OpenAI(model=\"gpt-4-turbo-preview\", temperature=0)<br>    return cls(<br>        critique_agent_worker=critique_agent_worker,<br>        critique_template=critique_template,<br>        correction_llm=correction_llm,<br>        max_iterations=max_iterations,<br>        stopping_callable=stopping_callable,<br>        callback_manager=callback_manager or CallbackManager([]),<br>        verbose=verbose,<br>        **kwargs,<br>    )<br>``` |\n\n### initialize\\_step [\\#](\\#llama_index.agent.introspective.ToolInteractiveReflectionAgentWorker.initialize_step \"Permanent link\")\n\n```\ninitialize_step(task: Task, **kwargs: Any) -> TaskStep\n\n```\n\nInitialize step from task.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/reflective/tool_interactive_reflection.py`\n\n|     |     |\n| --- | --- |\n| ```<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>``` | ```<br>def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>    \"\"\"Initialize step from task.\"\"\"<br>    # temporary memory for new messages<br>    new_memory = ChatMemoryBuffer.from_defaults()<br>    # put current history in new memory<br>    messages = task.memory.get()<br>    for message in messages:<br>        new_memory.put(message)<br>    # inject new input into memory<br>    new_memory.put(ChatMessage(content=task.input, role=MessageRole.USER))<br>    # initialize task state<br>    task_state = {<br>        \"new_memory\": new_memory,<br>        \"sources\": [],<br>    }<br>    task.extra_state.update(task_state)<br>    return TaskStep(<br>        task_id=task.task_id,<br>        step_id=str(uuid.uuid4()),<br>        input=task.input,<br>        step_state={\"count\": 0},<br>    )<br>``` |\n\n### run\\_step [\\#](\\#llama_index.agent.introspective.ToolInteractiveReflectionAgentWorker.run_step \"Permanent link\")\n\n```\nrun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/reflective/tool_interactive_reflection.py`\n\n|     |     |\n| --- | --- |\n| ```<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>``` | ```<br>@dispatcher.span<br>@trace_method(\"run_step\")<br>def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step.\"\"\"<br>    state = step.step_state<br>    state[\"count\"] += 1<br>    messages = task.extra_state[\"new_memory\"].get()<br>    prev_correct_str = messages[-1].content<br>    prev_correct_str_without_prefix = self._remove_correction_str_prefix(<br>        prev_correct_str<br>    )<br>    # critique phase<br>    critique_response = self._critique(input_str=prev_correct_str_without_prefix)<br>    task.extra_state[\"sources\"].extend(critique_response.sources)<br>    is_done = False<br>    if self.stopping_callable:<br>        is_done = self.stopping_callable(critique_str=critique_response.response)<br>    critique_msg = ChatMessage(<br>        role=MessageRole.USER, content=critique_response.response<br>    )<br>    task.extra_state[\"new_memory\"].put(critique_msg)<br>    # correction phase<br>    if is_done:<br>        # no correction to be made prev correction is sufficient<br>        agent_response = AgentChatResponse(<br>            response=prev_correct_str_without_prefix,<br>            sources=task.extra_state[\"sources\"],<br>        )<br>        task.extra_state[\"new_memory\"].put(<br>            ChatMessage(<br>                role=MessageRole.ASSISTANT, content=prev_correct_str_without_prefix<br>            )<br>        )<br>        new_steps = []<br>    else:<br>        # generate a new correction<br>        correct_msg = self._correct(<br>            input_str=prev_correct_str_without_prefix,<br>            critique=critique_response.response,<br>        )<br>        correct_str_without_prefix = self._remove_correction_str_prefix(<br>            correct_msg.content<br>        )<br>        # reached max iterations, no further reflection/correction cycles<br>        if self.max_iterations == state[\"count\"]:<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(<br>                    role=MessageRole.ASSISTANT, content=correct_str_without_prefix<br>                )<br>            )<br>            agent_response = AgentChatResponse(response=correct_str_without_prefix)<br>            new_steps = []<br>        else:<br>            # another round of reflection/correction will take place<br>            task.extra_state[\"new_memory\"].put(correct_msg)<br>            agent_response = AgentChatResponse(<br>                response=str(correct_msg), sources=critique_response.sources<br>            )<br>            new_steps = [<br>                step.get_next_step(<br>                    step_id=str(uuid.uuid4()),<br>                    # NOTE: input is unused<br>                    input=None,<br>                    step_state=state,<br>                )<br>            ]<br>    return TaskStepOutput(<br>        output=agent_response,<br>        task_step=step,<br>        is_last=is_done | (self.max_iterations == state[\"count\"]),<br>        next_steps=new_steps,<br>    )<br>``` |\n\n### arun\\_step`async`[\\#](\\#llama_index.agent.introspective.ToolInteractiveReflectionAgentWorker.arun_step \"Permanent link\")\n\n```\narun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async).\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/reflective/tool_interactive_reflection.py`\n\n|     |     |\n| --- | --- |\n| ```<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>``` | ```<br>@dispatcher.span<br>@trace_method(\"run_step\")<br>async def arun_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async).\"\"\"<br>    state = step.step_state<br>    state[\"count\"] += 1<br>    messages = task.extra_state[\"new_memory\"].get()<br>    prev_correct_str = messages[-1].content<br>    prev_correct_str_without_prefix = self._remove_correction_str_prefix(<br>        prev_correct_str<br>    )<br>    # critique phase<br>    critique_response = await self._acritique(<br>        input_str=prev_correct_str_without_prefix<br>    )<br>    task.extra_state[\"sources\"].extend(critique_response.sources)<br>    is_done = False<br>    if self.stopping_callable:<br>        is_done = self.stopping_callable(critique_str=critique_response.response)<br>    critique_msg = ChatMessage(<br>        role=MessageRole.USER, content=critique_response.response<br>    )<br>    task.extra_state[\"new_memory\"].put(critique_msg)<br>    # correction phase<br>    if is_done:<br>        # no correction to be made prev correction is sufficient<br>        agent_response = AgentChatResponse(<br>            response=prev_correct_str_without_prefix,<br>            sources=task.extra_state[\"sources\"],<br>        )<br>        task.extra_state[\"new_memory\"].put(<br>            ChatMessage(<br>                role=MessageRole.ASSISTANT, content=prev_correct_str_without_prefix<br>            )<br>        )<br>        new_steps = []<br>    else:<br>        # generate a new correction<br>        correct_msg = await self._acorrect(<br>            input_str=prev_correct_str_without_prefix,<br>            critique=critique_response.response,<br>        )<br>        correct_str_without_prefix = self._remove_correction_str_prefix(<br>            correct_msg.content<br>        )<br>        # reached max iterations, no further reflection/correction cycles<br>        if self.max_iterations == state[\"count\"]:<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(<br>                    role=MessageRole.ASSISTANT, content=correct_str_without_prefix<br>                )<br>            )<br>            agent_response = AgentChatResponse(response=correct_str_without_prefix)<br>            new_steps = []<br>        else:<br>            # another round of reflection/correction will take place<br>            task.extra_state[\"new_memory\"].put(correct_msg)<br>            agent_response = AgentChatResponse(<br>                response=str(correct_msg), sources=critique_response.sources<br>            )<br>            new_steps = [<br>                step.get_next_step(<br>                    step_id=str(uuid.uuid4()),<br>                    # NOTE: input is unused<br>                    input=None,<br>                    step_state=state,<br>                )<br>            ]<br>    return TaskStepOutput(<br>        output=agent_response,<br>        task_step=step,<br>        is_last=is_done | (self.max_iterations == state[\"count\"]),<br>        next_steps=new_steps,<br>    )<br>``` |\n\n### stream\\_step [\\#](\\#llama_index.agent.introspective.ToolInteractiveReflectionAgentWorker.stream_step \"Permanent link\")\n\n```\nstream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (stream).\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/reflective/tool_interactive_reflection.py`\n\n|     |     |\n| --- | --- |\n| ```<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>``` | ```<br>@dispatcher.span<br>@trace_method(\"run_step\")<br>def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step (stream).\"\"\"<br>    raise NotImplementedError(<br>        \"Stream not supported for tool-interactive reflection agent\"<br>    )<br>``` |\n\n### astream\\_step`async`[\\#](\\#llama_index.agent.introspective.ToolInteractiveReflectionAgentWorker.astream_step \"Permanent link\")\n\n```\nastream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async stream).\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/reflective/tool_interactive_reflection.py`\n\n|     |     |\n| --- | --- |\n| ```<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>``` | ```<br>@dispatcher.span<br>@trace_method(\"run_step\")<br>async def astream_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async stream).\"\"\"<br>    raise NotImplementedError(<br>        \"Stream not supported for tool-interactive reflection agent\"<br>    )<br>``` |\n\n### finalize\\_task [\\#](\\#llama_index.agent.introspective.ToolInteractiveReflectionAgentWorker.finalize_task \"Permanent link\")\n\n```\nfinalize_task(task: Task, **kwargs: Any) -> None\n\n```\n\nFinalize task, after all the steps are completed.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-introspective/llama_index/agent/introspective/reflective/tool_interactive_reflection.py`\n\n|     |     |\n| --- | --- |\n| ```<br>448<br>449<br>450<br>451<br>452<br>453<br>``` | ```<br>def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>    \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>    # add new messages to memory<br>    task.memory.set(task.extra_state[\"new_memory\"].get_all())<br>    # reset new memory<br>    task.extra_state[\"new_memory\"].reset()<br>``` |\n\nBack to top",
      "metadata": {
        "title": "Introspective - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/agent/introspective/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "\n\n# Alibabacloud aisearch\n\n## AlibabaCloudAISearchEmbedding [\\#](\\#llama_index.embeddings.alibabacloud_aisearch.AlibabaCloudAISearchEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nFor further details, please visit `https://help.aliyun.com/zh/open-search/search-platform/developer-reference/text-embedding-api-details`.\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-alibabacloud-aisearch/llama_index/embeddings/alibabacloud_aisearch/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>``` | ```<br>class AlibabaCloudAISearchEmbedding(BaseEmbedding):<br>    \"\"\"<br>    For further details, please visit `https://help.aliyun.com/zh/open-search/search-platform/developer-reference/text-embedding-api-details`.<br>    \"\"\"<br>    _client: Client = PrivateAttr()<br>    aisearch_api_key: str = Field(default=None, exclude=True)<br>    endpoint: str = None<br>    service_id: str = \"ops-text-embedding-002\"<br>    workspace_name: str = \"default\"<br>    def __init__(<br>        self, endpoint: str = None, aisearch_api_key: str = None, **kwargs: Any<br>    ) -> None:<br>        super().__init__(**kwargs)<br>        self.aisearch_api_key = get_from_param_or_env(<br>            \"aisearch_api_key\", aisearch_api_key, \"AISEARCH_API_KEY\"<br>        )<br>        self.endpoint = get_from_param_or_env(\"endpoint\", endpoint, \"AISEARCH_ENDPOINT\")<br>        config = AISearchConfig(<br>            bearer_token=self.aisearch_api_key,<br>            endpoint=self.endpoint,<br>            protocol=\"http\",<br>        )<br>        self._client = Client(config=config)<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"AlibabaCloudAISearchEmbedding\"<br>    @retry_decorator<br>    def _get_embedding(self, text: str, input_type: str) -> List[float]:<br>        request = GetTextEmbeddingRequest(input=text, input_type=input_type)<br>        response: GetTextEmbeddingResponse = self._client.get_text_embedding(<br>            workspace_name=self.workspace_name,<br>            service_id=self.service_id,<br>            request=request,<br>        )<br>        embeddings = response.body.result.embeddings<br>        return embeddings[0].embedding<br>    @aretry_decorator<br>    async def _aget_embedding(self, text: str, input_type: str) -> List[float]:<br>        request = GetTextEmbeddingRequest(input=text, input_type=input_type)<br>        response: GetTextEmbeddingResponse = (<br>            await self._client.get_text_embedding_async(<br>                workspace_name=self.workspace_name,<br>                service_id=self.service_id,<br>                request=request,<br>            )<br>        )<br>        embeddings = response.body.result.embeddings<br>        return embeddings[0].embedding<br>    @retry_decorator<br>    def _get_embeddings(self, texts: List[str], input_type: str) -> List[List[float]]:<br>        request = GetTextEmbeddingRequest(input=texts, input_type=input_type)<br>        response: GetTextEmbeddingResponse = self._client.get_text_embedding(<br>            workspace_name=self.workspace_name,<br>            service_id=self.service_id,<br>            request=request,<br>        )<br>        embeddings = response.body.result.embeddings<br>        return [emb.embedding for emb in embeddings]<br>    @aretry_decorator<br>    async def _aget_embeddings(<br>        self,<br>        texts: List[str],<br>        input_type: str,<br>    ) -> List[List[float]]:<br>        request = GetTextEmbeddingRequest(input=texts, input_type=input_type)<br>        response: GetTextEmbeddingResponse = (<br>            await self._client.get_text_embedding_async(<br>                workspace_name=self.workspace_name,<br>                service_id=self.service_id,<br>                request=request,<br>            )<br>        )<br>        embeddings = response.body.result.embeddings<br>        return [emb.embedding for emb in embeddings]<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self._get_embedding(<br>            query,<br>            input_type=\"query\",<br>        )<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        return await self._aget_embedding(<br>            query,<br>            input_type=\"query\",<br>        )<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._get_embedding(<br>            text,<br>            input_type=\"document\",<br>        )<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_text_embedding.\"\"\"<br>        return await self._aget_embedding(<br>            text,<br>            input_type=\"document\",<br>        )<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        return self._get_embeddings(<br>            texts,<br>            input_type=\"document\",<br>        )<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"The asynchronous version of _get_text_embeddings.\"\"\"<br>        return await self._aget_embeddings(<br>            texts,<br>            input_type=\"document\",<br>        )<br>``` |\n\nBack to top",
      "metadata": {
        "title": "Alibabacloud aisearch - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/alibabacloud_aisearch/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "\n\n# Deepinfra\n\n## DeepInfraEmbeddingModel [\\#](\\#llama_index.embeddings.deepinfra.DeepInfraEmbeddingModel \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nA wrapper class for accessing embedding models available via the DeepInfra API. This class allows for easy integration\nof DeepInfra embeddings into your projects, supporting both synchronous and asynchronous retrieval of text embeddings.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model_id` | `str` | Identifier for the model to be used for embeddings. Defaults to 'sentence-transformers/clip-ViT-B-32'. | `DEFAULT_MODEL_ID` |\n| `normalize` | `bool` | Flag to normalize embeddings post retrieval. Defaults to False. | `False` |\n| `api_token` | `str` | DeepInfra API token. If not provided, | `None` |\n\n**Examples:**\n\n```\n>>> from llama_index.embeddings.deepinfra import DeepInfraEmbeddingModel\n>>> model = DeepInfraEmbeddingModel()\n>>> print(model.get_text_embedding(\"Hello, world!\"))\n[0.1, 0.2, 0.3, ...]\n\n```\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-deepinfra/llama_index/embeddings/deepinfra/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>``` | ```<br>class DeepInfraEmbeddingModel(BaseEmbedding):<br>    \"\"\"<br>    A wrapper class for accessing embedding models available via the DeepInfra API. This class allows for easy integration<br>    of DeepInfra embeddings into your projects, supporting both synchronous and asynchronous retrieval of text embeddings.<br>    Args:<br>        model_id (str): Identifier for the model to be used for embeddings. Defaults to 'sentence-transformers/clip-ViT-B-32'.<br>        normalize (bool): Flag to normalize embeddings post retrieval. Defaults to False.<br>        api_token (str): DeepInfra API token. If not provided,<br>        the token is fetched from the environment variable 'DEEPINFRA_API_TOKEN'.<br>    Examples:<br>        >>> from llama_index.embeddings.deepinfra import DeepInfraEmbeddingModel<br>        >>> model = DeepInfraEmbeddingModel()<br>        >>> print(model.get_text_embedding(\"Hello, world!\"))<br>        [0.1, 0.2, 0.3, ...]<br>    \"\"\"<br>    \"\"\"model_id can be obtained from the DeepInfra website.\"\"\"<br>    _model_id: str = PrivateAttr()<br>    \"\"\"normalize flag to normalize embeddings post retrieval.\"\"\"<br>    _normalize: bool = PrivateAttr()<br>    \"\"\"api_token should be obtained from the DeepInfra website.\"\"\"<br>    _api_token: str = PrivateAttr()<br>    \"\"\"query_prefix is used to add a prefix to queries.\"\"\"<br>    _query_prefix: str = PrivateAttr()<br>    \"\"\"text_prefix is used to add a prefix to texts.\"\"\"<br>    _text_prefix: str = PrivateAttr()<br>    def __init__(<br>        self,<br>        model_id: str = DEFAULT_MODEL_ID,<br>        normalize: bool = False,<br>        api_token: str = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        query_prefix: str = \"\",<br>        text_prefix: str = \"\",<br>        embed_batch_size: int = MAX_BATCH_SIZE,<br>    ) -> None:<br>        \"\"\"<br>        Init params.<br>        \"\"\"<br>        super().__init__(<br>            callback_manager=callback_manager, embed_batch_size=embed_batch_size<br>        )<br>        self._model_id = model_id<br>        self._normalize = normalize<br>        self._api_token = api_token or os.getenv(ENV_VARIABLE, None)<br>        self._query_prefix = query_prefix<br>        self._text_prefix = text_prefix<br>    def _post(self, data: List[str]) -> List[List[float]]:<br>        \"\"\"<br>        Sends a POST request to the DeepInfra Inference API with the given data and returns the API response.<br>        Input data is chunked into batches to avoid exceeding the maximum batch size (1024).<br>        Args:<br>            data (List[str]): A list of strings to be embedded.<br>        Returns:<br>            dict: A dictionary containing embeddings from the API.<br>        \"\"\"<br>        url = self.get_url()<br>        chunked_data = _chunk(data, self.embed_batch_size)<br>        embeddings = []<br>        for chunk in chunked_data:<br>            response = requests.post(<br>                url,<br>                json={<br>                    \"inputs\": chunk,<br>                },<br>                headers=self._get_headers(),<br>            )<br>            response.raise_for_status()<br>            embeddings.extend(response.json()[\"embeddings\"])<br>        return embeddings<br>    def get_url(self):<br>        \"\"\"<br>        Get DeepInfra API URL.<br>        \"\"\"<br>        return f\"{INFERENCE_URL}/{self._model_id}\"<br>    async def _apost(self, data: List[str]) -> List[List[float]]:<br>        \"\"\"<br>        Sends a POST request to the DeepInfra Inference API with the given data and returns the API response.<br>        Input data is chunked into batches to avoid exceeding the maximum batch size (1024).<br>        Args:<br>            data (List[str]): A list of strings to be embedded.<br>        Output:<br>            List[float]: A list of embeddings from the API.<br>        \"\"\"<br>        url = self.get_url()<br>        chunked_data = _chunk(data, self.embed_batch_size)<br>        embeddings = []<br>        for chunk in chunked_data:<br>            async with aiohttp.ClientSession() as session:<br>                async with session.post(<br>                    url,<br>                    json={<br>                        \"inputs\": chunk,<br>                    },<br>                    headers=self._get_headers(),<br>                ) as resp:<br>                    response = await resp.json()<br>                    embeddings.extend(response[\"embeddings\"])<br>        return embeddings<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"<br>        Get query embedding.<br>        \"\"\"<br>        return self._post(self._add_query_prefix([query]))[0]<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"<br>        Async get query embedding.<br>        \"\"\"<br>        response = await self._apost(self._add_query_prefix([query]))<br>        return response[0]<br>    def _get_query_embeddings(self, queries: List[str]) -> List[List[float]]:<br>        \"\"\"<br>        Get query embeddings.<br>        \"\"\"<br>        return self._post(self._add_query_prefix(queries))<br>    async def _aget_query_embeddings(self, queries: List[str]) -> List[List[float]]:<br>        \"\"\"<br>        Async get query embeddings.<br>        \"\"\"<br>        return await self._apost(self._add_query_prefix(queries))<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"<br>        Get text embedding.<br>        \"\"\"<br>        return self._post(self._add_text_prefix([text]))[0]<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"<br>        Async get text embedding.<br>        \"\"\"<br>        response = await self._apost(self._add_text_prefix([text]))<br>        return response[0]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"<br>        Get text embedding.<br>        \"\"\"<br>        return self._post(self._add_text_prefix(texts))<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"<br>        Async get text embeddings.<br>        \"\"\"<br>        return await self._apost(self._add_text_prefix(texts))<br>    def _add_query_prefix(self, queries: List[str]) -> List[str]:<br>        \"\"\"<br>        Add query prefix to queries.<br>        \"\"\"<br>        return (<br>            [self._query_prefix + query for query in queries]<br>            if self._query_prefix<br>            else queries<br>        )<br>    def _add_text_prefix(self, texts: List[str]) -> List[str]:<br>        \"\"\"<br>        Add text prefix to texts.<br>        \"\"\"<br>        return (<br>            [self._text_prefix + text for text in texts] if self._text_prefix else texts<br>        )<br>    def _get_headers(self) -> dict:<br>        \"\"\"<br>        Get headers.<br>        \"\"\"<br>        return {<br>            \"Authorization\": f\"Bearer {self._api_token}\",<br>            \"Content-Type\": \"application/json\",<br>            \"User-Agent\": USER_AGENT,<br>        }<br>``` |\n\n### get\\_url [\\#](\\#llama_index.embeddings.deepinfra.DeepInfraEmbeddingModel.get_url \"Permanent link\")\n\n```\nget_url()\n\n```\n\nGet DeepInfra API URL.\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-deepinfra/llama_index/embeddings/deepinfra/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>104<br>105<br>106<br>107<br>108<br>``` | ```<br>def get_url(self):<br>    \"\"\"<br>    Get DeepInfra API URL.<br>    \"\"\"<br>    return f\"{INFERENCE_URL}/{self._model_id}\"<br>``` |\n\nBack to top",
      "metadata": {
        "title": "Deepinfra - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/deepinfra/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/aim/#llama_index.callbacks.aim.AimCallback)\n\n# Aim\n\n## AimCallback [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/aim/\\#llama_index.callbacks.aim.AimCallback \"Permanent link\")\n\nBases: `BaseCallbackHandler`\n\nAimCallback callback class.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `repo` |  | obj: `str`, optional):<br>Aim repository path or Repo object to which Run object is bound.<br>If skipped, default Repo is used. | `None` |\n| `experiment_name` |  | obj: `str`, optional):<br>Sets Run's `experiment` property. 'default' if not specified.<br>Can be used later to query runs/sequences. | `None` |\n| `system_tracking_interval` |  | obj: `int`, optional):<br>Sets the tracking interval in seconds for system usage<br>metrics (CPU, Memory, etc.). Set to `None` to disable<br>system metrics tracking. | `1` |\n| `log_system_params` |  | obj: `bool`, optional):<br>Enable/Disable logging of system params such as installed packages,<br>git info, environment variables, etc. | `True` |\n| `capture_terminal_logs` |  | obj: `bool`, optional):<br>Enable/Disable terminal stdout logging. | `True` |\n| `event_starts_to_ignore` | `Optional[List[CBEventType]]` | list of event types to ignore when tracking event starts. | `None` |\n| `event_ends_to_ignore` | `Optional[List[CBEventType]]` | list of event types to ignore when tracking event ends. | `None` |\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-aim/llama_index/callbacks/aim/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 13<br> 14<br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>``` | ```<br>class AimCallback(BaseCallbackHandler):<br>    \"\"\"<br>    AimCallback callback class.<br>    Args:<br>        repo (:obj:`str`, optional):<br>            Aim repository path or Repo object to which Run object is bound.<br>            If skipped, default Repo is used.<br>        experiment_name (:obj:`str`, optional):<br>            Sets Run's `experiment` property. 'default' if not specified.<br>            Can be used later to query runs/sequences.<br>        system_tracking_interval (:obj:`int`, optional):<br>            Sets the tracking interval in seconds for system usage<br>            metrics (CPU, Memory, etc.). Set to `None` to disable<br>            system metrics tracking.<br>        log_system_params (:obj:`bool`, optional):<br>            Enable/Disable logging of system params such as installed packages,<br>            git info, environment variables, etc.<br>        capture_terminal_logs (:obj:`bool`, optional):<br>            Enable/Disable terminal stdout logging.<br>        event_starts_to_ignore (Optional[List[CBEventType]]):<br>            list of event types to ignore when tracking event starts.<br>        event_ends_to_ignore (Optional[List[CBEventType]]):<br>            list of event types to ignore when tracking event ends.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        repo: Optional[str] = None,<br>        experiment_name: Optional[str] = None,<br>        system_tracking_interval: Optional[int] = 1,<br>        log_system_params: Optional[bool] = True,<br>        capture_terminal_logs: Optional[bool] = True,<br>        event_starts_to_ignore: Optional[List[CBEventType]] = None,<br>        event_ends_to_ignore: Optional[List[CBEventType]] = None,<br>        run_params: Optional[Dict[str, Any]] = None,<br>    ) -> None:<br>        if Run is None:<br>            raise ModuleNotFoundError(<br>                \"Please install aim to use the AimCallback: 'pip install aim'\"<br>            )<br>        event_starts_to_ignore = (<br>            event_starts_to_ignore if event_starts_to_ignore else []<br>        )<br>        event_ends_to_ignore = event_ends_to_ignore if event_ends_to_ignore else []<br>        super().__init__(<br>            event_starts_to_ignore=event_starts_to_ignore,<br>            event_ends_to_ignore=event_ends_to_ignore,<br>        )<br>        self.repo = repo<br>        self.experiment_name = experiment_name<br>        self.system_tracking_interval = system_tracking_interval<br>        self.log_system_params = log_system_params<br>        self.capture_terminal_logs = capture_terminal_logs<br>        self._run: Optional[Any] = None<br>        self._run_hash = None<br>        self._llm_response_step = 0<br>        self.setup(run_params)<br>    def on_event_start(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        parent_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> str:<br>        \"\"\"<br>        Args:<br>            event_type (CBEventType): event type to store.<br>            payload (Optional[Dict[str, Any]]): payload to store.<br>            event_id (str): event id to store.<br>            parent_id (str): parent event id.<br>        \"\"\"<br>        return \"\"<br>    def on_event_end(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> None:<br>        \"\"\"<br>        Args:<br>            event_type (CBEventType): event type to store.<br>            payload (Optional[Dict[str, Any]]): payload to store.<br>            event_id (str): event id to store.<br>        \"\"\"<br>        if not self._run:<br>            raise ValueError(\"AimCallback failed to init properly.\")<br>        if event_type is CBEventType.LLM and payload:<br>            if EventPayload.PROMPT in payload:<br>                llm_input = str(payload[EventPayload.PROMPT])<br>                llm_output = str(payload[EventPayload.COMPLETION])<br>            else:<br>                message = payload.get(EventPayload.MESSAGES, [])<br>                llm_input = \"\\n\".join([str(x) for x in message])<br>                llm_output = str(payload[EventPayload.RESPONSE])<br>            self._run.track(<br>                Text(llm_input),<br>                name=\"prompt\",<br>                step=self._llm_response_step,<br>                context={\"event_id\": event_id},<br>            )<br>            self._run.track(<br>                Text(llm_output),<br>                name=\"response\",<br>                step=self._llm_response_step,<br>                context={\"event_id\": event_id},<br>            )<br>            self._llm_response_step += 1<br>        elif event_type is CBEventType.CHUNKING and payload:<br>            for chunk_id, chunk in enumerate(payload[EventPayload.CHUNKS]):<br>                self._run.track(<br>                    Text(chunk),<br>                    name=\"chunk\",<br>                    step=self._llm_response_step,<br>                    context={\"chunk_id\": chunk_id, \"event_id\": event_id},<br>                )<br>    @property<br>    def experiment(self) -> Run:<br>        if not self._run:<br>            self.setup()<br>        return self._run<br>    def setup(self, args: Optional[Dict[str, Any]] = None) -> None:<br>        if not self._run:<br>            if self._run_hash:<br>                self._run = Run(<br>                    self._run_hash,<br>                    repo=self.repo,<br>                    system_tracking_interval=self.system_tracking_interval,<br>                    log_system_params=self.log_system_params,<br>                    capture_terminal_logs=self.capture_terminal_logs,<br>                )<br>            else:<br>                self._run = Run(<br>                    repo=self.repo,<br>                    experiment=self.experiment_name,<br>                    system_tracking_interval=self.system_tracking_interval,<br>                    log_system_params=self.log_system_params,<br>                    capture_terminal_logs=self.capture_terminal_logs,<br>                )<br>                self._run_hash = self._run.hash<br>        # Log config parameters<br>        if args:<br>            try:<br>                for key in args:<br>                    self._run.set(key, args[key], strict=False)<br>            except Exception as e:<br>                logger.warning(f\"Aim could not log config parameters -> {e}\")<br>    def __del__(self) -> None:<br>        if self._run and self._run.active:<br>            self._run.close()<br>    def start_trace(self, trace_id: Optional[str] = None) -> None:<br>        pass<br>    def end_trace(<br>        self,<br>        trace_id: Optional[str] = None,<br>        trace_map: Optional[Dict[str, List[str]]] = None,<br>    ) -> None:<br>        pass<br>``` |\n\n### on\\_event\\_start [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/aim/\\#llama_index.callbacks.aim.AimCallback.on_event_start \"Permanent link\")\n\n```\non_event_start(event_type: CBEventType, payload: Optional[Dict[str, Any]] = None, event_id: str = '', parent_id: str = '', **kwargs: Any) -> str\n\n```\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `event_type` | `CBEventType` | event type to store. | _required_ |\n| `payload` | `Optional[Dict[str, Any]]` | payload to store. | `None` |\n| `event_id` | `str` | event id to store. | `''` |\n| `parent_id` | `str` | parent event id. | `''` |\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-aim/llama_index/callbacks/aim/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>89<br>90<br>91<br>``` | ```<br>def on_event_start(<br>    self,<br>    event_type: CBEventType,<br>    payload: Optional[Dict[str, Any]] = None,<br>    event_id: str = \"\",<br>    parent_id: str = \"\",<br>    **kwargs: Any,<br>) -> str:<br>    \"\"\"<br>    Args:<br>        event_type (CBEventType): event type to store.<br>        payload (Optional[Dict[str, Any]]): payload to store.<br>        event_id (str): event id to store.<br>        parent_id (str): parent event id.<br>    \"\"\"<br>    return \"\"<br>``` |\n\n### on\\_event\\_end [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/aim/\\#llama_index.callbacks.aim.AimCallback.on_event_end \"Permanent link\")\n\n```\non_event_end(event_type: CBEventType, payload: Optional[Dict[str, Any]] = None, event_id: str = '', **kwargs: Any) -> None\n\n```\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `event_type` | `CBEventType` | event type to store. | _required_ |\n| `payload` | `Optional[Dict[str, Any]]` | payload to store. | `None` |\n| `event_id` | `str` | event id to store. | `''` |\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-aim/llama_index/callbacks/aim/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>``` | ```<br>def on_event_end(<br>    self,<br>    event_type: CBEventType,<br>    payload: Optional[Dict[str, Any]] = None,<br>    event_id: str = \"\",<br>    **kwargs: Any,<br>) -> None:<br>    \"\"\"<br>    Args:<br>        event_type (CBEventType): event type to store.<br>        payload (Optional[Dict[str, Any]]): payload to store.<br>        event_id (str): event id to store.<br>    \"\"\"<br>    if not self._run:<br>        raise ValueError(\"AimCallback failed to init properly.\")<br>    if event_type is CBEventType.LLM and payload:<br>        if EventPayload.PROMPT in payload:<br>            llm_input = str(payload[EventPayload.PROMPT])<br>            llm_output = str(payload[EventPayload.COMPLETION])<br>        else:<br>            message = payload.get(EventPayload.MESSAGES, [])<br>            llm_input = \"\\n\".join([str(x) for x in message])<br>            llm_output = str(payload[EventPayload.RESPONSE])<br>        self._run.track(<br>            Text(llm_input),<br>            name=\"prompt\",<br>            step=self._llm_response_step,<br>            context={\"event_id\": event_id},<br>        )<br>        self._run.track(<br>            Text(llm_output),<br>            name=\"response\",<br>            step=self._llm_response_step,<br>            context={\"event_id\": event_id},<br>        )<br>        self._llm_response_step += 1<br>    elif event_type is CBEventType.CHUNKING and payload:<br>        for chunk_id, chunk in enumerate(payload[EventPayload.CHUNKS]):<br>            self._run.track(<br>                Text(chunk),<br>                name=\"chunk\",<br>                step=self._llm_response_step,<br>                context={\"chunk_id\": chunk_id, \"event_id\": event_id},<br>            )<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Aim - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/aim/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "\n\n# Langchain\n\n## LangchainEmbedding [\\#](\\#llama_index.embeddings.langchain.LangchainEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nExternal embeddings (taken from Langchain).\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `langchain_embedding` | `Embeddings` | Langchain<br>embeddings class. | _required_ |\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-langchain/llama_index/embeddings/langchain/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>``` | ```<br>class LangchainEmbedding(BaseEmbedding):<br>    \"\"\"External embeddings (taken from Langchain).<br>    Args:<br>        langchain_embedding (langchain.embeddings.Embeddings): Langchain<br>            embeddings class.<br>    \"\"\"<br>    _langchain_embedding: \"LCEmbeddings\" = PrivateAttr()<br>    _async_not_implemented_warned: bool = PrivateAttr(default=False)<br>    def __init__(<br>        self,<br>        langchain_embeddings: \"LCEmbeddings\",<br>        model_name: Optional[str] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>    ):<br>        # attempt to get a useful model name<br>        if model_name is not None:<br>            model_name = model_name<br>        elif hasattr(langchain_embeddings, \"model_name\"):<br>            model_name = langchain_embeddings.model_name<br>        elif hasattr(langchain_embeddings, \"model\"):<br>            model_name = langchain_embeddings.model<br>        else:<br>            model_name = type(langchain_embeddings).__name__<br>        super().__init__(<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            model_name=model_name,<br>        )<br>        self._langchain_embedding = langchain_embeddings<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"LangchainEmbedding\"<br>    def _async_not_implemented_warn_once(self) -> None:<br>        if not self._async_not_implemented_warned:<br>            print(\"Async embedding not available, falling back to sync method.\")<br>            self._async_not_implemented_warned = True<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self._langchain_embedding.embed_query(query)<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        try:<br>            return await self._langchain_embedding.aembed_query(query)<br>        except NotImplementedError:<br>            # Warn the user that sync is being used<br>            self._async_not_implemented_warn_once()<br>            return self._get_query_embedding(query)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        try:<br>            embeds = await self._langchain_embedding.aembed_documents([text])<br>            return embeds[0]<br>        except NotImplementedError:<br>            # Warn the user that sync is being used<br>            self._async_not_implemented_warn_once()<br>            return self._get_text_embedding(text)<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._langchain_embedding.embed_documents([text])[0]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        return self._langchain_embedding.embed_documents(texts)<br>``` |\n\nBack to top",
      "metadata": {
        "title": "Langchain - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/langchain/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "\n\n# Context\n\n## ContextChatEngine [\\#](\\#llama_index.core.chat_engine.ContextChatEngine \"Permanent link\")\n\nBases: `BaseChatEngine`\n\nContext Chat Engine.\n\nUses a retriever to retrieve a context, set the context in the system prompt,\nand then uses an LLM to generate a response, for a fluid chat experience.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/context.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>``` | ```<br>class ContextChatEngine(BaseChatEngine):<br>    \"\"\"<br>    Context Chat Engine.<br>    Uses a retriever to retrieve a context, set the context in the system prompt,<br>    and then uses an LLM to generate a response, for a fluid chat experience.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        retriever: BaseRetriever,<br>        llm: LLM,<br>        memory: BaseMemory,<br>        prefix_messages: List[ChatMessage],<br>        node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,<br>        context_template: Optional[str] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>    ) -> None:<br>        self._retriever = retriever<br>        self._llm = llm<br>        self._memory = memory<br>        self._prefix_messages = prefix_messages<br>        self._node_postprocessors = node_postprocessors or []<br>        self._context_template = context_template or DEFAULT_CONTEXT_TEMPLATE<br>        self.callback_manager = callback_manager or CallbackManager([])<br>        for node_postprocessor in self._node_postprocessors:<br>            node_postprocessor.callback_manager = self.callback_manager<br>    @classmethod<br>    def from_defaults(<br>        cls,<br>        retriever: BaseRetriever,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        memory: Optional[BaseMemory] = None,<br>        system_prompt: Optional[str] = None,<br>        prefix_messages: Optional[List[ChatMessage]] = None,<br>        node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,<br>        context_template: Optional[str] = None,<br>        llm: Optional[LLM] = None,<br>        **kwargs: Any,<br>    ) -> \"ContextChatEngine\":<br>        \"\"\"Initialize a ContextChatEngine from default parameters.\"\"\"<br>        llm = llm or Settings.llm<br>        chat_history = chat_history or []<br>        memory = memory or ChatMemoryBuffer.from_defaults(<br>            chat_history=chat_history, token_limit=llm.metadata.context_window - 256<br>        )<br>        if system_prompt is not None:<br>            if prefix_messages is not None:<br>                raise ValueError(<br>                    \"Cannot specify both system_prompt and prefix_messages\"<br>                )<br>            prefix_messages = [<br>                ChatMessage(content=system_prompt, role=llm.metadata.system_role)<br>            ]<br>        prefix_messages = prefix_messages or []<br>        node_postprocessors = node_postprocessors or []<br>        return cls(<br>            retriever,<br>            llm=llm,<br>            memory=memory,<br>            prefix_messages=prefix_messages,<br>            node_postprocessors=node_postprocessors,<br>            callback_manager=Settings.callback_manager,<br>            context_template=context_template,<br>        )<br>    def _generate_context(self, message: str) -> Tuple[str, List[NodeWithScore]]:<br>        \"\"\"Generate context information from a message.\"\"\"<br>        nodes = self._retriever.retrieve(message)<br>        for postprocessor in self._node_postprocessors:<br>            nodes = postprocessor.postprocess_nodes(<br>                nodes, query_bundle=QueryBundle(message)<br>            )<br>        context_str = \"\\n\\n\".join(<br>            [n.node.get_content(metadata_mode=MetadataMode.LLM).strip() for n in nodes]<br>        )<br>        return self._context_template.format(context_str=context_str), nodes<br>    async def _agenerate_context(self, message: str) -> Tuple[str, List[NodeWithScore]]:<br>        \"\"\"Generate context information from a message.\"\"\"<br>        nodes = await self._retriever.aretrieve(message)<br>        for postprocessor in self._node_postprocessors:<br>            nodes = postprocessor.postprocess_nodes(<br>                nodes, query_bundle=QueryBundle(message)<br>            )<br>        context_str = \"\\n\\n\".join(<br>            [n.node.get_content(metadata_mode=MetadataMode.LLM).strip() for n in nodes]<br>        )<br>        return self._context_template.format(context_str=context_str), nodes<br>    def _get_prefix_messages_with_context(self, context_str: str) -> List[ChatMessage]:<br>        \"\"\"Get the prefix messages with context.\"\"\"<br>        # ensure we grab the user-configured system prompt<br>        system_prompt = \"\"<br>        prefix_messages = self._prefix_messages<br>        if (<br>            len(self._prefix_messages) != 0<br>            and self._prefix_messages[0].role == MessageRole.SYSTEM<br>        ):<br>            system_prompt = str(self._prefix_messages[0].content)<br>            prefix_messages = self._prefix_messages[1:]<br>        context_str_w_sys_prompt = system_prompt.strip() + \"\\n\" + context_str<br>        return [<br>            ChatMessage(<br>                content=context_str_w_sys_prompt, role=self._llm.metadata.system_role<br>            ),<br>            *prefix_messages,<br>        ]<br>    @trace_method(\"chat\")<br>    def chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        prev_chunks: Optional[List[NodeWithScore]] = None,<br>    ) -> AgentChatResponse:<br>        if chat_history is not None:<br>            self._memory.set(chat_history)<br>        self._memory.put(ChatMessage(content=message, role=\"user\"))<br>        context_str_template, nodes = self._generate_context(message)<br>        # If the fetched context is completely empty<br>        if len(nodes) == 0 and prev_chunks is not None:<br>            context_str = \"\\n\\n\".join(<br>                [<br>                    n.node.get_content(metadata_mode=MetadataMode.LLM).strip()<br>                    for n in prev_chunks<br>                ]<br>            )<br>            # Create a new context string template by using previous nodes<br>            context_str_template = self._context_template.format(<br>                context_str=context_str<br>            )<br>        prefix_messages = self._get_prefix_messages_with_context(context_str_template)<br>        if hasattr(self._memory, \"tokenizer_fn\"):<br>            prefix_messages_token_count = len(<br>                self._memory.tokenizer_fn(<br>                    \" \".join([(m.content or \"\") for m in prefix_messages])<br>                )<br>            )<br>        else:<br>            prefix_messages_token_count = 0<br>        all_messages = prefix_messages + self._memory.get(<br>            initial_token_count=prefix_messages_token_count<br>        )<br>        chat_response = self._llm.chat(all_messages)<br>        ai_message = chat_response.message<br>        self._memory.put(ai_message)<br>        return AgentChatResponse(<br>            response=str(chat_response.message.content),<br>            sources=[<br>                ToolOutput(<br>                    tool_name=\"retriever\",<br>                    content=str(prefix_messages[0]),<br>                    raw_input={\"message\": message},<br>                    raw_output=prefix_messages[0],<br>                )<br>            ],<br>            source_nodes=nodes,<br>        )<br>    @trace_method(\"chat\")<br>    def stream_chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        prev_chunks: Optional[List[NodeWithScore]] = None,<br>    ) -> StreamingAgentChatResponse:<br>        if chat_history is not None:<br>            self._memory.set(chat_history)<br>        self._memory.put(ChatMessage(content=message, role=\"user\"))<br>        context_str_template, nodes = self._generate_context(message)<br>        # If the fetched context is completely empty<br>        if len(nodes) == 0 and prev_chunks is not None:<br>            context_str = \"\\n\\n\".join(<br>                [<br>                    n.node.get_content(metadata_mode=MetadataMode.LLM).strip()<br>                    for n in prev_chunks<br>                ]<br>            )<br>            # Create a new context string template by using previous nodes<br>            context_str_template = self._context_template.format(<br>                context_str=context_str<br>            )<br>        prefix_messages = self._get_prefix_messages_with_context(context_str_template)<br>        if hasattr(self._memory, \"tokenizer_fn\"):<br>            initial_token_count = len(<br>                self._memory.tokenizer_fn(<br>                    \" \".join([(m.content or \"\") for m in prefix_messages])<br>                )<br>            )<br>        else:<br>            initial_token_count = 0<br>        all_messages = prefix_messages + self._memory.get(<br>            initial_token_count=initial_token_count<br>        )<br>        chat_response = StreamingAgentChatResponse(<br>            chat_stream=self._llm.stream_chat(all_messages),<br>            sources=[<br>                ToolOutput(<br>                    tool_name=\"retriever\",<br>                    content=str(prefix_messages[0]),<br>                    raw_input={\"message\": message},<br>                    raw_output=prefix_messages[0],<br>                )<br>            ],<br>            source_nodes=nodes,<br>        )<br>        thread = Thread(<br>            target=chat_response.write_response_to_history, args=(self._memory,)<br>        )<br>        thread.start()<br>        return chat_response<br>    @trace_method(\"chat\")<br>    async def achat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        prev_chunks: Optional[Sequence[NodeWithScore]] = None,<br>    ) -> AgentChatResponse:<br>        if chat_history is not None:<br>            self._memory.set(chat_history)<br>        self._memory.put(ChatMessage(content=message, role=\"user\"))<br>        context_str_template, nodes = await self._agenerate_context(message)<br>        # If the fetched context is completely empty<br>        if len(nodes) == 0 and prev_chunks is not None:<br>            context_str = \"\\n\\n\".join(<br>                [<br>                    n.node.get_content(metadata_mode=MetadataMode.LLM).strip()<br>                    for n in prev_chunks<br>                ]<br>            )<br>            # Create a new context string template by using previous nodes<br>            context_str_template = self._context_template.format(<br>                context_str=context_str<br>            )<br>        prefix_messages = self._get_prefix_messages_with_context(context_str_template)<br>        if hasattr(self._memory, \"tokenizer_fn\"):<br>            initial_token_count = len(<br>                self._memory.tokenizer_fn(<br>                    \" \".join([(m.content or \"\") for m in prefix_messages])<br>                )<br>            )<br>        else:<br>            initial_token_count = 0<br>        all_messages = prefix_messages + self._memory.get(<br>            initial_token_count=initial_token_count<br>        )<br>        chat_response = await self._llm.achat(all_messages)<br>        ai_message = chat_response.message<br>        self._memory.put(ai_message)<br>        return AgentChatResponse(<br>            response=str(chat_response.message.content),<br>            sources=[<br>                ToolOutput(<br>                    tool_name=\"retriever\",<br>                    content=str(prefix_messages[0]),<br>                    raw_input={\"message\": message},<br>                    raw_output=prefix_messages[0],<br>                )<br>            ],<br>            source_nodes=nodes,<br>        )<br>    @trace_method(\"chat\")<br>    async def astream_chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        prev_chunks: Optional[Sequence[NodeWithScore]] = None,<br>    ) -> StreamingAgentChatResponse:<br>        if chat_history is not None:<br>            self._memory.set(chat_history)<br>        self._memory.put(ChatMessage(content=message, role=\"user\"))<br>        context_str_template, nodes = await self._agenerate_context(message)<br>        # If the fetched context is completely empty<br>        if len(nodes) == 0 and prev_chunks is not None:<br>            context_str = \"\\n\\n\".join(<br>                [<br>                    n.node.get_content(metadata_mode=MetadataMode.LLM).strip()<br>                    for n in prev_chunks<br>                ]<br>            )<br>            # Create a new context string template by using previous nodes<br>            context_str_template = self._context_template.format(<br>                context_str=context_str<br>            )<br>        prefix_messages = self._get_prefix_messages_with_context(context_str_template)<br>        if hasattr(self._memory, \"tokenizer_fn\"):<br>            initial_token_count = len(<br>                self._memory.tokenizer_fn(<br>                    \" \".join([(m.content or \"\") for m in prefix_messages])<br>                )<br>            )<br>        else:<br>            initial_token_count = 0<br>        all_messages = prefix_messages + self._memory.get(<br>            initial_token_count=initial_token_count<br>        )<br>        chat_response = StreamingAgentChatResponse(<br>            achat_stream=await self._llm.astream_chat(all_messages),<br>            sources=[<br>                ToolOutput(<br>                    tool_name=\"retriever\",<br>                    content=str(prefix_messages[0]),<br>                    raw_input={\"message\": message},<br>                    raw_output=prefix_messages[0],<br>                )<br>            ],<br>            source_nodes=nodes,<br>        )<br>        asyncio.create_task(chat_response.awrite_response_to_history(self._memory))<br>        return chat_response<br>    def reset(self) -> None:<br>        self._memory.reset()<br>    @property<br>    def chat_history(self) -> List[ChatMessage]:<br>        \"\"\"Get chat history.\"\"\"<br>        return self._memory.get_all()<br>``` |\n\n### chat\\_history`property`[\\#](\\#llama_index.core.chat_engine.ContextChatEngine.chat_history \"Permanent link\")\n\n```\nchat_history: List[ChatMessage]\n\n```\n\nGet chat history.\n\n### from\\_defaults`classmethod`[\\#](\\#llama_index.core.chat_engine.ContextChatEngine.from_defaults \"Permanent link\")\n\n```\nfrom_defaults(retriever: BaseRetriever, chat_history: Optional[List[ChatMessage]] = None, memory: Optional[BaseMemory] = None, system_prompt: Optional[str] = None, prefix_messages: Optional[List[ChatMessage]] = None, node_postprocessors: Optional[List[BaseNodePostprocessor]] = None, context_template: Optional[str] = None, llm: Optional[LLM] = None, **kwargs: Any) -> ContextChatEngine\n\n```\n\nInitialize a ContextChatEngine from default parameters.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/context.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>``` | ```<br>@classmethod<br>def from_defaults(<br>    cls,<br>    retriever: BaseRetriever,<br>    chat_history: Optional[List[ChatMessage]] = None,<br>    memory: Optional[BaseMemory] = None,<br>    system_prompt: Optional[str] = None,<br>    prefix_messages: Optional[List[ChatMessage]] = None,<br>    node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,<br>    context_template: Optional[str] = None,<br>    llm: Optional[LLM] = None,<br>    **kwargs: Any,<br>) -> \"ContextChatEngine\":<br>    \"\"\"Initialize a ContextChatEngine from default parameters.\"\"\"<br>    llm = llm or Settings.llm<br>    chat_history = chat_history or []<br>    memory = memory or ChatMemoryBuffer.from_defaults(<br>        chat_history=chat_history, token_limit=llm.metadata.context_window - 256<br>    )<br>    if system_prompt is not None:<br>        if prefix_messages is not None:<br>            raise ValueError(<br>                \"Cannot specify both system_prompt and prefix_messages\"<br>            )<br>        prefix_messages = [<br>            ChatMessage(content=system_prompt, role=llm.metadata.system_role)<br>        ]<br>    prefix_messages = prefix_messages or []<br>    node_postprocessors = node_postprocessors or []<br>    return cls(<br>        retriever,<br>        llm=llm,<br>        memory=memory,<br>        prefix_messages=prefix_messages,<br>        node_postprocessors=node_postprocessors,<br>        callback_manager=Settings.callback_manager,<br>        context_template=context_template,<br>    )<br>``` |\n\nBack to top",
      "metadata": {
        "title": "Context - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/context/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/premai/#llama_index.embeddings.premai.PremAIEmbeddings)\n\n# Premai\n\n## PremAIEmbeddings [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/premai/\\#llama_index.embeddings.premai.PremAIEmbeddings \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nClass for PremAI embeddings.\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-premai/llama_index/embeddings/premai/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>``` | ```<br>class PremAIEmbeddings(BaseEmbedding):<br>    \"\"\"Class for PremAI embeddings.\"\"\"<br>    project_id: int = Field(<br>        description=(<br>            \"The project ID in which the experiments or deployments are carried out. can find all your projects here: https://app.premai.io/projects/\"<br>        )<br>    )<br>    premai_api_key: Optional[str] = Field(<br>        description=\"Prem AI API Key. Get it here: https://app.premai.io/api_keys/\"<br>    )<br>    model_name: str = Field(<br>        description=(\"The Embedding model to choose from\"),<br>    )<br>    # Instance variables initialized via Pydantic's mechanism<br>    _premai_client: \"Prem\" = PrivateAttr()<br>    def __init__(<br>        self,<br>        project_id: int,<br>        model_name: str,<br>        premai_api_key: Optional[str] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        **kwargs: Any,<br>    ):<br>        api_key = get_from_param_or_env(\"api_key\", premai_api_key, \"PREMAI_API_KEY\", \"\")<br>        if not api_key:<br>            raise ValueError(<br>                \"You must provide an API key to use PremAI. \"<br>                \"You can either pass it in as an argument or set it `PREMAI_API_KEY`.\"<br>            )<br>        super().__init__(<br>            project_id=project_id,<br>            model_name=model_name,<br>            callback_manager=callback_manager,<br>            **kwargs,<br>        )<br>        self._premai_client = Prem(api_key=api_key)<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"PremAIEmbeddings\"<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        embedding_response = self._premai_client.embeddings.create(<br>            project_id=self.project_id, model=self.model_name, input=query<br>        )<br>        return embedding_response.data[0].embedding<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        raise NotImplementedError(\"Async calls are not available in this version.\")<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        embedding_response = self._premai_client.embeddings.create(<br>            project_id=self.project_id, model=self.model_name, input=[text]<br>        )<br>        return embedding_response.data[0].embedding<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        embeddings = self._premai_client.embeddings.create(<br>            model=self.model_name, project_id=self.project_id, input=texts<br>        ).data<br>        return [embedding.embedding for embedding in embeddings]<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Premai - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/premai/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/huggingface_api/#llama_index.embeddings.huggingface_api.HuggingFaceInferenceAPIEmbedding)\n\n# Huggingface api\n\n## HuggingFaceInferenceAPIEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/huggingface_api/\\#llama_index.embeddings.huggingface_api.HuggingFaceInferenceAPIEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nWrapper on the Hugging Face's Inference API for embeddings.\n\nOverview of the design:\n\\- Uses the feature extraction task: https://huggingface.co/tasks/feature-extraction\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-huggingface-api/llama_index/embeddings/huggingface_api/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>``` | ```<br>class HuggingFaceInferenceAPIEmbedding(BaseEmbedding):  # type: ignore[misc]<br>    \"\"\"<br>    Wrapper on the Hugging Face's Inference API for embeddings.<br>    Overview of the design:<br>    - Uses the feature extraction task: https://huggingface.co/tasks/feature-extraction<br>    \"\"\"<br>    pooling: Optional[Pooling] = Field(<br>        default=Pooling.CLS,<br>        description=\"Pooling strategy. If None, the model's default pooling is used.\",<br>    )<br>    query_instruction: Optional[str] = Field(<br>        default=None, description=\"Instruction to prepend during query embedding.\"<br>    )<br>    text_instruction: Optional[str] = Field(<br>        default=None, description=\"Instruction to prepend during text embedding.\"<br>    )<br>    # Corresponds with huggingface_hub.InferenceClient<br>    model_name: Optional[str] = Field(<br>        default=None,<br>        description=\"Hugging Face model name. If None, the task will be used.\",<br>    )<br>    token: Union[str, bool, None] = Field(<br>        default=None,<br>        description=(<br>            \"Hugging Face token. Will default to the locally saved token. Pass \"<br>            \"token=False if you don\u2019t want to send your token to the server.\"<br>        ),<br>    )<br>    timeout: Optional[float] = Field(<br>        default=None,<br>        description=(<br>            \"The maximum number of seconds to wait for a response from the server.\"<br>            \" Loading a new model in Inference API can take up to several minutes.\"<br>            \" Defaults to None, meaning it will loop until the server is available.\"<br>        ),<br>    )<br>    headers: Dict[str, str] = Field(<br>        default=None,<br>        description=(<br>            \"Additional headers to send to the server. By default only the\"<br>            \" authorization and user-agent headers are sent. Values in this dictionary\"<br>            \" will override the default values.\"<br>        ),<br>    )<br>    cookies: Dict[str, str] = Field(<br>        default=None, description=\"Additional cookies to send to the server.\"<br>    )<br>    task: Optional[str] = Field(<br>        default=None,<br>        description=(<br>            \"Optional task to pick Hugging Face's recommended model, used when\"<br>            \" model_name is left as default of None.\"<br>        ),<br>    )<br>    _sync_client: \"InferenceClient\" = PrivateAttr()<br>    _async_client: \"AsyncInferenceClient\" = PrivateAttr()<br>    _get_model_info: \"Callable[..., ModelInfo]\" = PrivateAttr()<br>    def _get_inference_client_kwargs(self) -> Dict[str, Any]:<br>        \"\"\"Extract the Hugging Face InferenceClient construction parameters.\"\"\"<br>        return {<br>            \"model\": self.model_name,<br>            \"token\": self.token,<br>            \"timeout\": self.timeout,<br>            \"headers\": self.headers,<br>            \"cookies\": self.cookies,<br>        }<br>    def __init__(self, **kwargs: Any) -> None:<br>        \"\"\"Initialize.<br>        Args:<br>            kwargs: See the class-level Fields.<br>        \"\"\"<br>        if kwargs.get(\"model_name\") is None:<br>            task = kwargs.get(\"task\", \"\")<br>            # NOTE: task being None or empty string leads to ValueError,<br>            # which ensures model is present<br>            kwargs[\"model_name\"] = InferenceClient.get_recommended_model(task=task)<br>            logger.debug(<br>                f\"Using Hugging Face's recommended model {kwargs['model_name']}\"<br>                f\" given task {task}.\"<br>            )<br>            print(kwargs[\"model_name\"], flush=True)<br>        super().__init__(**kwargs)  # Populate pydantic Fields<br>        self._sync_client = InferenceClient(**self._get_inference_client_kwargs())<br>        self._async_client = AsyncInferenceClient(**self._get_inference_client_kwargs())<br>        self._get_model_info = model_info<br>    def validate_supported(self, task: str) -> None:<br>        \"\"\"<br>        Confirm the contained model_name is deployed on the Inference API service.<br>        Args:<br>            task: Hugging Face task to check within. A list of all tasks can be<br>                found here: https://huggingface.co/tasks<br>        \"\"\"<br>        all_models = self._sync_client.list_deployed_models(frameworks=\"all\")<br>        try:<br>            if self.model_name not in all_models[task]:<br>                raise ValueError(<br>                    \"The Inference API service doesn't have the model\"<br>                    f\" {self.model_name!r} deployed.\"<br>                )<br>        except KeyError as exc:<br>            raise KeyError(<br>                f\"Input task {task!r} not in possible tasks {list(all_models.keys())}.\"<br>            ) from exc<br>    def get_model_info(self, **kwargs: Any) -> \"ModelInfo\":<br>        \"\"\"Get metadata on the current model from Hugging Face.\"\"\"<br>        return self._get_model_info(self.model_name, **kwargs)<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"HuggingFaceInferenceAPIEmbedding\"<br>    async def _async_embed_single(self, text: str) -> Embedding:<br>        embedding = await self._async_client.feature_extraction(text)<br>        if len(embedding.shape) == 1:<br>            return embedding.tolist()<br>        embedding = embedding.squeeze(axis=0)<br>        if len(embedding.shape) == 1:  # Some models pool internally<br>            return embedding.tolist()<br>        try:<br>            return self.pooling(embedding).tolist()  # type: ignore[misc]<br>        except TypeError as exc:<br>            raise ValueError(<br>                f\"Pooling is required for {self.model_name} because it returned\"<br>                \" a > 1-D value, please specify pooling as not None.\"<br>            ) from exc<br>    async def _async_embed_bulk(self, texts: Sequence[str]) -> List[Embedding]:<br>        \"\"\"<br>        Embed a sequence of text, in parallel and asynchronously.<br>        NOTE: this uses an externally created asyncio event loop.<br>        \"\"\"<br>        tasks = [self._async_embed_single(text) for text in texts]<br>        return await asyncio.gather(*tasks)<br>    def _get_query_embedding(self, query: str) -> Embedding:<br>        \"\"\"<br>        Embed the input query synchronously.<br>        NOTE: a new asyncio event loop is created internally for this.<br>        \"\"\"<br>        return asyncio.run(self._aget_query_embedding(query))<br>    def _get_text_embedding(self, text: str) -> Embedding:<br>        \"\"\"<br>        Embed the text query synchronously.<br>        NOTE: a new asyncio event loop is created internally for this.<br>        \"\"\"<br>        return asyncio.run(self._aget_text_embedding(text))<br>    def _get_text_embeddings(self, texts: List[str]) -> List[Embedding]:<br>        \"\"\"<br>        Embed the input sequence of text synchronously and in parallel.<br>        NOTE: a new asyncio event loop is created internally for this.<br>        \"\"\"<br>        loop = asyncio.new_event_loop()<br>        try:<br>            tasks = [<br>                loop.create_task(self._aget_text_embedding(text)) for text in texts<br>            ]<br>            loop.run_until_complete(asyncio.wait(tasks))<br>        finally:<br>            loop.close()<br>        return [task.result() for task in tasks]<br>    async def _aget_query_embedding(self, query: str) -> Embedding:<br>        return await self._async_embed_single(<br>            text=format_query(query, self.model_name, self.query_instruction)<br>        )<br>    async def _aget_text_embedding(self, text: str) -> Embedding:<br>        return await self._async_embed_single(<br>            text=format_text(text, self.model_name, self.text_instruction)<br>        )<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[Embedding]:<br>        return await self._async_embed_bulk(<br>            texts=[<br>                format_text(text, self.model_name, self.text_instruction)<br>                for text in texts<br>            ]<br>        )<br>``` |\n\n### validate\\_supported [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/huggingface_api/\\#llama_index.embeddings.huggingface_api.HuggingFaceInferenceAPIEmbedding.validate_supported \"Permanent link\")\n\n```\nvalidate_supported(task: str) -> None\n\n```\n\nConfirm the contained model\\_name is deployed on the Inference API service.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `task` | `str` | Hugging Face task to check within. A list of all tasks can be<br>found here: https://huggingface.co/tasks | _required_ |\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-huggingface-api/llama_index/embeddings/huggingface_api/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>``` | ```<br>def validate_supported(self, task: str) -> None:<br>    \"\"\"<br>    Confirm the contained model_name is deployed on the Inference API service.<br>    Args:<br>        task: Hugging Face task to check within. A list of all tasks can be<br>            found here: https://huggingface.co/tasks<br>    \"\"\"<br>    all_models = self._sync_client.list_deployed_models(frameworks=\"all\")<br>    try:<br>        if self.model_name not in all_models[task]:<br>            raise ValueError(<br>                \"The Inference API service doesn't have the model\"<br>                f\" {self.model_name!r} deployed.\"<br>            )<br>    except KeyError as exc:<br>        raise KeyError(<br>            f\"Input task {task!r} not in possible tasks {list(all_models.keys())}.\"<br>        ) from exc<br>``` |\n\n### get\\_model\\_info [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/huggingface_api/\\#llama_index.embeddings.huggingface_api.HuggingFaceInferenceAPIEmbedding.get_model_info \"Permanent link\")\n\n```\nget_model_info(**kwargs: Any) -> ModelInfo\n\n```\n\nGet metadata on the current model from Hugging Face.\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-huggingface-api/llama_index/embeddings/huggingface_api/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>137<br>138<br>139<br>``` | ```<br>def get_model_info(self, **kwargs: Any) -> \"ModelInfo\":<br>    \"\"\"Get metadata on the current model from Hugging Face.\"\"\"<br>    return self._get_model_info(self.model_name, **kwargs)<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Huggingface api - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/huggingface_api/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/extractors/#llama_index.core.extractors.interface.BaseExtractor)\n\n# Index\n\nNode parser interface.\n\n## BaseExtractor [\\#](https://docs.llamaindex.ai/en/stable/api_reference/extractors/\\#llama_index.core.extractors.interface.BaseExtractor \"Permanent link\")\n\nBases: `TransformComponent`\n\nMetadata extractor.\n\nSource code in `llama-index-core/llama_index/core/extractors/interface.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>``` | ```<br>class BaseExtractor(TransformComponent):<br>    \"\"\"Metadata extractor.\"\"\"<br>    is_text_node_only: bool = True<br>    show_progress: bool = Field(default=True, description=\"Whether to show progress.\")<br>    metadata_mode: MetadataMode = Field(<br>        default=MetadataMode.ALL, description=\"Metadata mode to use when reading nodes.\"<br>    )<br>    node_text_template: str = Field(<br>        default=DEFAULT_NODE_TEXT_TEMPLATE,<br>        description=\"Template to represent how node text is mixed with metadata text.\",<br>    )<br>    disable_template_rewrite: bool = Field(<br>        default=False, description=\"Disable the node template rewrite.\"<br>    )<br>    in_place: bool = Field(<br>        default=True, description=\"Whether to process nodes in place.\"<br>    )<br>    num_workers: int = Field(<br>        default=4,<br>        description=\"Number of workers to use for concurrent async processing.\",<br>    )<br>    @classmethod<br>    def from_dict(cls, data: Dict[str, Any], **kwargs: Any) -> Self:  # type: ignore<br>        if isinstance(kwargs, dict):<br>            data.update(kwargs)<br>        data.pop(\"class_name\", None)<br>        llm_predictor = data.get(\"llm_predictor\", None)<br>        if llm_predictor:<br>            from llama_index.core.llm_predictor.loading import load_predictor<br>            llm_predictor = load_predictor(llm_predictor)<br>            data[\"llm_predictor\"] = llm_predictor<br>        llm = data.get(\"llm\", None)<br>        if llm:<br>            from llama_index.core.llms.loading import load_llm<br>            llm = load_llm(llm)<br>            data[\"llm\"] = llm<br>        return cls(**data)<br>    @classmethod<br>    def class_name(cls) -> str:<br>        \"\"\"Get class name.\"\"\"<br>        return \"MetadataExtractor\"<br>    @abstractmethod<br>    async def aextract(self, nodes: Sequence[BaseNode]) -> List[Dict]:<br>        \"\"\"Extracts metadata for a sequence of nodes, returning a list of<br>        metadata dictionaries corresponding to each node.<br>        Args:<br>            nodes (Sequence[Document]): nodes to extract metadata from<br>        \"\"\"<br>    def extract(self, nodes: Sequence[BaseNode]) -> List[Dict]:<br>        \"\"\"Extracts metadata for a sequence of nodes, returning a list of<br>        metadata dictionaries corresponding to each node.<br>        Args:<br>            nodes (Sequence[Document]): nodes to extract metadata from<br>        \"\"\"<br>        return asyncio_run(self.aextract(nodes))<br>    async def aprocess_nodes(<br>        self,<br>        nodes: Sequence[BaseNode],<br>        excluded_embed_metadata_keys: Optional[List[str]] = None,<br>        excluded_llm_metadata_keys: Optional[List[str]] = None,<br>        **kwargs: Any,<br>    ) -> List[BaseNode]:<br>        \"\"\"Post process nodes parsed from documents.<br>        Allows extractors to be chained.<br>        Args:<br>            nodes (List[BaseNode]): nodes to post-process<br>            excluded_embed_metadata_keys (Optional[List[str]]):<br>                keys to exclude from embed metadata<br>            excluded_llm_metadata_keys (Optional[List[str]]):<br>                keys to exclude from llm metadata<br>        \"\"\"<br>        if self.in_place:<br>            new_nodes = nodes<br>        else:<br>            new_nodes = [deepcopy(node) for node in nodes]<br>        cur_metadata_list = await self.aextract(new_nodes)<br>        for idx, node in enumerate(new_nodes):<br>            node.metadata.update(cur_metadata_list[idx])<br>        for idx, node in enumerate(new_nodes):<br>            if excluded_embed_metadata_keys is not None:<br>                node.excluded_embed_metadata_keys.extend(excluded_embed_metadata_keys)<br>            if excluded_llm_metadata_keys is not None:<br>                node.excluded_llm_metadata_keys.extend(excluded_llm_metadata_keys)<br>            if not self.disable_template_rewrite:<br>                if isinstance(node, TextNode):<br>                    cast(TextNode, node).text_template = self.node_text_template<br>        return new_nodes  # type: ignore<br>    def process_nodes(<br>        self,<br>        nodes: Sequence[BaseNode],<br>        excluded_embed_metadata_keys: Optional[List[str]] = None,<br>        excluded_llm_metadata_keys: Optional[List[str]] = None,<br>        **kwargs: Any,<br>    ) -> List[BaseNode]:<br>        return asyncio_run(<br>            self.aprocess_nodes(<br>                nodes,<br>                excluded_embed_metadata_keys=excluded_embed_metadata_keys,<br>                excluded_llm_metadata_keys=excluded_llm_metadata_keys,<br>                **kwargs,<br>            )<br>        )<br>    def __call__(self, nodes: Sequence[BaseNode], **kwargs: Any) -> List[BaseNode]:<br>        \"\"\"Post process nodes parsed from documents.<br>        Allows extractors to be chained.<br>        Args:<br>            nodes (List[BaseNode]): nodes to post-process<br>        \"\"\"<br>        return self.process_nodes(nodes, **kwargs)<br>    async def acall(self, nodes: Sequence[BaseNode], **kwargs: Any) -> List[BaseNode]:<br>        \"\"\"Post process nodes parsed from documents.<br>        Allows extractors to be chained.<br>        Args:<br>            nodes (List[BaseNode]): nodes to post-process<br>        \"\"\"<br>        return await self.aprocess_nodes(nodes, **kwargs)<br>``` |\n\n### class\\_name`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/extractors/\\#llama_index.core.extractors.interface.BaseExtractor.class_name \"Permanent link\")\n\n```\nclass_name() -> str\n\n```\n\nGet class name.\n\nSource code in `llama-index-core/llama_index/core/extractors/interface.py`\n\n|     |     |\n| --- | --- |\n| ```<br>73<br>74<br>75<br>76<br>``` | ```<br>@classmethod<br>def class_name(cls) -> str:<br>    \"\"\"Get class name.\"\"\"<br>    return \"MetadataExtractor\"<br>``` |\n\n### aextract`abstractmethod``async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/extractors/\\#llama_index.core.extractors.interface.BaseExtractor.aextract \"Permanent link\")\n\n```\naextract(nodes: Sequence[BaseNode]) -> List[Dict]\n\n```\n\nExtracts metadata for a sequence of nodes, returning a list of\nmetadata dictionaries corresponding to each node.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `nodes` | `Sequence[Document]` | nodes to extract metadata from | _required_ |\n\nSource code in `llama-index-core/llama_index/core/extractors/interface.py`\n\n|     |     |\n| --- | --- |\n| ```<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>``` | ```<br>@abstractmethod<br>async def aextract(self, nodes: Sequence[BaseNode]) -> List[Dict]:<br>    \"\"\"Extracts metadata for a sequence of nodes, returning a list of<br>    metadata dictionaries corresponding to each node.<br>    Args:<br>        nodes (Sequence[Document]): nodes to extract metadata from<br>    \"\"\"<br>``` |\n\n### extract [\\#](https://docs.llamaindex.ai/en/stable/api_reference/extractors/\\#llama_index.core.extractors.interface.BaseExtractor.extract \"Permanent link\")\n\n```\nextract(nodes: Sequence[BaseNode]) -> List[Dict]\n\n```\n\nExtracts metadata for a sequence of nodes, returning a list of\nmetadata dictionaries corresponding to each node.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `nodes` | `Sequence[Document]` | nodes to extract metadata from | _required_ |\n\nSource code in `llama-index-core/llama_index/core/extractors/interface.py`\n\n|     |     |\n| --- | --- |\n| ```<br>88<br>89<br>90<br>91<br>92<br>93<br>94<br>95<br>96<br>``` | ```<br>def extract(self, nodes: Sequence[BaseNode]) -> List[Dict]:<br>    \"\"\"Extracts metadata for a sequence of nodes, returning a list of<br>    metadata dictionaries corresponding to each node.<br>    Args:<br>        nodes (Sequence[Document]): nodes to extract metadata from<br>    \"\"\"<br>    return asyncio_run(self.aextract(nodes))<br>``` |\n\n### aprocess\\_nodes`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/extractors/\\#llama_index.core.extractors.interface.BaseExtractor.aprocess_nodes \"Permanent link\")\n\n```\naprocess_nodes(nodes: Sequence[BaseNode], excluded_embed_metadata_keys: Optional[List[str]] = None, excluded_llm_metadata_keys: Optional[List[str]] = None, **kwargs: Any) -> List[BaseNode]\n\n```\n\nPost process nodes parsed from documents.\n\nAllows extractors to be chained.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `nodes` | `List[BaseNode]` | nodes to post-process | _required_ |\n| `excluded_embed_metadata_keys` | `Optional[List[str]]` | keys to exclude from embed metadata | `None` |\n| `excluded_llm_metadata_keys` | `Optional[List[str]]` | keys to exclude from llm metadata | `None` |\n\nSource code in `llama-index-core/llama_index/core/extractors/interface.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>``` | ```<br>async def aprocess_nodes(<br>    self,<br>    nodes: Sequence[BaseNode],<br>    excluded_embed_metadata_keys: Optional[List[str]] = None,<br>    excluded_llm_metadata_keys: Optional[List[str]] = None,<br>    **kwargs: Any,<br>) -> List[BaseNode]:<br>    \"\"\"Post process nodes parsed from documents.<br>    Allows extractors to be chained.<br>    Args:<br>        nodes (List[BaseNode]): nodes to post-process<br>        excluded_embed_metadata_keys (Optional[List[str]]):<br>            keys to exclude from embed metadata<br>        excluded_llm_metadata_keys (Optional[List[str]]):<br>            keys to exclude from llm metadata<br>    \"\"\"<br>    if self.in_place:<br>        new_nodes = nodes<br>    else:<br>        new_nodes = [deepcopy(node) for node in nodes]<br>    cur_metadata_list = await self.aextract(new_nodes)<br>    for idx, node in enumerate(new_nodes):<br>        node.metadata.update(cur_metadata_list[idx])<br>    for idx, node in enumerate(new_nodes):<br>        if excluded_embed_metadata_keys is not None:<br>            node.excluded_embed_metadata_keys.extend(excluded_embed_metadata_keys)<br>        if excluded_llm_metadata_keys is not None:<br>            node.excluded_llm_metadata_keys.extend(excluded_llm_metadata_keys)<br>        if not self.disable_template_rewrite:<br>            if isinstance(node, TextNode):<br>                cast(TextNode, node).text_template = self.node_text_template<br>    return new_nodes  # type: ignore<br>``` |\n\n### acall`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/extractors/\\#llama_index.core.extractors.interface.BaseExtractor.acall \"Permanent link\")\n\n```\nacall(nodes: Sequence[BaseNode], **kwargs: Any) -> List[BaseNode]\n\n```\n\nPost process nodes parsed from documents.\n\nAllows extractors to be chained.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `nodes` | `List[BaseNode]` | nodes to post-process | _required_ |\n\nSource code in `llama-index-core/llama_index/core/extractors/interface.py`\n\n|     |     |\n| --- | --- |\n| ```<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>``` | ```<br>async def acall(self, nodes: Sequence[BaseNode], **kwargs: Any) -> List[BaseNode]:<br>    \"\"\"Post process nodes parsed from documents.<br>    Allows extractors to be chained.<br>    Args:<br>        nodes (List[BaseNode]): nodes to post-process<br>    \"\"\"<br>    return await self.aprocess_nodes(nodes, **kwargs)<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Index - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/extractors/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/text_embeddings_inference/#llama_index.embeddings.text_embeddings_inference.TextEmbeddingsInference)\n\n# Text embeddings inference\n\n## TextEmbeddingsInference [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/text_embeddings_inference/\\#llama_index.embeddings.text_embeddings_inference.TextEmbeddingsInference \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-text-embeddings-inference/llama_index/embeddings/text_embeddings_inference/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>``` | ```<br>class TextEmbeddingsInference(BaseEmbedding):<br>    base_url: str = Field(<br>        default=DEFAULT_URL,<br>        description=\"Base URL for the text embeddings service.\",<br>    )<br>    query_instruction: Optional[str] = Field(<br>        description=\"Instruction to prepend to query text.\"<br>    )<br>    text_instruction: Optional[str] = Field(<br>        description=\"Instruction to prepend to text.\"<br>    )<br>    timeout: float = Field(<br>        default=60.0,<br>        description=\"Timeout in seconds for the request.\",<br>    )<br>    truncate_text: bool = Field(<br>        default=True,<br>        description=\"Whether to truncate text or not when generating embeddings.\",<br>    )<br>    auth_token: Optional[Union[str, Callable[[str], str]]] = Field(<br>        default=None,<br>        description=\"Authentication token or authentication token generating function for authenticated requests\",<br>    )<br>    def __init__(<br>        self,<br>        model_name: str,<br>        base_url: str = DEFAULT_URL,<br>        text_instruction: Optional[str] = None,<br>        query_instruction: Optional[str] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        timeout: float = 60.0,<br>        truncate_text: bool = True,<br>        callback_manager: Optional[CallbackManager] = None,<br>        auth_token: Optional[Union[str, Callable[[str], str]]] = None,<br>    ):<br>        super().__init__(<br>            base_url=base_url,<br>            model_name=model_name,<br>            text_instruction=text_instruction,<br>            query_instruction=query_instruction,<br>            embed_batch_size=embed_batch_size,<br>            timeout=timeout,<br>            truncate_text=truncate_text,<br>            callback_manager=callback_manager,<br>            auth_token=auth_token,<br>        )<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"TextEmbeddingsInference\"<br>    def _call_api(self, texts: List[str]) -> List[List[float]]:<br>        import httpx<br>        headers = {\"Content-Type\": \"application/json\"}<br>        if self.auth_token is not None:<br>            if callable(self.auth_token):<br>                headers[\"Authorization\"] = self.auth_token(self.base_url)<br>            else:<br>                headers[\"Authorization\"] = self.auth_token<br>        json_data = {\"inputs\": texts, \"truncate\": self.truncate_text}<br>        with httpx.Client() as client:<br>            response = client.post(<br>                f\"{self.base_url}/embed\",<br>                headers=headers,<br>                json=json_data,<br>                timeout=self.timeout,<br>            )<br>        return response.json()<br>    async def _acall_api(self, texts: List[str]) -> List[List[float]]:<br>        import httpx<br>        headers = {\"Content-Type\": \"application/json\"}<br>        if self.auth_token is not None:<br>            if callable(self.auth_token):<br>                headers[\"Authorization\"] = self.auth_token(self.base_url)<br>            else:<br>                headers[\"Authorization\"] = self.auth_token<br>        json_data = {\"inputs\": texts, \"truncate\": self.truncate_text}<br>        async with httpx.AsyncClient() as client:<br>            response = await client.post(<br>                f\"{self.base_url}/embed\",<br>                headers=headers,<br>                json=json_data,<br>                timeout=self.timeout,<br>            )<br>        return response.json()<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        query = format_query(query, self.model_name, self.query_instruction)<br>        return self._call_api([query])[0]<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        text = format_text(text, self.model_name, self.text_instruction)<br>        return self._call_api([text])[0]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        texts = [<br>            format_text(text, self.model_name, self.text_instruction) for text in texts<br>        ]<br>        return self._call_api(texts)<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding async.\"\"\"<br>        query = format_query(query, self.model_name, self.query_instruction)<br>        return (await self._acall_api([query]))[0]<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding async.\"\"\"<br>        text = format_text(text, self.model_name, self.text_instruction)<br>        return (await self._acall_api([text]))[0]<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[Embedding]:<br>        texts = [<br>            format_text(text, self.model_name, self.text_instruction) for text in texts<br>        ]<br>        return await self._acall_api(texts)<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Text embeddings inference - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/text_embeddings_inference/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/honeyhive/#llama_index.callbacks.honeyhive.honeyhive_callback_handler)\n\n# Honeyhive\n\n## honeyhive\\_callback\\_handler [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/honeyhive/\\#llama_index.callbacks.honeyhive.honeyhive_callback_handler \"Permanent link\")\n\n```\nhoneyhive_callback_handler(**kwargs: Any) -> BaseCallbackHandler\n\n```\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-honeyhive/llama_index/callbacks/honeyhive/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>8<br>9<br>``` | ```<br>def honeyhive_callback_handler(**kwargs: Any) -> BaseCallbackHandler:<br>    return HoneyHiveLlamaIndexTracer(**kwargs)<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Honeyhive - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/honeyhive/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "\n\n# Multi modal\n\nEvaluation modules.\n\n## MultiModalRetrieverEvaluator [\\#](\\#llama_index.core.evaluation.MultiModalRetrieverEvaluator \"Permanent link\")\n\nBases: `BaseRetrievalEvaluator`\n\nRetriever evaluator.\n\nThis module will evaluate a retriever using a set of metrics.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `metrics` | `List[BaseRetrievalMetric]` | Sequence of metrics to evaluate | _required_ |\n| `retriever` |  | Retriever to evaluate. | _required_ |\n| `node_postprocessors` | `Optional[List[BaseNodePostprocessor]]` | Post-processor to apply after retrieval. | _required_ |\n\nSource code in `llama-index-core/llama_index/core/evaluation/retrieval/evaluator.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>``` | ```<br>class MultiModalRetrieverEvaluator(BaseRetrievalEvaluator):<br>    \"\"\"Retriever evaluator.<br>    This module will evaluate a retriever using a set of metrics.<br>    Args:<br>        metrics (List[BaseRetrievalMetric]): Sequence of metrics to evaluate<br>        retriever: Retriever to evaluate.<br>        node_postprocessors (Optional[List[BaseNodePostprocessor]]): Post-processor to apply after retrieval.<br>    \"\"\"<br>    retriever: BaseRetriever = Field(..., description=\"Retriever to evaluate\")<br>    node_postprocessors: Optional[List[SerializeAsAny[BaseNodePostprocessor]]] = Field(<br>        default=None, description=\"Optional post-processor\"<br>    )<br>    async def _aget_retrieved_ids_and_texts(<br>        self, query: str, mode: RetrievalEvalMode = RetrievalEvalMode.TEXT<br>    ) -> Tuple[List[str], List[str]]:<br>        \"\"\"Get retrieved ids.\"\"\"<br>        retrieved_nodes = await self.retriever.aretrieve(query)<br>        image_nodes: List[ImageNode] = []<br>        text_nodes: List[TextNode] = []<br>        if self.node_postprocessors:<br>            for node_postprocessor in self.node_postprocessors:<br>                retrieved_nodes = node_postprocessor.postprocess_nodes(<br>                    retrieved_nodes, query_str=query<br>                )<br>        for scored_node in retrieved_nodes:<br>            node = scored_node.node<br>            if isinstance(node, ImageNode):<br>                image_nodes.append(node)<br>            if node.text:<br>                text_nodes.append(node)<br>        if mode == \"text\":<br>            return (<br>                [node.node_id for node in text_nodes],<br>                [node.text for node in text_nodes],<br>            )<br>        elif mode == \"image\":<br>            return (<br>                [node.node_id for node in image_nodes],<br>                [node.text for node in image_nodes],<br>            )<br>        else:<br>            raise ValueError(\"Unsupported mode.\")<br>``` |\n\nBack to top",
      "metadata": {
        "title": "Multi modal - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/evaluation/multi_modal/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/condense_question/#llama_index.core.chat_engine.CondenseQuestionChatEngine)\n\n# Condense question\n\n## CondenseQuestionChatEngine [\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/condense_question/\\#llama_index.core.chat_engine.CondenseQuestionChatEngine \"Permanent link\")\n\nBases: `BaseChatEngine`\n\nCondense Question Chat Engine.\n\nFirst generate a standalone question from conversation context and last message,\nthen query the query engine for a response.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/condense_question.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>``` | ```<br>class CondenseQuestionChatEngine(BaseChatEngine):<br>    \"\"\"<br>    Condense Question Chat Engine.<br>    First generate a standalone question from conversation context and last message,<br>    then query the query engine for a response.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        query_engine: BaseQueryEngine,<br>        condense_question_prompt: BasePromptTemplate,<br>        memory: BaseMemory,<br>        llm: LLM,<br>        verbose: bool = False,<br>        callback_manager: Optional[CallbackManager] = None,<br>    ) -> None:<br>        self._query_engine = query_engine<br>        self._condense_question_prompt = condense_question_prompt<br>        self._memory = memory<br>        self._llm = llm<br>        self._verbose = verbose<br>        self.callback_manager = callback_manager or CallbackManager([])<br>    @classmethod<br>    def from_defaults(<br>        cls,<br>        query_engine: BaseQueryEngine,<br>        condense_question_prompt: Optional[BasePromptTemplate] = None,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        memory: Optional[BaseMemory] = None,<br>        memory_cls: Type[BaseMemory] = ChatMemoryBuffer,<br>        verbose: bool = False,<br>        system_prompt: Optional[str] = None,<br>        prefix_messages: Optional[List[ChatMessage]] = None,<br>        llm: Optional[LLM] = None,<br>        **kwargs: Any,<br>    ) -> \"CondenseQuestionChatEngine\":<br>        \"\"\"Initialize a CondenseQuestionChatEngine from default parameters.\"\"\"<br>        condense_question_prompt = condense_question_prompt or DEFAULT_PROMPT<br>        llm = llm or Settings.llm<br>        chat_history = chat_history or []<br>        memory = memory or memory_cls.from_defaults(chat_history=chat_history, llm=llm)<br>        if system_prompt is not None:<br>            raise NotImplementedError(<br>                \"system_prompt is not supported for CondenseQuestionChatEngine.\"<br>            )<br>        if prefix_messages is not None:<br>            raise NotImplementedError(<br>                \"prefix_messages is not supported for CondenseQuestionChatEngine.\"<br>            )<br>        return cls(<br>            query_engine,<br>            condense_question_prompt,<br>            memory,<br>            llm,<br>            verbose=verbose,<br>            callback_manager=Settings.callback_manager,<br>        )<br>    def _condense_question(<br>        self, chat_history: List[ChatMessage], last_message: str<br>    ) -> str:<br>        \"\"\"<br>        Generate standalone question from conversation context and last message.<br>        \"\"\"<br>        if not chat_history:<br>            # Keep the question as is if there's no conversation context.<br>            return last_message<br>        chat_history_str = messages_to_history_str(chat_history)<br>        logger.debug(chat_history_str)<br>        return self._llm.predict(<br>            self._condense_question_prompt,<br>            question=last_message,<br>            chat_history=chat_history_str,<br>        )<br>    async def _acondense_question(<br>        self, chat_history: List[ChatMessage], last_message: str<br>    ) -> str:<br>        \"\"\"<br>        Generate standalone question from conversation context and last message.<br>        \"\"\"<br>        if not chat_history:<br>            # Keep the question as is if there's no conversation context.<br>            return last_message<br>        chat_history_str = messages_to_history_str(chat_history)<br>        logger.debug(chat_history_str)<br>        return await self._llm.apredict(<br>            self._condense_question_prompt,<br>            question=last_message,<br>            chat_history=chat_history_str,<br>        )<br>    def _get_tool_output_from_response(<br>        self, query: str, response: RESPONSE_TYPE<br>    ) -> ToolOutput:<br>        if isinstance(response, (StreamingResponse, AsyncStreamingResponse)):<br>            return ToolOutput(<br>                content=\"\",<br>                tool_name=\"query_engine\",<br>                raw_input={\"query\": query},<br>                raw_output=response,<br>            )<br>        else:<br>            return ToolOutput(<br>                content=str(response),<br>                tool_name=\"query_engine\",<br>                raw_input={\"query\": query},<br>                raw_output=response,<br>            )<br>    @trace_method(\"chat\")<br>    def chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> AgentChatResponse:<br>        chat_history = chat_history or self._memory.get(input=message)<br>        # Generate standalone question from conversation context and last message<br>        condensed_question = self._condense_question(chat_history, message)<br>        log_str = f\"Querying with: {condensed_question}\"<br>        logger.info(log_str)<br>        if self._verbose:<br>            print(log_str)<br>        # TODO: right now, query engine uses class attribute to configure streaming,<br>        #       we are moving towards separate streaming and non-streaming methods.<br>        #       In the meanwhile, use this hack to toggle streaming.<br>        from llama_index.core.query_engine.retriever_query_engine import (<br>            RetrieverQueryEngine,<br>        )<br>        if isinstance(self._query_engine, RetrieverQueryEngine):<br>            is_streaming = self._query_engine._response_synthesizer._streaming<br>            self._query_engine._response_synthesizer._streaming = False<br>        # Query with standalone question<br>        query_response = self._query_engine.query(condensed_question)<br>        # NOTE: reset streaming flag<br>        if isinstance(self._query_engine, RetrieverQueryEngine):<br>            self._query_engine._response_synthesizer._streaming = is_streaming<br>        tool_output = self._get_tool_output_from_response(<br>            condensed_question, query_response<br>        )<br>        # Record response<br>        self._memory.put(ChatMessage(role=MessageRole.USER, content=message))<br>        self._memory.put(<br>            ChatMessage(role=MessageRole.ASSISTANT, content=str(query_response))<br>        )<br>        return AgentChatResponse(response=str(query_response), sources=[tool_output])<br>    @trace_method(\"chat\")<br>    def stream_chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> StreamingAgentChatResponse:<br>        chat_history = chat_history or self._memory.get(input=message)<br>        # Generate standalone question from conversation context and last message<br>        condensed_question = self._condense_question(chat_history, message)<br>        log_str = f\"Querying with: {condensed_question}\"<br>        logger.info(log_str)<br>        if self._verbose:<br>            print(log_str)<br>        # TODO: right now, query engine uses class attribute to configure streaming,<br>        #       we are moving towards separate streaming and non-streaming methods.<br>        #       In the meanwhile, use this hack to toggle streaming.<br>        from llama_index.core.query_engine.retriever_query_engine import (<br>            RetrieverQueryEngine,<br>        )<br>        if isinstance(self._query_engine, RetrieverQueryEngine):<br>            is_streaming = self._query_engine._response_synthesizer._streaming<br>            self._query_engine._response_synthesizer._streaming = True<br>        # Query with standalone question<br>        query_response = self._query_engine.query(condensed_question)<br>        # NOTE: reset streaming flag<br>        if isinstance(self._query_engine, RetrieverQueryEngine):<br>            self._query_engine._response_synthesizer._streaming = is_streaming<br>        tool_output = self._get_tool_output_from_response(<br>            condensed_question, query_response<br>        )<br>        # Record response<br>        if (<br>            isinstance(query_response, StreamingResponse)<br>            and query_response.response_gen is not None<br>        ):<br>            # override the generator to include writing to chat history<br>            self._memory.put(ChatMessage(role=MessageRole.USER, content=message))<br>            response = StreamingAgentChatResponse(<br>                chat_stream=response_gen_from_query_engine(query_response.response_gen),<br>                sources=[tool_output],<br>            )<br>            thread = Thread(<br>                target=response.write_response_to_history,<br>                args=(self._memory,),<br>            )<br>            thread.start()<br>        else:<br>            raise ValueError(\"Streaming is not enabled. Please use chat() instead.\")<br>        return response<br>    @trace_method(\"chat\")<br>    async def achat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> AgentChatResponse:<br>        chat_history = chat_history or self._memory.get(input=message)<br>        # Generate standalone question from conversation context and last message<br>        condensed_question = await self._acondense_question(chat_history, message)<br>        log_str = f\"Querying with: {condensed_question}\"<br>        logger.info(log_str)<br>        if self._verbose:<br>            print(log_str)<br>        # TODO: right now, query engine uses class attribute to configure streaming,<br>        #       we are moving towards separate streaming and non-streaming methods.<br>        #       In the meanwhile, use this hack to toggle streaming.<br>        from llama_index.core.query_engine.retriever_query_engine import (<br>            RetrieverQueryEngine,<br>        )<br>        if isinstance(self._query_engine, RetrieverQueryEngine):<br>            is_streaming = self._query_engine._response_synthesizer._streaming<br>            self._query_engine._response_synthesizer._streaming = False<br>        # Query with standalone question<br>        query_response = await self._query_engine.aquery(condensed_question)<br>        # NOTE: reset streaming flag<br>        if isinstance(self._query_engine, RetrieverQueryEngine):<br>            self._query_engine._response_synthesizer._streaming = is_streaming<br>        tool_output = self._get_tool_output_from_response(<br>            condensed_question, query_response<br>        )<br>        # Record response<br>        self._memory.put(ChatMessage(role=MessageRole.USER, content=message))<br>        self._memory.put(<br>            ChatMessage(role=MessageRole.ASSISTANT, content=str(query_response))<br>        )<br>        return AgentChatResponse(response=str(query_response), sources=[tool_output])<br>    @trace_method(\"chat\")<br>    async def astream_chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> StreamingAgentChatResponse:<br>        chat_history = chat_history or self._memory.get(input=message)<br>        # Generate standalone question from conversation context and last message<br>        condensed_question = await self._acondense_question(chat_history, message)<br>        log_str = f\"Querying with: {condensed_question}\"<br>        logger.info(log_str)<br>        if self._verbose:<br>            print(log_str)<br>        # TODO: right now, query engine uses class attribute to configure streaming,<br>        #       we are moving towards separate streaming and non-streaming methods.<br>        #       In the meanwhile, use this hack to toggle streaming.<br>        from llama_index.core.query_engine.retriever_query_engine import (<br>            RetrieverQueryEngine,<br>        )<br>        if isinstance(self._query_engine, RetrieverQueryEngine):<br>            is_streaming = self._query_engine._response_synthesizer._streaming<br>            self._query_engine._response_synthesizer._streaming = True<br>        # Query with standalone question<br>        query_response = await self._query_engine.aquery(condensed_question)<br>        # NOTE: reset streaming flag<br>        if isinstance(self._query_engine, RetrieverQueryEngine):<br>            self._query_engine._response_synthesizer._streaming = is_streaming<br>        tool_output = self._get_tool_output_from_response(<br>            condensed_question, query_response<br>        )<br>        # Record response<br>        if isinstance(query_response, AsyncStreamingResponse):<br>            # override the generator to include writing to chat history<br>            # TODO: query engine does not support async generator yet<br>            self._memory.put(ChatMessage(role=MessageRole.USER, content=message))<br>            response = StreamingAgentChatResponse(<br>                achat_stream=aresponse_gen_from_query_engine(<br>                    query_response.async_response_gen()<br>                ),<br>                sources=[tool_output],<br>            )<br>            asyncio.create_task(response.awrite_response_to_history(self._memory))<br>        else:<br>            raise ValueError(\"Streaming is not enabled. Please use achat() instead.\")<br>        return response<br>    def reset(self) -> None:<br>        # Clear chat history<br>        self._memory.reset()<br>    @property<br>    def chat_history(self) -> List[ChatMessage]:<br>        \"\"\"Get chat history.\"\"\"<br>        return self._memory.get_all()<br>``` |\n\n### chat\\_history`property`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/condense_question/\\#llama_index.core.chat_engine.CondenseQuestionChatEngine.chat_history \"Permanent link\")\n\n```\nchat_history: List[ChatMessage]\n\n```\n\nGet chat history.\n\n### from\\_defaults`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/condense_question/\\#llama_index.core.chat_engine.CondenseQuestionChatEngine.from_defaults \"Permanent link\")\n\n```\nfrom_defaults(query_engine: BaseQueryEngine, condense_question_prompt: Optional[BasePromptTemplate] = None, chat_history: Optional[List[ChatMessage]] = None, memory: Optional[BaseMemory] = None, memory_cls: Type[BaseMemory] = ChatMemoryBuffer, verbose: bool = False, system_prompt: Optional[str] = None, prefix_messages: Optional[List[ChatMessage]] = None, llm: Optional[LLM] = None, **kwargs: Any) -> CondenseQuestionChatEngine\n\n```\n\nInitialize a CondenseQuestionChatEngine from default parameters.\n\nSource code in `llama-index-core/llama_index/core/chat_engine/condense_question.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>``` | ```<br>@classmethod<br>def from_defaults(<br>    cls,<br>    query_engine: BaseQueryEngine,<br>    condense_question_prompt: Optional[BasePromptTemplate] = None,<br>    chat_history: Optional[List[ChatMessage]] = None,<br>    memory: Optional[BaseMemory] = None,<br>    memory_cls: Type[BaseMemory] = ChatMemoryBuffer,<br>    verbose: bool = False,<br>    system_prompt: Optional[str] = None,<br>    prefix_messages: Optional[List[ChatMessage]] = None,<br>    llm: Optional[LLM] = None,<br>    **kwargs: Any,<br>) -> \"CondenseQuestionChatEngine\":<br>    \"\"\"Initialize a CondenseQuestionChatEngine from default parameters.\"\"\"<br>    condense_question_prompt = condense_question_prompt or DEFAULT_PROMPT<br>    llm = llm or Settings.llm<br>    chat_history = chat_history or []<br>    memory = memory or memory_cls.from_defaults(chat_history=chat_history, llm=llm)<br>    if system_prompt is not None:<br>        raise NotImplementedError(<br>            \"system_prompt is not supported for CondenseQuestionChatEngine.\"<br>        )<br>    if prefix_messages is not None:<br>        raise NotImplementedError(<br>            \"prefix_messages is not supported for CondenseQuestionChatEngine.\"<br>        )<br>    return cls(<br>        query_engine,<br>        condense_question_prompt,<br>        memory,<br>        llm,<br>        verbose=verbose,<br>        callback_manager=Settings.callback_manager,<br>    )<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Condense question - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/chat_engines/condense_question/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/huggingface/#llama_index.embeddings.huggingface.HuggingFaceEmbedding)\n\n# Huggingface\n\n## HuggingFaceEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/huggingface/\\#llama_index.embeddings.huggingface.HuggingFaceEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nHuggingFace class for text embeddings.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model_name` | `str` | If it is a filepath on disc, it loads the model from that path.<br>If it is not a path, it first tries to download a pre-trained SentenceTransformer model.<br>If that fails, tries to construct a model from the Hugging Face Hub with that name.<br>Defaults to DEFAULT\\_HUGGINGFACE\\_EMBEDDING\\_MODEL. | `DEFAULT_HUGGINGFACE_EMBEDDING_MODEL` |\n| `max_length` | `Optional[int]` | Max sequence length to set in Model's config. If None,<br>it will use the Model's default max\\_seq\\_length. Defaults to None. | `None` |\n| `query_instruction` | `Optional[str]` | Instruction to prepend to query text.<br>Defaults to None. | `None` |\n| `text_instruction` | `Optional[str]` | Instruction to prepend to text.<br>Defaults to None. | `None` |\n| `normalize` | `bool` | Whether to normalize returned vectors.<br>Defaults to True. | `True` |\n| `embed_batch_size` | `int` | The batch size used for the computation.<br>Defaults to DEFAULT\\_EMBED\\_BATCH\\_SIZE. | `DEFAULT_EMBED_BATCH_SIZE` |\n| `cache_folder` | `Optional[str]` | Path to store models. Defaults to None. | `None` |\n| `trust_remote_code` | `bool` | Whether or not to allow for custom models defined on the<br>Hub in their own modeling files. This option should only be set to True for repositories<br>you trust and in which you have read the code, as it will execute code present on the Hub<br>on your local machine. Defaults to False. | `False` |\n| `device` | `Optional[str]` | Device (like \"cuda\", \"cpu\", \"mps\", \"npu\", ...) that should<br>be used for computation. If None, checks if a GPU can be used. Defaults to None. | `None` |\n| `callback_manager` | `Optional[CallbackManager]` | Callback Manager. Defaults to None. | `None` |\n| `parallel_process` | `bool` | If True it will start a multi-process pool to process the<br>encoding with several independent processes. Great for vast amount of texts.<br>Defaults to False. | `False` |\n| `target_devices` | `Optional[List[str]]` | PyTorch target devices, e.g.<br>\\[\"cuda:0\", \"cuda:1\", ...\\], \\[\"npu:0\", \"npu:1\", ...\\], or \\[\"cpu\", \"cpu\", \"cpu\", \"cpu\"\\].<br>If target\\_devices is None and CUDA/NPU is available, then all available CUDA/NPU devices<br>will be used. If target\\_devices is None and CUDA/NPU is not available, then 4 CPU devices<br>will be used. This parameter will only be used if `parallel_process = True`.<br>Defaults to None. | `None` |\n| `num_workers` | `int` | The number of workers to use for async embedding calls.<br>Defaults to None. | _required_ |\n| `**model_kwargs` |  | Other model kwargs to use | `{}` |\n| `tokenizer_name` | `Optional[str]` | \"Deprecated\" | `'deprecated'` |\n| `pooling` | `str` | \"Deprecated\" | `'deprecated'` |\n| `model` | `Optional[Any]` | \"Deprecated\" | `'deprecated'` |\n| `tokenizer` | `Optional[Any]` | \"Deprecated\" | `'deprecated'` |\n\n**Examples:**\n\n`pip install llama-index-embeddings-huggingface`\n\n```\nfrom llama_index.core import Settings\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\n# Set up the HuggingFaceEmbedding class with the required model to use with llamaindex core.\nembed_model  = HuggingFaceEmbedding(model_name = \"BAAI/bge-small-en\")\nSettings.embed_model = embed_model\n\n# Or if you want to Embed some text separately\nembeddings = embed_model.get_text_embedding(\"I want to Embed this text!\")\n\n```\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-huggingface/llama_index/embeddings/huggingface/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>``` | ````<br>class HuggingFaceEmbedding(BaseEmbedding):<br>    \"\"\"HuggingFace class for text embeddings.<br>    Args:<br>        model_name (str, optional): If it is a filepath on disc, it loads the model from that path.<br>            If it is not a path, it first tries to download a pre-trained SentenceTransformer model.<br>            If that fails, tries to construct a model from the Hugging Face Hub with that name.<br>            Defaults to DEFAULT_HUGGINGFACE_EMBEDDING_MODEL.<br>        max_length (Optional[int], optional): Max sequence length to set in Model's config. If None,<br>            it will use the Model's default max_seq_length. Defaults to None.<br>        query_instruction (Optional[str], optional): Instruction to prepend to query text.<br>            Defaults to None.<br>        text_instruction (Optional[str], optional): Instruction to prepend to text.<br>            Defaults to None.<br>        normalize (bool, optional): Whether to normalize returned vectors.<br>            Defaults to True.<br>        embed_batch_size (int, optional): The batch size used for the computation.<br>            Defaults to DEFAULT_EMBED_BATCH_SIZE.<br>        cache_folder (Optional[str], optional): Path to store models. Defaults to None.<br>        trust_remote_code (bool, optional): Whether or not to allow for custom models defined on the<br>            Hub in their own modeling files. This option should only be set to True for repositories<br>            you trust and in which you have read the code, as it will execute code present on the Hub<br>            on your local machine. Defaults to False.<br>        device (Optional[str], optional): Device (like \"cuda\", \"cpu\", \"mps\", \"npu\", ...) that should<br>            be used for computation. If None, checks if a GPU can be used. Defaults to None.<br>        callback_manager (Optional[CallbackManager], optional): Callback Manager. Defaults to None.<br>        parallel_process (bool, optional): If True it will start a multi-process pool to process the<br>            encoding with several independent processes. Great for vast amount of texts.<br>            Defaults to False.<br>        target_devices (Optional[List[str]], optional): PyTorch target devices, e.g.<br>            [\"cuda:0\", \"cuda:1\", ...], [\"npu:0\", \"npu:1\", ...], or [\"cpu\", \"cpu\", \"cpu\", \"cpu\"].<br>            If target_devices is None and CUDA/NPU is available, then all available CUDA/NPU devices<br>            will be used. If target_devices is None and CUDA/NPU is not available, then 4 CPU devices<br>            will be used. This parameter will only be used if `parallel_process = True`.<br>            Defaults to None.<br>        num_workers (int, optional): The number of workers to use for async embedding calls.<br>            Defaults to None.<br>        **model_kwargs: Other model kwargs to use<br>        tokenizer_name (Optional[str], optional): \"Deprecated\"<br>        pooling (str, optional): \"Deprecated\"<br>        model (Optional[Any], optional): \"Deprecated\"<br>        tokenizer (Optional[Any], optional): \"Deprecated\"<br>    Examples:<br>        `pip install llama-index-embeddings-huggingface`<br>        ```python<br>        from llama_index.core import Settings<br>        from llama_index.embeddings.huggingface import HuggingFaceEmbedding<br>        # Set up the HuggingFaceEmbedding class with the required model to use with llamaindex core.<br>        embed_model  = HuggingFaceEmbedding(model_name = \"BAAI/bge-small-en\")<br>        Settings.embed_model = embed_model<br>        # Or if you want to Embed some text separately<br>        embeddings = embed_model.get_text_embedding(\"I want to Embed this text!\")<br>        ```<br>    \"\"\"<br>    max_length: int = Field(<br>        default=DEFAULT_HUGGINGFACE_LENGTH, description=\"Maximum length of input.\", gt=0<br>    )<br>    normalize: bool = Field(default=True, description=\"Normalize embeddings or not.\")<br>    query_instruction: Optional[str] = Field(<br>        description=\"Instruction to prepend to query text.\", default=None<br>    )<br>    text_instruction: Optional[str] = Field(<br>        description=\"Instruction to prepend to text.\", default=None<br>    )<br>    cache_folder: Optional[str] = Field(<br>        description=\"Cache folder for Hugging Face files.\", default=None<br>    )<br>    _model: Any = PrivateAttr()<br>    _device: str = PrivateAttr()<br>    _parallel_process: bool = PrivateAttr()<br>    _target_devices: Optional[List[str]] = PrivateAttr()<br>    def __init__(<br>        self,<br>        model_name: str = DEFAULT_HUGGINGFACE_EMBEDDING_MODEL,<br>        tokenizer_name: Optional[str] = \"deprecated\",<br>        pooling: str = \"deprecated\",<br>        max_length: Optional[int] = None,<br>        query_instruction: Optional[str] = None,<br>        text_instruction: Optional[str] = None,<br>        normalize: bool = True,<br>        model: Optional[Any] = \"deprecated\",<br>        tokenizer: Optional[Any] = \"deprecated\",<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        cache_folder: Optional[str] = None,<br>        trust_remote_code: bool = False,<br>        device: Optional[str] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        parallel_process: bool = False,<br>        target_devices: Optional[List[str]] = None,<br>        **model_kwargs,<br>    ):<br>        device = device or infer_torch_device()<br>        cache_folder = cache_folder or get_cache_dir()<br>        for variable, value in [<br>            (\"model\", model),<br>            (\"tokenizer\", tokenizer),<br>            (\"pooling\", pooling),<br>            (\"tokenizer_name\", tokenizer_name),<br>        ]:<br>            if value != \"deprecated\":<br>                raise ValueError(<br>                    f\"{variable} is deprecated. Please remove it from the arguments.\"<br>                )<br>        if model_name is None:<br>            raise ValueError(\"The `model_name` argument must be provided.\")<br>        model = SentenceTransformer(<br>            model_name,<br>            device=device,<br>            cache_folder=cache_folder,<br>            trust_remote_code=trust_remote_code,<br>            prompts={<br>                \"query\": query_instruction<br>                or get_query_instruct_for_model_name(model_name),<br>                \"text\": text_instruction<br>                or get_text_instruct_for_model_name(model_name),<br>            },<br>            **model_kwargs,<br>        )<br>        if max_length:<br>            model.max_seq_length = max_length<br>        else:<br>            max_length = model.max_seq_length<br>        super().__init__(<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            model_name=model_name,<br>            max_length=max_length,<br>            normalize=normalize,<br>            query_instruction=query_instruction,<br>            text_instruction=text_instruction,<br>        )<br>        self._device = device<br>        self._model = model<br>        self._parallel_process = parallel_process<br>        self._target_devices = target_devices<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"HuggingFaceEmbedding\"<br>    def _embed(<br>        self,<br>        sentences: List[str],<br>        prompt_name: Optional[str] = None,<br>    ) -> List[List[float]]:<br>        \"\"\"Generates Embeddings either multiprocess or single process.<br>        Args:<br>            sentences (List[str]): Texts or Sentences to embed<br>            prompt_name (Optional[str], optional): The name of the prompt to use for encoding. Must be a key in the `prompts` dictionary i.e. \"query\" or \"text\" If ``prompt`` is also set, this argument is ignored. Defaults to None.<br>        Returns:<br>            List[List[float]]: a 2d numpy array with shape [num_inputs, output_dimension] is returned.<br>            If only one string input is provided, then the output is a 1d array with shape [output_dimension]<br>        \"\"\"<br>        if self._parallel_process:<br>            pool = self._model.start_multi_process_pool(<br>                target_devices=self._target_devices<br>            )<br>            emb = self._model.encode_multi_process(<br>                sentences=sentences,<br>                pool=pool,<br>                batch_size=self.embed_batch_size,<br>                prompt_name=prompt_name,<br>                normalize_embeddings=self.normalize,<br>            )<br>            self._model.stop_multi_process_pool(pool=pool)<br>        else:<br>            emb = self._model.encode(<br>                sentences,<br>                batch_size=self.embed_batch_size,<br>                prompt_name=prompt_name,<br>                normalize_embeddings=self.normalize,<br>            )<br>        return emb.tolist()<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Generates Embeddings for Query.<br>        Args:<br>            query (str): Query text/sentence<br>        Returns:<br>            List[float]: numpy array of embeddings<br>        \"\"\"<br>        return self._embed(query, prompt_name=\"query\")<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Generates Embeddings for Query Asynchronously.<br>        Args:<br>            query (str): Query text/sentence<br>        Returns:<br>            List[float]: numpy array of embeddings<br>        \"\"\"<br>        return self._get_query_embedding(query)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Generates Embeddings for text Asynchronously.<br>        Args:<br>            text (str): Text/Sentence<br>        Returns:<br>            List[float]: numpy array of embeddings<br>        \"\"\"<br>        return self._get_text_embedding(text)<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Generates Embeddings for text.<br>        Args:<br>            text (str): Text/sentences<br>        Returns:<br>            List[float]: numpy array of embeddings<br>        \"\"\"<br>        return self._embed(text, prompt_name=\"text\")<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Generates Embeddings for text.<br>        Args:<br>            texts (List[str]): Texts / Sentences<br>        Returns:<br>            List[List[float]]: numpy array of embeddings<br>        \"\"\"<br>        return self._embed(texts, prompt_name=\"text\")<br>```` |\n\n## HuggingFaceInferenceAPIEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/huggingface/\\#llama_index.embeddings.huggingface.HuggingFaceInferenceAPIEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nWrapper on the Hugging Face's Inference API for embeddings.\n\nOverview of the design:\n\\- Uses the feature extraction task: https://huggingface.co/tasks/feature-extraction\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-huggingface/llama_index/embeddings/huggingface/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>472<br>473<br>474<br>475<br>476<br>``` | ```<br>@deprecated(<br>    \"Deprecated in favor of `HuggingFaceInferenceAPIEmbedding` from `llama-index-embeddings-huggingface-api` which should be used instead.\",<br>    action=\"always\",<br>)<br>class HuggingFaceInferenceAPIEmbedding(BaseEmbedding):  # type: ignore[misc]<br>    \"\"\"<br>    Wrapper on the Hugging Face's Inference API for embeddings.<br>    Overview of the design:<br>    - Uses the feature extraction task: https://huggingface.co/tasks/feature-extraction<br>    \"\"\"<br>    pooling: Optional[Pooling] = Field(<br>        default=Pooling.CLS,<br>        description=\"Pooling strategy. If None, the model's default pooling is used.\",<br>    )<br>    query_instruction: Optional[str] = Field(<br>        default=None, description=\"Instruction to prepend during query embedding.\"<br>    )<br>    text_instruction: Optional[str] = Field(<br>        default=None, description=\"Instruction to prepend during text embedding.\"<br>    )<br>    # Corresponds with huggingface_hub.InferenceClient<br>    model_name: Optional[str] = Field(<br>        default=None,<br>        description=\"Hugging Face model name. If None, the task will be used.\",<br>    )<br>    token: Union[str, bool, None] = Field(<br>        default=None,<br>        description=(<br>            \"Hugging Face token. Will default to the locally saved token. Pass \"<br>            \"token=False if you don\u2019t want to send your token to the server.\"<br>        ),<br>    )<br>    timeout: Optional[float] = Field(<br>        default=None,<br>        description=(<br>            \"The maximum number of seconds to wait for a response from the server.\"<br>            \" Loading a new model in Inference API can take up to several minutes.\"<br>            \" Defaults to None, meaning it will loop until the server is available.\"<br>        ),<br>    )<br>    headers: Dict[str, str] = Field(<br>        default=None,<br>        description=(<br>            \"Additional headers to send to the server. By default only the\"<br>            \" authorization and user-agent headers are sent. Values in this dictionary\"<br>            \" will override the default values.\"<br>        ),<br>    )<br>    cookies: Dict[str, str] = Field(<br>        default=None, description=\"Additional cookies to send to the server.\"<br>    )<br>    task: Optional[str] = Field(<br>        default=None,<br>        description=(<br>            \"Optional task to pick Hugging Face's recommended model, used when\"<br>            \" model_name is left as default of None.\"<br>        ),<br>    )<br>    _sync_client: \"InferenceClient\" = PrivateAttr()<br>    _async_client: \"AsyncInferenceClient\" = PrivateAttr()<br>    _get_model_info: \"Callable[..., ModelInfo]\" = PrivateAttr()<br>    def _get_inference_client_kwargs(self) -> Dict[str, Any]:<br>        \"\"\"Extract the Hugging Face InferenceClient construction parameters.\"\"\"<br>        return {<br>            \"model\": self.model_name,<br>            \"token\": self.token,<br>            \"timeout\": self.timeout,<br>            \"headers\": self.headers,<br>            \"cookies\": self.cookies,<br>        }<br>    def __init__(self, **kwargs: Any) -> None:<br>        \"\"\"Initialize.<br>        Args:<br>            kwargs: See the class-level Fields.<br>        \"\"\"<br>        if kwargs.get(\"model_name\") is None:<br>            task = kwargs.get(\"task\", \"\")<br>            # NOTE: task being None or empty string leads to ValueError,<br>            # which ensures model is present<br>            kwargs[\"model_name\"] = InferenceClient.get_recommended_model(task=task)<br>            logger.debug(<br>                f\"Using Hugging Face's recommended model {kwargs['model_name']}\"<br>                f\" given task {task}.\"<br>            )<br>            print(kwargs[\"model_name\"], flush=True)<br>        super().__init__(**kwargs)  # Populate pydantic Fields<br>        self._sync_client = InferenceClient(**self._get_inference_client_kwargs())<br>        self._async_client = AsyncInferenceClient(**self._get_inference_client_kwargs())<br>        self._get_model_info = model_info<br>    def validate_supported(self, task: str) -> None:<br>        \"\"\"<br>        Confirm the contained model_name is deployed on the Inference API service.<br>        Args:<br>            task: Hugging Face task to check within. A list of all tasks can be<br>                found here: https://huggingface.co/tasks<br>        \"\"\"<br>        all_models = self._sync_client.list_deployed_models(frameworks=\"all\")<br>        try:<br>            if self.model_name not in all_models[task]:<br>                raise ValueError(<br>                    \"The Inference API service doesn't have the model\"<br>                    f\" {self.model_name!r} deployed.\"<br>                )<br>        except KeyError as exc:<br>            raise KeyError(<br>                f\"Input task {task!r} not in possible tasks {list(all_models.keys())}.\"<br>            ) from exc<br>    def get_model_info(self, **kwargs: Any) -> \"ModelInfo\":<br>        \"\"\"Get metadata on the current model from Hugging Face.\"\"\"<br>        return self._get_model_info(self.model_name, **kwargs)<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"HuggingFaceInferenceAPIEmbedding\"<br>    async def _async_embed_single(self, text: str) -> Embedding:<br>        embedding = await self._async_client.feature_extraction(text)<br>        if len(embedding.shape) == 1:<br>            return embedding.tolist()<br>        embedding = embedding.squeeze(axis=0)<br>        if len(embedding.shape) == 1:  # Some models pool internally<br>            return embedding.tolist()<br>        try:<br>            return self.pooling(embedding).tolist()  # type: ignore[misc]<br>        except TypeError as exc:<br>            raise ValueError(<br>                f\"Pooling is required for {self.model_name} because it returned\"<br>                \" a > 1-D value, please specify pooling as not None.\"<br>            ) from exc<br>    async def _async_embed_bulk(self, texts: Sequence[str]) -> List[Embedding]:<br>        \"\"\"<br>        Embed a sequence of text, in parallel and asynchronously.<br>        NOTE: this uses an externally created asyncio event loop.<br>        \"\"\"<br>        tasks = [self._async_embed_single(text) for text in texts]<br>        return await asyncio.gather(*tasks)<br>    def _get_query_embedding(self, query: str) -> Embedding:<br>        \"\"\"<br>        Embed the input query synchronously.<br>        NOTE: a new asyncio event loop is created internally for this.<br>        \"\"\"<br>        return asyncio.run(self._aget_query_embedding(query))<br>    def _get_text_embedding(self, text: str) -> Embedding:<br>        \"\"\"<br>        Embed the text query synchronously.<br>        NOTE: a new asyncio event loop is created internally for this.<br>        \"\"\"<br>        return asyncio.run(self._aget_text_embedding(text))<br>    def _get_text_embeddings(self, texts: List[str]) -> List[Embedding]:<br>        \"\"\"<br>        Embed the input sequence of text synchronously and in parallel.<br>        NOTE: a new asyncio event loop is created internally for this.<br>        \"\"\"<br>        loop = asyncio.new_event_loop()<br>        try:<br>            tasks = [<br>                loop.create_task(self._aget_text_embedding(text)) for text in texts<br>            ]<br>            loop.run_until_complete(asyncio.wait(tasks))<br>        finally:<br>            loop.close()<br>        return [task.result() for task in tasks]<br>    async def _aget_query_embedding(self, query: str) -> Embedding:<br>        return await self._async_embed_single(<br>            text=format_query(query, self.model_name, self.query_instruction)<br>        )<br>    async def _aget_text_embedding(self, text: str) -> Embedding:<br>        return await self._async_embed_single(<br>            text=format_text(text, self.model_name, self.text_instruction)<br>        )<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[Embedding]:<br>        return await self._async_embed_bulk(<br>            texts=[<br>                format_text(text, self.model_name, self.text_instruction)<br>                for text in texts<br>            ]<br>        )<br>``` |\n\n### validate\\_supported [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/huggingface/\\#llama_index.embeddings.huggingface.HuggingFaceInferenceAPIEmbedding.validate_supported \"Permanent link\")\n\n```\nvalidate_supported(task: str) -> None\n\n```\n\nConfirm the contained model\\_name is deployed on the Inference API service.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `task` | `str` | Hugging Face task to check within. A list of all tasks can be<br>found here: https://huggingface.co/tasks | _required_ |\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-huggingface/llama_index/embeddings/huggingface/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>``` | ```<br>def validate_supported(self, task: str) -> None:<br>    \"\"\"<br>    Confirm the contained model_name is deployed on the Inference API service.<br>    Args:<br>        task: Hugging Face task to check within. A list of all tasks can be<br>            found here: https://huggingface.co/tasks<br>    \"\"\"<br>    all_models = self._sync_client.list_deployed_models(frameworks=\"all\")<br>    try:<br>        if self.model_name not in all_models[task]:<br>            raise ValueError(<br>                \"The Inference API service doesn't have the model\"<br>                f\" {self.model_name!r} deployed.\"<br>            )<br>    except KeyError as exc:<br>        raise KeyError(<br>            f\"Input task {task!r} not in possible tasks {list(all_models.keys())}.\"<br>        ) from exc<br>``` |\n\n### get\\_model\\_info [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/huggingface/\\#llama_index.embeddings.huggingface.HuggingFaceInferenceAPIEmbedding.get_model_info \"Permanent link\")\n\n```\nget_model_info(**kwargs: Any) -> ModelInfo\n\n```\n\nGet metadata on the current model from Hugging Face.\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-huggingface/llama_index/embeddings/huggingface/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>396<br>397<br>398<br>``` | ```<br>def get_model_info(self, **kwargs: Any) -> \"ModelInfo\":<br>    \"\"\"Get metadata on the current model from Hugging Face.\"\"\"<br>    return self._get_model_info(self.model_name, **kwargs)<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Huggingface - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/huggingface/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/huggingface_optimum/#llama_index.embeddings.huggingface_optimum.OptimumEmbedding)\n\n# Huggingface optimum\n\n## OptimumEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/huggingface_optimum/\\#llama_index.embeddings.huggingface_optimum.OptimumEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-huggingface-optimum/llama_index/embeddings/huggingface_optimum/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>``` | ```<br>class OptimumEmbedding(BaseEmbedding):<br>    folder_name: str = Field(description=\"Folder name to load from.\")<br>    max_length: int = Field(description=\"Maximum length of input.\")<br>    pooling: str = Field(description=\"Pooling strategy. One of ['cls', 'mean'].\")<br>    normalize: bool = Field(default=True, description=\"Normalize embeddings or not.\")<br>    query_instruction: Optional[str] = Field(<br>        description=\"Instruction to prepend to query text.\"<br>    )<br>    text_instruction: Optional[str] = Field(<br>        description=\"Instruction to prepend to text.\"<br>    )<br>    cache_folder: Optional[str] = Field(<br>        description=\"Cache folder for huggingface files.\", default=None<br>    )<br>    _model: Any = PrivateAttr()<br>    _tokenizer: Any = PrivateAttr()<br>    _device: Any = PrivateAttr()<br>    def __init__(<br>        self,<br>        folder_name: str,<br>        pooling: str = \"cls\",<br>        max_length: Optional[int] = None,<br>        normalize: bool = True,<br>        query_instruction: Optional[str] = None,<br>        text_instruction: Optional[str] = None,<br>        model: Optional[Any] = None,<br>        tokenizer: Optional[Any] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>        device: Optional[str] = None,<br>    ):<br>        model = model or ORTModelForFeatureExtraction.from_pretrained(folder_name)<br>        tokenizer = tokenizer or AutoTokenizer.from_pretrained(folder_name)<br>        device = device or infer_torch_device()<br>        if max_length is None:<br>            try:<br>                max_length = int(model.config.max_position_embeddings)<br>            except Exception:<br>                raise ValueError(<br>                    \"Unable to find max_length from model config. \"<br>                    \"Please provide max_length.\"<br>                )<br>            try:<br>                max_length = min(max_length, int(tokenizer.model_max_length))<br>            except Exception as exc:<br>                print(f\"An error occurred while retrieving tokenizer max length: {exc}\")<br>        if pooling not in [\"cls\", \"mean\"]:<br>            raise ValueError(f\"Pooling {pooling} not supported.\")<br>        super().__init__(<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            folder_name=folder_name,<br>            max_length=max_length,<br>            pooling=pooling,<br>            normalize=normalize,<br>            query_instruction=query_instruction,<br>            text_instruction=text_instruction,<br>        )<br>        self._model = model<br>        self._device = device<br>        self._tokenizer = tokenizer<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"OptimumEmbedding\"<br>    @classmethod<br>    def create_and_save_optimum_model(<br>        cls,<br>        model_name_or_path: str,<br>        output_path: str,<br>        export_kwargs: Optional[dict] = None,<br>    ) -> None:<br>        try:<br>            from optimum.onnxruntime import ORTModelForFeatureExtraction<br>            from transformers import AutoTokenizer<br>        except ImportError:<br>            raise ImportError(<br>                \"OptimumEmbedding requires transformers to be installed.\\n\"<br>                \"Please install transformers with \"<br>                \"`pip install transformers optimum[exporters]`.\"<br>            )<br>        export_kwargs = export_kwargs or {}<br>        model = ORTModelForFeatureExtraction.from_pretrained(<br>            model_name_or_path, export=True, **export_kwargs<br>        )<br>        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)<br>        model.save_pretrained(output_path)<br>        tokenizer.save_pretrained(output_path)<br>        print(<br>            f\"Saved optimum model to {output_path}. Use it with \"<br>            f\"`embed_model = OptimumEmbedding(folder_name='{output_path}')`.\"<br>        )<br>    def _mean_pooling(self, model_output: Any, attention_mask: Any) -> Any:<br>        \"\"\"Mean Pooling - Take attention mask into account for correct averaging.\"\"\"<br>        import torch<br>        # First element of model_output contains all token embeddings<br>        token_embeddings = model_output[0]<br>        input_mask_expanded = (<br>            attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()<br>        )<br>        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(<br>            input_mask_expanded.sum(1), min=1e-9<br>        )<br>    def _cls_pooling(self, model_output: list) -> Any:<br>        \"\"\"Use the CLS token as the pooling token.\"\"\"<br>        return model_output[0][:, 0]<br>    def _embed(self, sentences: List[str]) -> List[List[float]]:<br>        \"\"\"Embed sentences.\"\"\"<br>        encoded_input = self._tokenizer(<br>            sentences,<br>            padding=True,<br>            max_length=self.max_length,<br>            truncation=True,<br>            return_tensors=\"pt\",<br>        )<br>        model_output = self._model(**encoded_input)<br>        if self.pooling == \"cls\":<br>            embeddings = self._cls_pooling(model_output)<br>        else:<br>            embeddings = self._mean_pooling(<br>                model_output, encoded_input[\"attention_mask\"].to(self._device)<br>            )<br>        if self.normalize:<br>            import torch<br>            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)<br>        return embeddings.tolist()<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        query = format_query(query, self.model_name, self.query_instruction)<br>        return self._embed([query])[0]<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding async.\"\"\"<br>        return self._get_query_embedding(query)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding async.\"\"\"<br>        return self._get_text_embedding(text)<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        text = format_text(text, self.model_name, self.text_instruction)<br>        return self._embed([text])[0]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        texts = [<br>            format_text(text, self.model_name, self.text_instruction) for text in texts<br>        ]<br>        return self._embed(texts)<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Huggingface optimum - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/huggingface_optimum/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/azure_inference/#llama_index.embeddings.azure_inference.AzureAIEmbeddingsModel)\n\n# Azure inference\n\n## AzureAIEmbeddingsModel [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/azure_inference/\\#llama_index.embeddings.azure_inference.AzureAIEmbeddingsModel \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nAzure AI model inference for embeddings.\n\n**Examples:**\n\n```\nfrom llama_index.core import Settings\nfrom llama_index.embeddings.azure_inference import AzureAIEmbeddingsModel\n\nllm = AzureAIEmbeddingsModel(\n    endpoint=\"https://[your-endpoint].inference.ai.azure.com\",\n    credential=\"your-api-key\",\n)\n\n# # If using Microsoft Entra ID authentication, you can create the\n# # client as follows\n#\n# from azure.identity import DefaultAzureCredential\n#\n# embed_model = AzureAIEmbeddingsModel(\n#     endpoint=\"https://[your-endpoint].inference.ai.azure.com\",\n#     credential=DefaultAzureCredential()\n# )\n#\n# # If you plan to use asynchronous calling, make sure to use the async\n# # credentials as follows\n#\n# from azure.identity.aio import DefaultAzureCredential as DefaultAzureCredentialAsync\n#\n# embed_model = AzureAIEmbeddingsModel(\n#     endpoint=\"https://[your-endpoint].inference.ai.azure.com\",\n#     credential=DefaultAzureCredentialAsync()\n# )\n\n# Once the client is instantiated, you can set the context to use the model\nSettings.embed_model = embed_model\n\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\n\n```\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-azure-inference/llama_index/embeddings/azure_inference/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>``` | ````<br>class AzureAIEmbeddingsModel(BaseEmbedding):<br>    \"\"\"Azure AI model inference for embeddings.<br>    Examples:<br>        ```python<br>        from llama_index.core import Settings<br>        from llama_index.embeddings.azure_inference import AzureAIEmbeddingsModel<br>        llm = AzureAIEmbeddingsModel(<br>            endpoint=\"https://[your-endpoint].inference.ai.azure.com\",<br>            credential=\"your-api-key\",<br>        )<br>        # # If using Microsoft Entra ID authentication, you can create the<br>        # # client as follows<br>        #<br>        # from azure.identity import DefaultAzureCredential<br>        #<br>        # embed_model = AzureAIEmbeddingsModel(<br>        #     endpoint=\"https://[your-endpoint].inference.ai.azure.com\",<br>        #     credential=DefaultAzureCredential()<br>        # )<br>        #<br>        # # If you plan to use asynchronous calling, make sure to use the async<br>        # # credentials as follows<br>        #<br>        # from azure.identity.aio import DefaultAzureCredential as DefaultAzureCredentialAsync<br>        #<br>        # embed_model = AzureAIEmbeddingsModel(<br>        #     endpoint=\"https://[your-endpoint].inference.ai.azure.com\",<br>        #     credential=DefaultAzureCredentialAsync()<br>        # )<br>        # Once the client is instantiated, you can set the context to use the model<br>        Settings.embed_model = embed_model<br>        documents = SimpleDirectoryReader(\"./data\").load_data()<br>        index = VectorStoreIndex.from_documents(documents)<br>        ```<br>    \"\"\"<br>    model_kwargs: Dict[str, Any] = Field(<br>        default_factory=dict, description=\"Additional kwargs model parameters.\"<br>    )<br>    _client: EmbeddingsClient = PrivateAttr()<br>    _async_client: EmbeddingsClientAsync = PrivateAttr()<br>    def __init__(<br>        self,<br>        endpoint: str = None,<br>        credential: Union[str, AzureKeyCredential, \"TokenCredential\"] = None,<br>        model_name: str = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>        num_workers: Optional[int] = None,<br>        client_kwargs: Optional[Dict[str, Any]] = None,<br>        **kwargs: Any,<br>    ):<br>        client_kwargs = client_kwargs or {}<br>        endpoint = get_from_param_or_env(<br>            \"endpoint\", endpoint, \"AZURE_INFERENCE_ENDPOINT\", None<br>        )<br>        credential = get_from_param_or_env(<br>            \"credential\", credential, \"AZURE_INFERENCE_CREDENTIAL\", None<br>        )<br>        credential = (<br>            AzureKeyCredential(credential)<br>            if isinstance(credential, str)<br>            else credential<br>        )<br>        if not endpoint:<br>            raise ValueError(<br>                \"You must provide an endpoint to use the Azure AI model inference LLM.\"<br>                \"Pass the endpoint as a parameter or set the AZURE_INFERENCE_ENDPOINT\"<br>                \"environment variable.\"<br>            )<br>        if not credential:<br>            raise ValueError(<br>                \"You must provide an credential to use the Azure AI model inference LLM.\"<br>                \"Pass the credential as a parameter or set the AZURE_INFERENCE_CREDENTIAL\"<br>            )<br>        client = EmbeddingsClient(<br>            endpoint=endpoint,<br>            credential=credential,<br>            user_agent=\"llamaindex\",<br>            **client_kwargs,<br>        )<br>        async_client = EmbeddingsClientAsync(<br>            endpoint=endpoint,<br>            credential=credential,<br>            user_agent=\"llamaindex\",<br>            **client_kwargs,<br>        )<br>        if not model_name:<br>            try:<br>                # Get model info from the endpoint. This method may not be supported by all<br>                # endpoints.<br>                model_info = client.get_model_info()<br>                model_name = model_info.get(\"model_name\", None)<br>            except HttpResponseError:<br>                logger.warning(<br>                    f\"Endpoint '{self._client._config.endpoint}' does not support model metadata retrieval. \"<br>                    \"Unable to populate model attributes.\"<br>                )<br>        super().__init__(<br>            model_name=model_name or \"unknown\",<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            num_workers=num_workers,<br>            **kwargs,<br>        )<br>        self._client = client<br>        self._async_client = async_client<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"AzureAIEmbeddingsModel\"<br>    @property<br>    def _model_kwargs(self) -> Dict[str, Any]:<br>        additional_kwargs = {}<br>        if self.model_name and self.model_name != \"unknown\":<br>            additional_kwargs[\"model\"] = self.model_name<br>        if self.model_kwargs:<br>            # pass any extra model parameters<br>            additional_kwargs.update(self.model_kwargs)<br>        return additional_kwargs<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self._client.embed(input=[query], **self._model_kwargs).data[0].embedding<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        return (<br>            (await self._async_client.embed(input=[query], **self._model_kwargs))<br>            .data[0]<br>            .embedding<br>        )<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._client.embed(input=[text], **self._model_kwargs).data[0].embedding<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously get text embedding.\"\"\"<br>        return (<br>            (await self._async_client.embed(input=[text], **self._model_kwargs))<br>            .data[0]<br>            .embedding<br>        )<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        embedding_response = self._client.embed(input=texts, **self._model_kwargs).data<br>        return [embed.embedding for embed in embedding_response]<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Asynchronously get text embeddings.\"\"\"<br>        embedding_response = await self._async_client.embed(<br>            input=texts, **self._model_kwargs<br>        )<br>        return [embed.embedding for embed in embedding_response.data]<br>```` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Azure inference - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/azure_inference/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/#llama_index.core.embeddings.BaseEmbedding)\n\n# Index\n\n## BaseEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/\\#llama_index.core.embeddings.BaseEmbedding \"Permanent link\")\n\nBases: `TransformComponent`, `DispatcherSpanMixin`\n\nBase class for embeddings.\n\nSource code in `llama-index-core/llama_index/core/base/embeddings/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br>``` | ```<br>class BaseEmbedding(TransformComponent, DispatcherSpanMixin):<br>    \"\"\"Base class for embeddings.\"\"\"<br>    model_config = ConfigDict(<br>        protected_namespaces=(\"pydantic_model_\",), arbitrary_types_allowed=True<br>    )<br>    model_name: str = Field(<br>        default=\"unknown\", description=\"The name of the embedding model.\"<br>    )<br>    embed_batch_size: int = Field(<br>        default=DEFAULT_EMBED_BATCH_SIZE,<br>        description=\"The batch size for embedding calls.\",<br>        gt=0,<br>        le=2048,<br>    )<br>    callback_manager: CallbackManager = Field(<br>        default_factory=lambda: CallbackManager([]), exclude=True<br>    )<br>    num_workers: Optional[int] = Field(<br>        default=None,<br>        description=\"The number of workers to use for async embedding calls.\",<br>    )<br>    @field_validator(\"callback_manager\")<br>    @classmethod<br>    def check_callback_manager(cls, v: CallbackManager) -> CallbackManager:<br>        if v is None:<br>            return CallbackManager([])<br>        return v<br>    @abstractmethod<br>    def _get_query_embedding(self, query: str) -> Embedding:<br>        \"\"\"<br>        Embed the input query synchronously.<br>        Subclasses should implement this method. Reference get_query_embedding's<br>        docstring for more information.<br>        \"\"\"<br>    @abstractmethod<br>    async def _aget_query_embedding(self, query: str) -> Embedding:<br>        \"\"\"<br>        Embed the input query asynchronously.<br>        Subclasses should implement this method. Reference get_query_embedding's<br>        docstring for more information.<br>        \"\"\"<br>    @dispatcher.span<br>    def get_query_embedding(self, query: str) -> Embedding:<br>        \"\"\"<br>        Embed the input query.<br>        When embedding a query, depending on the model, a special instruction<br>        can be prepended to the raw query string. For example, \"Represent the<br>        question for retrieving supporting documents: \". If you're curious,<br>        other examples of predefined instructions can be found in<br>        embeddings/huggingface_utils.py.<br>        \"\"\"<br>        model_dict = self.to_dict()<br>        model_dict.pop(\"api_key\", None)<br>        dispatcher.event(<br>            EmbeddingStartEvent(<br>                model_dict=model_dict,<br>            )<br>        )<br>        with self.callback_manager.event(<br>            CBEventType.EMBEDDING, payload={EventPayload.SERIALIZED: self.to_dict()}<br>        ) as event:<br>            query_embedding = self._get_query_embedding(query)<br>            event.on_end(<br>                payload={<br>                    EventPayload.CHUNKS: [query],<br>                    EventPayload.EMBEDDINGS: [query_embedding],<br>                },<br>            )<br>        dispatcher.event(<br>            EmbeddingEndEvent(<br>                chunks=[query],<br>                embeddings=[query_embedding],<br>            )<br>        )<br>        return query_embedding<br>    @dispatcher.span<br>    async def aget_query_embedding(self, query: str) -> Embedding:<br>        \"\"\"Get query embedding.\"\"\"<br>        model_dict = self.to_dict()<br>        model_dict.pop(\"api_key\", None)<br>        dispatcher.event(<br>            EmbeddingStartEvent(<br>                model_dict=model_dict,<br>            )<br>        )<br>        with self.callback_manager.event(<br>            CBEventType.EMBEDDING, payload={EventPayload.SERIALIZED: self.to_dict()}<br>        ) as event:<br>            query_embedding = await self._aget_query_embedding(query)<br>            event.on_end(<br>                payload={<br>                    EventPayload.CHUNKS: [query],<br>                    EventPayload.EMBEDDINGS: [query_embedding],<br>                },<br>            )<br>        dispatcher.event(<br>            EmbeddingEndEvent(<br>                chunks=[query],<br>                embeddings=[query_embedding],<br>            )<br>        )<br>        return query_embedding<br>    def get_agg_embedding_from_queries(<br>        self,<br>        queries: List[str],<br>        agg_fn: Optional[Callable[..., Embedding]] = None,<br>    ) -> Embedding:<br>        \"\"\"Get aggregated embedding from multiple queries.\"\"\"<br>        query_embeddings = [self.get_query_embedding(query) for query in queries]<br>        agg_fn = agg_fn or mean_agg<br>        return agg_fn(query_embeddings)<br>    async def aget_agg_embedding_from_queries(<br>        self,<br>        queries: List[str],<br>        agg_fn: Optional[Callable[..., Embedding]] = None,<br>    ) -> Embedding:<br>        \"\"\"Async get aggregated embedding from multiple queries.\"\"\"<br>        query_embeddings = [await self.aget_query_embedding(query) for query in queries]<br>        agg_fn = agg_fn or mean_agg<br>        return agg_fn(query_embeddings)<br>    @abstractmethod<br>    def _get_text_embedding(self, text: str) -> Embedding:<br>        \"\"\"<br>        Embed the input text synchronously.<br>        Subclasses should implement this method. Reference get_text_embedding's<br>        docstring for more information.<br>        \"\"\"<br>    async def _aget_text_embedding(self, text: str) -> Embedding:<br>        \"\"\"<br>        Embed the input text asynchronously.<br>        Subclasses can implement this method if there is a true async<br>        implementation. Reference get_text_embedding's docstring for more<br>        information.<br>        \"\"\"<br>        # Default implementation just falls back on _get_text_embedding<br>        return self._get_text_embedding(text)<br>    def _get_text_embeddings(self, texts: List[str]) -> List[Embedding]:<br>        \"\"\"<br>        Embed the input sequence of text synchronously.<br>        Subclasses can implement this method if batch queries are supported.<br>        \"\"\"<br>        # Default implementation just loops over _get_text_embedding<br>        return [self._get_text_embedding(text) for text in texts]<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[Embedding]:<br>        \"\"\"<br>        Embed the input sequence of text asynchronously.<br>        Subclasses can implement this method if batch queries are supported.<br>        \"\"\"<br>        return await asyncio.gather(<br>            *[self._aget_text_embedding(text) for text in texts]<br>        )<br>    @dispatcher.span<br>    def get_text_embedding(self, text: str) -> Embedding:<br>        \"\"\"<br>        Embed the input text.<br>        When embedding text, depending on the model, a special instruction<br>        can be prepended to the raw text string. For example, \"Represent the<br>        document for retrieval: \". If you're curious, other examples of<br>        predefined instructions can be found in embeddings/huggingface_utils.py.<br>        \"\"\"<br>        model_dict = self.to_dict()<br>        model_dict.pop(\"api_key\", None)<br>        dispatcher.event(<br>            EmbeddingStartEvent(<br>                model_dict=model_dict,<br>            )<br>        )<br>        with self.callback_manager.event(<br>            CBEventType.EMBEDDING, payload={EventPayload.SERIALIZED: self.to_dict()}<br>        ) as event:<br>            text_embedding = self._get_text_embedding(text)<br>            event.on_end(<br>                payload={<br>                    EventPayload.CHUNKS: [text],<br>                    EventPayload.EMBEDDINGS: [text_embedding],<br>                }<br>            )<br>        dispatcher.event(<br>            EmbeddingEndEvent(<br>                chunks=[text],<br>                embeddings=[text_embedding],<br>            )<br>        )<br>        return text_embedding<br>    @dispatcher.span<br>    async def aget_text_embedding(self, text: str) -> Embedding:<br>        \"\"\"Async get text embedding.\"\"\"<br>        model_dict = self.to_dict()<br>        model_dict.pop(\"api_key\", None)<br>        dispatcher.event(<br>            EmbeddingStartEvent(<br>                model_dict=model_dict,<br>            )<br>        )<br>        with self.callback_manager.event(<br>            CBEventType.EMBEDDING, payload={EventPayload.SERIALIZED: self.to_dict()}<br>        ) as event:<br>            text_embedding = await self._aget_text_embedding(text)<br>            event.on_end(<br>                payload={<br>                    EventPayload.CHUNKS: [text],<br>                    EventPayload.EMBEDDINGS: [text_embedding],<br>                }<br>            )<br>        dispatcher.event(<br>            EmbeddingEndEvent(<br>                chunks=[text],<br>                embeddings=[text_embedding],<br>            )<br>        )<br>        return text_embedding<br>    @dispatcher.span<br>    def get_text_embedding_batch(<br>        self,<br>        texts: List[str],<br>        show_progress: bool = False,<br>        **kwargs: Any,<br>    ) -> List[Embedding]:<br>        \"\"\"Get a list of text embeddings, with batching.\"\"\"<br>        cur_batch: List[str] = []<br>        result_embeddings: List[Embedding] = []<br>        queue_with_progress = enumerate(<br>            get_tqdm_iterable(texts, show_progress, \"Generating embeddings\")<br>        )<br>        model_dict = self.to_dict()<br>        model_dict.pop(\"api_key\", None)<br>        for idx, text in queue_with_progress:<br>            cur_batch.append(text)<br>            if idx == len(texts) - 1 or len(cur_batch) == self.embed_batch_size:<br>                # flush<br>                dispatcher.event(<br>                    EmbeddingStartEvent(<br>                        model_dict=model_dict,<br>                    )<br>                )<br>                with self.callback_manager.event(<br>                    CBEventType.EMBEDDING,<br>                    payload={EventPayload.SERIALIZED: self.to_dict()},<br>                ) as event:<br>                    embeddings = self._get_text_embeddings(cur_batch)<br>                    result_embeddings.extend(embeddings)<br>                    event.on_end(<br>                        payload={<br>                            EventPayload.CHUNKS: cur_batch,<br>                            EventPayload.EMBEDDINGS: embeddings,<br>                        },<br>                    )<br>                dispatcher.event(<br>                    EmbeddingEndEvent(<br>                        chunks=cur_batch,<br>                        embeddings=embeddings,<br>                    )<br>                )<br>                cur_batch = []<br>        return result_embeddings<br>    @dispatcher.span<br>    async def aget_text_embedding_batch(<br>        self, texts: List[str], show_progress: bool = False<br>    ) -> List[Embedding]:<br>        \"\"\"Asynchronously get a list of text embeddings, with batching.\"\"\"<br>        num_workers = self.num_workers<br>        model_dict = self.to_dict()<br>        model_dict.pop(\"api_key\", None)<br>        cur_batch: List[str] = []<br>        callback_payloads: List[Tuple[str, List[str]]] = []<br>        result_embeddings: List[Embedding] = []<br>        embeddings_coroutines: List[Coroutine] = []<br>        for idx, text in enumerate(texts):<br>            cur_batch.append(text)<br>            if idx == len(texts) - 1 or len(cur_batch) == self.embed_batch_size:<br>                # flush<br>                dispatcher.event(<br>                    EmbeddingStartEvent(<br>                        model_dict=model_dict,<br>                    )<br>                )<br>                event_id = self.callback_manager.on_event_start(<br>                    CBEventType.EMBEDDING,<br>                    payload={EventPayload.SERIALIZED: self.to_dict()},<br>                )<br>                callback_payloads.append((event_id, cur_batch))<br>                embeddings_coroutines.append(self._aget_text_embeddings(cur_batch))<br>                cur_batch = []<br>        # flatten the results of asyncio.gather, which is a list of embeddings lists<br>        nested_embeddings = []<br>        if num_workers and num_workers > 1:<br>            nested_embeddings = await run_jobs(<br>                embeddings_coroutines,<br>                show_progress=show_progress,<br>                workers=self.num_workers,<br>                desc=\"Generating embeddings\",<br>            )<br>        else:<br>            if show_progress:<br>                try:<br>                    from tqdm.asyncio import tqdm_asyncio<br>                    nested_embeddings = await tqdm_asyncio.gather(<br>                        *embeddings_coroutines,<br>                        total=len(embeddings_coroutines),<br>                        desc=\"Generating embeddings\",<br>                    )<br>                except ImportError:<br>                    nested_embeddings = await asyncio.gather(*embeddings_coroutines)<br>            else:<br>                nested_embeddings = await asyncio.gather(*embeddings_coroutines)<br>        result_embeddings = [<br>            embedding for embeddings in nested_embeddings for embedding in embeddings<br>        ]<br>        for (event_id, text_batch), embeddings in zip(<br>            callback_payloads, nested_embeddings<br>        ):<br>            dispatcher.event(<br>                EmbeddingEndEvent(<br>                    chunks=text_batch,<br>                    embeddings=embeddings,<br>                )<br>            )<br>            self.callback_manager.on_event_end(<br>                CBEventType.EMBEDDING,<br>                payload={<br>                    EventPayload.CHUNKS: text_batch,<br>                    EventPayload.EMBEDDINGS: embeddings,<br>                },<br>                event_id=event_id,<br>            )<br>        return result_embeddings<br>    def similarity(<br>        self,<br>        embedding1: Embedding,<br>        embedding2: Embedding,<br>        mode: SimilarityMode = SimilarityMode.DEFAULT,<br>    ) -> float:<br>        \"\"\"Get embedding similarity.\"\"\"<br>        return similarity(embedding1=embedding1, embedding2=embedding2, mode=mode)<br>    def __call__(self, nodes: Sequence[BaseNode], **kwargs: Any) -> Sequence[BaseNode]:<br>        embeddings = self.get_text_embedding_batch(<br>            [node.get_content(metadata_mode=MetadataMode.EMBED) for node in nodes],<br>            **kwargs,<br>        )<br>        for node, embedding in zip(nodes, embeddings):<br>            node.embedding = embedding<br>        return nodes<br>    async def acall(<br>        self, nodes: Sequence[BaseNode], **kwargs: Any<br>    ) -> Sequence[BaseNode]:<br>        embeddings = await self.aget_text_embedding_batch(<br>            [node.get_content(metadata_mode=MetadataMode.EMBED) for node in nodes],<br>            **kwargs,<br>        )<br>        for node, embedding in zip(nodes, embeddings):<br>            node.embedding = embedding<br>        return nodes<br>``` |\n\n### get\\_query\\_embedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/\\#llama_index.core.embeddings.BaseEmbedding.get_query_embedding \"Permanent link\")\n\n```\nget_query_embedding(query: str) -> Embedding\n\n```\n\nEmbed the input query.\n\nWhen embedding a query, depending on the model, a special instruction\ncan be prepended to the raw query string. For example, \"Represent the\nquestion for retrieving supporting documents: \". If you're curious,\nother examples of predefined instructions can be found in\nembeddings/huggingface\\_utils.py.\n\nSource code in `llama-index-core/llama_index/core/base/embeddings/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>``` | ```<br>@dispatcher.span<br>def get_query_embedding(self, query: str) -> Embedding:<br>    \"\"\"<br>    Embed the input query.<br>    When embedding a query, depending on the model, a special instruction<br>    can be prepended to the raw query string. For example, \"Represent the<br>    question for retrieving supporting documents: \". If you're curious,<br>    other examples of predefined instructions can be found in<br>    embeddings/huggingface_utils.py.<br>    \"\"\"<br>    model_dict = self.to_dict()<br>    model_dict.pop(\"api_key\", None)<br>    dispatcher.event(<br>        EmbeddingStartEvent(<br>            model_dict=model_dict,<br>        )<br>    )<br>    with self.callback_manager.event(<br>        CBEventType.EMBEDDING, payload={EventPayload.SERIALIZED: self.to_dict()}<br>    ) as event:<br>        query_embedding = self._get_query_embedding(query)<br>        event.on_end(<br>            payload={<br>                EventPayload.CHUNKS: [query],<br>                EventPayload.EMBEDDINGS: [query_embedding],<br>            },<br>        )<br>    dispatcher.event(<br>        EmbeddingEndEvent(<br>            chunks=[query],<br>            embeddings=[query_embedding],<br>        )<br>    )<br>    return query_embedding<br>``` |\n\n### aget\\_query\\_embedding`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/\\#llama_index.core.embeddings.BaseEmbedding.aget_query_embedding \"Permanent link\")\n\n```\naget_query_embedding(query: str) -> Embedding\n\n```\n\nGet query embedding.\n\nSource code in `llama-index-core/llama_index/core/base/embeddings/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>``` | ```<br>@dispatcher.span<br>async def aget_query_embedding(self, query: str) -> Embedding:<br>    \"\"\"Get query embedding.\"\"\"<br>    model_dict = self.to_dict()<br>    model_dict.pop(\"api_key\", None)<br>    dispatcher.event(<br>        EmbeddingStartEvent(<br>            model_dict=model_dict,<br>        )<br>    )<br>    with self.callback_manager.event(<br>        CBEventType.EMBEDDING, payload={EventPayload.SERIALIZED: self.to_dict()}<br>    ) as event:<br>        query_embedding = await self._aget_query_embedding(query)<br>        event.on_end(<br>            payload={<br>                EventPayload.CHUNKS: [query],<br>                EventPayload.EMBEDDINGS: [query_embedding],<br>            },<br>        )<br>    dispatcher.event(<br>        EmbeddingEndEvent(<br>            chunks=[query],<br>            embeddings=[query_embedding],<br>        )<br>    )<br>    return query_embedding<br>``` |\n\n### get\\_agg\\_embedding\\_from\\_queries [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/\\#llama_index.core.embeddings.BaseEmbedding.get_agg_embedding_from_queries \"Permanent link\")\n\n```\nget_agg_embedding_from_queries(queries: List[str], agg_fn: Optional[Callable[..., Embedding]] = None) -> Embedding\n\n```\n\nGet aggregated embedding from multiple queries.\n\nSource code in `llama-index-core/llama_index/core/base/embeddings/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>``` | ```<br>def get_agg_embedding_from_queries(<br>    self,<br>    queries: List[str],<br>    agg_fn: Optional[Callable[..., Embedding]] = None,<br>) -> Embedding:<br>    \"\"\"Get aggregated embedding from multiple queries.\"\"\"<br>    query_embeddings = [self.get_query_embedding(query) for query in queries]<br>    agg_fn = agg_fn or mean_agg<br>    return agg_fn(query_embeddings)<br>``` |\n\n### aget\\_agg\\_embedding\\_from\\_queries`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/\\#llama_index.core.embeddings.BaseEmbedding.aget_agg_embedding_from_queries \"Permanent link\")\n\n```\naget_agg_embedding_from_queries(queries: List[str], agg_fn: Optional[Callable[..., Embedding]] = None) -> Embedding\n\n```\n\nAsync get aggregated embedding from multiple queries.\n\nSource code in `llama-index-core/llama_index/core/base/embeddings/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>``` | ```<br>async def aget_agg_embedding_from_queries(<br>    self,<br>    queries: List[str],<br>    agg_fn: Optional[Callable[..., Embedding]] = None,<br>) -> Embedding:<br>    \"\"\"Async get aggregated embedding from multiple queries.\"\"\"<br>    query_embeddings = [await self.aget_query_embedding(query) for query in queries]<br>    agg_fn = agg_fn or mean_agg<br>    return agg_fn(query_embeddings)<br>``` |\n\n### get\\_text\\_embedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/\\#llama_index.core.embeddings.BaseEmbedding.get_text_embedding \"Permanent link\")\n\n```\nget_text_embedding(text: str) -> Embedding\n\n```\n\nEmbed the input text.\n\nWhen embedding text, depending on the model, a special instruction\ncan be prepended to the raw text string. For example, \"Represent the\ndocument for retrieval: \". If you're curious, other examples of\npredefined instructions can be found in embeddings/huggingface\\_utils.py.\n\nSource code in `llama-index-core/llama_index/core/base/embeddings/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>``` | ```<br>@dispatcher.span<br>def get_text_embedding(self, text: str) -> Embedding:<br>    \"\"\"<br>    Embed the input text.<br>    When embedding text, depending on the model, a special instruction<br>    can be prepended to the raw text string. For example, \"Represent the<br>    document for retrieval: \". If you're curious, other examples of<br>    predefined instructions can be found in embeddings/huggingface_utils.py.<br>    \"\"\"<br>    model_dict = self.to_dict()<br>    model_dict.pop(\"api_key\", None)<br>    dispatcher.event(<br>        EmbeddingStartEvent(<br>            model_dict=model_dict,<br>        )<br>    )<br>    with self.callback_manager.event(<br>        CBEventType.EMBEDDING, payload={EventPayload.SERIALIZED: self.to_dict()}<br>    ) as event:<br>        text_embedding = self._get_text_embedding(text)<br>        event.on_end(<br>            payload={<br>                EventPayload.CHUNKS: [text],<br>                EventPayload.EMBEDDINGS: [text_embedding],<br>            }<br>        )<br>    dispatcher.event(<br>        EmbeddingEndEvent(<br>            chunks=[text],<br>            embeddings=[text_embedding],<br>        )<br>    )<br>    return text_embedding<br>``` |\n\n### aget\\_text\\_embedding`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/\\#llama_index.core.embeddings.BaseEmbedding.aget_text_embedding \"Permanent link\")\n\n```\naget_text_embedding(text: str) -> Embedding\n\n```\n\nAsync get text embedding.\n\nSource code in `llama-index-core/llama_index/core/base/embeddings/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>``` | ```<br>@dispatcher.span<br>async def aget_text_embedding(self, text: str) -> Embedding:<br>    \"\"\"Async get text embedding.\"\"\"<br>    model_dict = self.to_dict()<br>    model_dict.pop(\"api_key\", None)<br>    dispatcher.event(<br>        EmbeddingStartEvent(<br>            model_dict=model_dict,<br>        )<br>    )<br>    with self.callback_manager.event(<br>        CBEventType.EMBEDDING, payload={EventPayload.SERIALIZED: self.to_dict()}<br>    ) as event:<br>        text_embedding = await self._aget_text_embedding(text)<br>        event.on_end(<br>            payload={<br>                EventPayload.CHUNKS: [text],<br>                EventPayload.EMBEDDINGS: [text_embedding],<br>            }<br>        )<br>    dispatcher.event(<br>        EmbeddingEndEvent(<br>            chunks=[text],<br>            embeddings=[text_embedding],<br>        )<br>    )<br>    return text_embedding<br>``` |\n\n### get\\_text\\_embedding\\_batch [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/\\#llama_index.core.embeddings.BaseEmbedding.get_text_embedding_batch \"Permanent link\")\n\n```\nget_text_embedding_batch(texts: List[str], show_progress: bool = False, **kwargs: Any) -> List[Embedding]\n\n```\n\nGet a list of text embeddings, with batching.\n\nSource code in `llama-index-core/llama_index/core/base/embeddings/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>``` | ```<br>@dispatcher.span<br>def get_text_embedding_batch(<br>    self,<br>    texts: List[str],<br>    show_progress: bool = False,<br>    **kwargs: Any,<br>) -> List[Embedding]:<br>    \"\"\"Get a list of text embeddings, with batching.\"\"\"<br>    cur_batch: List[str] = []<br>    result_embeddings: List[Embedding] = []<br>    queue_with_progress = enumerate(<br>        get_tqdm_iterable(texts, show_progress, \"Generating embeddings\")<br>    )<br>    model_dict = self.to_dict()<br>    model_dict.pop(\"api_key\", None)<br>    for idx, text in queue_with_progress:<br>        cur_batch.append(text)<br>        if idx == len(texts) - 1 or len(cur_batch) == self.embed_batch_size:<br>            # flush<br>            dispatcher.event(<br>                EmbeddingStartEvent(<br>                    model_dict=model_dict,<br>                )<br>            )<br>            with self.callback_manager.event(<br>                CBEventType.EMBEDDING,<br>                payload={EventPayload.SERIALIZED: self.to_dict()},<br>            ) as event:<br>                embeddings = self._get_text_embeddings(cur_batch)<br>                result_embeddings.extend(embeddings)<br>                event.on_end(<br>                    payload={<br>                        EventPayload.CHUNKS: cur_batch,<br>                        EventPayload.EMBEDDINGS: embeddings,<br>                    },<br>                )<br>            dispatcher.event(<br>                EmbeddingEndEvent(<br>                    chunks=cur_batch,<br>                    embeddings=embeddings,<br>                )<br>            )<br>            cur_batch = []<br>    return result_embeddings<br>``` |\n\n### aget\\_text\\_embedding\\_batch`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/\\#llama_index.core.embeddings.BaseEmbedding.aget_text_embedding_batch \"Permanent link\")\n\n```\naget_text_embedding_batch(texts: List[str], show_progress: bool = False) -> List[Embedding]\n\n```\n\nAsynchronously get a list of text embeddings, with batching.\n\nSource code in `llama-index-core/llama_index/core/base/embeddings/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>``` | ```<br>@dispatcher.span<br>async def aget_text_embedding_batch(<br>    self, texts: List[str], show_progress: bool = False<br>) -> List[Embedding]:<br>    \"\"\"Asynchronously get a list of text embeddings, with batching.\"\"\"<br>    num_workers = self.num_workers<br>    model_dict = self.to_dict()<br>    model_dict.pop(\"api_key\", None)<br>    cur_batch: List[str] = []<br>    callback_payloads: List[Tuple[str, List[str]]] = []<br>    result_embeddings: List[Embedding] = []<br>    embeddings_coroutines: List[Coroutine] = []<br>    for idx, text in enumerate(texts):<br>        cur_batch.append(text)<br>        if idx == len(texts) - 1 or len(cur_batch) == self.embed_batch_size:<br>            # flush<br>            dispatcher.event(<br>                EmbeddingStartEvent(<br>                    model_dict=model_dict,<br>                )<br>            )<br>            event_id = self.callback_manager.on_event_start(<br>                CBEventType.EMBEDDING,<br>                payload={EventPayload.SERIALIZED: self.to_dict()},<br>            )<br>            callback_payloads.append((event_id, cur_batch))<br>            embeddings_coroutines.append(self._aget_text_embeddings(cur_batch))<br>            cur_batch = []<br>    # flatten the results of asyncio.gather, which is a list of embeddings lists<br>    nested_embeddings = []<br>    if num_workers and num_workers > 1:<br>        nested_embeddings = await run_jobs(<br>            embeddings_coroutines,<br>            show_progress=show_progress,<br>            workers=self.num_workers,<br>            desc=\"Generating embeddings\",<br>        )<br>    else:<br>        if show_progress:<br>            try:<br>                from tqdm.asyncio import tqdm_asyncio<br>                nested_embeddings = await tqdm_asyncio.gather(<br>                    *embeddings_coroutines,<br>                    total=len(embeddings_coroutines),<br>                    desc=\"Generating embeddings\",<br>                )<br>            except ImportError:<br>                nested_embeddings = await asyncio.gather(*embeddings_coroutines)<br>        else:<br>            nested_embeddings = await asyncio.gather(*embeddings_coroutines)<br>    result_embeddings = [<br>        embedding for embeddings in nested_embeddings for embedding in embeddings<br>    ]<br>    for (event_id, text_batch), embeddings in zip(<br>        callback_payloads, nested_embeddings<br>    ):<br>        dispatcher.event(<br>            EmbeddingEndEvent(<br>                chunks=text_batch,<br>                embeddings=embeddings,<br>            )<br>        )<br>        self.callback_manager.on_event_end(<br>            CBEventType.EMBEDDING,<br>            payload={<br>                EventPayload.CHUNKS: text_batch,<br>                EventPayload.EMBEDDINGS: embeddings,<br>            },<br>            event_id=event_id,<br>        )<br>    return result_embeddings<br>``` |\n\n### similarity [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/\\#llama_index.core.embeddings.BaseEmbedding.similarity \"Permanent link\")\n\n```\nsimilarity(embedding1: Embedding, embedding2: Embedding, mode: SimilarityMode = SimilarityMode.DEFAULT) -> float\n\n```\n\nGet embedding similarity.\n\nSource code in `llama-index-core/llama_index/core/base/embeddings/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>``` | ```<br>def similarity(<br>    self,<br>    embedding1: Embedding,<br>    embedding2: Embedding,<br>    mode: SimilarityMode = SimilarityMode.DEFAULT,<br>) -> float:<br>    \"\"\"Get embedding similarity.\"\"\"<br>    return similarity(embedding1=embedding1, embedding2=embedding2, mode=mode)<br>``` |\n\n## resolve\\_embed\\_model [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/\\#llama_index.core.embeddings.resolve_embed_model \"Permanent link\")\n\n```\nresolve_embed_model(embed_model: Optional[EmbedType] = None, callback_manager: Optional[CallbackManager] = None) -> BaseEmbedding\n\n```\n\nResolve embed model.\n\nSource code in `llama-index-core/llama_index/core/embeddings/utils.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>``` | ```<br>def resolve_embed_model(<br>    embed_model: Optional[EmbedType] = None,<br>    callback_manager: Optional[CallbackManager] = None,<br>) -> BaseEmbedding:<br>    \"\"\"Resolve embed model.\"\"\"<br>    from llama_index.core.settings import Settings<br>    try:<br>        from llama_index.core.bridge.langchain import Embeddings as LCEmbeddings<br>    except ImportError:<br>        LCEmbeddings = None  # type: ignore<br>    if embed_model == \"default\":<br>        if os.getenv(\"IS_TESTING\"):<br>            embed_model = MockEmbedding(embed_dim=8)<br>            embed_model.callback_manager = callback_manager or Settings.callback_manager<br>            return embed_model<br>        try:<br>            from llama_index.embeddings.openai import (<br>                OpenAIEmbedding,<br>            )  # pants: no-infer-dep<br>            from llama_index.embeddings.openai.utils import (<br>                validate_openai_api_key,<br>            )  # pants: no-infer-dep<br>            embed_model = OpenAIEmbedding()<br>            validate_openai_api_key(embed_model.api_key)  # type: ignore<br>        except ImportError:<br>            raise ImportError(<br>                \"`llama-index-embeddings-openai` package not found, \"<br>                \"please run `pip install llama-index-embeddings-openai`\"<br>            )<br>        except ValueError as e:<br>            raise ValueError(<br>                \"\\n******\\n\"<br>                \"Could not load OpenAI embedding model. \"<br>                \"If you intended to use OpenAI, please check your OPENAI_API_KEY.\\n\"<br>                \"Original error:\\n\"<br>                f\"{e!s}\"<br>                \"\\nConsider using embed_model='local'.\\n\"<br>                \"Visit our documentation for more embedding options: \"<br>                \"https://docs.llamaindex.ai/en/stable/module_guides/models/\"<br>                \"embeddings.html#modules\"<br>                \"\\n******\"<br>            )<br>    # for image multi-modal embeddings<br>    elif isinstance(embed_model, str) and embed_model.startswith(\"clip\"):<br>        try:<br>            from llama_index.embeddings.clip import ClipEmbedding  # pants: no-infer-dep<br>            clip_model_name = (<br>                embed_model.split(\":\")[1] if \":\" in embed_model else \"ViT-B/32\"<br>            )<br>            embed_model = ClipEmbedding(model_name=clip_model_name)<br>        except ImportError as e:<br>            raise ImportError(<br>                \"`llama-index-embeddings-clip` package not found, \"<br>                \"please run `pip install llama-index-embeddings-clip` and `pip install git+https://github.com/openai/CLIP.git`\"<br>            )<br>    if isinstance(embed_model, str):<br>        try:<br>            from llama_index.embeddings.huggingface import (<br>                HuggingFaceEmbedding,<br>            )  # pants: no-infer-dep<br>            splits = embed_model.split(\":\", 1)<br>            is_local = splits[0]<br>            model_name = splits[1] if len(splits) > 1 else None<br>            if is_local != \"local\":<br>                raise ValueError(<br>                    \"embed_model must start with str 'local' or of type BaseEmbedding\"<br>                )<br>            cache_folder = os.path.join(get_cache_dir(), \"models\")<br>            os.makedirs(cache_folder, exist_ok=True)<br>            embed_model = HuggingFaceEmbedding(<br>                model_name=model_name, cache_folder=cache_folder<br>            )<br>        except ImportError:<br>            raise ImportError(<br>                \"`llama-index-embeddings-huggingface` package not found, \"<br>                \"please run `pip install llama-index-embeddings-huggingface`\"<br>            )<br>    if LCEmbeddings is not None and isinstance(embed_model, LCEmbeddings):<br>        try:<br>            from llama_index.embeddings.langchain import (<br>                LangchainEmbedding,<br>            )  # pants: no-infer-dep<br>            embed_model = LangchainEmbedding(embed_model)<br>        except ImportError as e:<br>            raise ImportError(<br>                \"`llama-index-embeddings-langchain` package not found, \"<br>                \"please run `pip install llama-index-embeddings-langchain`\"<br>            )<br>    if embed_model is None:<br>        print(\"Embeddings have been explicitly disabled. Using MockEmbedding.\")<br>        embed_model = MockEmbedding(embed_dim=1)<br>    assert isinstance(embed_model, BaseEmbedding)<br>    embed_model.callback_manager = callback_manager or Settings.callback_manager<br>    return embed_model<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Index - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/uptrain/#llama_index.callbacks.uptrain.UpTrainCallbackHandler)\n\n# Uptrain\n\n## UpTrainCallbackHandler [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/uptrain/\\#llama_index.callbacks.uptrain.UpTrainCallbackHandler \"Permanent link\")\n\nBases: `BaseCallbackHandler`\n\nUpTrain callback handler.\n\nThis class is responsible for handling the UpTrain API and logging events to UpTrain.\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-uptrain/llama_index/callbacks/uptrain/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>``` | ```<br>class UpTrainCallbackHandler(BaseCallbackHandler):<br>    \"\"\"<br>    UpTrain callback handler.<br>    This class is responsible for handling the UpTrain API and logging events to UpTrain.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        api_key: str,<br>        key_type: Literal[\"uptrain\", \"openai\"],<br>        project_name: str = \"uptrain_llamaindex\",<br>    ) -> None:<br>        \"\"\"Initialize the UpTrain callback handler.\"\"\"<br>        try:<br>            from uptrain import APIClient, EvalLLM, Settings<br>        except ImportError:<br>            raise ImportError(<br>                \"UpTrainCallbackHandler requires the 'uptrain' package. \"<br>                \"Please install it using 'pip install uptrain'.\"<br>            )<br>        nest_asyncio.apply()<br>        super().__init__(<br>            event_starts_to_ignore=[],<br>            event_ends_to_ignore=[],<br>        )<br>        self.schema = UpTrainDataSchema(project_name=project_name)<br>        self._event_pairs_by_id: Dict[str, List[CBEvent]] = defaultdict(list)<br>        self._trace_map: Dict[str, List[str]] = defaultdict(list)<br>        # Based on whether the user enters an UpTrain API key or an OpenAI API key, the client is initialized<br>        # If both are entered, the UpTrain API key is used<br>        if key_type == \"uptrain\":<br>            settings = Settings(uptrain_access_token=api_key)<br>            self.uptrain_client = APIClient(settings=settings)<br>        elif key_type == \"openai\":<br>            settings = Settings(openai_api_key=api_key)<br>            self.uptrain_client = EvalLLM(settings=settings)<br>        else:<br>            raise ValueError(\"Invalid key type: Must be 'uptrain' or 'openai'\")<br>    def uptrain_evaluate(<br>        self,<br>        evaluation_name: str,<br>        data: List[Dict[str, str]],<br>        checks: List[str],<br>    ) -> None:<br>        \"\"\"Run an evaluation on the UpTrain server using UpTrain client.\"\"\"<br>        if self.uptrain_client.__class__.__name__ == \"APIClient\":<br>            uptrain_result = self.uptrain_client.log_and_evaluate(<br>                project_name=self.schema.project_name,<br>                evaluation_name=evaluation_name,<br>                data=data,<br>                checks=checks,<br>            )<br>        else:<br>            uptrain_result = self.uptrain_client.evaluate(<br>                project_name=self.schema.project_name,<br>                evaluation_name=evaluation_name,<br>                data=data,<br>                checks=checks,<br>            )<br>        self.schema.uptrain_results[self.schema.project_name].append(uptrain_result)<br>        score_name_map = {<br>            \"score_context_relevance\": \"Context Relevance Score\",<br>            \"score_factual_accuracy\": \"Factual Accuracy Score\",<br>            \"score_response_completeness\": \"Response Completeness Score\",<br>            \"score_sub_query_completeness\": \"Sub Query Completeness Score\",<br>            \"score_context_reranking\": \"Context Reranking Score\",<br>            \"score_context_conciseness\": \"Context Conciseness Score\",<br>        }<br>        # Print the results<br>        for row in uptrain_result:<br>            columns = list(row.keys())<br>            for column in columns:<br>                if column == \"question\":<br>                    print(f\"\\nQuestion: {row[column]}\")<br>                elif column == \"response\":<br>                    print(f\"Response: {row[column]}\\n\")<br>                elif column.startswith(\"score\"):<br>                    if column in score_name_map:<br>                        print(f\"{score_name_map[column]}: {row[column]}\")<br>                    else:<br>                        print(f\"{column}: {row[column]}\")<br>            print()<br>    def on_event_start(<br>        self,<br>        event_type: CBEventType,<br>        payload: Any = None,<br>        event_id: str = \"\",<br>        parent_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> str:<br>        \"\"\"Run when an event starts and return id of event.\"\"\"<br>        event = CBEvent(event_type, payload=payload, id_=event_id)<br>        self._event_pairs_by_id[event.id_].append(event)<br>        if event_type is CBEventType.QUERY:<br>            self.schema.question = payload[\"query_str\"]<br>        if event_type is CBEventType.TEMPLATING and \"template_vars\" in payload:<br>            template_vars = payload[\"template_vars\"]<br>            self.schema.context = template_vars.get(\"context_str\", \"\")<br>        elif event_type is CBEventType.RERANKING and \"nodes\" in payload:<br>            self.schema.eval_types.add(\"reranking\")<br>            # Store old context data<br>            self.schema.old_context = [node.text for node in payload[\"nodes\"]]<br>        elif event_type is CBEventType.SUB_QUESTION:<br>            # For the first sub question, store parent question and parent id<br>            if \"sub_question\" not in self.schema.eval_types:<br>                self.schema.parent_question = self.schema.question<br>                self.schema.eval_types.add(\"sub_question\")<br>            # Store sub question data - question and parent id<br>            self.schema.sub_question_parent_id = parent_id<br>        return event_id<br>    def on_event_end(<br>        self,<br>        event_type: CBEventType,<br>        payload: Any = None,<br>        event_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> None:<br>        \"\"\"Run when an event ends.\"\"\"<br>        try:<br>            from uptrain import Evals<br>        except ImportError:<br>            raise ImportError(<br>                \"UpTrainCallbackHandler requires the 'uptrain' package. \"<br>                \"Please install it using 'pip install uptrain'.\"<br>            )<br>        event = CBEvent(event_type, payload=payload, id_=event_id)<br>        self._event_pairs_by_id[event.id_].append(event)<br>        self._trace_map = defaultdict(list)<br>        if event_id == self.schema.sub_question_parent_id:<br>            # Perform individual evaluations for sub questions (but send all sub questions at once)<br>            self.uptrain_evaluate(<br>                evaluation_name=\"sub_question_answering\",<br>                data=list(self.schema.sub_question_map.values()),<br>                checks=[<br>                    Evals.CONTEXT_RELEVANCE,<br>                    Evals.FACTUAL_ACCURACY,<br>                    Evals.RESPONSE_COMPLETENESS,<br>                ],<br>            )<br>            # Perform evaluation for question and all sub questions (as a whole)<br>            sub_questions = [<br>                sub_question[\"question\"]<br>                for sub_question in self.schema.sub_question_map.values()<br>            ]<br>            sub_questions_formatted = \"\\n\".join(<br>                [<br>                    f\"{index}. {string}\"<br>                    for index, string in enumerate(sub_questions, start=1)<br>                ]<br>            )<br>            self.uptrain_evaluate(<br>                evaluation_name=\"sub_query_completeness\",<br>                data=[<br>                    {<br>                        \"question\": self.schema.parent_question,<br>                        \"sub_questions\": sub_questions_formatted,<br>                    }<br>                ],<br>                checks=[Evals.SUB_QUERY_COMPLETENESS],<br>            )<br>            self.schema.eval_types.remove(\"sub_question\")<br>        # Should not be called for sub questions<br>        if (<br>            event_type is CBEventType.SYNTHESIZE<br>            and \"sub_question\" not in self.schema.eval_types<br>        ):<br>            self.schema.response = payload[\"response\"].response<br>            # Perform evaluation for synthesization<br>            if \"reranking\" in self.schema.eval_types:<br>                if self.schema.reranking_type == \"rerank\":<br>                    evaluation_name = \"question_answering_rerank\"<br>                else:<br>                    evaluation_name = \"question_answering_resize\"<br>                self.schema.eval_types.remove(\"reranking\")<br>            else:<br>                evaluation_name = \"question_answering\"<br>            self.uptrain_evaluate(<br>                evaluation_name=evaluation_name,<br>                data=[<br>                    {<br>                        \"question\": self.schema.question,<br>                        \"context\": self.schema.context,<br>                        \"response\": self.schema.response,<br>                    }<br>                ],<br>                checks=[<br>                    Evals.CONTEXT_RELEVANCE,<br>                    Evals.FACTUAL_ACCURACY,<br>                    Evals.RESPONSE_COMPLETENESS,<br>                ],<br>            )<br>        elif event_type is CBEventType.RERANKING:<br>            # Store new context data<br>            self.schema.new_context = [node.text for node in payload[\"nodes\"]]<br>            if len(self.schema.old_context) == len(self.schema.new_context):<br>                self.schema.reranking_type = \"rerank\"<br>                context = \"\\n\".join(<br>                    [<br>                        f\"{index}. {string}\"<br>                        for index, string in enumerate(self.schema.old_context, start=1)<br>                    ]<br>                )<br>                reranked_context = \"\\n\".join(<br>                    [<br>                        f\"{index}. {string}\"<br>                        for index, string in enumerate(self.schema.new_context, start=1)<br>                    ]<br>                )<br>                # Perform evaluation for reranking<br>                self.uptrain_evaluate(<br>                    evaluation_name=\"context_reranking\",<br>                    data=[<br>                        {<br>                            \"question\": self.schema.question,<br>                            \"context\": context,<br>                            \"reranked_context\": reranked_context,<br>                        }<br>                    ],<br>                    checks=[<br>                        Evals.CONTEXT_RERANKING,<br>                    ],<br>                )<br>            else:<br>                self.schema.reranking_type = \"resize\"<br>                context = \"\\n\".join(self.schema.old_context)<br>                concise_context = \"\\n\".join(self.schema.new_context)<br>                # Perform evaluation for resizing<br>                self.uptrain_evaluate(<br>                    evaluation_name=\"context_conciseness\",<br>                    data=[<br>                        {<br>                            \"question\": self.schema.question,<br>                            \"context\": context,<br>                            \"concise_context\": concise_context,<br>                        }<br>                    ],<br>                    checks=[<br>                        Evals.CONTEXT_CONCISENESS,<br>                    ],<br>                )<br>        elif event_type is CBEventType.SUB_QUESTION:<br>            # Store sub question data<br>            self.schema.sub_question_map[event_id][\"question\"] = payload[<br>                \"sub_question\"<br>            ].sub_q.sub_question<br>            self.schema.sub_question_map[event_id][\"context\"] = (<br>                payload[\"sub_question\"].sources[0].node.text<br>            )<br>            self.schema.sub_question_map[event_id][\"response\"] = payload[<br>                \"sub_question\"<br>            ].answer<br>    def start_trace(self, trace_id: Optional[str] = None) -> None:<br>        self._trace_map = defaultdict(list)<br>        return super().start_trace(trace_id)<br>    def end_trace(<br>        self,<br>        trace_id: Optional[str] = None,<br>        trace_map: Optional[Dict[str, List[str]]] = None,<br>    ) -> None:<br>        self._trace_map = trace_map or defaultdict(list)<br>        return super().end_trace(trace_id, trace_map)<br>    def build_trace_map(<br>        self,<br>        cur_event_id: str,<br>        trace_map: Any,<br>    ) -> Dict[str, Any]:<br>        event_pair = self._event_pairs_by_id[cur_event_id]<br>        if event_pair:<br>            event_data = {<br>                \"event_type\": event_pair[0].event_type,<br>                \"event_id\": event_pair[0].id_,<br>                \"children\": {},<br>            }<br>            trace_map[cur_event_id] = event_data<br>        child_event_ids = self._trace_map[cur_event_id]<br>        for child_event_id in child_event_ids:<br>            self.build_trace_map(child_event_id, event_data[\"children\"])<br>        return trace_map<br>``` |\n\n### uptrain\\_evaluate [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/uptrain/\\#llama_index.callbacks.uptrain.UpTrainCallbackHandler.uptrain_evaluate \"Permanent link\")\n\n```\nuptrain_evaluate(evaluation_name: str, data: List[Dict[str, str]], checks: List[str]) -> None\n\n```\n\nRun an evaluation on the UpTrain server using UpTrain client.\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-uptrain/llama_index/callbacks/uptrain/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>``` | ```<br>def uptrain_evaluate(<br>    self,<br>    evaluation_name: str,<br>    data: List[Dict[str, str]],<br>    checks: List[str],<br>) -> None:<br>    \"\"\"Run an evaluation on the UpTrain server using UpTrain client.\"\"\"<br>    if self.uptrain_client.__class__.__name__ == \"APIClient\":<br>        uptrain_result = self.uptrain_client.log_and_evaluate(<br>            project_name=self.schema.project_name,<br>            evaluation_name=evaluation_name,<br>            data=data,<br>            checks=checks,<br>        )<br>    else:<br>        uptrain_result = self.uptrain_client.evaluate(<br>            project_name=self.schema.project_name,<br>            evaluation_name=evaluation_name,<br>            data=data,<br>            checks=checks,<br>        )<br>    self.schema.uptrain_results[self.schema.project_name].append(uptrain_result)<br>    score_name_map = {<br>        \"score_context_relevance\": \"Context Relevance Score\",<br>        \"score_factual_accuracy\": \"Factual Accuracy Score\",<br>        \"score_response_completeness\": \"Response Completeness Score\",<br>        \"score_sub_query_completeness\": \"Sub Query Completeness Score\",<br>        \"score_context_reranking\": \"Context Reranking Score\",<br>        \"score_context_conciseness\": \"Context Conciseness Score\",<br>    }<br>    # Print the results<br>    for row in uptrain_result:<br>        columns = list(row.keys())<br>        for column in columns:<br>            if column == \"question\":<br>                print(f\"\\nQuestion: {row[column]}\")<br>            elif column == \"response\":<br>                print(f\"Response: {row[column]}\\n\")<br>            elif column.startswith(\"score\"):<br>                if column in score_name_map:<br>                    print(f\"{score_name_map[column]}: {row[column]}\")<br>                else:<br>                    print(f\"{column}: {row[column]}\")<br>        print()<br>``` |\n\n### on\\_event\\_start [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/uptrain/\\#llama_index.callbacks.uptrain.UpTrainCallbackHandler.on_event_start \"Permanent link\")\n\n```\non_event_start(event_type: CBEventType, payload: Any = None, event_id: str = '', parent_id: str = '', **kwargs: Any) -> str\n\n```\n\nRun when an event starts and return id of event.\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-uptrain/llama_index/callbacks/uptrain/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>``` | ```<br>def on_event_start(<br>    self,<br>    event_type: CBEventType,<br>    payload: Any = None,<br>    event_id: str = \"\",<br>    parent_id: str = \"\",<br>    **kwargs: Any,<br>) -> str:<br>    \"\"\"Run when an event starts and return id of event.\"\"\"<br>    event = CBEvent(event_type, payload=payload, id_=event_id)<br>    self._event_pairs_by_id[event.id_].append(event)<br>    if event_type is CBEventType.QUERY:<br>        self.schema.question = payload[\"query_str\"]<br>    if event_type is CBEventType.TEMPLATING and \"template_vars\" in payload:<br>        template_vars = payload[\"template_vars\"]<br>        self.schema.context = template_vars.get(\"context_str\", \"\")<br>    elif event_type is CBEventType.RERANKING and \"nodes\" in payload:<br>        self.schema.eval_types.add(\"reranking\")<br>        # Store old context data<br>        self.schema.old_context = [node.text for node in payload[\"nodes\"]]<br>    elif event_type is CBEventType.SUB_QUESTION:<br>        # For the first sub question, store parent question and parent id<br>        if \"sub_question\" not in self.schema.eval_types:<br>            self.schema.parent_question = self.schema.question<br>            self.schema.eval_types.add(\"sub_question\")<br>        # Store sub question data - question and parent id<br>        self.schema.sub_question_parent_id = parent_id<br>    return event_id<br>``` |\n\n### on\\_event\\_end [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/uptrain/\\#llama_index.callbacks.uptrain.UpTrainCallbackHandler.on_event_end \"Permanent link\")\n\n```\non_event_end(event_type: CBEventType, payload: Any = None, event_id: str = '', **kwargs: Any) -> None\n\n```\n\nRun when an event ends.\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-uptrain/llama_index/callbacks/uptrain/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>``` | ```<br>def on_event_end(<br>    self,<br>    event_type: CBEventType,<br>    payload: Any = None,<br>    event_id: str = \"\",<br>    **kwargs: Any,<br>) -> None:<br>    \"\"\"Run when an event ends.\"\"\"<br>    try:<br>        from uptrain import Evals<br>    except ImportError:<br>        raise ImportError(<br>            \"UpTrainCallbackHandler requires the 'uptrain' package. \"<br>            \"Please install it using 'pip install uptrain'.\"<br>        )<br>    event = CBEvent(event_type, payload=payload, id_=event_id)<br>    self._event_pairs_by_id[event.id_].append(event)<br>    self._trace_map = defaultdict(list)<br>    if event_id == self.schema.sub_question_parent_id:<br>        # Perform individual evaluations for sub questions (but send all sub questions at once)<br>        self.uptrain_evaluate(<br>            evaluation_name=\"sub_question_answering\",<br>            data=list(self.schema.sub_question_map.values()),<br>            checks=[<br>                Evals.CONTEXT_RELEVANCE,<br>                Evals.FACTUAL_ACCURACY,<br>                Evals.RESPONSE_COMPLETENESS,<br>            ],<br>        )<br>        # Perform evaluation for question and all sub questions (as a whole)<br>        sub_questions = [<br>            sub_question[\"question\"]<br>            for sub_question in self.schema.sub_question_map.values()<br>        ]<br>        sub_questions_formatted = \"\\n\".join(<br>            [<br>                f\"{index}. {string}\"<br>                for index, string in enumerate(sub_questions, start=1)<br>            ]<br>        )<br>        self.uptrain_evaluate(<br>            evaluation_name=\"sub_query_completeness\",<br>            data=[<br>                {<br>                    \"question\": self.schema.parent_question,<br>                    \"sub_questions\": sub_questions_formatted,<br>                }<br>            ],<br>            checks=[Evals.SUB_QUERY_COMPLETENESS],<br>        )<br>        self.schema.eval_types.remove(\"sub_question\")<br>    # Should not be called for sub questions<br>    if (<br>        event_type is CBEventType.SYNTHESIZE<br>        and \"sub_question\" not in self.schema.eval_types<br>    ):<br>        self.schema.response = payload[\"response\"].response<br>        # Perform evaluation for synthesization<br>        if \"reranking\" in self.schema.eval_types:<br>            if self.schema.reranking_type == \"rerank\":<br>                evaluation_name = \"question_answering_rerank\"<br>            else:<br>                evaluation_name = \"question_answering_resize\"<br>            self.schema.eval_types.remove(\"reranking\")<br>        else:<br>            evaluation_name = \"question_answering\"<br>        self.uptrain_evaluate(<br>            evaluation_name=evaluation_name,<br>            data=[<br>                {<br>                    \"question\": self.schema.question,<br>                    \"context\": self.schema.context,<br>                    \"response\": self.schema.response,<br>                }<br>            ],<br>            checks=[<br>                Evals.CONTEXT_RELEVANCE,<br>                Evals.FACTUAL_ACCURACY,<br>                Evals.RESPONSE_COMPLETENESS,<br>            ],<br>        )<br>    elif event_type is CBEventType.RERANKING:<br>        # Store new context data<br>        self.schema.new_context = [node.text for node in payload[\"nodes\"]]<br>        if len(self.schema.old_context) == len(self.schema.new_context):<br>            self.schema.reranking_type = \"rerank\"<br>            context = \"\\n\".join(<br>                [<br>                    f\"{index}. {string}\"<br>                    for index, string in enumerate(self.schema.old_context, start=1)<br>                ]<br>            )<br>            reranked_context = \"\\n\".join(<br>                [<br>                    f\"{index}. {string}\"<br>                    for index, string in enumerate(self.schema.new_context, start=1)<br>                ]<br>            )<br>            # Perform evaluation for reranking<br>            self.uptrain_evaluate(<br>                evaluation_name=\"context_reranking\",<br>                data=[<br>                    {<br>                        \"question\": self.schema.question,<br>                        \"context\": context,<br>                        \"reranked_context\": reranked_context,<br>                    }<br>                ],<br>                checks=[<br>                    Evals.CONTEXT_RERANKING,<br>                ],<br>            )<br>        else:<br>            self.schema.reranking_type = \"resize\"<br>            context = \"\\n\".join(self.schema.old_context)<br>            concise_context = \"\\n\".join(self.schema.new_context)<br>            # Perform evaluation for resizing<br>            self.uptrain_evaluate(<br>                evaluation_name=\"context_conciseness\",<br>                data=[<br>                    {<br>                        \"question\": self.schema.question,<br>                        \"context\": context,<br>                        \"concise_context\": concise_context,<br>                    }<br>                ],<br>                checks=[<br>                    Evals.CONTEXT_CONCISENESS,<br>                ],<br>            )<br>    elif event_type is CBEventType.SUB_QUESTION:<br>        # Store sub question data<br>        self.schema.sub_question_map[event_id][\"question\"] = payload[<br>            \"sub_question\"<br>        ].sub_q.sub_question<br>        self.schema.sub_question_map[event_id][\"context\"] = (<br>            payload[\"sub_question\"].sources[0].node.text<br>        )<br>        self.schema.sub_question_map[event_id][\"response\"] = payload[<br>            \"sub_question\"<br>        ].answer<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Uptrain - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/uptrain/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/dashscope/#llama_index.embeddings.dashscope.DashScopeEmbedding)\n\n# Dashscope\n\n## DashScopeEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/dashscope/\\#llama_index.embeddings.dashscope.DashScopeEmbedding \"Permanent link\")\n\nBases: `MultiModalEmbedding`\n\nDashScope class for text embedding.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model_name` | `str` | Model name for embedding.<br>Defaults to DashScopeTextEmbeddingModels.TEXT\\_EMBEDDING\\_V2.<br>Options are:<br>```<br>- DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V1<br>- DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V2<br>``` | `TEXT_EMBEDDING_V2` |\n| `text_type` | `str` | The input type, \\['query', 'document'\\],<br>For asymmetric tasks such as retrieval, in order to achieve better<br>retrieval results, it is recommended to distinguish between query<br>text (query) and base text (document) types, clustering Symmetric<br>tasks such as classification and classification do not need to<br>be specially specified, and the system default<br>value \"document\" can be used. | `'document'` |\n| `api_key` | `str` | The DashScope api key. | `None` |\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-dashscope/llama_index/embeddings/dashscope/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>``` | ```<br>class DashScopeEmbedding(MultiModalEmbedding):<br>    \"\"\"DashScope class for text embedding.<br>    Args:<br>        model_name (str): Model name for embedding.<br>            Defaults to DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V2.<br>                Options are:<br>                - DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V1<br>                - DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V2<br>        text_type (str): The input type, ['query', 'document'],<br>            For asymmetric tasks such as retrieval, in order to achieve better<br>            retrieval results, it is recommended to distinguish between query<br>            text (query) and base text (document) types, clustering Symmetric<br>            tasks such as classification and classification do not need to<br>            be specially specified, and the system default<br>            value \"document\" can be used.<br>        api_key (str): The DashScope api key.<br>    \"\"\"<br>    _api_key: Optional[str] = PrivateAttr()<br>    _text_type: Optional[str] = PrivateAttr()<br>    def __init__(<br>        self,<br>        model_name: str = DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V2,<br>        text_type: str = \"document\",<br>        api_key: Optional[str] = None,<br>        embed_batch_size: int = EMBED_MAX_BATCH_SIZE,<br>        **kwargs: Any,<br>    ) -> None:<br>        super().__init__(<br>            model_name=model_name,<br>            embed_batch_size=embed_batch_size,<br>            **kwargs,<br>        )<br>        self._api_key = api_key<br>        self._text_type = text_type<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"DashScopeEmbedding\"<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        emb = get_text_embedding(<br>            self.model_name,<br>            query,<br>            api_key=self._api_key,<br>            text_type=\"query\",<br>        )<br>        if len(emb) > 0 and emb[0] is not None:<br>            return emb[0]<br>        else:<br>            return []<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        emb = get_text_embedding(<br>            self.model_name,<br>            text,<br>            api_key=self._api_key,<br>            text_type=self._text_type,<br>        )<br>        if len(emb) > 0 and emb[0] is not None:<br>            return emb[0]<br>        else:<br>            return []<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        return get_text_embedding(<br>            self.model_name,<br>            texts,<br>            api_key=self._api_key,<br>            text_type=self._text_type,<br>        )<br>    # TODO: use proper async methods<br>    async def _aget_text_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._get_text_embedding(query)<br>    # TODO: user proper async methods<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self._get_query_embedding(query)<br>    def get_batch_query_embedding(self, embedding_file_url: str) -> Optional[str]:<br>        \"\"\"Get batch query embeddings.<br>        Args:<br>            embedding_file_url (str): The url of the file to embedding which with lines of text to embedding.<br>        Returns:<br>            str: The url of the embedding result, format ref:<br>                 https://help.aliyun.com/zh/dashscope/developer-reference/text-embedding-async-api-details.<br>        \"\"\"<br>        return get_batch_text_embedding(<br>            self.model_name,<br>            embedding_file_url,<br>            api_key=self._api_key,<br>            text_type=self._text_type,<br>        )<br>    def get_batch_text_embedding(self, embedding_file_url: str) -> Optional[str]:<br>        \"\"\"Get batch text embeddings.<br>        Args:<br>            embedding_file_url (str): The url of the file to embedding which with lines of text to embedding.<br>        Returns:<br>            str: The url of the embedding result, format ref:<br>                 https://help.aliyun.com/zh/dashscope/developer-reference/text-embedding-async-api-details.<br>        \"\"\"<br>        return get_batch_text_embedding(<br>            self.model_name,<br>            embedding_file_url,<br>            api_key=self._api_key,<br>            text_type=self._text_type,<br>        )<br>    def _get_image_embedding(self, img_file_path: ImageType) -> List[float]:<br>        \"\"\"<br>        Embed the input image synchronously.<br>        \"\"\"<br>        input = [{\"image\": img_file_path}]<br>        return get_multimodal_embedding(<br>            self.model_name, input=input, api_key=self._api_key<br>        )<br>    async def _aget_image_embedding(self, img_file_path: ImageType) -> List[float]:<br>        \"\"\"<br>        Embed the input image asynchronously.<br>        \"\"\"<br>        return self._get_image_embedding(img_file_path=img_file_path)<br>    def get_multimodal_embedding(<br>        self, input: List[Dict], auto_truncation: bool = False<br>    ) -> List[float]:<br>        \"\"\"Call DashScope multimodal embedding.<br>        ref: https://help.aliyun.com/zh/dashscope/developer-reference/one-peace-multimodal-embedding-api-details.<br>        Args:<br>            input (str): The input of the multimodal embedding, eg:<br>                [{'factor': 1, 'text': '\u4f60\u597d'},<br>                {'factor': 2, 'audio': 'https://dashscope.oss-cn-beijing.aliyuncs.com/audios/cow.flac'},<br>                {'factor': 3, 'image': 'https://dashscope.oss-cn-beijing.aliyuncs.com/images/256_1.png'}]<br>        Raises:<br>            ImportError: Need install dashscope package.<br>        Returns:<br>            List[float]: The embedding result<br>        \"\"\"<br>        return get_multimodal_embedding(<br>            self.model_name,<br>            input=input,<br>            api_key=self._api_key,<br>            auto_truncation=auto_truncation,<br>        )<br>``` |\n\n### get\\_batch\\_query\\_embedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/dashscope/\\#llama_index.embeddings.dashscope.DashScopeEmbedding.get_batch_query_embedding \"Permanent link\")\n\n```\nget_batch_query_embedding(embedding_file_url: str) -> Optional[str]\n\n```\n\nGet batch query embeddings.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `embedding_file_url` | `str` | The url of the file to embedding which with lines of text to embedding. | _required_ |\n\n**Returns:**\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `str` | `Optional[str]` | The url of the embedding result, format ref:<br>https://help.aliyun.com/zh/dashscope/developer-reference/text-embedding-async-api-details. |\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-dashscope/llama_index/embeddings/dashscope/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>``` | ```<br>def get_batch_query_embedding(self, embedding_file_url: str) -> Optional[str]:<br>    \"\"\"Get batch query embeddings.<br>    Args:<br>        embedding_file_url (str): The url of the file to embedding which with lines of text to embedding.<br>    Returns:<br>        str: The url of the embedding result, format ref:<br>             https://help.aliyun.com/zh/dashscope/developer-reference/text-embedding-async-api-details.<br>    \"\"\"<br>    return get_batch_text_embedding(<br>        self.model_name,<br>        embedding_file_url,<br>        api_key=self._api_key,<br>        text_type=self._text_type,<br>    )<br>``` |\n\n### get\\_batch\\_text\\_embedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/dashscope/\\#llama_index.embeddings.dashscope.DashScopeEmbedding.get_batch_text_embedding \"Permanent link\")\n\n```\nget_batch_text_embedding(embedding_file_url: str) -> Optional[str]\n\n```\n\nGet batch text embeddings.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `embedding_file_url` | `str` | The url of the file to embedding which with lines of text to embedding. | _required_ |\n\n**Returns:**\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `str` | `Optional[str]` | The url of the embedding result, format ref:<br>https://help.aliyun.com/zh/dashscope/developer-reference/text-embedding-async-api-details. |\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-dashscope/llama_index/embeddings/dashscope/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>``` | ```<br>def get_batch_text_embedding(self, embedding_file_url: str) -> Optional[str]:<br>    \"\"\"Get batch text embeddings.<br>    Args:<br>        embedding_file_url (str): The url of the file to embedding which with lines of text to embedding.<br>    Returns:<br>        str: The url of the embedding result, format ref:<br>             https://help.aliyun.com/zh/dashscope/developer-reference/text-embedding-async-api-details.<br>    \"\"\"<br>    return get_batch_text_embedding(<br>        self.model_name,<br>        embedding_file_url,<br>        api_key=self._api_key,<br>        text_type=self._text_type,<br>    )<br>``` |\n\n### get\\_multimodal\\_embedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/dashscope/\\#llama_index.embeddings.dashscope.DashScopeEmbedding.get_multimodal_embedding \"Permanent link\")\n\n```\nget_multimodal_embedding(input: List[Dict], auto_truncation: bool = False) -> List[float]\n\n```\n\nCall DashScope multimodal embedding.\nref: https://help.aliyun.com/zh/dashscope/developer-reference/one-peace-multimodal-embedding-api-details.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `input` | `str` | The input of the multimodal embedding, eg:<br>\\[{'factor': 1, 'text': '\u4f60\u597d'},<br>{'factor': 2, 'audio': 'https://dashscope.oss-cn-beijing.aliyuncs.com/audios/cow.flac'},<br>{'factor': 3, 'image': 'https://dashscope.oss-cn-beijing.aliyuncs.com/images/256\\_1.png'}\\] | _required_ |\n\n**Raises:**\n\n| Type | Description |\n| --- | --- |\n| `ImportError` | Need install dashscope package. |\n\n**Returns:**\n\n| Type | Description |\n| --- | --- |\n| `List[float]` | List\\[float\\]: The embedding result |\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-dashscope/llama_index/embeddings/dashscope/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>``` | ```<br>def get_multimodal_embedding(<br>    self, input: List[Dict], auto_truncation: bool = False<br>) -> List[float]:<br>    \"\"\"Call DashScope multimodal embedding.<br>    ref: https://help.aliyun.com/zh/dashscope/developer-reference/one-peace-multimodal-embedding-api-details.<br>    Args:<br>        input (str): The input of the multimodal embedding, eg:<br>            [{'factor': 1, 'text': '\u4f60\u597d'},<br>            {'factor': 2, 'audio': 'https://dashscope.oss-cn-beijing.aliyuncs.com/audios/cow.flac'},<br>            {'factor': 3, 'image': 'https://dashscope.oss-cn-beijing.aliyuncs.com/images/256_1.png'}]<br>    Raises:<br>        ImportError: Need install dashscope package.<br>    Returns:<br>        List[float]: The embedding result<br>    \"\"\"<br>    return get_multimodal_embedding(<br>        self.model_name,<br>        input=input,<br>        api_key=self._api_key,<br>        auto_truncation=auto_truncation,<br>    )<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Dashscope - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/dashscope/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "\n\n# Sagemaker endpoint\n\n## SageMakerEmbedding [\\#](\\#llama_index.embeddings.sagemaker_endpoint.SageMakerEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-sagemaker-endpoint/llama_index/embeddings/sagemaker_endpoint/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>``` | ```<br>class SageMakerEmbedding(BaseEmbedding):<br>    endpoint_name: str = Field(description=\"SageMaker Embedding endpoint name\")<br>    endpoint_kwargs: Dict[str, Any] = Field(<br>        default={},<br>        description=\"Additional kwargs for the invoke_endpoint request.\",<br>    )<br>    model_kwargs: Dict[str, Any] = Field(<br>        default={},<br>        description=\"kwargs to pass to the model.\",<br>    )<br>    content_handler: BaseIOHandler = Field(<br>        default=DEFAULT_IO_HANDLER,<br>        description=\"used to serialize input, deserialize output, and remove a prefix.\",<br>    )<br>    profile_name: Optional[str] = Field(<br>        description=\"The name of aws profile to use. If not given, then the default profile is used.\"<br>    )<br>    aws_access_key_id: Optional[str] = Field(description=\"AWS Access Key ID to use\")<br>    aws_secret_access_key: Optional[str] = Field(<br>        description=\"AWS Secret Access Key to use\"<br>    )<br>    aws_session_token: Optional[str] = Field(description=\"AWS Session Token to use\")<br>    region_name: Optional[str] = Field(<br>        description=\"AWS region name to use. Uses region configured in AWS CLI if not passed\"<br>    )<br>    max_retries: Optional[int] = Field(<br>        default=3,<br>        description=\"The maximum number of API retries.\",<br>        gte=0,<br>    )<br>    timeout: Optional[float] = Field(<br>        default=60.0,<br>        description=\"The timeout, in seconds, for API requests.\",<br>        gte=0,<br>    )<br>    _client: Any = PrivateAttr()<br>    _verbose: bool = PrivateAttr()<br>    def __init__(<br>        self,<br>        endpoint_name: str,<br>        endpoint_kwargs: Optional[Dict[str, Any]] = {},<br>        model_kwargs: Optional[Dict[str, Any]] = {},<br>        content_handler: BaseIOHandler = DEFAULT_IO_HANDLER,<br>        profile_name: Optional[str] = None,<br>        aws_access_key_id: Optional[str] = None,<br>        aws_secret_access_key: Optional[str] = None,<br>        aws_session_token: Optional[str] = None,<br>        region_name: Optional[str] = None,<br>        max_retries: Optional[int] = 3,<br>        timeout: Optional[float] = 60.0,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>        pydantic_program_mode: PydanticProgramMode = PydanticProgramMode.DEFAULT,<br>        verbose: bool = False,<br>    ):<br>        if not endpoint_name:<br>            raise ValueError(<br>                \"Missing required argument:`endpoint_name`\"<br>                \" Please specify the endpoint_name\"<br>            )<br>        endpoint_kwargs = endpoint_kwargs or {}<br>        model_kwargs = model_kwargs or {}<br>        content_handler = content_handler<br>        super().__init__(<br>            endpoint_name=endpoint_name,<br>            endpoint_kwargs=endpoint_kwargs,<br>            model_kwargs=model_kwargs,<br>            content_handler=content_handler,<br>            embed_batch_size=embed_batch_size,<br>            profile_name=profile_name,<br>            region_name=region_name,<br>            aws_access_key_id=aws_access_key_id,<br>            aws_secret_access_key=aws_secret_access_key,<br>            aws_session_token=aws_session_token,<br>            pydantic_program_mode=pydantic_program_mode,<br>            callback_manager=callback_manager,<br>        )<br>        self._client = get_aws_service_client(<br>            service_name=\"sagemaker-runtime\",<br>            profile_name=profile_name,<br>            region_name=region_name,<br>            aws_access_key_id=aws_access_key_id,<br>            aws_secret_access_key=aws_secret_access_key,<br>            aws_session_token=aws_session_token,<br>            max_retries=max_retries,<br>            timeout=timeout,<br>        )<br>        self._verbose = verbose<br>    @classmethod<br>    def class_name(self) -> str:<br>        return \"SageMakerEmbedding\"<br>    def _get_embedding(self, payload: List[str], **kwargs: Any) -> List[Embedding]:<br>        model_kwargs = {**self.model_kwargs, **kwargs}<br>        request_body = self.content_handler.serialize_input(<br>            request=payload, model_kwargs=model_kwargs<br>        )<br>        response = self._client.invoke_endpoint(<br>            EndpointName=self.endpoint_name,<br>            Body=request_body,<br>            ContentType=self.content_handler.content_type,<br>            Accept=self.content_handler.accept,<br>            **self.endpoint_kwargs,<br>        )[\"Body\"]<br>        return self.content_handler.deserialize_output(response=response)<br>    def _get_query_embedding(self, query: str, **kwargs: Any) -> Embedding:<br>        query = query.replace(\"\\n\", \" \")<br>        return self._get_embedding([query], **kwargs)[0]<br>    def _get_text_embedding(self, text: str, **kwargs: Any) -> Embedding:<br>        text = text.replace(\"\\n\", \" \")<br>        return self._get_embedding([text], **kwargs)[0]<br>    def _get_text_embeddings(self, texts: List[str], **kwargs: Any) -> List[Embedding]:<br>        \"\"\"<br>        Embed the input sequence of text synchronously.<br>        Subclasses can implement this method if batch queries are supported.<br>        \"\"\"<br>        texts = [text.replace(\"\\n\", \" \") for text in texts]<br>        # Default implementation just loops over _get_text_embedding<br>        return self._get_embedding(texts, **kwargs)<br>    async def _aget_query_embedding(self, query: str, **kwargs: Any) -> Embedding:<br>        raise NotImplementedError<br>    async def _aget_text_embedding(self, text: str, **kwargs: Any) -> Embedding:<br>        raise NotImplementedError<br>    async def _aget_text_embeddings(<br>        self, texts: List[str], **kwargs: Any<br>    ) -> List[Embedding]:<br>        raise NotImplementedError<br>``` |\n\nBack to top",
      "metadata": {
        "title": "Sagemaker endpoint - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/sagemaker_endpoint/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/argilla/#llama_index.callbacks.argilla.argilla_callback_handler)\n\n# Argilla\n\n## argilla\\_callback\\_handler [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/argilla/\\#llama_index.callbacks.argilla.argilla_callback_handler \"Permanent link\")\n\n```\nargilla_callback_handler(**kwargs: Any) -> BaseCallbackHandler\n\n```\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-argilla/llama_index/callbacks/argilla/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>``` | ```<br>def argilla_callback_handler(**kwargs: Any) -> BaseCallbackHandler:<br>    try:<br>        # lazy import<br>        from argilla_llama_index import ArgillaCallbackHandler<br>    except ImportError:<br>        raise ImportError(\"Please install Argilla with `pip install argilla`\")<br>    return ArgillaCallbackHandler(**kwargs)<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Argilla - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/argilla/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/dataset_generation/#llama_index.core.evaluation.DatasetGenerator)\n\n# Dataset generation\n\nEvaluation modules.\n\n## DatasetGenerator [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/dataset_generation/\\#llama_index.core.evaluation.DatasetGenerator \"Permanent link\")\n\nBases: `PromptMixin`\n\nGenerate dataset (question/ question-answer pairs) based on the given documents.\n\nNOTE: this is a beta feature, subject to change!\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `nodes` | `List[Node]` | List of nodes. (Optional) | _required_ |\n| `llm` | `LLM` | Language model. | `None` |\n| `callback_manager` | `CallbackManager` | Callback manager. | `None` |\n| `num_questions_per_chunk` | `int` | number of question to be generated per chunk. Each document is chunked of size 512 words. | `10` |\n| `text_question_template` | `BasePromptTemplate | None` | Question generation template. | `None` |\n| `question_gen_query` | `str | None` | Question generation query. | `None` |\n\nSource code in `llama-index-core/llama_index/core/evaluation/dataset_generation.py`\n\n|     |     |\n| --- | --- |\n| ```<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>``` | ```<br>@deprecated(<br>    \"Deprecated in favor of `RagDatasetGenerator` which should be used instead.\",<br>    action=\"always\",<br>)<br>class DatasetGenerator(PromptMixin):<br>    \"\"\"Generate dataset (question/ question-answer pairs) \\<br>    based on the given documents.<br>    NOTE: this is a beta feature, subject to change!<br>    Args:<br>        nodes (List[Node]): List of nodes. (Optional)<br>        llm (LLM): Language model.<br>        callback_manager (CallbackManager): Callback manager.<br>        num_questions_per_chunk: number of question to be \\<br>        generated per chunk. Each document is chunked of size 512 words.<br>        text_question_template: Question generation template.<br>        question_gen_query: Question generation query.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        nodes: List[BaseNode],<br>        llm: Optional[LLM] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        num_questions_per_chunk: int = 10,<br>        text_question_template: BasePromptTemplate | None = None,<br>        text_qa_template: BasePromptTemplate | None = None,<br>        question_gen_query: str | None = None,<br>        metadata_mode: MetadataMode = MetadataMode.NONE,<br>        show_progress: bool = False,<br>    ) -> None:<br>        \"\"\"Init params.\"\"\"<br>        self.llm = llm or Settings.llm<br>        self.callback_manager = callback_manager or Settings.callback_manager<br>        self.text_question_template = text_question_template or PromptTemplate(<br>            DEFAULT_QUESTION_GENERATION_PROMPT<br>        )<br>        self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT<br>        self.question_gen_query = (<br>            question_gen_query<br>            or f\"You are a Teacher/Professor. Your task is to setup \\<br>                        {num_questions_per_chunk} questions for an upcoming \\<br>                        quiz/examination. The questions should be diverse in nature \\<br>                            across the document. Restrict the questions to the \\<br>                                context information provided.\"<br>        )<br>        self.nodes = nodes<br>        self._metadata_mode = metadata_mode<br>        self._show_progress = show_progress<br>    @classmethod<br>    def from_documents(<br>        cls,<br>        documents: List[Document],<br>        llm: Optional[LLM] = None,<br>        transformations: Optional[List[TransformComponent]] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        num_questions_per_chunk: int = 10,<br>        text_question_template: BasePromptTemplate | None = None,<br>        text_qa_template: BasePromptTemplate | None = None,<br>        question_gen_query: str | None = None,<br>        required_keywords: List[str] | None = None,<br>        exclude_keywords: List[str] | None = None,<br>        show_progress: bool = False,<br>    ) -> DatasetGenerator:<br>        \"\"\"Generate dataset from documents.\"\"\"<br>        llm = llm or Settings.llm<br>        transformations = transformations or Settings.transformations<br>        callback_manager = callback_manager or Settings.callback_manager<br>        nodes = run_transformations(<br>            documents, transformations, show_progress=show_progress<br>        )<br>        # use node postprocessor to filter nodes<br>        required_keywords = required_keywords or []<br>        exclude_keywords = exclude_keywords or []<br>        node_postprocessor = KeywordNodePostprocessor(<br>            callback_manager=callback_manager,<br>            required_keywords=required_keywords,<br>            exclude_keywords=exclude_keywords,<br>        )<br>        node_with_scores = [NodeWithScore(node=node) for node in nodes]<br>        node_with_scores = node_postprocessor.postprocess_nodes(node_with_scores)<br>        nodes = [node_with_score.node for node_with_score in node_with_scores]<br>        return cls(<br>            nodes=nodes,<br>            llm=llm,<br>            callback_manager=callback_manager,<br>            num_questions_per_chunk=num_questions_per_chunk,<br>            text_question_template=text_question_template,<br>            text_qa_template=text_qa_template,<br>            question_gen_query=question_gen_query,<br>            show_progress=show_progress,<br>        )<br>    async def _agenerate_dataset(<br>        self,<br>        nodes: List[BaseNode],<br>        num: int | None = None,<br>        generate_response: bool = False,<br>    ) -> QueryResponseDataset:<br>        \"\"\"Node question generator.\"\"\"<br>        query_tasks: List[Coroutine] = []<br>        queries: Dict[str, str] = {}<br>        responses_dict: Dict[str, str] = {}<br>        if self._show_progress:<br>            from tqdm.asyncio import tqdm_asyncio<br>            async_module = tqdm_asyncio<br>        else:<br>            async_module = asyncio<br>        summary_indices: List[SummaryIndex] = []<br>        for node in nodes:<br>            if num is not None and len(query_tasks) >= num:<br>                break<br>            index = SummaryIndex.from_documents(<br>                [<br>                    Document(<br>                        text=node.get_content(metadata_mode=self._metadata_mode),<br>                        metadata=node.metadata,  # type: ignore<br>                    )<br>                ],<br>                callback_manager=self.callback_manager,<br>            )<br>            query_engine = index.as_query_engine(<br>                llm=self.llm,<br>                text_qa_template=self.text_question_template,<br>                use_async=True,<br>            )<br>            task = query_engine.aquery(<br>                self.question_gen_query,<br>            )<br>            query_tasks.append(task)<br>            summary_indices.append(index)<br>        responses = await async_module.gather(*query_tasks)<br>        for idx, response in enumerate(responses):<br>            result = str(response).strip().split(\"\\n\")<br>            cleaned_questions = [<br>                re.sub(r\"^\\d+[\\).\\s]\", \"\", question).strip() for question in result<br>            ]<br>            cleaned_questions = [<br>                question for question in cleaned_questions if len(question) > 0<br>            ]<br>            cur_queries = {<br>                str(uuid.uuid4()): question for question in cleaned_questions<br>            }<br>            queries.update(cur_queries)<br>            if generate_response:<br>                index = summary_indices[idx]<br>                qr_tasks = []<br>                cur_query_items = list(cur_queries.items())<br>                cur_query_keys = [query_id for query_id, _ in cur_query_items]<br>                for query_id, query in cur_query_items:<br>                    qa_query_engine = index.as_query_engine(<br>                        llm=self.llm,<br>                        text_qa_template=self.text_qa_template,<br>                    )<br>                    qr_task = qa_query_engine.aquery(query)<br>                    qr_tasks.append(qr_task)<br>                qr_responses = await async_module.gather(*qr_tasks)<br>                for query_id, qa_response in zip(cur_query_keys, qr_responses):<br>                    responses_dict[query_id] = str(qa_response)<br>            else:<br>                pass<br>        query_ids = list(queries.keys())<br>        if num is not None:<br>            query_ids = query_ids[:num]<br>            # truncate queries, responses to the subset of query ids<br>            queries = {query_id: queries[query_id] for query_id in query_ids}<br>            if generate_response:<br>                responses_dict = {<br>                    query_id: responses_dict[query_id] for query_id in query_ids<br>                }<br>        return QueryResponseDataset(queries=queries, responses=responses_dict)<br>    async def agenerate_questions_from_nodes(self, num: int | None = None) -> List[str]:<br>        \"\"\"Generates questions for each document.\"\"\"<br>        dataset = await self._agenerate_dataset(<br>            self.nodes, num=num, generate_response=False<br>        )<br>        return dataset.questions<br>    async def agenerate_dataset_from_nodes(<br>        self, num: int | None = None<br>    ) -> QueryResponseDataset:<br>        \"\"\"Generates questions for each document.\"\"\"<br>        return await self._agenerate_dataset(<br>            self.nodes, num=num, generate_response=True<br>        )<br>    def generate_questions_from_nodes(self, num: int | None = None) -> List[str]:<br>        \"\"\"Generates questions for each document.\"\"\"<br>        return asyncio_run(self.agenerate_questions_from_nodes(num=num))<br>    def generate_dataset_from_nodes(<br>        self, num: int | None = None<br>    ) -> QueryResponseDataset:<br>        \"\"\"Generates questions for each document.\"\"\"<br>        return asyncio_run(self.agenerate_dataset_from_nodes(num=num))<br>    def _get_prompts(self) -> PromptDictType:<br>        \"\"\"Get prompts.\"\"\"<br>        return {<br>            \"text_question_template\": self.text_question_template,<br>            \"text_qa_template\": self.text_qa_template,<br>        }<br>    def _get_prompt_modules(self) -> PromptMixinType:<br>        \"\"\"Get prompt modules.\"\"\"<br>        return {}<br>    def _update_prompts(self, prompts: PromptDictType) -> None:<br>        \"\"\"Update prompts.\"\"\"<br>        if \"text_question_template\" in prompts:<br>            self.text_question_template = prompts[\"text_question_template\"]<br>        if \"text_qa_template\" in prompts:<br>            self.text_qa_template = prompts[\"text_qa_template\"]<br>``` |\n\n### from\\_documents`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/dataset_generation/\\#llama_index.core.evaluation.DatasetGenerator.from_documents \"Permanent link\")\n\n```\nfrom_documents(documents: List[Document], llm: Optional[LLM] = None, transformations: Optional[List[TransformComponent]] = None, callback_manager: Optional[CallbackManager] = None, num_questions_per_chunk: int = 10, text_question_template: BasePromptTemplate | None = None, text_qa_template: BasePromptTemplate | None = None, question_gen_query: str | None = None, required_keywords: List[str] | None = None, exclude_keywords: List[str] | None = None, show_progress: bool = False) -> DatasetGenerator\n\n```\n\nGenerate dataset from documents.\n\nSource code in `llama-index-core/llama_index/core/evaluation/dataset_generation.py`\n\n|     |     |\n| --- | --- |\n| ```<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>``` | ```<br>@classmethod<br>def from_documents(<br>    cls,<br>    documents: List[Document],<br>    llm: Optional[LLM] = None,<br>    transformations: Optional[List[TransformComponent]] = None,<br>    callback_manager: Optional[CallbackManager] = None,<br>    num_questions_per_chunk: int = 10,<br>    text_question_template: BasePromptTemplate | None = None,<br>    text_qa_template: BasePromptTemplate | None = None,<br>    question_gen_query: str | None = None,<br>    required_keywords: List[str] | None = None,<br>    exclude_keywords: List[str] | None = None,<br>    show_progress: bool = False,<br>) -> DatasetGenerator:<br>    \"\"\"Generate dataset from documents.\"\"\"<br>    llm = llm or Settings.llm<br>    transformations = transformations or Settings.transformations<br>    callback_manager = callback_manager or Settings.callback_manager<br>    nodes = run_transformations(<br>        documents, transformations, show_progress=show_progress<br>    )<br>    # use node postprocessor to filter nodes<br>    required_keywords = required_keywords or []<br>    exclude_keywords = exclude_keywords or []<br>    node_postprocessor = KeywordNodePostprocessor(<br>        callback_manager=callback_manager,<br>        required_keywords=required_keywords,<br>        exclude_keywords=exclude_keywords,<br>    )<br>    node_with_scores = [NodeWithScore(node=node) for node in nodes]<br>    node_with_scores = node_postprocessor.postprocess_nodes(node_with_scores)<br>    nodes = [node_with_score.node for node_with_score in node_with_scores]<br>    return cls(<br>        nodes=nodes,<br>        llm=llm,<br>        callback_manager=callback_manager,<br>        num_questions_per_chunk=num_questions_per_chunk,<br>        text_question_template=text_question_template,<br>        text_qa_template=text_qa_template,<br>        question_gen_query=question_gen_query,<br>        show_progress=show_progress,<br>    )<br>``` |\n\n### agenerate\\_questions\\_from\\_nodes`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/dataset_generation/\\#llama_index.core.evaluation.DatasetGenerator.agenerate_questions_from_nodes \"Permanent link\")\n\n```\nagenerate_questions_from_nodes(num: int | None = None) -> List[str]\n\n```\n\nGenerates questions for each document.\n\nSource code in `llama-index-core/llama_index/core/evaluation/dataset_generation.py`\n\n|     |     |\n| --- | --- |\n| ```<br>298<br>299<br>300<br>301<br>302<br>303<br>``` | ```<br>async def agenerate_questions_from_nodes(self, num: int | None = None) -> List[str]:<br>    \"\"\"Generates questions for each document.\"\"\"<br>    dataset = await self._agenerate_dataset(<br>        self.nodes, num=num, generate_response=False<br>    )<br>    return dataset.questions<br>``` |\n\n### agenerate\\_dataset\\_from\\_nodes`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/dataset_generation/\\#llama_index.core.evaluation.DatasetGenerator.agenerate_dataset_from_nodes \"Permanent link\")\n\n```\nagenerate_dataset_from_nodes(num: int | None = None) -> QueryResponseDataset\n\n```\n\nGenerates questions for each document.\n\nSource code in `llama-index-core/llama_index/core/evaluation/dataset_generation.py`\n\n|     |     |\n| --- | --- |\n| ```<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>``` | ```<br>async def agenerate_dataset_from_nodes(<br>    self, num: int | None = None<br>) -> QueryResponseDataset:<br>    \"\"\"Generates questions for each document.\"\"\"<br>    return await self._agenerate_dataset(<br>        self.nodes, num=num, generate_response=True<br>    )<br>``` |\n\n### generate\\_questions\\_from\\_nodes [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/dataset_generation/\\#llama_index.core.evaluation.DatasetGenerator.generate_questions_from_nodes \"Permanent link\")\n\n```\ngenerate_questions_from_nodes(num: int | None = None) -> List[str]\n\n```\n\nGenerates questions for each document.\n\nSource code in `llama-index-core/llama_index/core/evaluation/dataset_generation.py`\n\n|     |     |\n| --- | --- |\n| ```<br>313<br>314<br>315<br>``` | ```<br>def generate_questions_from_nodes(self, num: int | None = None) -> List[str]:<br>    \"\"\"Generates questions for each document.\"\"\"<br>    return asyncio_run(self.agenerate_questions_from_nodes(num=num))<br>``` |\n\n### generate\\_dataset\\_from\\_nodes [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/dataset_generation/\\#llama_index.core.evaluation.DatasetGenerator.generate_dataset_from_nodes \"Permanent link\")\n\n```\ngenerate_dataset_from_nodes(num: int | None = None) -> QueryResponseDataset\n\n```\n\nGenerates questions for each document.\n\nSource code in `llama-index-core/llama_index/core/evaluation/dataset_generation.py`\n\n|     |     |\n| --- | --- |\n| ```<br>317<br>318<br>319<br>320<br>321<br>``` | ```<br>def generate_dataset_from_nodes(<br>    self, num: int | None = None<br>) -> QueryResponseDataset:<br>    \"\"\"Generates questions for each document.\"\"\"<br>    return asyncio_run(self.agenerate_dataset_from_nodes(num=num))<br>``` |\n\n## QueryResponseDataset [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/dataset_generation/\\#llama_index.core.evaluation.QueryResponseDataset \"Permanent link\")\n\nBases: `BaseModel`\n\nQuery Response Dataset.\n\nThe response can be empty if the dataset is generated from documents.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `queries` | `Dict[str, str]` | Query id -> query. | _required_ |\n| `responses` | `Dict[str, str]` | Query id -> response. | _required_ |\n\nSource code in `llama-index-core/llama_index/core/evaluation/dataset_generation.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>``` | ```<br>@deprecated(<br>    \"Deprecated in favor of `LabelledRagDataset` which should be used instead.\",<br>    action=\"always\",<br>)<br>class QueryResponseDataset(BaseModel):<br>    \"\"\"Query Response Dataset.<br>    The response can be empty if the dataset is generated from documents.<br>    Args:<br>        queries (Dict[str, str]): Query id -> query.<br>        responses (Dict[str, str]): Query id -> response.<br>    \"\"\"<br>    queries: Dict[str, str] = Field(<br>        default_factory=dict, description=\"Query id -> query\"<br>    )<br>    responses: Dict[str, str] = Field(<br>        default_factory=dict, description=\"Query id -> response\"<br>    )<br>    @classmethod<br>    def from_qr_pairs(<br>        cls,<br>        qr_pairs: List[Tuple[str, str]],<br>    ) -> QueryResponseDataset:<br>        \"\"\"Create from qr pairs.\"\"\"<br>        # define ids as simple integers<br>        queries = {str(idx): query for idx, (query, _) in enumerate(qr_pairs)}<br>        responses = {str(idx): response for idx, (_, response) in enumerate(qr_pairs)}<br>        return cls(queries=queries, responses=responses)<br>    @property<br>    def qr_pairs(self) -> List[Tuple[str, str]]:<br>        \"\"\"Get pairs.\"\"\"<br>        # if query_id not in response, throw error<br>        for query_id in self.queries:<br>            if query_id not in self.responses:<br>                raise ValueError(f\"Query id {query_id} not in responses\")<br>        return [<br>            (self.queries[query_id], self.responses[query_id])<br>            for query_id in self.queries<br>        ]<br>    @property<br>    def questions(self) -> List[str]:<br>        \"\"\"Get questions.\"\"\"<br>        return list(self.queries.values())<br>    def save_json(self, path: str) -> None:<br>        \"\"\"Save json.\"\"\"<br>        with open(path, \"w\") as f:<br>            json.dump(self.model_dump(), f, indent=4)<br>    @classmethod<br>    def from_json(cls, path: str) -> QueryResponseDataset:<br>        \"\"\"Load json.\"\"\"<br>        with open(path) as f:<br>            data = json.load(f)<br>        return cls(**data)<br>``` |\n\n### qr\\_pairs`property`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/dataset_generation/\\#llama_index.core.evaluation.QueryResponseDataset.qr_pairs \"Permanent link\")\n\n```\nqr_pairs: List[Tuple[str, str]]\n\n```\n\nGet pairs.\n\n### questions`property`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/dataset_generation/\\#llama_index.core.evaluation.QueryResponseDataset.questions \"Permanent link\")\n\n```\nquestions: List[str]\n\n```\n\nGet questions.\n\n### from\\_qr\\_pairs`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/dataset_generation/\\#llama_index.core.evaluation.QueryResponseDataset.from_qr_pairs \"Permanent link\")\n\n```\nfrom_qr_pairs(qr_pairs: List[Tuple[str, str]]) -> QueryResponseDataset\n\n```\n\nCreate from qr pairs.\n\nSource code in `llama-index-core/llama_index/core/evaluation/dataset_generation.py`\n\n|     |     |\n| --- | --- |\n| ```<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>``` | ```<br>@classmethod<br>def from_qr_pairs(<br>    cls,<br>    qr_pairs: List[Tuple[str, str]],<br>) -> QueryResponseDataset:<br>    \"\"\"Create from qr pairs.\"\"\"<br>    # define ids as simple integers<br>    queries = {str(idx): query for idx, (query, _) in enumerate(qr_pairs)}<br>    responses = {str(idx): response for idx, (_, response) in enumerate(qr_pairs)}<br>    return cls(queries=queries, responses=responses)<br>``` |\n\n### save\\_json [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/dataset_generation/\\#llama_index.core.evaluation.QueryResponseDataset.save_json \"Permanent link\")\n\n```\nsave_json(path: str) -> None\n\n```\n\nSave json.\n\nSource code in `llama-index-core/llama_index/core/evaluation/dataset_generation.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 99<br>100<br>101<br>102<br>``` | ```<br>def save_json(self, path: str) -> None:<br>    \"\"\"Save json.\"\"\"<br>    with open(path, \"w\") as f:<br>        json.dump(self.model_dump(), f, indent=4)<br>``` |\n\n### from\\_json`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/dataset_generation/\\#llama_index.core.evaluation.QueryResponseDataset.from_json \"Permanent link\")\n\n```\nfrom_json(path: str) -> QueryResponseDataset\n\n```\n\nLoad json.\n\nSource code in `llama-index-core/llama_index/core/evaluation/dataset_generation.py`\n\n|     |     |\n| --- | --- |\n| ```<br>104<br>105<br>106<br>107<br>108<br>109<br>``` | ```<br>@classmethod<br>def from_json(cls, path: str) -> QueryResponseDataset:<br>    \"\"\"Load json.\"\"\"<br>    with open(path) as f:<br>        data = json.load(f)<br>    return cls(**data)<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Dataset generation - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/evaluation/dataset_generation/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/pairwise_comparison/#llama_index.core.evaluation.PairwiseComparisonEvaluator)\n\n# Pairwise comparison\n\nEvaluation modules.\n\n## PairwiseComparisonEvaluator [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/pairwise_comparison/\\#llama_index.core.evaluation.PairwiseComparisonEvaluator \"Permanent link\")\n\nBases: `BaseEvaluator`\n\nPairwise comparison evaluator.\n\nEvaluates the quality of a response vs. a \"reference\" response given a question by\nhaving an LLM judge which response is better.\n\nOutputs whether the `response` given is better than the `reference` response.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `eval_template` | `Optional[Union[str, BasePromptTemplate]]` | The template to use for evaluation. | `None` |\n| `enforce_consensus` | `bool` | Whether to enforce consensus (consistency if we<br>flip the order of the answers). Defaults to True. | `True` |\n\nSource code in `llama-index-core/llama_index/core/evaluation/pairwise.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>``` | ```<br>class PairwiseComparisonEvaluator(BaseEvaluator):<br>    \"\"\"Pairwise comparison evaluator.<br>    Evaluates the quality of a response vs. a \"reference\" response given a question by<br>    having an LLM judge which response is better.<br>    Outputs whether the `response` given is better than the `reference` response.<br>    Args:<br>        eval_template (Optional[Union[str, BasePromptTemplate]]):<br>            The template to use for evaluation.<br>        enforce_consensus (bool): Whether to enforce consensus (consistency if we<br>            flip the order of the answers). Defaults to True.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        llm: Optional[LLM] = None,<br>        eval_template: Optional[Union[BasePromptTemplate, str]] = None,<br>        parser_function: Callable[<br>            [str], Tuple[Optional[bool], Optional[float], Optional[str]]<br>        ] = _default_parser_function,<br>        enforce_consensus: bool = True,<br>    ) -> None:<br>        self._llm = llm or Settings.llm<br>        self._eval_template: BasePromptTemplate<br>        if isinstance(eval_template, str):<br>            self._eval_template = PromptTemplate(eval_template)<br>        else:<br>            self._eval_template = eval_template or DEFAULT_EVAL_TEMPLATE<br>        self._enforce_consensus = enforce_consensus<br>        self._parser_function = parser_function<br>    def _get_prompts(self) -> PromptDictType:<br>        \"\"\"Get prompts.\"\"\"<br>        return {<br>            \"eval_template\": self._eval_template,<br>        }<br>    def _update_prompts(self, prompts: PromptDictType) -> None:<br>        \"\"\"Update prompts.\"\"\"<br>        if \"eval_template\" in prompts:<br>            self._eval_template = prompts[\"eval_template\"]<br>    async def _get_eval_result(<br>        self,<br>        query: str,<br>        response: str,<br>        second_response: str,<br>        reference: Optional[str],<br>    ) -> EvaluationResult:<br>        \"\"\"Get evaluation result.\"\"\"<br>        eval_response = await self._llm.apredict(<br>            prompt=self._eval_template,<br>            query=query,<br>            answer_1=response,<br>            answer_2=second_response,<br>            reference=reference or \"\",<br>        )<br>        # Extract from response<br>        passing, score, feedback = self._parser_function(eval_response)<br>        if passing is None and score is None and feedback is None:<br>            return EvaluationResult(<br>                query=query,<br>                invalid_result=True,<br>                invalid_reason=\"Output cannot be parsed\",<br>                feedback=eval_response,<br>            )<br>        else:<br>            return EvaluationResult(<br>                query=query,<br>                response=eval_response,<br>                passing=passing,<br>                score=score,<br>                feedback=eval_response,<br>                pairwise_source=EvaluationSource.ORIGINAL,<br>            )<br>    async def _resolve_results(<br>        self,<br>        eval_result: EvaluationResult,<br>        flipped_eval_result: EvaluationResult,<br>    ) -> EvaluationResult:<br>        \"\"\"Resolve eval results from evaluation + flipped evaluation.<br>        Args:<br>            eval_result (EvaluationResult): Result when answer_1 is shown first<br>            flipped_eval_result (EvaluationResult): Result when answer_2 is shown first<br>        Returns:<br>            EvaluationResult: The final evaluation result<br>        \"\"\"<br>        # add pairwise_source to eval_result and flipped_eval_result<br>        eval_result.pairwise_source = EvaluationSource.ORIGINAL<br>        flipped_eval_result.pairwise_source = EvaluationSource.FLIPPED<br>        # count the votes for each of the 2 answers<br>        votes_1 = 0.0<br>        votes_2 = 0.0<br>        if eval_result.score is not None and flipped_eval_result.score is not None:<br>            votes_1 = eval_result.score + (1 - flipped_eval_result.score)<br>            votes_2 = (1 - eval_result.score) + flipped_eval_result.score<br>        if votes_1 + votes_2 != 2:  # each round, the judge can give a total of 1 vote<br>            raise ValueError(\"Impossible score results. Total amount of votes is 2.\")<br>        # get the judges (original and flipped) who voted for answer_1<br>        voters_1 = [eval_result] * (eval_result.score == 1.0) + [<br>            flipped_eval_result<br>        ] * (flipped_eval_result.score == 0.0)<br>        # get the judges (original and flipped) who voted for answer_2<br>        voters_2 = [eval_result] * (eval_result.score == 0.0) + [<br>            flipped_eval_result<br>        ] * (flipped_eval_result.score == 1.0)<br>        if votes_1 > votes_2:<br>            return voters_1[0]  # return any voter for answer_1<br>        elif votes_2 > votes_1:<br>            return voters_2[0]  # return any vote for answer_2<br>        else:<br>            if (<br>                eval_result.score == 0.5<br>            ):  # votes_1 == votes_2 can only happen if both are 1.0 (so actual tie)<br>                # doesn't matter which one we return here<br>                return eval_result<br>            else:  # Inconclusive case!<br>                return EvaluationResult(<br>                    query=eval_result.query,<br>                    response=\"\",<br>                    passing=None,<br>                    score=0.5,<br>                    feedback=\"\",<br>                    pairwise_source=EvaluationSource.NEITHER,<br>                )<br>    async def aevaluate(<br>        self,<br>        query: Optional[str] = None,<br>        response: Optional[str] = None,<br>        contexts: Optional[Sequence[str]] = None,<br>        second_response: Optional[str] = None,<br>        reference: Optional[str] = None,<br>        sleep_time_in_seconds: int = 0,<br>        **kwargs: Any,<br>    ) -> EvaluationResult:<br>        del kwargs  # Unused<br>        del contexts  # Unused<br>        if query is None or response is None or second_response is None:<br>            raise ValueError(<br>                \"query, response, second_response, and reference must be provided\"<br>            )<br>        await asyncio.sleep(sleep_time_in_seconds)<br>        eval_result = await self._get_eval_result(<br>            query, response, second_response, reference<br>        )<br>        if self._enforce_consensus and not eval_result.invalid_result:<br>            # Flip the order of the answers and see if the answer is consistent<br>            # (which means that the score should flip from 0 to 1 and vice-versa)<br>            # if not, then we return a tie<br>            flipped_eval_result = await self._get_eval_result(<br>                query, second_response, response, reference<br>            )<br>            if not flipped_eval_result.invalid_result:<br>                resolved_eval_result = await self._resolve_results(<br>                    eval_result, flipped_eval_result<br>                )<br>            else:<br>                resolved_eval_result = EvaluationResult(<br>                    query=eval_result.query,<br>                    response=eval_result.response,<br>                    feedback=flipped_eval_result.response,<br>                    invalid_result=True,<br>                    invalid_reason=\"Output cannot be parsed.\",<br>                )<br>        else:<br>            resolved_eval_result = eval_result<br>        return resolved_eval_result<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Pairwise comparison - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/evaluation/pairwise_comparison/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/agent/llm_compiler/#llama_index.agent.llm_compiler.LLMCompilerAgentWorker)\n\n# Llm compiler\n\n## LLMCompilerAgentWorker [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/llm_compiler/\\#llama_index.agent.llm_compiler.LLMCompilerAgentWorker \"Permanent link\")\n\nBases: `BaseAgentWorker`\n\nLLMCompiler Agent Worker.\n\nLLMCompiler is an agent framework that allows async multi-function calling and query planning.\nHere is the implementation.\n\nSource Repo (paper linked): https://github.com/SqueezeAILab/LLMCompiler?tab=readme-ov-file\n\nSource code in `llama-index-integrations/agent/llama-index-agent-llm-compiler/llama_index/agent/llm_compiler/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>``` | ```<br>class LLMCompilerAgentWorker(BaseAgentWorker):<br>    \"\"\"LLMCompiler Agent Worker.<br>    LLMCompiler is an agent framework that allows async multi-function calling and query planning.<br>    Here is the implementation.<br>    Source Repo (paper linked): https://github.com/SqueezeAILab/LLMCompiler?tab=readme-ov-file<br>    \"\"\"<br>    def __init__(<br>        self,<br>        tools: Sequence[BaseTool],<br>        llm: LLM,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        planner_example_prompt_str: Optional[str] = None,<br>        stop: Optional[List[str]] = None,<br>        joiner_prompt: Optional[PromptTemplate] = None,<br>        max_replans: int = 3,<br>    ) -> None:<br>        self.callback_manager = callback_manager or llm.callback_manager<br>        self.planner_example_prompt_str = (<br>            planner_example_prompt_str or PLANNER_EXAMPLE_PROMPT<br>        )<br>        self.system_prompt = generate_llm_compiler_prompt(<br>            tools, example_prompt=self.planner_example_prompt_str<br>        )<br>        self.system_prompt_replan = generate_llm_compiler_prompt(<br>            tools, is_replan=True, example_prompt=self.planner_example_prompt_str<br>        )<br>        self.llm = llm<br>        # TODO: make tool_retriever work<br>        self.tools = tools<br>        self.output_parser = LLMCompilerPlanParser(tools=tools)<br>        self.stop = stop<br>        self.max_replans = max_replans<br>        self.verbose = verbose<br>        # joiner program<br>        self.joiner_prompt = joiner_prompt or PromptTemplate(OUTPUT_PROMPT)<br>        self.joiner_program = LLMTextCompletionProgram.from_defaults(<br>            output_parser=LLMCompilerJoinerParser(),<br>            output_cls=JoinerOutput,<br>            prompt=self.joiner_prompt,<br>            llm=self.llm,<br>            verbose=verbose,<br>        )<br>        # if len(tools) > 0 and tool_retriever is not None:<br>        #     raise ValueError(\"Cannot specify both tools and tool_retriever\")<br>        # elif len(tools) > 0:<br>        #     self._get_tools = lambda _: tools<br>        # elif tool_retriever is not None:<br>        #     tool_retriever_c = cast(ObjectRetriever[BaseTool], tool_retriever)<br>        #     self._get_tools = lambda message: tool_retriever_c.retrieve(message)<br>        # else:<br>        #     self._get_tools = lambda _: []<br>    @classmethod<br>    def from_tools(<br>        cls,<br>        tools: Optional[Sequence[BaseTool]] = None,<br>        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>        llm: Optional[LLM] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>        **kwargs: Any,<br>    ) -> \"LLMCompilerAgentWorker\":<br>        \"\"\"Convenience constructor method from set of of BaseTools (Optional).<br>        Returns:<br>            LLMCompilerAgentWorker: the LLMCompilerAgentWorker instance<br>        \"\"\"<br>        llm = llm or OpenAI(model=DEFAULT_MODEL_NAME)<br>        if callback_manager is not None:<br>            llm.callback_manager = callback_manager<br>        return cls(<br>            tools=tools or [],<br>            tool_retriever=tool_retriever,<br>            llm=llm,<br>            callback_manager=callback_manager,<br>            verbose=verbose,<br>        )<br>    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>        \"\"\"Initialize step from task.\"\"\"<br>        sources: List[ToolOutput] = []<br>        # temporary memory for new messages<br>        new_memory = ChatMemoryBuffer.from_defaults()<br>        # put user message in memory<br>        new_memory.put(ChatMessage(content=task.input, role=MessageRole.USER))<br>        # initialize task state<br>        task_state = {<br>            \"sources\": sources,<br>            \"new_memory\": new_memory,<br>        }<br>        task.extra_state.update(task_state)<br>        return TaskStep(<br>            task_id=task.task_id,<br>            step_id=str(uuid.uuid4()),<br>            input=task.input,<br>            step_state={\"is_replan\": False, \"contexts\": [], \"replans\": 0},<br>        )<br>    def get_tools(self, input: str) -> List[AsyncBaseTool]:<br>        \"\"\"Get tools.\"\"\"<br>        # return [adapt_to_async_tool(t) for t in self._get_tools(input)]<br>        return [adapt_to_async_tool(t) for t in self.tools]<br>    async def arun_llm(<br>        self,<br>        input: str,<br>        previous_context: Optional[str] = None,<br>        is_replan: bool = False,<br>    ) -> ChatResponse:<br>        \"\"\"Run LLM.\"\"\"<br>        if is_replan:<br>            system_prompt = self.system_prompt_replan<br>            assert previous_context is not None, \"previous_context cannot be None\"<br>            human_prompt = f\"Question: {input}\\n{previous_context}\\n\"<br>        else:<br>            system_prompt = self.system_prompt<br>            human_prompt = f\"Question: {input}\"<br>        messages = [<br>            ChatMessage(role=MessageRole.SYSTEM, content=system_prompt),<br>            ChatMessage(role=MessageRole.USER, content=human_prompt),<br>        ]<br>        return await self.llm.achat(messages)<br>    async def ajoin(<br>        self,<br>        input: str,<br>        tasks: Dict[int, LLMCompilerTask],<br>        is_final: bool = False,<br>    ) -> JoinerOutput:<br>        \"\"\"Join answer using LLM/agent.\"\"\"<br>        agent_scratchpad = \"\\n\\n\"<br>        agent_scratchpad += \"\".join(<br>            [<br>                task.get_thought_action_observation(<br>                    include_action=True, include_thought=True<br>                )<br>                for task in tasks.values()<br>                if not task.is_join<br>            ]<br>        )<br>        agent_scratchpad = agent_scratchpad.strip()<br>        output = self.joiner_program(<br>            query_str=input,<br>            context_str=agent_scratchpad,<br>        )<br>        output = cast(JoinerOutput, output)<br>        if self.verbose:<br>            print_text(f\"> Thought: {output.thought}\\n\", color=\"pink\")<br>            print_text(f\"> Answer: {output.answer}\\n\", color=\"pink\")<br>        if is_final:<br>            output.is_replan = False<br>        return output<br>    def _get_task_step_response(<br>        self,<br>        task: Task,<br>        llmc_tasks: Dict[int, LLMCompilerTask],<br>        answer: str,<br>        joiner_thought: str,<br>        step: TaskStep,<br>        is_replan: bool,<br>    ) -> TaskStepOutput:<br>        \"\"\"Get task step response.\"\"\"<br>        agent_answer = AgentChatResponse(response=answer, sources=[])<br>        if not is_replan:<br>            # generate final answer<br>            new_steps = []<br>            # put in memory<br>            task.extra_state[\"new_memory\"].put(<br>                ChatMessage(content=answer, role=MessageRole.ASSISTANT)<br>            )<br>        else:<br>            # Collect contexts for the subsequent replanner<br>            context = generate_context_for_replanner(<br>                tasks=llmc_tasks, joiner_thought=joiner_thought<br>            )<br>            new_contexts = step.step_state[\"contexts\"] + [context]<br>            # TODO: generate new steps<br>            new_steps = [<br>                step.get_next_step(<br>                    step_id=str(uuid.uuid4()),<br>                    input=None,<br>                    step_state={<br>                        \"is_replan\": is_replan,<br>                        \"contexts\": new_contexts,<br>                        \"replans\": step.step_state[\"replans\"] + 1,<br>                    },<br>                )<br>            ]<br>        return TaskStepOutput(<br>            output=agent_answer,<br>            task_step=step,<br>            next_steps=new_steps,<br>            is_last=not is_replan,<br>        )<br>    async def _arun_step(<br>        self,<br>        step: TaskStep,<br>        task: Task,<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        if self.verbose:<br>            print(<br>                f\"> Running step {step.step_id} for task {task.task_id}.\\n\"<br>                f\"> Step count: {step.step_state['replans']}\"<br>            )<br>        is_final_iter = (<br>            step.step_state[\"is_replan\"]<br>            and step.step_state[\"replans\"] >= self.max_replans<br>        )<br>        if len(step.step_state[\"contexts\"]) == 0:<br>            formatted_contexts = None<br>        else:<br>            formatted_contexts = format_contexts(step.step_state[\"contexts\"])<br>        llm_response = await self.arun_llm(<br>            task.input,<br>            previous_context=formatted_contexts,<br>            is_replan=step.step_state[\"is_replan\"],<br>        )<br>        if self.verbose:<br>            print_text(f\"> Plan: {llm_response.message.content}\\n\", color=\"pink\")<br>        # return task dict (will generate plan, parse into dictionary)<br>        task_dict = self.output_parser.parse(cast(str, llm_response.message.content))<br>        # execute via task executor<br>        task_fetching_unit = TaskFetchingUnit.from_tasks(<br>            task_dict, verbose=self.verbose<br>        )<br>        await task_fetching_unit.schedule()<br>        ## join tasks - get response<br>        tasks = cast(Dict[int, LLMCompilerTask], task_fetching_unit.tasks)<br>        joiner_output = await self.ajoin(<br>            task.input,<br>            tasks,<br>            is_final=is_final_iter,<br>        )<br>        # get task step response (with new steps planned)<br>        return self._get_task_step_response(<br>            task,<br>            llmc_tasks=tasks,<br>            answer=joiner_output.answer,<br>            joiner_thought=joiner_output.thought,<br>            step=step,<br>            is_replan=joiner_output.is_replan,<br>        )<br>    @trace_method(\"run_step\")<br>    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step.\"\"\"<br>        return asyncio.run(self.arun_step(step=step, task=task, **kwargs))<br>    @trace_method(\"run_step\")<br>    async def arun_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        \"\"\"Run step (async).\"\"\"<br>        return await self._arun_step(step, task)<br>    @trace_method(\"run_step\")<br>    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>        \"\"\"Run step (stream).\"\"\"<br>        # # TODO: figure out if we need a different type for TaskStepOutput<br>        # return self._run_step_stream(step, task)<br>        raise NotImplementedError<br>    @trace_method(\"run_step\")<br>    async def astream_step(<br>        self, step: TaskStep, task: Task, **kwargs: Any<br>    ) -> TaskStepOutput:<br>        raise NotImplementedError<br>        # \"\"\"Run step (async stream).\"\"\"<br>        # return await self._arun_step_stream(step, task)<br>    def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>        \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>        # add new messages to memory<br>        task.memory.put_messages(task.extra_state[\"new_memory\"].get_all())<br>        # reset new memory<br>        task.extra_state[\"new_memory\"].reset()<br>``` |\n\n### from\\_tools`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/llm_compiler/\\#llama_index.agent.llm_compiler.LLMCompilerAgentWorker.from_tools \"Permanent link\")\n\n```\nfrom_tools(tools: Optional[Sequence[BaseTool]] = None, tool_retriever: Optional[ObjectRetriever[BaseTool]] = None, llm: Optional[LLM] = None, callback_manager: Optional[CallbackManager] = None, verbose: bool = False, **kwargs: Any) -> LLMCompilerAgentWorker\n\n```\n\nConvenience constructor method from set of of BaseTools (Optional).\n\n**Returns:**\n\n| Name | Type | Description |\n| --- | --- | --- |\n| `LLMCompilerAgentWorker` | `LLMCompilerAgentWorker` | the LLMCompilerAgentWorker instance |\n\nSource code in `llama-index-integrations/agent/llama-index-agent-llm-compiler/llama_index/agent/llm_compiler/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>``` | ```<br>@classmethod<br>def from_tools(<br>    cls,<br>    tools: Optional[Sequence[BaseTool]] = None,<br>    tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,<br>    llm: Optional[LLM] = None,<br>    callback_manager: Optional[CallbackManager] = None,<br>    verbose: bool = False,<br>    **kwargs: Any,<br>) -> \"LLMCompilerAgentWorker\":<br>    \"\"\"Convenience constructor method from set of of BaseTools (Optional).<br>    Returns:<br>        LLMCompilerAgentWorker: the LLMCompilerAgentWorker instance<br>    \"\"\"<br>    llm = llm or OpenAI(model=DEFAULT_MODEL_NAME)<br>    if callback_manager is not None:<br>        llm.callback_manager = callback_manager<br>    return cls(<br>        tools=tools or [],<br>        tool_retriever=tool_retriever,<br>        llm=llm,<br>        callback_manager=callback_manager,<br>        verbose=verbose,<br>    )<br>``` |\n\n### initialize\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/llm_compiler/\\#llama_index.agent.llm_compiler.LLMCompilerAgentWorker.initialize_step \"Permanent link\")\n\n```\ninitialize_step(task: Task, **kwargs: Any) -> TaskStep\n\n```\n\nInitialize step from task.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-llm-compiler/llama_index/agent/llm_compiler/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>``` | ```<br>def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:<br>    \"\"\"Initialize step from task.\"\"\"<br>    sources: List[ToolOutput] = []<br>    # temporary memory for new messages<br>    new_memory = ChatMemoryBuffer.from_defaults()<br>    # put user message in memory<br>    new_memory.put(ChatMessage(content=task.input, role=MessageRole.USER))<br>    # initialize task state<br>    task_state = {<br>        \"sources\": sources,<br>        \"new_memory\": new_memory,<br>    }<br>    task.extra_state.update(task_state)<br>    return TaskStep(<br>        task_id=task.task_id,<br>        step_id=str(uuid.uuid4()),<br>        input=task.input,<br>        step_state={\"is_replan\": False, \"contexts\": [], \"replans\": 0},<br>    )<br>``` |\n\n### get\\_tools [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/llm_compiler/\\#llama_index.agent.llm_compiler.LLMCompilerAgentWorker.get_tools \"Permanent link\")\n\n```\nget_tools(input: str) -> List[AsyncBaseTool]\n\n```\n\nGet tools.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-llm-compiler/llama_index/agent/llm_compiler/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>239<br>240<br>241<br>242<br>``` | ```<br>def get_tools(self, input: str) -> List[AsyncBaseTool]:<br>    \"\"\"Get tools.\"\"\"<br>    # return [adapt_to_async_tool(t) for t in self._get_tools(input)]<br>    return [adapt_to_async_tool(t) for t in self.tools]<br>``` |\n\n### arun\\_llm`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/llm_compiler/\\#llama_index.agent.llm_compiler.LLMCompilerAgentWorker.arun_llm \"Permanent link\")\n\n```\narun_llm(input: str, previous_context: Optional[str] = None, is_replan: bool = False) -> ChatResponse\n\n```\n\nRun LLM.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-llm-compiler/llama_index/agent/llm_compiler/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>``` | ```<br>async def arun_llm(<br>    self,<br>    input: str,<br>    previous_context: Optional[str] = None,<br>    is_replan: bool = False,<br>) -> ChatResponse:<br>    \"\"\"Run LLM.\"\"\"<br>    if is_replan:<br>        system_prompt = self.system_prompt_replan<br>        assert previous_context is not None, \"previous_context cannot be None\"<br>        human_prompt = f\"Question: {input}\\n{previous_context}\\n\"<br>    else:<br>        system_prompt = self.system_prompt<br>        human_prompt = f\"Question: {input}\"<br>    messages = [<br>        ChatMessage(role=MessageRole.SYSTEM, content=system_prompt),<br>        ChatMessage(role=MessageRole.USER, content=human_prompt),<br>    ]<br>    return await self.llm.achat(messages)<br>``` |\n\n### ajoin`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/llm_compiler/\\#llama_index.agent.llm_compiler.LLMCompilerAgentWorker.ajoin \"Permanent link\")\n\n```\najoin(input: str, tasks: Dict[int, LLMCompilerTask], is_final: bool = False) -> JoinerOutput\n\n```\n\nJoin answer using LLM/agent.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-llm-compiler/llama_index/agent/llm_compiler/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>``` | ```<br>async def ajoin(<br>    self,<br>    input: str,<br>    tasks: Dict[int, LLMCompilerTask],<br>    is_final: bool = False,<br>) -> JoinerOutput:<br>    \"\"\"Join answer using LLM/agent.\"\"\"<br>    agent_scratchpad = \"\\n\\n\"<br>    agent_scratchpad += \"\".join(<br>        [<br>            task.get_thought_action_observation(<br>                include_action=True, include_thought=True<br>            )<br>            for task in tasks.values()<br>            if not task.is_join<br>        ]<br>    )<br>    agent_scratchpad = agent_scratchpad.strip()<br>    output = self.joiner_program(<br>        query_str=input,<br>        context_str=agent_scratchpad,<br>    )<br>    output = cast(JoinerOutput, output)<br>    if self.verbose:<br>        print_text(f\"> Thought: {output.thought}\\n\", color=\"pink\")<br>        print_text(f\"> Answer: {output.answer}\\n\", color=\"pink\")<br>    if is_final:<br>        output.is_replan = False<br>    return output<br>``` |\n\n### run\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/llm_compiler/\\#llama_index.agent.llm_compiler.LLMCompilerAgentWorker.run_step \"Permanent link\")\n\n```\nrun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-llm-compiler/llama_index/agent/llm_compiler/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>398<br>399<br>400<br>401<br>``` | ```<br>@trace_method(\"run_step\")<br>def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step.\"\"\"<br>    return asyncio.run(self.arun_step(step=step, task=task, **kwargs))<br>``` |\n\n### arun\\_step`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/llm_compiler/\\#llama_index.agent.llm_compiler.LLMCompilerAgentWorker.arun_step \"Permanent link\")\n\n```\narun_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (async).\n\nSource code in `llama-index-integrations/agent/llama-index-agent-llm-compiler/llama_index/agent/llm_compiler/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>403<br>404<br>405<br>406<br>407<br>408<br>``` | ```<br>@trace_method(\"run_step\")<br>async def arun_step(<br>    self, step: TaskStep, task: Task, **kwargs: Any<br>) -> TaskStepOutput:<br>    \"\"\"Run step (async).\"\"\"<br>    return await self._arun_step(step, task)<br>``` |\n\n### stream\\_step [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/llm_compiler/\\#llama_index.agent.llm_compiler.LLMCompilerAgentWorker.stream_step \"Permanent link\")\n\n```\nstream_step(step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput\n\n```\n\nRun step (stream).\n\nSource code in `llama-index-integrations/agent/llama-index-agent-llm-compiler/llama_index/agent/llm_compiler/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>410<br>411<br>412<br>413<br>414<br>415<br>``` | ```<br>@trace_method(\"run_step\")<br>def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:<br>    \"\"\"Run step (stream).\"\"\"<br>    # # TODO: figure out if we need a different type for TaskStepOutput<br>    # return self._run_step_stream(step, task)<br>    raise NotImplementedError<br>``` |\n\n### finalize\\_task [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/llm_compiler/\\#llama_index.agent.llm_compiler.LLMCompilerAgentWorker.finalize_task \"Permanent link\")\n\n```\nfinalize_task(task: Task, **kwargs: Any) -> None\n\n```\n\nFinalize task, after all the steps are completed.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-llm-compiler/llama_index/agent/llm_compiler/step.py`\n\n|     |     |\n| --- | --- |\n| ```<br>425<br>426<br>427<br>428<br>429<br>430<br>``` | ```<br>def finalize_task(self, task: Task, **kwargs: Any) -> None:<br>    \"\"\"Finalize task, after all the steps are completed.\"\"\"<br>    # add new messages to memory<br>    task.memory.put_messages(task.extra_state[\"new_memory\"].get_all())<br>    # reset new memory<br>    task.extra_state[\"new_memory\"].reset()<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Llm compiler - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/agent/llm_compiler/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai_legacy/#llama_index.agent.openai_legacy.ContextRetrieverOpenAIAgent)\n\n# Openai legacy\n\n## ContextRetrieverOpenAIAgent [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai_legacy/\\#llama_index.agent.openai_legacy.ContextRetrieverOpenAIAgent \"Permanent link\")\n\nBases: `BaseOpenAIAgent`\n\nContextRetriever OpenAI Agent.\n\nThis agent performs retrieval from BaseRetriever before\ncalling the LLM. Allows it to augment user message with context.\n\nNOTE: this is a beta feature, function interfaces might change.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `tools` | `List[BaseTool]` | A list of tools. | _required_ |\n| `retriever` | `BaseRetriever` | A retriever. | _required_ |\n| `qa_prompt` | `Optional[PromptTemplate]` | A QA prompt. | _required_ |\n| `context_separator` | `str` | A context separator. | _required_ |\n| `llm` | `Optional[OpenAI]` | An OpenAI LLM. | _required_ |\n| `chat_history` | `Optional[List[ChatMessage]]` | A chat history. | _required_ |\n| `prefix_messages` | `List[ChatMessage]` | List\\[ChatMessage\\]: A list of prefix messages. | _required_ |\n| `verbose` | `bool` | Whether to print debug statements. | `False` |\n| `max_function_calls` | `int` | Maximum number of function calls. | `DEFAULT_MAX_FUNCTION_CALLS` |\n| `callback_manager` | `Optional[CallbackManager]` | A callback manager. | `None` |\n\nSource code in `llama-index-integrations/agent/llama-index-agent-openai-legacy/llama_index/agent/openai_legacy/context_retriever_agent.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>``` | ```<br>class ContextRetrieverOpenAIAgent(BaseOpenAIAgent):<br>    \"\"\"ContextRetriever OpenAI Agent.<br>    This agent performs retrieval from BaseRetriever before<br>    calling the LLM. Allows it to augment user message with context.<br>    NOTE: this is a beta feature, function interfaces might change.<br>    Args:<br>        tools (List[BaseTool]): A list of tools.<br>        retriever (BaseRetriever): A retriever.<br>        qa_prompt (Optional[PromptTemplate]): A QA prompt.<br>        context_separator (str): A context separator.<br>        llm (Optional[OpenAI]): An OpenAI LLM.<br>        chat_history (Optional[List[ChatMessage]]): A chat history.<br>        prefix_messages: List[ChatMessage]: A list of prefix messages.<br>        verbose (bool): Whether to print debug statements.<br>        max_function_calls (int): Maximum number of function calls.<br>        callback_manager (Optional[CallbackManager]): A callback manager.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        tools: List[BaseTool],<br>        retriever: BaseRetriever,<br>        qa_prompt: PromptTemplate,<br>        context_separator: str,<br>        llm: OpenAI,<br>        memory: BaseMemory,<br>        prefix_messages: List[ChatMessage],<br>        verbose: bool = False,<br>        max_function_calls: int = DEFAULT_MAX_FUNCTION_CALLS,<br>        callback_manager: Optional[CallbackManager] = None,<br>    ) -> None:<br>        super().__init__(<br>            llm=llm,<br>            memory=memory,<br>            prefix_messages=prefix_messages,<br>            verbose=verbose,<br>            max_function_calls=max_function_calls,<br>            callback_manager=callback_manager,<br>        )<br>        self._tools = tools<br>        self._qa_prompt = qa_prompt<br>        self._retriever = retriever<br>        self._context_separator = context_separator<br>    @classmethod<br>    def from_tools_and_retriever(<br>        cls,<br>        tools: List[BaseTool],<br>        retriever: BaseRetriever,<br>        qa_prompt: Optional[PromptTemplate] = None,<br>        context_separator: str = \"\\n\",<br>        llm: Optional[LLM] = None,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        memory: Optional[BaseMemory] = None,<br>        memory_cls: Type[BaseMemory] = ChatMemoryBuffer,<br>        verbose: bool = False,<br>        max_function_calls: int = DEFAULT_MAX_FUNCTION_CALLS,<br>        callback_manager: Optional[CallbackManager] = None,<br>        system_prompt: Optional[str] = None,<br>        prefix_messages: Optional[List[ChatMessage]] = None,<br>    ) -> \"ContextRetrieverOpenAIAgent\":<br>        \"\"\"Create a ContextRetrieverOpenAIAgent from a retriever.<br>        Args:<br>            retriever (BaseRetriever): A retriever.<br>            qa_prompt (Optional[PromptTemplate]): A QA prompt.<br>            context_separator (str): A context separator.<br>            llm (Optional[OpenAI]): An OpenAI LLM.<br>            chat_history (Optional[ChatMessageHistory]): A chat history.<br>            verbose (bool): Whether to print debug statements.<br>            max_function_calls (int): Maximum number of function calls.<br>            callback_manager (Optional[CallbackManager]): A callback manager.<br>        \"\"\"<br>        qa_prompt = qa_prompt or DEFAULT_QA_PROMPT<br>        chat_history = chat_history or []<br>        llm = llm or Settings.llm<br>        if not isinstance(llm, OpenAI):<br>            raise ValueError(\"llm must be a OpenAI instance\")<br>        if callback_manager is not None:<br>            llm.callback_manager = callback_manager<br>        memory = memory or memory_cls.from_defaults(chat_history=chat_history, llm=llm)<br>        if not is_function_calling_model(llm.model):<br>            raise ValueError(<br>                f\"Model name {llm.model} does not support function calling API.\"<br>            )<br>        if system_prompt is not None:<br>            if prefix_messages is not None:<br>                raise ValueError(<br>                    \"Cannot specify both system_prompt and prefix_messages\"<br>                )<br>            prefix_messages = [ChatMessage(content=system_prompt, role=\"system\")]<br>        prefix_messages = prefix_messages or []<br>        return cls(<br>            tools=tools,<br>            retriever=retriever,<br>            qa_prompt=qa_prompt,<br>            context_separator=context_separator,<br>            llm=llm,<br>            memory=memory,<br>            prefix_messages=prefix_messages,<br>            verbose=verbose,<br>            max_function_calls=max_function_calls,<br>            callback_manager=callback_manager,<br>        )<br>    def _get_tools(self, message: str) -> List[BaseTool]:<br>        \"\"\"Get tools.\"\"\"<br>        return self._tools<br>    def _build_formatted_message(self, message: str) -> str:<br>        # augment user message<br>        retrieved_nodes_w_scores: List[NodeWithScore] = self._retriever.retrieve(<br>            message<br>        )<br>        retrieved_nodes = [node.node for node in retrieved_nodes_w_scores]<br>        retrieved_texts = [node.get_content() for node in retrieved_nodes]<br>        # format message<br>        context_str = self._context_separator.join(retrieved_texts)<br>        return self._qa_prompt.format(context_str=context_str, query_str=message)<br>    def chat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Union[str, dict] = \"auto\",<br>    ) -> AgentChatResponse:<br>        \"\"\"Chat.\"\"\"<br>        formatted_message = self._build_formatted_message(message)<br>        if self._verbose:<br>            print_text(formatted_message + \"\\n\", color=\"yellow\")<br>        return super().chat(<br>            formatted_message, chat_history=chat_history, tool_choice=tool_choice<br>        )<br>    async def achat(<br>        self,<br>        message: str,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        tool_choice: Union[str, dict] = \"auto\",<br>    ) -> AgentChatResponse:<br>        \"\"\"Chat.\"\"\"<br>        formatted_message = self._build_formatted_message(message)<br>        if self._verbose:<br>            print_text(formatted_message + \"\\n\", color=\"yellow\")<br>        return await super().achat(<br>            formatted_message, chat_history=chat_history, tool_choice=tool_choice<br>        )<br>    def get_tools(self, message: str) -> List[BaseTool]:<br>        \"\"\"Get tools.\"\"\"<br>        return self._get_tools(message)<br>``` |\n\n### from\\_tools\\_and\\_retriever`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai_legacy/\\#llama_index.agent.openai_legacy.ContextRetrieverOpenAIAgent.from_tools_and_retriever \"Permanent link\")\n\n```\nfrom_tools_and_retriever(tools: List[BaseTool], retriever: BaseRetriever, qa_prompt: Optional[PromptTemplate] = None, context_separator: str = '\\n', llm: Optional[LLM] = None, chat_history: Optional[List[ChatMessage]] = None, memory: Optional[BaseMemory] = None, memory_cls: Type[BaseMemory] = ChatMemoryBuffer, verbose: bool = False, max_function_calls: int = DEFAULT_MAX_FUNCTION_CALLS, callback_manager: Optional[CallbackManager] = None, system_prompt: Optional[str] = None, prefix_messages: Optional[List[ChatMessage]] = None) -> ContextRetrieverOpenAIAgent\n\n```\n\nCreate a ContextRetrieverOpenAIAgent from a retriever.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `retriever` | `BaseRetriever` | A retriever. | _required_ |\n| `qa_prompt` | `Optional[PromptTemplate]` | A QA prompt. | `None` |\n| `context_separator` | `str` | A context separator. | `'\\n'` |\n| `llm` | `Optional[OpenAI]` | An OpenAI LLM. | `None` |\n| `chat_history` | `Optional[ChatMessageHistory]` | A chat history. | `None` |\n| `verbose` | `bool` | Whether to print debug statements. | `False` |\n| `max_function_calls` | `int` | Maximum number of function calls. | `DEFAULT_MAX_FUNCTION_CALLS` |\n| `callback_manager` | `Optional[CallbackManager]` | A callback manager. | `None` |\n\nSource code in `llama-index-integrations/agent/llama-index-agent-openai-legacy/llama_index/agent/openai_legacy/context_retriever_agent.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>``` | ```<br>@classmethod<br>def from_tools_and_retriever(<br>    cls,<br>    tools: List[BaseTool],<br>    retriever: BaseRetriever,<br>    qa_prompt: Optional[PromptTemplate] = None,<br>    context_separator: str = \"\\n\",<br>    llm: Optional[LLM] = None,<br>    chat_history: Optional[List[ChatMessage]] = None,<br>    memory: Optional[BaseMemory] = None,<br>    memory_cls: Type[BaseMemory] = ChatMemoryBuffer,<br>    verbose: bool = False,<br>    max_function_calls: int = DEFAULT_MAX_FUNCTION_CALLS,<br>    callback_manager: Optional[CallbackManager] = None,<br>    system_prompt: Optional[str] = None,<br>    prefix_messages: Optional[List[ChatMessage]] = None,<br>) -> \"ContextRetrieverOpenAIAgent\":<br>    \"\"\"Create a ContextRetrieverOpenAIAgent from a retriever.<br>    Args:<br>        retriever (BaseRetriever): A retriever.<br>        qa_prompt (Optional[PromptTemplate]): A QA prompt.<br>        context_separator (str): A context separator.<br>        llm (Optional[OpenAI]): An OpenAI LLM.<br>        chat_history (Optional[ChatMessageHistory]): A chat history.<br>        verbose (bool): Whether to print debug statements.<br>        max_function_calls (int): Maximum number of function calls.<br>        callback_manager (Optional[CallbackManager]): A callback manager.<br>    \"\"\"<br>    qa_prompt = qa_prompt or DEFAULT_QA_PROMPT<br>    chat_history = chat_history or []<br>    llm = llm or Settings.llm<br>    if not isinstance(llm, OpenAI):<br>        raise ValueError(\"llm must be a OpenAI instance\")<br>    if callback_manager is not None:<br>        llm.callback_manager = callback_manager<br>    memory = memory or memory_cls.from_defaults(chat_history=chat_history, llm=llm)<br>    if not is_function_calling_model(llm.model):<br>        raise ValueError(<br>            f\"Model name {llm.model} does not support function calling API.\"<br>        )<br>    if system_prompt is not None:<br>        if prefix_messages is not None:<br>            raise ValueError(<br>                \"Cannot specify both system_prompt and prefix_messages\"<br>            )<br>        prefix_messages = [ChatMessage(content=system_prompt, role=\"system\")]<br>    prefix_messages = prefix_messages or []<br>    return cls(<br>        tools=tools,<br>        retriever=retriever,<br>        qa_prompt=qa_prompt,<br>        context_separator=context_separator,<br>        llm=llm,<br>        memory=memory,<br>        prefix_messages=prefix_messages,<br>        verbose=verbose,<br>        max_function_calls=max_function_calls,<br>        callback_manager=callback_manager,<br>    )<br>``` |\n\n### chat [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai_legacy/\\#llama_index.agent.openai_legacy.ContextRetrieverOpenAIAgent.chat \"Permanent link\")\n\n```\nchat(message: str, chat_history: Optional[List[ChatMessage]] = None, tool_choice: Union[str, dict] = 'auto') -> AgentChatResponse\n\n```\n\nChat.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-openai-legacy/llama_index/agent/openai_legacy/context_retriever_agent.py`\n\n|     |     |\n| --- | --- |\n| ```<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>``` | ```<br>def chat(<br>    self,<br>    message: str,<br>    chat_history: Optional[List[ChatMessage]] = None,<br>    tool_choice: Union[str, dict] = \"auto\",<br>) -> AgentChatResponse:<br>    \"\"\"Chat.\"\"\"<br>    formatted_message = self._build_formatted_message(message)<br>    if self._verbose:<br>        print_text(formatted_message + \"\\n\", color=\"yellow\")<br>    return super().chat(<br>        formatted_message, chat_history=chat_history, tool_choice=tool_choice<br>    )<br>``` |\n\n### achat`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai_legacy/\\#llama_index.agent.openai_legacy.ContextRetrieverOpenAIAgent.achat \"Permanent link\")\n\n```\nachat(message: str, chat_history: Optional[List[ChatMessage]] = None, tool_choice: Union[str, dict] = 'auto') -> AgentChatResponse\n\n```\n\nChat.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-openai-legacy/llama_index/agent/openai_legacy/context_retriever_agent.py`\n\n|     |     |\n| --- | --- |\n| ```<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>``` | ```<br>async def achat(<br>    self,<br>    message: str,<br>    chat_history: Optional[List[ChatMessage]] = None,<br>    tool_choice: Union[str, dict] = \"auto\",<br>) -> AgentChatResponse:<br>    \"\"\"Chat.\"\"\"<br>    formatted_message = self._build_formatted_message(message)<br>    if self._verbose:<br>        print_text(formatted_message + \"\\n\", color=\"yellow\")<br>    return await super().achat(<br>        formatted_message, chat_history=chat_history, tool_choice=tool_choice<br>    )<br>``` |\n\n### get\\_tools [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai_legacy/\\#llama_index.agent.openai_legacy.ContextRetrieverOpenAIAgent.get_tools \"Permanent link\")\n\n```\nget_tools(message: str) -> List[BaseTool]\n\n```\n\nGet tools.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-openai-legacy/llama_index/agent/openai_legacy/context_retriever_agent.py`\n\n|     |     |\n| --- | --- |\n| ```<br>197<br>198<br>199<br>``` | ```<br>def get_tools(self, message: str) -> List[BaseTool]:<br>    \"\"\"Get tools.\"\"\"<br>    return self._get_tools(message)<br>``` |\n\n## FnRetrieverOpenAIAgent [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/openai_legacy/\\#llama_index.agent.openai_legacy.FnRetrieverOpenAIAgent \"Permanent link\")\n\nBases: `OpenAIAgent`\n\nFunction Retriever OpenAI Agent.\n\nUses our object retriever module to retrieve openai agent.\n\nNOTE: This is deprecated, you can just use the base `OpenAIAgent` class by\nspecifying the following:\n\n```\nagent = OpenAIAgent.from_tools(tool_retriever=retriever, ...)\n\n```\n\nSource code in `llama-index-integrations/agent/llama-index-agent-openai-legacy/llama_index/agent/openai_legacy/retriever_openai_agent.py`\n\n|     |     |\n| --- | --- |\n| ```<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>``` | ````<br>class FnRetrieverOpenAIAgent(OpenAIAgent):<br>    \"\"\"Function Retriever OpenAI Agent.<br>    Uses our object retriever module to retrieve openai agent.<br>    NOTE: This is deprecated, you can just use the base `OpenAIAgent` class by<br>    specifying the following:<br>    ```<br>    agent = OpenAIAgent.from_tools(tool_retriever=retriever, ...)<br>    ```<br>    \"\"\"<br>    @classmethod<br>    def from_retriever(<br>        cls, retriever: ObjectRetriever[BaseTool], **kwargs: Any<br>    ) -> \"FnRetrieverOpenAIAgent\":<br>        return cast(<br>            FnRetrieverOpenAIAgent, cls.from_tools(tool_retriever=retriever, **kwargs)<br>        )<br>```` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Openai legacy - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/agent/openai_legacy/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/deepeval/#llama_index.callbacks.deepeval.deepeval_callback_handler)\n\n# Deepeval\n\n## deepeval\\_callback\\_handler [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/deepeval/\\#llama_index.callbacks.deepeval.deepeval_callback_handler \"Permanent link\")\n\n```\ndeepeval_callback_handler(**eval_params: Any) -> BaseCallbackHandler\n\n```\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-deepeval/llama_index/callbacks/deepeval/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>8<br>9<br>``` | ```<br>def deepeval_callback_handler(**eval_params: Any) -> BaseCallbackHandler:<br>    return LlamaIndexCallbackHandler(**eval_params)<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Deepeval - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/deepeval/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/instructor/#llama_index.embeddings.instructor.InstructorEmbedding)\n\n# Instructor\n\n## InstructorEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/instructor/\\#llama_index.embeddings.instructor.InstructorEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-instructor/llama_index/embeddings/instructor/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>89<br>90<br>91<br>92<br>93<br>94<br>95<br>96<br>97<br>``` | ```<br>class InstructorEmbedding(BaseEmbedding):<br>    query_instruction: Optional[str] = Field(<br>        description=\"Instruction to prepend to query text.\"<br>    )<br>    text_instruction: Optional[str] = Field(<br>        description=\"Instruction to prepend to text.\"<br>    )<br>    cache_folder: Optional[str] = Field(<br>        description=\"Cache folder for huggingface files.\"<br>    )<br>    _model: Any = PrivateAttr()<br>    def __init__(<br>        self,<br>        model_name: str = DEFAULT_INSTRUCT_MODEL,<br>        query_instruction: Optional[str] = None,<br>        text_instruction: Optional[str] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        cache_folder: Optional[str] = None,<br>        device: Optional[str] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>    ):<br>        super().__init__(<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            model_name=model_name,<br>            query_instruction=query_instruction,<br>            text_instruction=text_instruction,<br>            cache_folder=cache_folder,<br>        )<br>        self._model = INSTRUCTOR(model_name, cache_folder=cache_folder, device=device)<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"InstructorEmbedding\"<br>    def _format_query_text(self, query_text: str) -> List[str]:<br>        \"\"\"Format query text.\"\"\"<br>        instruction = self.query_instruction<br>        if instruction is None:<br>            instruction = get_query_instruct_for_model_name(self.model_name)<br>        return [instruction, query_text]<br>    def _format_text(self, text: str) -> List[str]:<br>        \"\"\"Format text.\"\"\"<br>        instruction = self.text_instruction<br>        if instruction is None:<br>            instruction = get_text_instruct_for_model_name(self.model_name)<br>        return [instruction, text]<br>    def _embed(self, instruct_sentence_pairs: List[List[str]]) -> List[List[float]]:<br>        \"\"\"Embed sentences.\"\"\"<br>        return self._model.encode(instruct_sentence_pairs).tolist()<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        query_pair = self._format_query_text(query)<br>        return self._embed([query_pair])[0]<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding async.\"\"\"<br>        return self._get_query_embedding(query)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding async.\"\"\"<br>        return self._get_text_embedding(text)<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        text_pair = self._format_text(text)<br>        return self._embed([text_pair])[0]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        text_pairs = [self._format_text(text) for text in texts]<br>        return self._embed(text_pairs)<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Instructor - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/instructor/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/bedrock/#llama_index.embeddings.bedrock.BedrockEmbedding)\n\n# Bedrock\n\n## BedrockEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/bedrock/\\#llama_index.embeddings.bedrock.BedrockEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-bedrock/llama_index/embeddings/bedrock/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>``` | ```<br>class BedrockEmbedding(BaseEmbedding):<br>    model_name: str = Field(description=\"The modelId of the Bedrock model to use.\")<br>    profile_name: Optional[str] = Field(<br>        description=\"The name of aws profile to use. If not given, then the default profile is used.\",<br>    )<br>    aws_access_key_id: Optional[str] = Field(description=\"AWS Access Key ID to use\")<br>    aws_secret_access_key: Optional[str] = Field(<br>        description=\"AWS Secret Access Key to use\"<br>    )<br>    aws_session_token: Optional[str] = Field(description=\"AWS Session Token to use\")<br>    region_name: Optional[str] = Field(<br>        description=\"AWS region name to use. Uses region configured in AWS CLI if not passed\",<br>    )<br>    botocore_session: Optional[Any] = Field(<br>        description=\"Use this Botocore session instead of creating a new default one.\",<br>        exclude=True,<br>    )<br>    botocore_config: Optional[Any] = Field(<br>        description=\"Custom configuration object to use instead of the default generated one.\",<br>        exclude=True,<br>    )<br>    max_retries: int = Field(<br>        default=10, description=\"The maximum number of API retries.\", gt=0<br>    )<br>    timeout: float = Field(<br>        default=60.0,<br>        description=\"The timeout for the Bedrock API request in seconds. It will be used for both connect and read timeouts.\",<br>    )<br>    additional_kwargs: Dict[str, Any] = Field(<br>        default_factory=dict, description=\"Additional kwargs for the bedrock client.\"<br>    )<br>    _client: Any = PrivateAttr()<br>    def __init__(<br>        self,<br>        model_name: str = Models.TITAN_EMBEDDING,<br>        profile_name: Optional[str] = None,<br>        aws_access_key_id: Optional[str] = None,<br>        aws_secret_access_key: Optional[str] = None,<br>        aws_session_token: Optional[str] = None,<br>        region_name: Optional[str] = None,<br>        client: Optional[Any] = None,<br>        botocore_session: Optional[Any] = None,<br>        botocore_config: Optional[Any] = None,<br>        additional_kwargs: Optional[Dict[str, Any]] = None,<br>        max_retries: int = 10,<br>        timeout: float = 60.0,<br>        callback_manager: Optional[CallbackManager] = None,<br>        # base class<br>        system_prompt: Optional[str] = None,<br>        messages_to_prompt: Optional[Callable[[Sequence[ChatMessage]], str]] = None,<br>        completion_to_prompt: Optional[Callable[[str], str]] = None,<br>        pydantic_program_mode: PydanticProgramMode = PydanticProgramMode.DEFAULT,<br>        output_parser: Optional[BaseOutputParser] = None,<br>        **kwargs: Any,<br>    ):<br>        additional_kwargs = additional_kwargs or {}<br>        session_kwargs = {<br>            \"profile_name\": profile_name,<br>            \"region_name\": region_name,<br>            \"aws_access_key_id\": aws_access_key_id,<br>            \"aws_secret_access_key\": aws_secret_access_key,<br>            \"aws_session_token\": aws_session_token,<br>            \"botocore_session\": botocore_session,<br>        }<br>        try:<br>            import boto3<br>            from botocore.config import Config<br>            config = (<br>                Config(<br>                    retries={\"max_attempts\": max_retries, \"mode\": \"standard\"},<br>                    connect_timeout=timeout,<br>                    read_timeout=timeout,<br>                )<br>                if botocore_config is None<br>                else botocore_config<br>            )<br>            session = boto3.Session(**session_kwargs)<br>        except ImportError:<br>            raise ImportError(<br>                \"boto3 package not found, install with\" \"'pip install boto3'\"<br>            )<br>        super().__init__(<br>            model_name=model_name,<br>            max_retries=max_retries,<br>            timeout=timeout,<br>            botocore_config=config,<br>            profile_name=profile_name,<br>            aws_access_key_id=aws_access_key_id,<br>            aws_secret_access_key=aws_secret_access_key,<br>            aws_session_token=aws_session_token,<br>            region_name=region_name,<br>            botocore_session=botocore_session,<br>            additional_kwargs=additional_kwargs,<br>            callback_manager=callback_manager,<br>            system_prompt=system_prompt,<br>            messages_to_prompt=messages_to_prompt,<br>            completion_to_prompt=completion_to_prompt,<br>            pydantic_program_mode=pydantic_program_mode,<br>            output_parser=output_parser,<br>            **kwargs,<br>        )<br>        # Prior to general availability, custom boto3 wheel files were<br>        # distributed that used the bedrock service to invokeModel.<br>        # This check prevents any services still using those wheel files<br>        # from breaking<br>        if client is not None:<br>            self._client = client<br>        elif \"bedrock-runtime\" in session.get_available_services():<br>            self._client = session.client(\"bedrock-runtime\", config=config)<br>        else:<br>            self._client = session.client(\"bedrock\", config=config)<br>    @staticmethod<br>    def list_supported_models() -> Dict[str, List[str]]:<br>        list_models = {}<br>        for provider in PROVIDERS:<br>            list_models[provider.value] = [<br>                m.value for m in Models if provider.value in m.value<br>            ]<br>        return list_models<br>    @classmethod<br>    def class_name(self) -> str:<br>        return \"BedrockEmbedding\"<br>    @deprecated(<br>        version=\"0.9.48\",<br>        reason=(<br>            \"Use the provided kwargs in the constructor, \"<br>            \"set_credentials will be removed in future releases.\"<br>        ),<br>        action=\"once\",<br>    )<br>    def set_credentials(<br>        self,<br>        aws_region: Optional[str] = None,<br>        aws_access_key_id: Optional[str] = None,<br>        aws_secret_access_key: Optional[str] = None,<br>        aws_session_token: Optional[str] = None,<br>        aws_profile: Optional[str] = None,<br>    ) -> None:<br>        aws_region = aws_region or os.getenv(\"AWS_REGION\")<br>        aws_access_key_id = aws_access_key_id or os.getenv(\"AWS_ACCESS_KEY_ID\")<br>        aws_secret_access_key = aws_secret_access_key or os.getenv(<br>            \"AWS_SECRET_ACCESS_KEY\"<br>        )<br>        aws_session_token = aws_session_token or os.getenv(\"AWS_SESSION_TOKEN\")<br>        if aws_region is None:<br>            warnings.warn(<br>                \"AWS_REGION not found. Set environment variable AWS_REGION or set aws_region\"<br>            )<br>        if aws_access_key_id is None:<br>            warnings.warn(<br>                \"AWS_ACCESS_KEY_ID not found. Set environment variable AWS_ACCESS_KEY_ID or set aws_access_key_id\"<br>            )<br>            assert aws_access_key_id is not None<br>        if aws_secret_access_key is None:<br>            warnings.warn(<br>                \"AWS_SECRET_ACCESS_KEY not found. Set environment variable AWS_SECRET_ACCESS_KEY or set aws_secret_access_key\"<br>            )<br>            assert aws_secret_access_key is not None<br>        if aws_session_token is None:<br>            warnings.warn(<br>                \"AWS_SESSION_TOKEN not found. Set environment variable AWS_SESSION_TOKEN or set aws_session_token\"<br>            )<br>            assert aws_session_token is not None<br>        session_kwargs = {<br>            \"profile_name\": aws_profile,<br>            \"region_name\": aws_region,<br>            \"aws_access_key_id\": aws_access_key_id,<br>            \"aws_secret_access_key\": aws_secret_access_key,<br>            \"aws_session_token\": aws_session_token,<br>        }<br>        try:<br>            import boto3<br>            session = boto3.Session(**session_kwargs)<br>        except ImportError:<br>            raise ImportError(<br>                \"boto3 package not found, install with\" \"'pip install boto3'\"<br>            )<br>        if \"bedrock-runtime\" in session.get_available_services():<br>            self._client = session.client(\"bedrock-runtime\")<br>        else:<br>            self._client = session.client(\"bedrock\")<br>    @classmethod<br>    @deprecated(<br>        version=\"0.9.48\",<br>        reason=(<br>            \"Use the provided kwargs in the constructor, \"<br>            \"set_credentials will be removed in future releases.\"<br>        ),<br>        action=\"once\",<br>    )<br>    def from_credentials(<br>        cls,<br>        model_name: str = Models.TITAN_EMBEDDING,<br>        aws_region: Optional[str] = None,<br>        aws_access_key_id: Optional[str] = None,<br>        aws_secret_access_key: Optional[str] = None,<br>        aws_session_token: Optional[str] = None,<br>        aws_profile: Optional[str] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>        verbose: bool = False,<br>    ) -> \"BedrockEmbedding\":<br>        \"\"\"<br>        Instantiate using AWS credentials.<br>        Args:<br>            model_name (str) : Name of the model<br>            aws_access_key_id (str): AWS access key ID<br>            aws_secret_access_key (str): AWS secret access key<br>            aws_session_token (str): AWS session token<br>            aws_region (str): AWS region where the service is located<br>            aws_profile (str): AWS profile, when None, default profile is chosen automatically<br>        Example:<br>                .. code-block:: python<br>                    from llama_index.embeddings import BedrockEmbedding<br>                    # Define the model name<br>                    model_name = \"your_model_name\"<br>                    embeddings = BedrockEmbedding.from_credentials(<br>                        model_name,<br>                        aws_access_key_id,<br>                        aws_secret_access_key,<br>                        aws_session_token,<br>                        aws_region,<br>                        aws_profile,<br>                    )<br>        \"\"\"<br>        session_kwargs = {<br>            \"profile_name\": aws_profile,<br>            \"region_name\": aws_region,<br>            \"aws_access_key_id\": aws_access_key_id,<br>            \"aws_secret_access_key\": aws_secret_access_key,<br>            \"aws_session_token\": aws_session_token,<br>        }<br>        try:<br>            import boto3<br>            session = boto3.Session(**session_kwargs)<br>        except ImportError:<br>            raise ImportError(<br>                \"boto3 package not found, install with\" \"'pip install boto3'\"<br>            )<br>        if \"bedrock-runtime\" in session.get_available_services():<br>            client = session.client(\"bedrock-runtime\")<br>        else:<br>            client = session.client(\"bedrock\")<br>        return cls(<br>            client=client,<br>            model=model_name,<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            verbose=verbose,<br>        )<br>    def _get_embedding(<br>        self, payload: Union[str, List[str]], type: Literal[\"text\", \"query\"]<br>    ) -> Union[Embedding, List[Embedding]]:<br>        \"\"\"Get the embedding for the given payload.<br>        Args:<br>            payload (Union[str, List[str]]): The text or list of texts for which the embeddings are to be obtained.<br>            type (Literal[&quot;text&quot;, &quot;query&quot;]): The type of the payload. It can be either \"text\" or \"query\".<br>        Returns:<br>            Union[Embedding, List[Embedding]]: The embedding or list of embeddings for the given payload. If the payload is a list of strings, then the response will be a list of embeddings.<br>        \"\"\"<br>        if self._client is None:<br>            self.set_credentials()<br>        if self._client is None:<br>            raise ValueError(\"Client not set\")<br>        provider = self.model_name.split(\".\")[0]<br>        request_body = self._get_request_body(provider, payload, type)<br>        response = self._client.invoke_model(<br>            body=request_body,<br>            modelId=self.model_name,<br>            accept=\"application/json\",<br>            contentType=\"application/json\",<br>        )<br>        resp = json.loads(response.get(\"body\").read().decode(\"utf-8\"))<br>        identifiers = PROVIDER_SPECIFIC_IDENTIFIERS.get(provider, None)<br>        if identifiers is None:<br>            raise ValueError(\"Provider not supported\")<br>        return identifiers[\"get_embeddings_func\"](resp, isinstance(payload, list))<br>    def _get_query_embedding(self, query: str) -> Embedding:<br>        return self._get_embedding(query, \"query\")<br>    def _get_text_embedding(self, text: str) -> Embedding:<br>        return self._get_embedding(text, \"text\")<br>    def _get_text_embeddings(self, texts: List[str]) -> List[Embedding]:<br>        provider = self.model_name.split(\".\")[0]<br>        if provider == PROVIDERS.COHERE:<br>            return self._get_embedding(texts, \"text\")<br>        return super()._get_text_embeddings(texts)<br>    def _get_request_body(<br>        self,<br>        provider: str,<br>        payload: Union[str, List[str]],<br>        input_type: Literal[\"text\", \"query\"],<br>    ) -> Any:<br>        \"\"\"Build the request body as per the provider.<br>        Currently supported providers are amazon, cohere.<br>        amazon:<br>            Sample Payload of type str<br>            \"Hello World!\"<br>        cohere:<br>            Sample Payload of type dict of following format<br>            {<br>                'texts': [\"This is a test document\", \"This is another document\"],<br>                'input_type': 'search_document'<br>            }<br>        \"\"\"<br>        if provider == PROVIDERS.AMAZON:<br>            if isinstance(payload, list):<br>                raise ValueError(\"Amazon provider does not support list of texts\")<br>            titan_body_request = {\"inputText\": payload}<br>            # Titan Embedding V2.0 has additional body parameters to check.<br>            if \"dimensions\" in self.additional_kwargs:<br>                if self.model_name == Models.TITAN_EMBEDDING_V2_0:<br>                    titan_body_request[\"dimensions\"] = self.additional_kwargs[<br>                        \"dimensions\"<br>                    ]<br>                else:<br>                    raise ValueError(<br>                        \"'dimensions' param not supported outside of 'titan-embed-text-v2:0' model.\"<br>                    )<br>            if \"normalize\" in self.additional_kwargs:<br>                if self.model_name == Models.TITAN_EMBEDDING_V2_0:<br>                    titan_body_request[\"normalize\"] = self.additional_kwargs[<br>                        \"normalize\"<br>                    ]<br>                else:<br>                    raise ValueError(<br>                        \"'normalize' param not supported outside of 'titan-embed-text-v2:0' model.\"<br>                    )<br>            request_body = json.dumps(titan_body_request)<br>        elif provider == PROVIDERS.COHERE:<br>            input_types = {<br>                \"text\": \"search_document\",<br>                \"query\": \"search_query\",<br>            }<br>            payload = [payload] if isinstance(payload, str) else payload<br>            payload = [p[:2048] if len(p) > 2048 else p for p in payload]<br>            request_body = json.dumps(<br>                {<br>                    \"texts\": payload,<br>                    \"input_type\": input_types[input_type],<br>                }<br>            )<br>        else:<br>            raise ValueError(\"Provider not supported\")<br>        return request_body<br>    async def _aget_query_embedding(self, query: str) -> Embedding:<br>        return self._get_embedding(query, \"query\")<br>    async def _aget_text_embedding(self, text: str) -> Embedding:<br>        return self._get_embedding(text, \"text\")<br>``` |\n\n### from\\_credentials`classmethod`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/bedrock/\\#llama_index.embeddings.bedrock.BedrockEmbedding.from_credentials \"Permanent link\")\n\n```\nfrom_credentials(model_name: str = Models.TITAN_EMBEDDING, aws_region: Optional[str] = None, aws_access_key_id: Optional[str] = None, aws_secret_access_key: Optional[str] = None, aws_session_token: Optional[str] = None, aws_profile: Optional[str] = None, embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE, callback_manager: Optional[CallbackManager] = None, verbose: bool = False) -> BedrockEmbedding\n\n```\n\nInstantiate using AWS credentials.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model_name` | `str) ` | Name of the model | `TITAN_EMBEDDING` |\n| `aws_access_key_id` | `str` | AWS access key ID | `None` |\n| `aws_secret_access_key` | `str` | AWS secret access key | `None` |\n| `aws_session_token` | `str` | AWS session token | `None` |\n| `aws_region` | `str` | AWS region where the service is located | `None` |\n| `aws_profile` | `str` | AWS profile, when None, default profile is chosen automatically | `None` |\n\nExample\n\n.. code-block:: python\n\n```\nfrom llama_index.embeddings import BedrockEmbedding\n\n# Define the model name\nmodel_name = \"your_model_name\"\n\nembeddings = BedrockEmbedding.from_credentials(\n    model_name,\n    aws_access_key_id,\n    aws_secret_access_key,\n    aws_session_token,\n    aws_region,\n    aws_profile,\n)\n\n```\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-bedrock/llama_index/embeddings/bedrock/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>``` | ```<br>@classmethod<br>@deprecated(<br>    version=\"0.9.48\",<br>    reason=(<br>        \"Use the provided kwargs in the constructor, \"<br>        \"set_credentials will be removed in future releases.\"<br>    ),<br>    action=\"once\",<br>)<br>def from_credentials(<br>    cls,<br>    model_name: str = Models.TITAN_EMBEDDING,<br>    aws_region: Optional[str] = None,<br>    aws_access_key_id: Optional[str] = None,<br>    aws_secret_access_key: Optional[str] = None,<br>    aws_session_token: Optional[str] = None,<br>    aws_profile: Optional[str] = None,<br>    embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>    callback_manager: Optional[CallbackManager] = None,<br>    verbose: bool = False,<br>) -> \"BedrockEmbedding\":<br>    \"\"\"<br>    Instantiate using AWS credentials.<br>    Args:<br>        model_name (str) : Name of the model<br>        aws_access_key_id (str): AWS access key ID<br>        aws_secret_access_key (str): AWS secret access key<br>        aws_session_token (str): AWS session token<br>        aws_region (str): AWS region where the service is located<br>        aws_profile (str): AWS profile, when None, default profile is chosen automatically<br>    Example:<br>            .. code-block:: python<br>                from llama_index.embeddings import BedrockEmbedding<br>                # Define the model name<br>                model_name = \"your_model_name\"<br>                embeddings = BedrockEmbedding.from_credentials(<br>                    model_name,<br>                    aws_access_key_id,<br>                    aws_secret_access_key,<br>                    aws_session_token,<br>                    aws_region,<br>                    aws_profile,<br>                )<br>    \"\"\"<br>    session_kwargs = {<br>        \"profile_name\": aws_profile,<br>        \"region_name\": aws_region,<br>        \"aws_access_key_id\": aws_access_key_id,<br>        \"aws_secret_access_key\": aws_secret_access_key,<br>        \"aws_session_token\": aws_session_token,<br>    }<br>    try:<br>        import boto3<br>        session = boto3.Session(**session_kwargs)<br>    except ImportError:<br>        raise ImportError(<br>            \"boto3 package not found, install with\" \"'pip install boto3'\"<br>        )<br>    if \"bedrock-runtime\" in session.get_available_services():<br>        client = session.client(\"bedrock-runtime\")<br>    else:<br>        client = session.client(\"bedrock\")<br>    return cls(<br>        client=client,<br>        model=model_name,<br>        embed_batch_size=embed_batch_size,<br>        callback_manager=callback_manager,<br>        verbose=verbose,<br>    )<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Bedrock - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/bedrock/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/alephalpha/#llama_index.embeddings.alephalpha.AlephAlphaEmbedding)\n\n# Alephalpha\n\n## AlephAlphaEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/alephalpha/\\#llama_index.embeddings.alephalpha.AlephAlphaEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nAlephAlphaEmbedding uses the Aleph Alpha API to generate embeddings for text.\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-alephalpha/llama_index/embeddings/alephalpha/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>``` | ```<br>class AlephAlphaEmbedding(BaseEmbedding):<br>    \"\"\"AlephAlphaEmbedding uses the Aleph Alpha API to generate embeddings for text.\"\"\"<br>    model: str = Field(<br>        default=DEFAULT_ALEPHALPHA_MODEL, description=\"The Aleph Alpha model to use.\"<br>    )<br>    token: str = Field(default=None, description=\"The Aleph Alpha API token.\")<br>    representation: Optional[str] = Field(<br>        default=SemanticRepresentation.Query,<br>        description=\"The representation type to use for generating embeddings.\",<br>    )<br>    compress_to_size: Optional[int] = Field(<br>        default=None,<br>        description=\"The size to compress the embeddings to.\",<br>        gt=0,<br>    )<br>    base_url: Optional[str] = Field(<br>        default=DEFAULT_ALEPHALPHA_HOST, description=\"The hostname of the API base_url.\"<br>    )<br>    timeout: Optional[float] = Field(<br>        default=None, description=\"The timeout to use in seconds.\", gte=0<br>    )<br>    max_retries: int = Field(<br>        default=10, description=\"The maximum number of API retries.\", gte=0<br>    )<br>    normalize: Optional[bool] = Field(<br>        default=False, description=\"Return normalized embeddings.\"<br>    )<br>    hosting: Optional[str] = Field(default=None, description=\"The hosting to use.\")<br>    nice: bool = Field(default=False, description=\"Whether to be nice to the API.\")<br>    verify_ssl: bool = Field(default=True, description=\"Whether to verify SSL.\")<br>    additional_kwargs: Dict[str, Any] = Field(<br>        default_factory=dict, description=\"Additional kwargs for the Aleph Alpha API.\"<br>    )<br>    # Instance variables initialized via Pydantic's mechanism<br>    _client: Any = PrivateAttr()<br>    _aclient: Any = PrivateAttr()<br>    def __init__(<br>        self,<br>        model: str = DEFAULT_ALEPHALPHA_MODEL,<br>        token: Optional[str] = None,<br>        representation: Optional[str] = None,<br>        base_url: Optional[str] = DEFAULT_ALEPHALPHA_HOST,<br>        hosting: Optional[str] = None,<br>        timeout: Optional[float] = None,<br>        max_retries: int = 10,<br>        nice: bool = False,<br>        verify_ssl: bool = True,<br>        additional_kwargs: Optional[Dict[str, Any]] = None,<br>    ):<br>        \"\"\"<br>        A class representation for generating embeddings using the AlephAlpha API.<br>        Args:<br>            token: The token to use for the AlephAlpha API.<br>            model: The model to use for generating embeddings.<br>            base_url: The base URL of the AlephAlpha API.<br>            nice: Whether to use the \"nice\" mode for the AlephAlpha API.<br>            additional_kwargs: Additional kwargs for the AlephAlpha API.<br>        \"\"\"<br>        additional_kwargs = additional_kwargs or {}<br>        super().__init__(<br>            model=model,<br>            representation=representation,<br>            base_url=base_url,<br>            token=token,<br>            nice=nice,<br>            additional_kwargs=additional_kwargs,<br>        )<br>        self.token = get_from_param_or_env(\"aa_token\", token, \"AA_TOKEN\", \"\")<br>        if representation is not None and isinstance(representation, str):<br>            try:<br>                representation_enum = SemanticRepresentation[<br>                    representation.capitalize()<br>                ]<br>            except KeyError:<br>                raise ValueError(<br>                    f\"{representation} is not a valid representation type. Available types are: {list(SemanticRepresentation.__members__.keys())}\"<br>                )<br>            self.representation = representation_enum<br>        else:<br>            self.representation = representation<br>        self._client = None<br>        self._aclient = None<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"AlephAlphaEmbedding\"<br>    def _get_credential_kwargs(self) -> Dict[str, Any]:<br>        return {<br>            \"token\": self.token,<br>            \"host\": self.base_url,<br>            \"hosting\": self.hosting,<br>            \"request_timeout_seconds\": self.timeout,<br>            \"total_retries\": self.max_retries,<br>            \"nice\": self.nice,<br>            \"verify_ssl\": self.verify_ssl,<br>        }<br>    def _get_client(self) -> Client:<br>        if self._client is None:<br>            self._client = Client(**self._get_credential_kwargs())<br>        return self._client<br>    def _get_aclient(self) -> AsyncClient:<br>        if self._aclient is None:<br>            self._aclient = AsyncClient(**self._get_credential_kwargs())<br>        return self._aclient<br>    def _get_embedding(self, text: str, representation: str) -> List[float]:<br>        \"\"\"Embed sentence using AlephAlpha.\"\"\"<br>        client = self._get_client()<br>        request = SemanticEmbeddingRequest(<br>            prompt=Prompt.from_text(text),<br>            representation=representation or self.representation,<br>            compress_to_size=self.compress_to_size,<br>            normalize=self.normalize,<br>        )<br>        result = client.semantic_embed(request=request, model=self.model)<br>        return result.embedding<br>    async def _aget_embedding(self, text: str, representation: str) -> List[float]:<br>        \"\"\"Get embedding async.\"\"\"<br>        aclient = self._get_aclient()<br>        request = SemanticEmbeddingRequest(<br>            prompt=Prompt.from_text(text),<br>            representation=representation or self.representation,<br>            compress_to_size=self.compress_to_size,<br>            normalize=self.normalize,<br>        )<br>        result = await aclient.semantic_embed(request=request, model=self.model)<br>        return result.embedding<br>    def _get_embeddings(<br>        self, texts: List[str], representation: str<br>    ) -> List[List[float]]:<br>        \"\"\"Embed sentences using AlephAlpha.\"\"\"<br>        client = self._get_client()<br>        request = BatchSemanticEmbeddingRequest(<br>            prompts=[Prompt.from_text(text) for text in texts],<br>            representation=representation or self.representation,<br>            compress_to_size=self.compress_to_size,<br>            normalize=self.normalize,<br>        )<br>        result: BatchSemanticEmbeddingResponse = client.batch_semantic_embed(<br>            request=request, model=self.model<br>        )<br>        return result.embeddings<br>    async def _aget_embeddings(<br>        self, texts: List[str], representation: str<br>    ) -> List[List[float]]:<br>        \"\"\"Get embeddings async.\"\"\"<br>        aclient = self._get_aclient()<br>        request = BatchSemanticEmbeddingRequest(<br>            prompts=[Prompt.from_text(text) for text in texts],<br>            representation=representation or self.representation,<br>            compress_to_size=self.compress_to_size,<br>            normalize=self.normalize,<br>        )<br>        result: BatchSemanticEmbeddingResponse = await aclient.batch_semantic_embed(<br>            request=request, model=self.model<br>        )<br>        return result.embeddings<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding. For query embeddings, representation='query'.\"\"\"<br>        return self._get_embedding(query, SemanticRepresentation.Query)<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding async. For query embeddings, representation='query'.\"\"\"<br>        return self._aget_embedding(query, SemanticRepresentation.Query)<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding. For text embeddings, representation='document'.\"\"\"<br>        return self._get_embedding(text, SemanticRepresentation.Document)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding async.\"\"\"<br>        return self._aget_embedding(text, SemanticRepresentation.Document)<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        return self._get_embeddings(texts, SemanticRepresentation.Document)<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings async.\"\"\"<br>        return self._aget_embeddings(texts, SemanticRepresentation.Document)<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Alephalpha - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/alephalpha/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/anyscale/#llama_index.embeddings.anyscale.AnyscaleEmbedding)\n\n# Anyscale\n\n## AnyscaleEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/anyscale/\\#llama_index.embeddings.anyscale.AnyscaleEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nAnyscale class for embeddings.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model` | `str` | Model for embedding.<br>Defaults to \"thenlper/gte-large\" | `DEFAULT_MODEL` |\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-anyscale/llama_index/embeddings/anyscale/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>``` | ```<br>class AnyscaleEmbedding(BaseEmbedding):<br>    \"\"\"<br>    Anyscale class for embeddings.<br>    Args:<br>        model (str): Model for embedding.<br>            Defaults to \"thenlper/gte-large\"<br>    \"\"\"<br>    additional_kwargs: Dict[str, Any] = Field(<br>        default_factory=dict, description=\"Additional kwargs for the OpenAI API.\"<br>    )<br>    api_key: str = Field(description=\"The Anyscale API key.\")<br>    api_base: str = Field(description=\"The base URL for Anyscale API.\")<br>    api_version: str = Field(description=\"The version for OpenAI API.\")<br>    max_retries: int = Field(<br>        default=10, description=\"Maximum number of retries.\", gte=0<br>    )<br>    timeout: float = Field(default=60.0, description=\"Timeout for each request.\", gte=0)<br>    default_headers: Optional[Dict[str, str]] = Field(<br>        default=None, description=\"The default headers for API requests.\"<br>    )<br>    reuse_client: bool = Field(<br>        default=True,<br>        description=(<br>            \"Reuse the Anyscale client between requests. When doing anything with large \"<br>            \"volumes of async API calls, setting this to false can improve stability.\"<br>        ),<br>    )<br>    _query_engine: Optional[str] = PrivateAttr()<br>    _text_engine: Optional[str] = PrivateAttr()<br>    _client: Optional[OpenAI] = PrivateAttr()<br>    _aclient: Optional[AsyncOpenAI] = PrivateAttr()<br>    _http_client: Optional[httpx.Client] = PrivateAttr()<br>    def __init__(<br>        self,<br>        model: str = DEFAULT_MODEL,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        additional_kwargs: Optional[Dict[str, Any]] = None,<br>        api_key: Optional[str] = None,<br>        api_base: Optional[str] = DEFAULT_API_BASE,<br>        api_version: Optional[str] = None,<br>        max_retries: int = 10,<br>        timeout: float = 60.0,<br>        reuse_client: bool = True,<br>        callback_manager: Optional[CallbackManager] = None,<br>        default_headers: Optional[Dict[str, str]] = None,<br>        http_client: Optional[httpx.Client] = None,<br>        **kwargs: Any,<br>    ) -> None:<br>        additional_kwargs = additional_kwargs or {}<br>        api_key, api_base, api_version = resolve_anyscale_credentials(<br>            api_key=api_key,<br>            api_base=api_base,<br>            api_version=api_version,<br>        )<br>        if \"model_name\" in kwargs:<br>            model_name = kwargs.pop(\"model_name\")<br>        else:<br>            model_name = model<br>        super().__init__(<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            model_name=model_name,<br>            additional_kwargs=additional_kwargs,<br>            api_key=api_key,<br>            api_base=api_base,<br>            api_version=api_version,<br>            max_retries=max_retries,<br>            reuse_client=reuse_client,<br>            timeout=timeout,<br>            default_headers=default_headers,<br>            **kwargs,<br>        )<br>        self._query_engine = model_name<br>        self._text_engine = model_name<br>        self._client = None<br>        self._aclient = None<br>        self._http_client = http_client<br>    def _get_client(self) -> OpenAI:<br>        if not self.reuse_client:<br>            return OpenAI(**self._get_credential_kwargs())<br>        if self._client is None:<br>            self._client = OpenAI(**self._get_credential_kwargs())<br>        return self._client<br>    def _get_aclient(self) -> AsyncOpenAI:<br>        if not self.reuse_client:<br>            return AsyncOpenAI(**self._get_credential_kwargs())<br>        if self._aclient is None:<br>            self._aclient = AsyncOpenAI(**self._get_credential_kwargs())<br>        return self._aclient<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"AnyscaleEmbedding\"<br>    def _get_credential_kwargs(self) -> Dict[str, Any]:<br>        return {<br>            \"api_key\": self.api_key,<br>            \"base_url\": self.api_base,<br>            \"max_retries\": self.max_retries,<br>            \"timeout\": self.timeout,<br>            \"default_headers\": self.default_headers,<br>            \"http_client\": self._http_client,<br>        }<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        client = self._get_client()<br>        return get_embedding(<br>            client,<br>            query,<br>            engine=self._query_engine,<br>            **self.additional_kwargs,<br>        )<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        aclient = self._get_aclient()<br>        return await aget_embedding(<br>            aclient,<br>            query,<br>            engine=self._query_engine,<br>            **self.additional_kwargs,<br>        )<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        client = self._get_client()<br>        return get_embedding(<br>            client,<br>            text,<br>            engine=self._text_engine,<br>            **self.additional_kwargs,<br>        )<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously get text embedding.\"\"\"<br>        aclient = self._get_aclient()<br>        return await aget_embedding(<br>            aclient,<br>            text,<br>            engine=self._text_engine,<br>            **self.additional_kwargs,<br>        )<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"<br>        Get text embeddings.<br>        By default, this is a wrapper around _get_text_embedding.<br>        Can be overridden for batch queries.<br>        \"\"\"<br>        client = self._get_client()<br>        return get_embeddings(<br>            client,<br>            texts,<br>            engine=self._text_engine,<br>            **self.additional_kwargs,<br>        )<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Asynchronously get text embeddings.\"\"\"<br>        aclient = self._get_aclient()<br>        return await aget_embeddings(<br>            aclient,<br>            texts,<br>            engine=self._text_engine,<br>            **self.additional_kwargs,<br>        )<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Anyscale - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/anyscale/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/mistralai/#llama_index.embeddings.mistralai.MistralAIEmbedding)\n\n# Mistralai\n\n## MistralAIEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/mistralai/\\#llama_index.embeddings.mistralai.MistralAIEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nClass for MistralAI embeddings.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model_name` | `str` | Model for embedding.<br>Defaults to \"mistral-embed\". | `'mistral-embed'` |\n| `api_key` | `Optional[str]` | API key to access the model. Defaults to None. | `None` |\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-mistralai/llama_index/embeddings/mistralai/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>``` | ```<br>class MistralAIEmbedding(BaseEmbedding):<br>    \"\"\"Class for MistralAI embeddings.<br>    Args:<br>        model_name (str): Model for embedding.<br>            Defaults to \"mistral-embed\".<br>        api_key (Optional[str]): API key to access the model. Defaults to None.<br>    \"\"\"<br>    # Instance variables initialized via Pydantic's mechanism<br>    _client: Mistral = PrivateAttr()<br>    def __init__(<br>        self,<br>        model_name: str = \"mistral-embed\",<br>        api_key: Optional[str] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>        **kwargs: Any,<br>    ):<br>        super().__init__(<br>            model_name=model_name,<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            **kwargs,<br>        )<br>        api_key = get_from_param_or_env(\"api_key\", api_key, \"MISTRAL_API_KEY\", \"\")<br>        if not api_key:<br>            raise ValueError(<br>                \"You must provide an API key to use mistralai. \"<br>                \"You can either pass it in as an argument or set it `MISTRAL_API_KEY`.\"<br>            )<br>        self._client = Mistral(api_key=api_key)<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"MistralAIEmbedding\"<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return (<br>            self._client.embeddings.create(model=self.model_name, inputs=[query])<br>            .data[0]<br>            .embedding<br>        )<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        return (<br>            (<br>                await self._client.embeddings.create_async(<br>                    model=self.model_name, inputs=[query]<br>                )<br>            )<br>            .data[0]<br>            .embedding<br>        )<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return (<br>            self._client.embeddings.create(model=self.model_name, inputs=[text])<br>            .data[0]<br>            .embedding<br>        )<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously get text embedding.\"\"\"<br>        return (<br>            await self._client.embeddings.create(<br>                model=self.model_name,<br>                inputs=[text],<br>            )<br>            .data[0]<br>            .embedding<br>        )<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        embedding_response = self._client.embeddings.create(<br>            model=self.model_name, inputs=texts<br>        ).data<br>        return [embed.embedding for embed in embedding_response]<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Asynchronously get text embeddings.\"\"\"<br>        embedding_response = await self._client.embeddings.create_async(<br>            model=self.model_name, inputs=texts<br>        )<br>        return [embed.embedding for embed in embedding_response.data]<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Mistralai - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/mistralai/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/openinference/#llama_index.callbacks.openinference.OpenInferenceCallbackHandler)\n\n# Openinference\n\n## OpenInferenceCallbackHandler [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/openinference/\\#llama_index.callbacks.openinference.OpenInferenceCallbackHandler \"Permanent link\")\n\nBases: `BaseCallbackHandler`\n\nCallback handler for storing generation data in OpenInference format.\nOpenInference is an open standard for capturing and storing AI model\ninferences. It enables production LLMapp servers to seamlessly integrate\nwith LLM observability solutions such as Arize and Phoenix.\n\nFor more information on the specification, see\nhttps://github.com/Arize-ai/open-inference-spec\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-openinference/llama_index/callbacks/openinference/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>``` | ```<br>class OpenInferenceCallbackHandler(BaseCallbackHandler):<br>    \"\"\"Callback handler for storing generation data in OpenInference format.<br>    OpenInference is an open standard for capturing and storing AI model<br>    inferences. It enables production LLMapp servers to seamlessly integrate<br>    with LLM observability solutions such as Arize and Phoenix.<br>    For more information on the specification, see<br>    https://github.com/Arize-ai/open-inference-spec<br>    \"\"\"<br>    def __init__(<br>        self,<br>        callback: Optional[Callable[[List[QueryData], List[NodeData]], None]] = None,<br>    ) -> None:<br>        \"\"\"Initializes the OpenInferenceCallbackHandler.<br>        Args:<br>            callback (Optional[Callable[[List[QueryData], List[NodeData]], None]], optional): A<br>            callback function that will be called when a query trace is<br>            completed, often used for logging or persisting query data.<br>        \"\"\"<br>        super().__init__(event_starts_to_ignore=[], event_ends_to_ignore=[])<br>        self._callback = callback<br>        self._trace_data = TraceData()<br>        self._query_data_buffer: List[QueryData] = []<br>        self._node_data_buffer: List[NodeData] = []<br>    def start_trace(self, trace_id: Optional[str] = None) -> None:<br>        if trace_id == \"query\" or trace_id == \"chat\":<br>            self._trace_data = TraceData()<br>            self._trace_data.query_data.timestamp = datetime.now().isoformat()<br>            self._trace_data.query_data.id = _generate_random_id()<br>    def end_trace(<br>        self,<br>        trace_id: Optional[str] = None,<br>        trace_map: Optional[Dict[str, List[str]]] = None,<br>    ) -> None:<br>        if trace_id == \"query\" or trace_id == \"chat\":<br>            self._query_data_buffer.append(self._trace_data.query_data)<br>            self._node_data_buffer.extend(self._trace_data.node_datas)<br>            self._trace_data = TraceData()<br>            if self._callback is not None:<br>                self._callback(self._query_data_buffer, self._node_data_buffer)<br>    def on_event_start(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        parent_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> str:<br>        if payload is not None:<br>            if event_type is CBEventType.QUERY:<br>                query_text = payload[EventPayload.QUERY_STR]<br>                self._trace_data.query_data.query_text = query_text<br>            elif event_type is CBEventType.LLM:<br>                if prompt := payload.get(EventPayload.PROMPT, None):<br>                    self._trace_data.query_data.llm_prompt = prompt<br>                if messages := payload.get(EventPayload.MESSAGES, None):<br>                    self._trace_data.query_data.llm_messages = [<br>                        (m.role.value, m.content) for m in messages<br>                    ]<br>                    # For chat engines there is no query event and thus the<br>                    # query text will be None, in this case we set the query<br>                    # text to the last message passed to the LLM<br>                    if self._trace_data.query_data.query_text is None:<br>                        self._trace_data.query_data.query_text = messages[-1].content<br>        return event_id<br>    def on_event_end(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> None:<br>        if payload is None:<br>            return<br>        if event_type is CBEventType.RETRIEVE:<br>            for node_with_score in payload[EventPayload.NODES]:<br>                node = node_with_score.node<br>                score = node_with_score.score<br>                self._trace_data.query_data.node_ids.append(node.hash)<br>                self._trace_data.query_data.scores.append(score)<br>                self._trace_data.node_datas.append(<br>                    NodeData(<br>                        id=node.hash,<br>                        node_text=node.text,<br>                    )<br>                )<br>        elif event_type is CBEventType.LLM:<br>            if self._trace_data.query_data.response_text is None:<br>                if response := payload.get(EventPayload.RESPONSE, None):<br>                    if isinstance(response, ChatResponse):<br>                        # If the response is of class ChatResponse the string<br>                        # representation has the format \"<role>: <message>\",<br>                        # but we want just the message<br>                        response_text = response.message.content<br>                    else:<br>                        response_text = str(response)<br>                    self._trace_data.query_data.response_text = response_text<br>                elif completion := payload.get(EventPayload.COMPLETION, None):<br>                    self._trace_data.query_data.response_text = str(completion)<br>        elif event_type is CBEventType.EMBEDDING:<br>            self._trace_data.query_data.query_embedding = payload[<br>                EventPayload.EMBEDDINGS<br>            ][0]<br>    def flush_query_data_buffer(self) -> List[QueryData]:<br>        \"\"\"Clears the query data buffer and returns the data.<br>        Returns:<br>            List[QueryData]: The query data.<br>        \"\"\"<br>        query_data_buffer = self._query_data_buffer<br>        self._query_data_buffer = []<br>        return query_data_buffer<br>    def flush_node_data_buffer(self) -> List[NodeData]:<br>        \"\"\"Clears the node data buffer and returns the data.<br>        Returns:<br>            List[NodeData]: The node data.<br>        \"\"\"<br>        node_data_buffer = self._node_data_buffer<br>        self._node_data_buffer = []<br>        return node_data_buffer<br>``` |\n\n### flush\\_query\\_data\\_buffer [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/openinference/\\#llama_index.callbacks.openinference.OpenInferenceCallbackHandler.flush_query_data_buffer \"Permanent link\")\n\n```\nflush_query_data_buffer() -> List[QueryData]\n\n```\n\nClears the query data buffer and returns the data.\n\n**Returns:**\n\n| Type | Description |\n| --- | --- |\n| `List[QueryData]` | List\\[QueryData\\]: The query data. |\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-openinference/llama_index/callbacks/openinference/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>``` | ```<br>def flush_query_data_buffer(self) -> List[QueryData]:<br>    \"\"\"Clears the query data buffer and returns the data.<br>    Returns:<br>        List[QueryData]: The query data.<br>    \"\"\"<br>    query_data_buffer = self._query_data_buffer<br>    self._query_data_buffer = []<br>    return query_data_buffer<br>``` |\n\n### flush\\_node\\_data\\_buffer [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/openinference/\\#llama_index.callbacks.openinference.OpenInferenceCallbackHandler.flush_node_data_buffer \"Permanent link\")\n\n```\nflush_node_data_buffer() -> List[NodeData]\n\n```\n\nClears the node data buffer and returns the data.\n\n**Returns:**\n\n| Type | Description |\n| --- | --- |\n| `List[NodeData]` | List\\[NodeData\\]: The node data. |\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-openinference/llama_index/callbacks/openinference/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>``` | ```<br>def flush_node_data_buffer(self) -> List[NodeData]:<br>    \"\"\"Clears the node data buffer and returns the data.<br>    Returns:<br>        List[NodeData]: The node data.<br>    \"\"\"<br>    node_data_buffer = self._node_data_buffer<br>    self._node_data_buffer = []<br>    return node_data_buffer<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Openinference - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/openinference/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/#api-reference)\n\n# API Reference [\\#](https://docs.llamaindex.ai/en/stable/api_reference/\\#api-reference \"Permanent link\")\n\nLlamaIndex provides thorough documentation of modules and integrations used in the framework.\n\nUse the navigation or search to find the classes you are interested in!\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "API Reference - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/correctness/#llama_index.core.evaluation.CorrectnessEvaluator)\n\n# Correctness\n\nEvaluation modules.\n\n## CorrectnessEvaluator [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/correctness/\\#llama_index.core.evaluation.CorrectnessEvaluator \"Permanent link\")\n\nBases: `BaseEvaluator`\n\nCorrectness evaluator.\n\nEvaluates the correctness of a question answering system.\nThis evaluator depends on `reference` answer to be provided, in addition to the\nquery string and response string.\n\nIt outputs a score between 1 and 5, where 1 is the worst and 5 is the best,\nalong with a reasoning for the score.\nPassing is defined as a score greater than or equal to the given threshold.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `eval_template` | `Optional[Union[BasePromptTemplate, str]]` | Template for the evaluation prompt. | `None` |\n| `score_threshold` | `float` | Numerical threshold for passing the evaluation,<br>defaults to 4.0. | `4.0` |\n\nSource code in `llama-index-core/llama_index/core/evaluation/correctness.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>``` | ```<br>class CorrectnessEvaluator(BaseEvaluator):<br>    \"\"\"Correctness evaluator.<br>    Evaluates the correctness of a question answering system.<br>    This evaluator depends on `reference` answer to be provided, in addition to the<br>    query string and response string.<br>    It outputs a score between 1 and 5, where 1 is the worst and 5 is the best,<br>    along with a reasoning for the score.<br>    Passing is defined as a score greater than or equal to the given threshold.<br>    Args:<br>        eval_template (Optional[Union[BasePromptTemplate, str]]):<br>            Template for the evaluation prompt.<br>        score_threshold (float): Numerical threshold for passing the evaluation,<br>            defaults to 4.0.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        llm: Optional[LLM] = None,<br>        eval_template: Optional[Union[BasePromptTemplate, str]] = None,<br>        score_threshold: float = 4.0,<br>        parser_function: Callable[<br>            [str], Tuple[Optional[float], Optional[str]]<br>        ] = default_parser,<br>    ) -> None:<br>        self._llm = llm or Settings.llm<br>        self._eval_template: BasePromptTemplate<br>        if isinstance(eval_template, str):<br>            self._eval_template = PromptTemplate(eval_template)<br>        else:<br>            self._eval_template = eval_template or DEFAULT_EVAL_TEMPLATE<br>        self._score_threshold = score_threshold<br>        self.parser_function = parser_function<br>    def _get_prompts(self) -> PromptDictType:<br>        \"\"\"Get prompts.\"\"\"<br>        return {<br>            \"eval_template\": self._eval_template,<br>        }<br>    def _update_prompts(self, prompts: PromptDictType) -> None:<br>        \"\"\"Update prompts.\"\"\"<br>        if \"eval_template\" in prompts:<br>            self._eval_template = prompts[\"eval_template\"]<br>    async def aevaluate(<br>        self,<br>        query: Optional[str] = None,<br>        response: Optional[str] = None,<br>        contexts: Optional[Sequence[str]] = None,<br>        reference: Optional[str] = None,<br>        sleep_time_in_seconds: int = 0,<br>        **kwargs: Any,<br>    ) -> EvaluationResult:<br>        del kwargs  # Unused<br>        del contexts  # Unused<br>        await asyncio.sleep(sleep_time_in_seconds)<br>        if query is None or response is None:<br>            raise ValueError(\"query, and response must be provided\")<br>        eval_response = await self._llm.apredict(<br>            prompt=self._eval_template,<br>            query=query,<br>            generated_answer=response,<br>            reference_answer=reference or \"(NO REFERENCE ANSWER SUPPLIED)\",<br>        )<br>        # Use the parser function<br>        score, reasoning = self.parser_function(eval_response)<br>        return EvaluationResult(<br>            query=query,<br>            response=response,<br>            passing=score >= self._score_threshold if score is not None else None,<br>            score=score,<br>            feedback=reasoning,<br>        )<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Correctness - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/evaluation/correctness/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/openai/#llama_index.embeddings.openai.OpenAIEmbedding)\n\n# Openai\n\n## OpenAIEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/openai/\\#llama_index.embeddings.openai.OpenAIEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nOpenAI class for embeddings.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `mode` | `str` | Mode for embedding.<br>Defaults to OpenAIEmbeddingMode.TEXT\\_SEARCH\\_MODE.<br>Options are:<br>- OpenAIEmbeddingMode.SIMILARITY\\_MODE<br>- OpenAIEmbeddingMode.TEXT\\_SEARCH\\_MODE | `TEXT_SEARCH_MODE` |\n| `model` | `str` | Model for embedding.<br>Defaults to OpenAIEmbeddingModelType.TEXT\\_EMBED\\_ADA\\_002.<br>Options are:<br>- OpenAIEmbeddingModelType.DAVINCI<br>- OpenAIEmbeddingModelType.CURIE<br>- OpenAIEmbeddingModelType.BABBAGE<br>- OpenAIEmbeddingModelType.ADA<br>- OpenAIEmbeddingModelType.TEXT\\_EMBED\\_ADA\\_002 | `TEXT_EMBED_ADA_002` |\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-openai/llama_index/embeddings/openai/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>``` | ```<br>class OpenAIEmbedding(BaseEmbedding):<br>    \"\"\"OpenAI class for embeddings.<br>    Args:<br>        mode (str): Mode for embedding.<br>            Defaults to OpenAIEmbeddingMode.TEXT_SEARCH_MODE.<br>            Options are:<br>            - OpenAIEmbeddingMode.SIMILARITY_MODE<br>            - OpenAIEmbeddingMode.TEXT_SEARCH_MODE<br>        model (str): Model for embedding.<br>            Defaults to OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002.<br>            Options are:<br>            - OpenAIEmbeddingModelType.DAVINCI<br>            - OpenAIEmbeddingModelType.CURIE<br>            - OpenAIEmbeddingModelType.BABBAGE<br>            - OpenAIEmbeddingModelType.ADA<br>            - OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002<br>    \"\"\"<br>    additional_kwargs: Dict[str, Any] = Field(<br>        default_factory=dict, description=\"Additional kwargs for the OpenAI API.\"<br>    )<br>    api_key: str = Field(description=\"The OpenAI API key.\")<br>    api_base: Optional[str] = Field(<br>        default=DEFAULT_OPENAI_API_BASE, description=\"The base URL for OpenAI API.\"<br>    )<br>    api_version: Optional[str] = Field(<br>        default=DEFAULT_OPENAI_API_VERSION, description=\"The version for OpenAI API.\"<br>    )<br>    max_retries: int = Field(<br>        default=10, description=\"Maximum number of retries.\", gte=0<br>    )<br>    timeout: float = Field(default=60.0, description=\"Timeout for each request.\", gte=0)<br>    default_headers: Optional[Dict[str, str]] = Field(<br>        default=None, description=\"The default headers for API requests.\"<br>    )<br>    reuse_client: bool = Field(<br>        default=True,<br>        description=(<br>            \"Reuse the OpenAI client between requests. When doing anything with large \"<br>            \"volumes of async API calls, setting this to false can improve stability.\"<br>        ),<br>    )<br>    dimensions: Optional[int] = Field(<br>        default=None,<br>        description=(<br>            \"The number of dimensions on the output embedding vectors. \"<br>            \"Works only with v3 embedding models.\"<br>        ),<br>    )<br>    _query_engine: str = PrivateAttr()<br>    _text_engine: str = PrivateAttr()<br>    _client: Optional[OpenAI] = PrivateAttr()<br>    _aclient: Optional[AsyncOpenAI] = PrivateAttr()<br>    _http_client: Optional[httpx.Client] = PrivateAttr()<br>    _async_http_client: Optional[httpx.AsyncClient] = PrivateAttr()<br>    def __init__(<br>        self,<br>        mode: str = OpenAIEmbeddingMode.TEXT_SEARCH_MODE,<br>        model: str = OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002,<br>        embed_batch_size: int = 100,<br>        dimensions: Optional[int] = None,<br>        additional_kwargs: Optional[Dict[str, Any]] = None,<br>        api_key: Optional[str] = None,<br>        api_base: Optional[str] = None,<br>        api_version: Optional[str] = None,<br>        max_retries: int = 10,<br>        timeout: float = 60.0,<br>        reuse_client: bool = True,<br>        callback_manager: Optional[CallbackManager] = None,<br>        default_headers: Optional[Dict[str, str]] = None,<br>        http_client: Optional[httpx.Client] = None,<br>        async_http_client: Optional[httpx.AsyncClient] = None,<br>        num_workers: Optional[int] = None,<br>        **kwargs: Any,<br>    ) -> None:<br>        additional_kwargs = additional_kwargs or {}<br>        if dimensions is not None:<br>            additional_kwargs[\"dimensions\"] = dimensions<br>        api_key, api_base, api_version = self._resolve_credentials(<br>            api_key=api_key,<br>            api_base=api_base,<br>            api_version=api_version,<br>        )<br>        query_engine = get_engine(mode, model, _QUERY_MODE_MODEL_DICT)<br>        text_engine = get_engine(mode, model, _TEXT_MODE_MODEL_DICT)<br>        if \"model_name\" in kwargs:<br>            model_name = kwargs.pop(\"model_name\")<br>            query_engine = text_engine = model_name<br>        else:<br>            model_name = model<br>        super().__init__(<br>            embed_batch_size=embed_batch_size,<br>            dimensions=dimensions,<br>            callback_manager=callback_manager,<br>            model_name=model_name,<br>            additional_kwargs=additional_kwargs,<br>            api_key=api_key,<br>            api_base=api_base,<br>            api_version=api_version,<br>            max_retries=max_retries,<br>            reuse_client=reuse_client,<br>            timeout=timeout,<br>            default_headers=default_headers,<br>            num_workers=num_workers,<br>            **kwargs,<br>        )<br>        self._query_engine = query_engine<br>        self._text_engine = text_engine<br>        self._client = None<br>        self._aclient = None<br>        self._http_client = http_client<br>        self._async_http_client = async_http_client<br>    def _resolve_credentials(<br>        self,<br>        api_key: Optional[str] = None,<br>        api_base: Optional[str] = None,<br>        api_version: Optional[str] = None,<br>    ) -> Tuple[Optional[str], str, str]:<br>        return resolve_openai_credentials(api_key, api_base, api_version)<br>    def _get_client(self) -> OpenAI:<br>        if not self.reuse_client:<br>            return OpenAI(**self._get_credential_kwargs())<br>        if self._client is None:<br>            self._client = OpenAI(**self._get_credential_kwargs())<br>        return self._client<br>    def _get_aclient(self) -> AsyncOpenAI:<br>        if not self.reuse_client:<br>            return AsyncOpenAI(**self._get_credential_kwargs(is_async=True))<br>        if self._aclient is None:<br>            self._aclient = AsyncOpenAI(**self._get_credential_kwargs(is_async=True))<br>        return self._aclient<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"OpenAIEmbedding\"<br>    def _get_credential_kwargs(self, is_async: bool = False) -> Dict[str, Any]:<br>        return {<br>            \"api_key\": self.api_key,<br>            \"base_url\": self.api_base,<br>            \"max_retries\": self.max_retries,<br>            \"timeout\": self.timeout,<br>            \"default_headers\": self.default_headers,<br>            \"http_client\": self._async_http_client if is_async else self._http_client,<br>        }<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        client = self._get_client()<br>        return get_embedding(<br>            client,<br>            query,<br>            engine=self._query_engine,<br>            **self.additional_kwargs,<br>        )<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        aclient = self._get_aclient()<br>        return await aget_embedding(<br>            aclient,<br>            query,<br>            engine=self._query_engine,<br>            **self.additional_kwargs,<br>        )<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        client = self._get_client()<br>        return get_embedding(<br>            client,<br>            text,<br>            engine=self._text_engine,<br>            **self.additional_kwargs,<br>        )<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously get text embedding.\"\"\"<br>        aclient = self._get_aclient()<br>        return await aget_embedding(<br>            aclient,<br>            text,<br>            engine=self._text_engine,<br>            **self.additional_kwargs,<br>        )<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.<br>        By default, this is a wrapper around _get_text_embedding.<br>        Can be overridden for batch queries.<br>        \"\"\"<br>        client = self._get_client()<br>        return get_embeddings(<br>            client,<br>            texts,<br>            engine=self._text_engine,<br>            **self.additional_kwargs,<br>        )<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Asynchronously get text embeddings.\"\"\"<br>        aclient = self._get_aclient()<br>        return await aget_embeddings(<br>            aclient,<br>            texts,<br>            engine=self._text_engine,<br>            **self.additional_kwargs,<br>        )<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Openai - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/openai/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/llm_rails/#llama_index.embeddings.llm_rails.LLMRailsEmbedding)\n\n# Llm rails\n\n## LLMRailsEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/llm_rails/\\#llama_index.embeddings.llm_rails.LLMRailsEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nLLMRails embedding models.\n\nThis class provides an interface to generate embeddings using a model deployed\nin an LLMRails cluster. It requires a model\\_id of the model deployed in the cluster and api key you can obtain\nfrom https://console.llmrails.com/api-keys.\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-llm-rails/llama_index/embeddings/llm_rails/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 11<br> 12<br> 13<br> 14<br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>``` | ```<br>class LLMRailsEmbedding(BaseEmbedding):<br>    \"\"\"LLMRails embedding models.<br>    This class provides an interface to generate embeddings using a model deployed<br>    in an LLMRails cluster. It requires a model_id of the model deployed in the cluster and api key you can obtain<br>    from https://console.llmrails.com/api-keys.<br>    \"\"\"<br>    model_id: str<br>    api_key: str<br>    session: requests.Session<br>    @classmethod<br>    def class_name(self) -> str:<br>        return \"LLMRailsEmbedding\"<br>    def __init__(<br>        self,<br>        api_key: str,<br>        model_id: str = \"embedding-english-v1\",  # or embedding-multi-v1<br>        **kwargs: Any,<br>    ):<br>        retry = Retry(<br>            total=3,<br>            connect=3,<br>            read=2,<br>            allowed_methods=[\"POST\"],<br>            backoff_factor=2,<br>            status_forcelist=[502, 503, 504],<br>        )<br>        session = requests.Session()<br>        session.mount(\"https://api.llmrails.com\", HTTPAdapter(max_retries=retry))<br>        session.headers = {\"X-API-KEY\": api_key}<br>        super().__init__(model_id=model_id, api_key=api_key, session=session, **kwargs)<br>    def _get_embedding(self, text: str) -> List[float]:<br>        \"\"\"<br>        Generate an embedding for a single query text.<br>        Args:<br>            text (str): The query text to generate an embedding for.<br>        Returns:<br>            List[float]: The embedding for the input query text.<br>        \"\"\"<br>        try:<br>            response = self.session.post(<br>                \"https://api.llmrails.com/v1/embeddings\",<br>                json={\"input\": [text], \"model\": self.model_id},<br>            )<br>            response.raise_for_status()<br>            return response.json()[\"data\"][0][\"embedding\"]<br>        except requests.exceptions.HTTPError as e:<br>            logger.error(f\"Error while embedding text {e}.\")<br>            raise ValueError(f\"Unable to embed given text {e}\")<br>    async def _aget_embedding(self, text: str) -> List[float]:<br>        \"\"\"<br>        Generate an embedding for a single query text.<br>        Args:<br>            text (str): The query text to generate an embedding for.<br>        Returns:<br>            List[float]: The embedding for the input query text.<br>        \"\"\"<br>        try:<br>            import httpx<br>        except ImportError:<br>            raise ImportError(<br>                \"The httpx library is required to use the async version of \"<br>                \"this function. Install it with `pip install httpx`.\"<br>            )<br>        try:<br>            async with httpx.AsyncClient() as client:<br>                response = await client.post(<br>                    \"https://api.llmrails.com/v1/embeddings\",<br>                    headers={\"X-API-KEY\": self.api_key},<br>                    json={\"input\": [text], \"model\": self.model_id},<br>                )<br>                response.raise_for_status()<br>            return response.json()[\"data\"][0][\"embedding\"]<br>        except httpx._exceptions.HTTPError as e:<br>            logger.error(f\"Error while embedding text {e}.\")<br>            raise ValueError(f\"Unable to embed given text {e}\")<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        return self._get_embedding(text)<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        return self._get_embedding(query)<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        return await self._aget_embedding(query)<br>    async def _aget_text_embedding(self, query: str) -> List[float]:<br>        return await self._aget_embedding(query)<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Llm rails - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/llm_rails/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/ollama/#llama_index.embeddings.ollama.OllamaEmbedding)\n\n# Ollama\n\n## OllamaEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/ollama/\\#llama_index.embeddings.ollama.OllamaEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nClass for Ollama embeddings.\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-ollama/llama_index/embeddings/ollama/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>89<br>90<br>91<br>92<br>93<br>94<br>95<br>96<br>97<br>98<br>``` | ```<br>class OllamaEmbedding(BaseEmbedding):<br>    \"\"\"Class for Ollama embeddings.\"\"\"<br>    base_url: str = Field(description=\"Base url the model is hosted by Ollama\")<br>    model_name: str = Field(description=\"The Ollama model to use.\")<br>    embed_batch_size: int = Field(<br>        default=DEFAULT_EMBED_BATCH_SIZE,<br>        description=\"The batch size for embedding calls.\",<br>        gt=0,<br>        lte=2048,<br>    )<br>    ollama_additional_kwargs: Dict[str, Any] = Field(<br>        default_factory=dict, description=\"Additional kwargs for the Ollama API.\"<br>    )<br>    _client: Client = PrivateAttr()<br>    _async_client: AsyncClient = PrivateAttr()<br>    def __init__(<br>        self,<br>        model_name: str,<br>        base_url: str = \"http://localhost:11434\",<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        ollama_additional_kwargs: Optional[Dict[str, Any]] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        **kwargs: Any,<br>    ) -> None:<br>        super().__init__(<br>            model_name=model_name,<br>            base_url=base_url,<br>            embed_batch_size=embed_batch_size,<br>            ollama_additional_kwargs=ollama_additional_kwargs or {},<br>            callback_manager=callback_manager,<br>            **kwargs,<br>        )<br>        self._client = Client(host=self.base_url)<br>        self._async_client = AsyncClient(host=self.base_url)<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"OllamaEmbedding\"<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self.get_general_text_embedding(query)<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        return await self.aget_general_text_embedding(query)<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self.get_general_text_embedding(text)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously get text embedding.\"\"\"<br>        return await self.aget_general_text_embedding(text)<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        embeddings_list: List[List[float]] = []<br>        for text in texts:<br>            embeddings = self.get_general_text_embedding(text)<br>            embeddings_list.append(embeddings)<br>        return embeddings_list<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Asynchronously get text embeddings.\"\"\"<br>        return await asyncio.gather(<br>            *[self.aget_general_text_embedding(text) for text in texts]<br>        )<br>    def get_general_text_embedding(self, texts: str) -> List[float]:<br>        \"\"\"Get Ollama embedding.\"\"\"<br>        result = self._client.embeddings(<br>            model=self.model_name, prompt=texts, options=self.ollama_additional_kwargs<br>        )<br>        return result[\"embedding\"]<br>    async def aget_general_text_embedding(self, prompt: str) -> List[float]:<br>        \"\"\"Asynchronously get Ollama embedding.\"\"\"<br>        result = await self._async_client.embeddings(<br>            model=self.model_name, prompt=prompt, options=self.ollama_additional_kwargs<br>        )<br>        return result[\"embedding\"]<br>``` |\n\n### get\\_general\\_text\\_embedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/ollama/\\#llama_index.embeddings.ollama.OllamaEmbedding.get_general_text_embedding \"Permanent link\")\n\n```\nget_general_text_embedding(texts: str) -> List[float]\n\n```\n\nGet Ollama embedding.\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-ollama/llama_index/embeddings/ollama/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>86<br>87<br>88<br>89<br>90<br>91<br>``` | ```<br>def get_general_text_embedding(self, texts: str) -> List[float]:<br>    \"\"\"Get Ollama embedding.\"\"\"<br>    result = self._client.embeddings(<br>        model=self.model_name, prompt=texts, options=self.ollama_additional_kwargs<br>    )<br>    return result[\"embedding\"]<br>``` |\n\n### aget\\_general\\_text\\_embedding`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/ollama/\\#llama_index.embeddings.ollama.OllamaEmbedding.aget_general_text_embedding \"Permanent link\")\n\n```\naget_general_text_embedding(prompt: str) -> List[float]\n\n```\n\nAsynchronously get Ollama embedding.\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-ollama/llama_index/embeddings/ollama/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>93<br>94<br>95<br>96<br>97<br>98<br>``` | ```<br>async def aget_general_text_embedding(self, prompt: str) -> List[float]:<br>    \"\"\"Asynchronously get Ollama embedding.\"\"\"<br>    result = await self._async_client.embeddings(<br>        model=self.model_name, prompt=prompt, options=self.ollama_additional_kwargs<br>    )<br>    return result[\"embedding\"]<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Ollama - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/ollama/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/vertex/#llama_index.embeddings.vertex.VertexMultiModalEmbedding)\n\n# Vertex\n\n## VertexMultiModalEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/vertex/\\#llama_index.embeddings.vertex.VertexMultiModalEmbedding \"Permanent link\")\n\nBases: `MultiModalEmbedding`\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-vertex/llama_index/embeddings/vertex/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>``` | ```<br>class VertexMultiModalEmbedding(MultiModalEmbedding):<br>    embed_dimension: int = Field(description=\"The vertex output embedding dimension.\")<br>    additional_kwargs: Dict[str, Any] = Field(<br>        default_factory=dict, description=\"Additional kwargs for the Vertex.\"<br>    )<br>    _model: MultiModalEmbeddingModel = PrivateAttr()<br>    _embed_dimension: int = PrivateAttr()<br>    def __init__(<br>        self,<br>        model_name: str = \"multimodalembedding\",<br>        project: Optional[str] = None,<br>        location: Optional[str] = None,<br>        credentials: Optional[Any] = None,<br>        embed_dimension: int = 1408,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>        additional_kwargs: Optional[Dict[str, Any]] = None,<br>    ) -> None:<br>        init_vertexai(project=project, location=location, credentials=credentials)<br>        callback_manager = callback_manager or CallbackManager([])<br>        additional_kwargs = additional_kwargs or {}<br>        super().__init__(<br>            embed_dimension=embed_dimension,<br>            additional_kwargs=additional_kwargs,<br>            model_name=model_name,<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>        )<br>        self._model = MultiModalEmbeddingModel.from_pretrained(model_name)<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"VertexMultiModalEmbedding\"<br>    def _get_text_embedding(self, text: str) -> Embedding:<br>        return self._model.get_embeddings(<br>            contextual_text=text,<br>            dimension=self.embed_dimension,<br>            **self.additional_kwargs<br>        ).text_embedding<br>    def _get_image_embedding(self, img_file_path: ImageType) -> Embedding:<br>        if isinstance(img_file_path, str):<br>            image = Image.load_from_file(img_file_path)<br>        else:<br>            image = Image(image_bytes=img_file_path.getvalue())<br>        embeddings = self._model.get_embeddings(<br>            image=image, dimension=self.embed_dimension, **self.additional_kwargs<br>        )<br>        return embeddings.image_embedding<br>    def _get_query_embedding(self, query: str) -> Embedding:<br>        return self._get_text_embedding(query)<br>    # Vertex AI SDK does not support async variants yet<br>    async def _aget_text_embedding(self, text: str) -> Embedding:<br>        return self._get_text_embedding(text)<br>    async def _aget_image_embedding(self, img_file_path: ImageType) -> Embedding:<br>        return self._get_image_embedding(img_file_path)<br>    async def _aget_query_embedding(self, query: str) -> Embedding:<br>        return self._get_query_embedding(query)<br>``` |\n\n## VertexTextEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/vertex/\\#llama_index.embeddings.vertex.VertexTextEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-vertex/llama_index/embeddings/vertex/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>``` | ```<br>class VertexTextEmbedding(BaseEmbedding):<br>    embed_mode: VertexEmbeddingMode = Field(description=\"The embedding mode to use.\")<br>    additional_kwargs: Dict[str, Any] = Field(<br>        default_factory=dict, description=\"Additional kwargs for the Vertex.\"<br>    )<br>    client_email: Optional[str] = Field(<br>        description=\"The client email for the VertexAI credentials.\"<br>    )<br>    token_uri: Optional[str] = Field(<br>        description=\"The token URI for the VertexAI credentials.\"<br>    )<br>    private_key_id: Optional[str] = Field(<br>        description=\"The private key ID for the VertexAI credentials.\"<br>    )<br>    private_key: Optional[str] = Field(<br>        description=\"The private key for the VertexAI credentials.\"<br>    )<br>    _model: TextEmbeddingModel = PrivateAttr()<br>    def __init__(<br>        self,<br>        model_name: str = \"textembedding-gecko@003\",<br>        project: Optional[str] = None,<br>        location: Optional[str] = None,<br>        credentials: Optional[auth_credentials.Credentials] = None,<br>        embed_mode: VertexEmbeddingMode = VertexEmbeddingMode.RETRIEVAL_MODE,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        callback_manager: Optional[CallbackManager] = None,<br>        additional_kwargs: Optional[Dict[str, Any]] = None,<br>        num_workers: Optional[int] = None,<br>        client_email: Optional[str] = None,<br>        token_uri: Optional[str] = None,<br>        private_key_id: Optional[str] = None,<br>        private_key: Optional[str] = None,<br>    ) -> None:<br>        if credentials is None:<br>            if client_email and token_uri and private_key_id and private_key:<br>                info = {<br>                    \"client_email\": client_email,<br>                    \"token_uri\": token_uri,<br>                    \"private_key_id\": private_key_id,<br>                    \"private_key\": private_key.replace(\"\\\\n\", \"\\n\"),<br>                }<br>                credentials = service_account.Credentials.from_service_account_info(<br>                    info<br>                )<br>            else:<br>                raise ValueError(<br>                    \"Either provide credentials or all of client_email, token_uri, private_key_id, and private_key.\"<br>                )<br>        init_vertexai(project=project, location=location, credentials=credentials)<br>        callback_manager = callback_manager or CallbackManager([])<br>        additional_kwargs = additional_kwargs or {}<br>        super().__init__(<br>            embed_mode=embed_mode,<br>            project=project,<br>            location=location,<br>            credentials=credentials,<br>            additional_kwargs=additional_kwargs,<br>            model_name=model_name,<br>            embed_batch_size=embed_batch_size,<br>            callback_manager=callback_manager,<br>            num_workers=num_workers,<br>            client_email=client_email,<br>            token_uri=token_uri,<br>            private_key_id=private_key_id,<br>            private_key=private_key,<br>        )<br>        self._model = TextEmbeddingModel.from_pretrained(model_name)<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"VertexTextEmbedding\"<br>    def _get_text_embeddings(self, texts: List[str]) -> List[Embedding]:<br>        texts = _get_embedding_request(<br>            texts=texts,<br>            embed_mode=self.embed_mode,<br>            is_query=False,<br>            model_name=self.model_name,<br>        )<br>        embeddings = self._model.get_embeddings(texts, **self.additional_kwargs)<br>        return [embedding.values for embedding in embeddings]<br>    def _get_text_embedding(self, text: str) -> Embedding:<br>        return self._get_text_embeddings([text])[0]<br>    async def _aget_text_embedding(self, text: str) -> Embedding:<br>        return (await self._aget_text_embeddings([text]))[0]<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[Embedding]:<br>        texts = _get_embedding_request(<br>            texts=texts,<br>            embed_mode=self.embed_mode,<br>            is_query=False,<br>            model_name=self.model_name,<br>        )<br>        embeddings = await self._model.get_embeddings_async(<br>            texts, **self.additional_kwargs<br>        )<br>        return [embedding.values for embedding in embeddings]<br>    def _get_query_embedding(self, query: str) -> Embedding:<br>        texts = _get_embedding_request(<br>            texts=[query],<br>            embed_mode=self.embed_mode,<br>            is_query=True,<br>            model_name=self.model_name,<br>        )<br>        embeddings = self._model.get_embeddings(texts, **self.additional_kwargs)<br>        return embeddings[0].values<br>    async def _aget_query_embedding(self, query: str) -> Embedding:<br>        texts = _get_embedding_request(<br>            texts=[query],<br>            embed_mode=self.embed_mode,<br>            is_query=True,<br>            model_name=self.model_name,<br>        )<br>        embeddings = await self._model.get_embeddings_async(<br>            texts, **self.additional_kwargs<br>        )<br>        return embeddings[0].values<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Vertex - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/vertex/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/agent/dashscope/#llama_index.agent.dashscope.DashScopeAgent)\n\n# Dashscope\n\n## DashScopeAgent [\\#](https://docs.llamaindex.ai/en/stable/api_reference/agent/dashscope/\\#llama_index.agent.dashscope.DashScopeAgent \"Permanent link\")\n\nBases: `BaseAgent`\n\nDashScope agent simple wrapper for Alibaba cloud bailian high-level agent api.\n\nSource code in `llama-index-integrations/agent/llama-index-agent-dashscope/llama_index/agent/dashscope/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>``` | ```<br>class DashScopeAgent(BaseAgent):<br>    \"\"\"<br>    DashScope agent simple wrapper for Alibaba cloud bailian high-level agent api.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        app_id: str,<br>        chat_session: bool = True,<br>        workspace: str = None,<br>        api_key: str = None,<br>        verbose: bool = False,<br>    ) -> None:<br>        \"\"\"Init params.<br>        Args:<br>            app_id (str): id of Alibaba cloud bailian application<br>            chat_session (bool): When need to keep chat session, defaults to True.<br>            workspace(str, `optional`): Workspace of Alibaba cloud bailian<br>            api_key (str, optional): The api api_key, can be None,<br>                if None, will get from ENV DASHSCOPE_API_KEY.<br>            verbose: Output verbose info or not.<br>        \"\"\"<br>        self.app_id = app_id<br>        self.chat_session = chat_session<br>        self.workspace = workspace<br>        self.api_key = api_key<br>        self._verbose = verbose<br>        self._session_id = None<br>    @trace_method(\"chat\")<br>    def chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None, **kwargs<br>    ) -> AgentChatResponse:<br>        return self._chat(message=message, stream=False, **kwargs)<br>    async def achat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> AgentChatResponse:<br>        raise NotImplementedError(\"achat not implemented\")<br>    @trace_method(\"chat\")<br>    def stream_chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None, **kwargs<br>    ) -> StreamingAgentChatResponse:<br>        return self._chat(message=message, stream=True, **kwargs)<br>    async def astream_chat(<br>        self, message: str, chat_history: Optional[List[ChatMessage]] = None<br>    ) -> StreamingAgentChatResponse:<br>        raise NotImplementedError(\"astream_chat not implemented\")<br>    def reset(self) -> None:<br>        self._session_id = None<br>    @property<br>    def chat_history(self) -> List[ChatMessage]:<br>        raise NotImplementedError(\"chat_history not implemented\")<br>    @property<br>    def get_session_id(self) -> str:<br>        return self._session_id<br>    def _chat(<br>        self,<br>        message: str,<br>        stream: bool = False,<br>        chat_history: Optional[List[ChatMessage]] = None,<br>        **kwargs,<br>    ) -> Union[AgentChatResponse, StreamingAgentChatResponse]:<br>        \"\"\"Call app completion service.<br>        Args:<br>            message (str): Message for chatting with LLM.<br>            chat_history (List[ChatMessage], `optional`): The user provided chat history. Defaults to None.<br>            **kwargs:<br>                session_id(str, `optional`): Session if for multiple rounds call.<br>                biz_params(dict, `optional`): The extra parameters for flow or plugin.<br>        Raises:<br>            ValueError: The request failed with http code and message.<br>        Returns:<br>            Union[AgentChatResponse, StreamingAgentChatResponse]<br>        \"\"\"<br>        if stream:<br>            kwargs[\"stream\"] = True<br>        if self.chat_session:<br>            kwargs[\"session_id\"] = self._session_id<br>        response = Application.call(<br>            app_id=self.app_id,<br>            prompt=message,<br>            history=None,<br>            workspace=self.workspace,<br>            api_key=self.api_key,<br>            **kwargs,<br>        )<br>        if stream:<br>            return StreamingAgentChatResponse(<br>                chat_stream=(self.from_dashscope_response(rsp) for rsp in response)<br>            )<br>        else:<br>            if response.status_code != HTTPStatus.OK:<br>                raise ValueError(<br>                    f\"Chat failed with status: {response.status_code}, request id: {response.request_id}, \"<br>                    f\"code: {response.code}, message: {response.message}\"<br>                )<br>            if self._verbose:<br>                print(\"Got chat response: %s\" % response)<br>            self._session_id = response.output.session_id<br>            return AgentChatResponse(response=response.output.text)<br>    def from_dashscope_response(self, response: ApplicationResponse) -> ChatResponse:<br>        if response.status_code != HTTPStatus.OK:<br>            raise ValueError(<br>                f\"Chat failed with status: {response.status_code}, request id: {response.request_id}, \"<br>                f\"code: {response.code}, message: {response.message}\"<br>            )<br>        if self._verbose and response.output.finish_reason == \"stop\":<br>            print(\"Got final chat response: %s\" % response)<br>        self._session_id = response.output.session_id<br>        return ChatResponse(<br>            message=ChatMessage(<br>                role=MessageRole.ASSISTANT, content=response.output.text<br>            )<br>        )<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Dashscope - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/agent/dashscope/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/token_counter/#llama_index.core.callbacks.token_counting.TokenCountingHandler)\n\n# Token counter\n\n## TokenCountingHandler [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/token_counter/\\#llama_index.core.callbacks.token_counting.TokenCountingHandler \"Permanent link\")\n\nBases: `PythonicallyPrintingBaseHandler`\n\nCallback handler for counting tokens in LLM and Embedding events.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `tokenizer` | `Optional[Callable[[str], List]]` | Tokenizer to use. Defaults to the global tokenizer<br>(see llama\\_index.core.utils.globals\\_helper). | `None` |\n| `event_starts_to_ignore` | `Optional[List[CBEventType]]` | List of event types to ignore at the start of a trace. | `None` |\n| `event_ends_to_ignore` | `Optional[List[CBEventType]]` | List of event types to ignore at the end of a trace. | `None` |\n\nSource code in `llama-index-core/llama_index/core/callbacks/token_counting.py`\n\n|     |     |\n| --- | --- |\n| ```<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>``` | ```<br>class TokenCountingHandler(PythonicallyPrintingBaseHandler):<br>    \"\"\"Callback handler for counting tokens in LLM and Embedding events.<br>    Args:<br>        tokenizer:<br>            Tokenizer to use. Defaults to the global tokenizer<br>            (see llama_index.core.utils.globals_helper).<br>        event_starts_to_ignore: List of event types to ignore at the start of a trace.<br>        event_ends_to_ignore: List of event types to ignore at the end of a trace.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        tokenizer: Optional[Callable[[str], List]] = None,<br>        event_starts_to_ignore: Optional[List[CBEventType]] = None,<br>        event_ends_to_ignore: Optional[List[CBEventType]] = None,<br>        verbose: bool = False,<br>        logger: Optional[logging.Logger] = None,<br>    ) -> None:<br>        self.llm_token_counts: List[TokenCountingEvent] = []<br>        self.embedding_token_counts: List[TokenCountingEvent] = []<br>        self.tokenizer = tokenizer or get_tokenizer()<br>        self._token_counter = TokenCounter(tokenizer=self.tokenizer)<br>        self._verbose = verbose<br>        super().__init__(<br>            event_starts_to_ignore=event_starts_to_ignore or [],<br>            event_ends_to_ignore=event_ends_to_ignore or [],<br>            logger=logger,<br>        )<br>    def start_trace(self, trace_id: Optional[str] = None) -> None:<br>        return<br>    def end_trace(<br>        self,<br>        trace_id: Optional[str] = None,<br>        trace_map: Optional[Dict[str, List[str]]] = None,<br>    ) -> None:<br>        return<br>    def on_event_start(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        parent_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> str:<br>        return event_id<br>    def on_event_end(<br>        self,<br>        event_type: CBEventType,<br>        payload: Optional[Dict[str, Any]] = None,<br>        event_id: str = \"\",<br>        **kwargs: Any,<br>    ) -> None:<br>        \"\"\"Count the LLM or Embedding tokens as needed.\"\"\"<br>        if (<br>            event_type == CBEventType.LLM<br>            and event_type not in self.event_ends_to_ignore<br>            and payload is not None<br>        ):<br>            self.llm_token_counts.append(<br>                get_llm_token_counts(<br>                    token_counter=self._token_counter,<br>                    payload=payload,<br>                    event_id=event_id,<br>                )<br>            )<br>            if self._verbose:<br>                self._print(<br>                    \"LLM Prompt Token Usage: \"<br>                    f\"{self.llm_token_counts[-1].prompt_token_count}\\n\"<br>                    \"LLM Completion Token Usage: \"<br>                    f\"{self.llm_token_counts[-1].completion_token_count}\",<br>                )<br>        elif (<br>            event_type == CBEventType.EMBEDDING<br>            and event_type not in self.event_ends_to_ignore<br>            and payload is not None<br>        ):<br>            total_chunk_tokens = 0<br>            for chunk in payload.get(EventPayload.CHUNKS, []):<br>                self.embedding_token_counts.append(<br>                    TokenCountingEvent(<br>                        event_id=event_id,<br>                        prompt=chunk,<br>                        prompt_token_count=self._token_counter.get_string_tokens(chunk),<br>                        completion=\"\",<br>                        completion_token_count=0,<br>                    )<br>                )<br>                total_chunk_tokens += self.embedding_token_counts[-1].total_token_count<br>            if self._verbose:<br>                self._print(f\"Embedding Token Usage: {total_chunk_tokens}\")<br>    @property<br>    def total_llm_token_count(self) -> int:<br>        \"\"\"Get the current total LLM token count.\"\"\"<br>        return sum([x.total_token_count for x in self.llm_token_counts])<br>    @property<br>    def prompt_llm_token_count(self) -> int:<br>        \"\"\"Get the current total LLM prompt token count.\"\"\"<br>        return sum([x.prompt_token_count for x in self.llm_token_counts])<br>    @property<br>    def completion_llm_token_count(self) -> int:<br>        \"\"\"Get the current total LLM completion token count.\"\"\"<br>        return sum([x.completion_token_count for x in self.llm_token_counts])<br>    @property<br>    def total_embedding_token_count(self) -> int:<br>        \"\"\"Get the current total Embedding token count.\"\"\"<br>        return sum([x.total_token_count for x in self.embedding_token_counts])<br>    def reset_counts(self) -> None:<br>        \"\"\"Reset the token counts.\"\"\"<br>        self.llm_token_counts = []<br>        self.embedding_token_counts = []<br>``` |\n\n### total\\_llm\\_token\\_count`property`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/token_counter/\\#llama_index.core.callbacks.token_counting.TokenCountingHandler.total_llm_token_count \"Permanent link\")\n\n```\ntotal_llm_token_count: int\n\n```\n\nGet the current total LLM token count.\n\n### prompt\\_llm\\_token\\_count`property`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/token_counter/\\#llama_index.core.callbacks.token_counting.TokenCountingHandler.prompt_llm_token_count \"Permanent link\")\n\n```\nprompt_llm_token_count: int\n\n```\n\nGet the current total LLM prompt token count.\n\n### completion\\_llm\\_token\\_count`property`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/token_counter/\\#llama_index.core.callbacks.token_counting.TokenCountingHandler.completion_llm_token_count \"Permanent link\")\n\n```\ncompletion_llm_token_count: int\n\n```\n\nGet the current total LLM completion token count.\n\n### total\\_embedding\\_token\\_count`property`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/token_counter/\\#llama_index.core.callbacks.token_counting.TokenCountingHandler.total_embedding_token_count \"Permanent link\")\n\n```\ntotal_embedding_token_count: int\n\n```\n\nGet the current total Embedding token count.\n\n### on\\_event\\_end [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/token_counter/\\#llama_index.core.callbacks.token_counting.TokenCountingHandler.on_event_end \"Permanent link\")\n\n```\non_event_end(event_type: CBEventType, payload: Optional[Dict[str, Any]] = None, event_id: str = '', **kwargs: Any) -> None\n\n```\n\nCount the LLM or Embedding tokens as needed.\n\nSource code in `llama-index-core/llama_index/core/callbacks/token_counting.py`\n\n|     |     |\n| --- | --- |\n| ```<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>``` | ```<br>def on_event_end(<br>    self,<br>    event_type: CBEventType,<br>    payload: Optional[Dict[str, Any]] = None,<br>    event_id: str = \"\",<br>    **kwargs: Any,<br>) -> None:<br>    \"\"\"Count the LLM or Embedding tokens as needed.\"\"\"<br>    if (<br>        event_type == CBEventType.LLM<br>        and event_type not in self.event_ends_to_ignore<br>        and payload is not None<br>    ):<br>        self.llm_token_counts.append(<br>            get_llm_token_counts(<br>                token_counter=self._token_counter,<br>                payload=payload,<br>                event_id=event_id,<br>            )<br>        )<br>        if self._verbose:<br>            self._print(<br>                \"LLM Prompt Token Usage: \"<br>                f\"{self.llm_token_counts[-1].prompt_token_count}\\n\"<br>                \"LLM Completion Token Usage: \"<br>                f\"{self.llm_token_counts[-1].completion_token_count}\",<br>            )<br>    elif (<br>        event_type == CBEventType.EMBEDDING<br>        and event_type not in self.event_ends_to_ignore<br>        and payload is not None<br>    ):<br>        total_chunk_tokens = 0<br>        for chunk in payload.get(EventPayload.CHUNKS, []):<br>            self.embedding_token_counts.append(<br>                TokenCountingEvent(<br>                    event_id=event_id,<br>                    prompt=chunk,<br>                    prompt_token_count=self._token_counter.get_string_tokens(chunk),<br>                    completion=\"\",<br>                    completion_token_count=0,<br>                )<br>            )<br>            total_chunk_tokens += self.embedding_token_counts[-1].total_token_count<br>        if self._verbose:<br>            self._print(f\"Embedding Token Usage: {total_chunk_tokens}\")<br>``` |\n\n### reset\\_counts [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/token_counter/\\#llama_index.core.callbacks.token_counting.TokenCountingHandler.reset_counts \"Permanent link\")\n\n```\nreset_counts() -> None\n\n```\n\nReset the token counts.\n\nSource code in `llama-index-core/llama_index/core/callbacks/token_counting.py`\n\n|     |     |\n| --- | --- |\n| ```<br>260<br>261<br>262<br>263<br>``` | ```<br>def reset_counts(self) -> None:<br>    \"\"\"Reset the token counts.\"\"\"<br>    self.llm_token_counts = []<br>    self.embedding_token_counts = []<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Token counter - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/token_counter/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/litellm/#llama_index.embeddings.litellm.LiteLLMEmbedding)\n\n# Litellm\n\n## LiteLLMEmbedding [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/litellm/\\#llama_index.embeddings.litellm.LiteLLMEmbedding \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-litellm/llama_index/embeddings/litellm/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>71<br>72<br>73<br>74<br>75<br>76<br>77<br>78<br>79<br>80<br>81<br>82<br>83<br>84<br>85<br>86<br>87<br>88<br>``` | ```<br>class LiteLLMEmbedding(BaseEmbedding):<br>    model_name: str = Field(description=\"The name of the embedding model.\")<br>    api_key: Optional[str] = Field(<br>        default=None,<br>        description=\"OpenAI key. If not provided, the proxy server must be configured with the key.\",<br>    )<br>    api_base: Optional[str] = Field(<br>        default=None, description=\"The base URL of the LiteLLM proxy.\"<br>    )<br>    dimensions: Optional[int] = Field(<br>        default=None,<br>        description=(<br>            \"The number of dimensions the resulting output embeddings should have. \"<br>            \"Only supported in text-embedding-3 and later models.\"<br>        ),<br>    )<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"lite-llm\"<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        return self._get_query_embedding(query)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        return self._get_text_embedding(text)<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        embeddings = get_embeddings(<br>            api_key=self.api_key,<br>            api_base=self.api_base,<br>            model_name=self.model_name,<br>            dimensions=self.dimensions,<br>            input=[query],<br>        )<br>        return embeddings[0]<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        embeddings = get_embeddings(<br>            api_key=self.api_key,<br>            api_base=self.api_base,<br>            model_name=self.model_name,<br>            dimensions=self.dimensions,<br>            input=[text],<br>        )<br>        return embeddings[0]<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        return get_embeddings(<br>            api_key=self.api_key,<br>            api_base=self.api_base,<br>            model_name=self.model_name,<br>            dimensions=self.dimensions,<br>            input=texts,<br>        )<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Litellm - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/litellm/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/literalai/#llama_index.callbacks.literalai.literalai_callback_handler)\n\n# Literalai\n\n## literalai\\_callback\\_handler [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/literalai/\\#llama_index.callbacks.literalai.literalai_callback_handler \"Permanent link\")\n\n```\nliteralai_callback_handler(batch_size: int = 5, api_key: Optional[str] = None, url: Optional[str] = None, environment: Optional[str] = None, disabled: bool = False) -> BaseCallbackHandler\n\n```\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-literalai/llama_index/callbacks/literalai/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>37<br>38<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>``` | ```<br>def literalai_callback_handler(<br>    batch_size: int = 5,<br>    api_key: Optional[str] = None,<br>    url: Optional[str] = None,<br>    environment: Optional[str] = None,<br>    disabled: bool = False,<br>) -> BaseCallbackHandler:<br>    try:<br>        from literalai import LiteralClient<br>        from literalai.my_types import Environment<br>        literalai_client = LiteralClient(<br>            batch_size=batch_size,<br>            api_key=api_key,<br>            url=url,<br>            environment=cast(Environment, environment),<br>            disabled=disabled,<br>        )<br>        literalai_client.instrument_llamaindex()<br>        class QueryEndEventHandler(BaseEventHandler):<br>            \"\"\"This handler will flush the Literal Client cache to Literal AI at the end of each query.\"\"\"<br>            @classmethod<br>            def class_name(cls) -> str:<br>                \"\"\"Class name.\"\"\"<br>                return \"QueryEndEventHandler\"<br>            def handle(self, event: BaseEvent, **kwargs) -> None:<br>                \"\"\"Flushes the Literal cache when receiving the QueryEnd event.\"\"\"<br>                try:<br>                    if isinstance(event, QueryEndEvent):<br>                        literalai_client.flush()<br>                except Exception as e:<br>                    logging.error(<br>                        \"Error in Literal AI global handler : %s\",<br>                        str(e),<br>                        exc_info=True,<br>                    )<br>        dispatcher = get_dispatcher()<br>        event_handler = QueryEndEventHandler()<br>        dispatcher.add_event_handler(event_handler)<br>    except ImportError:<br>        raise ImportError(<br>            \"Please install the Literal AI Python SDK with `pip install -U literalai`\"<br>        )<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Literalai - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/literalai/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/ibm/#llama_index.embeddings.ibm.WatsonxEmbeddings)\n\n# Ibm\n\n## WatsonxEmbeddings [\\#](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/ibm/\\#llama_index.embeddings.ibm.WatsonxEmbeddings \"Permanent link\")\n\nBases: `BaseEmbedding`\n\nIBM watsonx.ai embeddings.\n\nExample\n\n`pip install llama-index-embeddings-ibm`\n\n```\nfrom llama_index.embeddings.ibm import WatsonxEmbeddings\nwatsonx_llm = WatsonxEmbeddings(\n    model_id=\"ibm/slate-125m-english-rtrvr\",\n    url=\"https://us-south.ml.cloud.ibm.com\",\n    apikey=\"*****\",\n    project_id=\"*****\",\n)\n\n```\n\nSource code in `llama-index-integrations/embeddings/llama-index-embeddings-ibm/llama_index/embeddings/ibm/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>``` | ````<br>class WatsonxEmbeddings(BaseEmbedding):<br>    \"\"\"<br>    IBM watsonx.ai embeddings.<br>    Example:<br>        `pip install llama-index-embeddings-ibm`<br>        ```python<br>        from llama_index.embeddings.ibm import WatsonxEmbeddings<br>        watsonx_llm = WatsonxEmbeddings(<br>            model_id=\"ibm/slate-125m-english-rtrvr\",<br>            url=\"https://us-south.ml.cloud.ibm.com\",<br>            apikey=\"*****\",<br>            project_id=\"*****\",<br>        )<br>        ```<br>    \"\"\"<br>    model_id: str = Field(<br>        default=DEFAULT_EMBED_MODEL,<br>        description=\"\"\"Type of model to use.\"\"\",<br>        allow_mutation=False,<br>    )<br>    truncate_input_tokens: Optional[int] = Field(<br>        default=None,<br>        description=\"\"\"Represents the maximum number of input tokens accepted.\"\"\",<br>    )<br>    project_id: Optional[str] = Field(<br>        default=None,<br>        description=\"ID of the Watson Studio project.\",<br>        allow_mutation=False,<br>    )<br>    space_id: Optional[str] = Field(<br>        default=None,<br>        description=\"\"\"ID of the Watson Studio space.\"\"\",<br>        allow_mutation=False,<br>    )<br>    url: Optional[SecretStr] = Field(<br>        default=None,<br>        description=\"\"\"Url to Watson Machine Learning or CPD instance\"\"\",<br>        allow_mutation=False,<br>    )<br>    apikey: Optional[SecretStr] = Field(<br>        default=None,<br>        description=\"\"\"Apikey to Watson Machine Learning or CPD instance\"\"\",<br>        allow_mutation=False,<br>    )<br>    token: Optional[SecretStr] = Field(<br>        default=None, description=\"\"\"Token to CPD instance\"\"\", allow_mutation=False<br>    )<br>    password: Optional[SecretStr] = Field(<br>        default=None, description=\"\"\"Password to CPD instance\"\"\", allow_mutation=False<br>    )<br>    username: Optional[SecretStr] = Field(<br>        default=None, description=\"\"\"Username to CPD instance\"\"\", allow_mutation=False<br>    )<br>    instance_id: Optional[SecretStr] = Field(<br>        default=None,<br>        description=\"\"\"Instance_id of CPD instance\"\"\",<br>        allow_mutation=False,<br>    )<br>    version: Optional[SecretStr] = Field(<br>        default=None, description=\"\"\"Version of CPD instance\"\"\", allow_mutation=False<br>    )<br>    verify: Union[str, bool, None] = Field(<br>        default=None,<br>        description=\"\"\"User can pass as verify one of following:<br>        the path to a CA_BUNDLE file<br>        the path of directory with certificates of trusted CAs<br>        True - default path to truststore will be taken<br>        False - no verification will be made\"\"\",<br>        allow_mutation=False,<br>    )<br>    _embed_model: Embeddings = PrivateAttr()<br>    def __init__(<br>        self,<br>        model_id: str,<br>        truncate_input_tokens: Optional[int] = None,<br>        embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,<br>        project_id: Optional[str] = None,<br>        space_id: Optional[str] = None,<br>        url: Optional[str] = None,<br>        apikey: Optional[str] = None,<br>        token: Optional[str] = None,<br>        password: Optional[str] = None,<br>        username: Optional[str] = None,<br>        instance_id: Optional[str] = None,<br>        version: Optional[str] = None,<br>        verify: Union[str, bool, None] = None,<br>        api_client: Optional[APIClient] = None,<br>        callback_manager: Optional[CallbackManager] = None,<br>        **kwargs: Any,<br>    ):<br>        callback_manager = callback_manager or CallbackManager([])<br>        if isinstance(api_client, APIClient):<br>            project_id = api_client.default_project_id or project_id<br>            space_id = api_client.default_space_id or space_id<br>            creds = {}<br>        else:<br>            creds = resolve_watsonx_credentials(<br>                url=url,<br>                apikey=apikey,<br>                token=token,<br>                username=username,<br>                password=password,<br>                instance_id=instance_id,<br>            )<br>        url = creds.get(\"url\").get_secret_value() if creds.get(\"url\") else None<br>        apikey = creds.get(\"apikey\").get_secret_value() if creds.get(\"apikey\") else None<br>        token = creds.get(\"token\").get_secret_value() if creds.get(\"token\") else None<br>        password = (<br>            creds.get(\"password\").get_secret_value() if creds.get(\"password\") else None<br>        )<br>        username = (<br>            creds.get(\"username\").get_secret_value() if creds.get(\"username\") else None<br>        )<br>        instance_id = (<br>            creds.get(\"instance_id\").get_secret_value()<br>            if creds.get(\"instance_id\")<br>            else None<br>        )<br>        super().__init__(<br>            model_id=model_id,<br>            truncate_input_tokens=truncate_input_tokens,<br>            project_id=project_id,<br>            space_id=space_id,<br>            url=url,<br>            apikey=apikey,<br>            token=token,<br>            password=password,<br>            username=username,<br>            instance_id=instance_id,<br>            version=version,<br>            verify=verify,<br>            callback_manager=callback_manager,<br>            embed_batch_size=embed_batch_size,<br>            **kwargs,<br>        )<br>        self._embed_model = Embeddings(<br>            model_id=model_id,<br>            params=self.params,<br>            credentials=(<br>                Credentials.from_dict(<br>                    {<br>                        key: value.get_secret_value() if value else None<br>                        for key, value in self._get_credential_kwargs().items()<br>                    },<br>                    _verify=self.verify,<br>                )<br>                if creds<br>                else None<br>            ),<br>            project_id=self.project_id,<br>            space_id=self.space_id,<br>            api_client=api_client,<br>        )<br>    class Config:<br>        validate_assignment = True<br>    @classmethod<br>    def class_name(cls) -> str:<br>        return \"WatsonxEmbedding\"<br>    def _get_credential_kwargs(self) -> Dict[str, SecretStr | None]:<br>        return {<br>            \"url\": self.url,<br>            \"apikey\": self.apikey,<br>            \"token\": self.token,<br>            \"password\": self.password,<br>            \"username\": self.username,<br>            \"instance_id\": self.instance_id,<br>            \"version\": self.version,<br>        }<br>    @property<br>    def params(self) -> Dict[str, int] | None:<br>        return (<br>            {\"truncate_input_tokens\": self.truncate_input_tokens}<br>            if self.truncate_input_tokens<br>            else None<br>        )<br>    def _get_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"Get query embedding.\"\"\"<br>        return self._embed_model.embed_query(text=query, params=self.params)<br>    def _get_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Get text embedding.\"\"\"<br>        return self._get_query_embedding(query=text)<br>    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Get text embeddings.\"\"\"<br>        return self._embed_model.embed_documents(texts=texts, params=self.params)<br>    ### Async methods<br>    # Asynchronous evaluation is not yet supported for watsonx.ai embeddings<br>    async def _aget_query_embedding(self, query: str) -> List[float]:<br>        \"\"\"The asynchronous version of _get_query_embedding.\"\"\"<br>        return self._get_query_embedding(query)<br>    async def _aget_text_embedding(self, text: str) -> List[float]:<br>        \"\"\"Asynchronously get text embedding.\"\"\"<br>        return self._get_text_embedding(text)<br>    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:<br>        \"\"\"Asynchronously get text embeddings.\"\"\"<br>        return self._get_text_embeddings(texts)<br>```` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Ibm - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/embeddings/ibm/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/faithfullness/#llama_index.core.evaluation.FaithfulnessEvaluator)\n\n# Faithfullness\n\nEvaluation modules.\n\n## FaithfulnessEvaluator [\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/faithfullness/\\#llama_index.core.evaluation.FaithfulnessEvaluator \"Permanent link\")\n\nBases: `BaseEvaluator`\n\nFaithfulness evaluator.\n\nEvaluates whether a response is faithful to the contexts\n(i.e. whether the response is supported by the contexts or hallucinated.)\n\nThis evaluator only considers the response string and the list of context strings.\n\n**Parameters:**\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `raise_error(bool)` |  | Whether to raise an error when the response is invalid.<br>Defaults to False. | _required_ |\n| `eval_template(Optional[Union[str,` | `BasePromptTemplate]]` | The template to use for evaluation. | _required_ |\n| `refine_template(Optional[Union[str,` | `BasePromptTemplate]]` | The template to use for refining the evaluation. | _required_ |\n\nSource code in `llama-index-core/llama_index/core/evaluation/faithfulness.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>``` | ```<br>class FaithfulnessEvaluator(BaseEvaluator):<br>    \"\"\"<br>    Faithfulness evaluator.<br>    Evaluates whether a response is faithful to the contexts<br>    (i.e. whether the response is supported by the contexts or hallucinated.)<br>    This evaluator only considers the response string and the list of context strings.<br>    Args:<br>        raise_error(bool): Whether to raise an error when the response is invalid.<br>            Defaults to False.<br>        eval_template(Optional[Union[str, BasePromptTemplate]]):<br>            The template to use for evaluation.<br>        refine_template(Optional[Union[str, BasePromptTemplate]]):<br>            The template to use for refining the evaluation.<br>    \"\"\"<br>    def __init__(<br>        self,<br>        llm: Optional[LLM] = None,<br>        raise_error: bool = False,<br>        eval_template: Optional[Union[str, BasePromptTemplate]] = None,<br>        refine_template: Optional[Union[str, BasePromptTemplate]] = None,<br>    ) -> None:<br>        \"\"\"Init params.\"\"\"<br>        self._llm = llm or Settings.llm<br>        self._raise_error = raise_error<br>        self._eval_template: BasePromptTemplate<br>        if isinstance(eval_template, str):<br>            self._eval_template = PromptTemplate(eval_template)<br>        if isinstance(eval_template, BasePromptTemplate):<br>            self._eval_template = eval_template<br>        else:<br>            model_name = self._llm.metadata.model_name<br>            self._eval_template = TEMPLATES_CATALOG.get(<br>                model_name, DEFAULT_EVAL_TEMPLATE<br>            )<br>        self._refine_template: BasePromptTemplate<br>        if isinstance(refine_template, str):<br>            self._refine_template = PromptTemplate(refine_template)<br>        else:<br>            self._refine_template = refine_template or DEFAULT_REFINE_TEMPLATE<br>    def _get_prompts(self) -> PromptDictType:<br>        \"\"\"Get prompts.\"\"\"<br>        return {<br>            \"eval_template\": self._eval_template,<br>            \"refine_template\": self._refine_template,<br>        }<br>    def _update_prompts(self, prompts: PromptDictType) -> None:<br>        \"\"\"Update prompts.\"\"\"<br>        if \"eval_template\" in prompts:<br>            self._eval_template = prompts[\"eval_template\"]<br>        if \"refine_template\" in prompts:<br>            self._refine_template = prompts[\"refine_template\"]<br>    async def aevaluate(<br>        self,<br>        query: str | None = None,<br>        response: str | None = None,<br>        contexts: Sequence[str] | None = None,<br>        sleep_time_in_seconds: int = 0,<br>        **kwargs: Any,<br>    ) -> EvaluationResult:<br>        \"\"\"Evaluate whether the response is faithful to the contexts.\"\"\"<br>        del kwargs  # Unused<br>        await asyncio.sleep(sleep_time_in_seconds)<br>        if contexts is None or response is None:<br>            raise ValueError(\"contexts and response must be provided\")<br>        docs = [Document(text=context) for context in contexts]<br>        index = SummaryIndex.from_documents(docs)<br>        query_engine = index.as_query_engine(<br>            llm=self._llm,<br>            text_qa_template=self._eval_template,<br>            refine_template=self._refine_template,<br>        )<br>        response_obj = await query_engine.aquery(response)<br>        raw_response_txt = str(response_obj)<br>        if \"yes\" in raw_response_txt.lower():<br>            passing = True<br>        else:<br>            passing = False<br>            if self._raise_error:<br>                raise ValueError(\"The response is invalid\")<br>        return EvaluationResult(<br>            query=query,<br>            response=response,<br>            contexts=contexts,<br>            passing=passing,<br>            score=1.0 if passing else 0.0,<br>            feedback=raw_response_txt,<br>        )<br>``` |\n\n### aevaluate`async`[\\#](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/faithfullness/\\#llama_index.core.evaluation.FaithfulnessEvaluator.aevaluate \"Permanent link\")\n\n```\naevaluate(query: str | None = None, response: str | None = None, contexts: Sequence[str] | None = None, sleep_time_in_seconds: int = 0, **kwargs: Any) -> EvaluationResult\n\n```\n\nEvaluate whether the response is faithful to the contexts.\n\nSource code in `llama-index-core/llama_index/core/evaluation/faithfulness.py`\n\n|     |     |\n| --- | --- |\n| ```<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>``` | ```<br>async def aevaluate(<br>    self,<br>    query: str | None = None,<br>    response: str | None = None,<br>    contexts: Sequence[str] | None = None,<br>    sleep_time_in_seconds: int = 0,<br>    **kwargs: Any,<br>) -> EvaluationResult:<br>    \"\"\"Evaluate whether the response is faithful to the contexts.\"\"\"<br>    del kwargs  # Unused<br>    await asyncio.sleep(sleep_time_in_seconds)<br>    if contexts is None or response is None:<br>        raise ValueError(\"contexts and response must be provided\")<br>    docs = [Document(text=context) for context in contexts]<br>    index = SummaryIndex.from_documents(docs)<br>    query_engine = index.as_query_engine(<br>        llm=self._llm,<br>        text_qa_template=self._eval_template,<br>        refine_template=self._refine_template,<br>    )<br>    response_obj = await query_engine.aquery(response)<br>    raw_response_txt = str(response_obj)<br>    if \"yes\" in raw_response_txt.lower():<br>        passing = True<br>    else:<br>        passing = False<br>        if self._raise_error:<br>            raise ValueError(\"The response is invalid\")<br>    return EvaluationResult(<br>        query=query,<br>        response=response,<br>        contexts=contexts,<br>        passing=passing,<br>        score=1.0 if passing else 0.0,<br>        feedback=raw_response_txt,<br>    )<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Faithfullness - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/evaluation/faithfullness/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    },
    {
      "markdown": "[Skip to content](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/arize_phoenix/#llama_index.callbacks.arize_phoenix.arize_phoenix_callback_handler)\n\n# Arize phoenix\n\n## arize\\_phoenix\\_callback\\_handler [\\#](https://docs.llamaindex.ai/en/stable/api_reference/callbacks/arize_phoenix/\\#llama_index.callbacks.arize_phoenix.arize_phoenix_callback_handler \"Permanent link\")\n\n```\narize_phoenix_callback_handler(**kwargs: Any) -> BaseCallbackHandler\n\n```\n\nSource code in `llama-index-integrations/callbacks/llama-index-callbacks-arize-phoenix/llama_index/callbacks/arize_phoenix/base.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 6<br> 7<br> 8<br> 9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36<br>``` | ```<br>def arize_phoenix_callback_handler(**kwargs: Any) -> BaseCallbackHandler:<br>    # newer versions of arize, v2.x<br>    try:<br>        from openinference.instrumentation.llama_index import LlamaIndexInstrumentor<br>        from opentelemetry.exporter.otlp.proto.http.trace_exporter import (<br>            OTLPSpanExporter,<br>        )<br>        from opentelemetry.sdk import trace as trace_sdk<br>        from opentelemetry.sdk.trace.export import SimpleSpanProcessor<br>        endpoint = kwargs.get(\"endpoint\", \"http://127.0.0.1:6006/v1/traces\")<br>        tracer_provider = trace_sdk.TracerProvider()<br>        tracer_provider.add_span_processor(<br>            SimpleSpanProcessor(OTLPSpanExporter(endpoint))<br>        )<br>        return LlamaIndexInstrumentor().instrument(<br>            tracer_provider=kwargs.get(\"tracer_provider\", tracer_provider)<br>        )<br>    except ImportError:<br>        # using an older version of arize<br>        pass<br>    # older versions of arize, v1.x<br>    try:<br>        from phoenix.trace.llama_index import OpenInferenceTraceCallbackHandler<br>    except ImportError:<br>        raise ImportError(<br>            \"Please install Arize Phoenix with `pip install -q arize-phoenix`\"<br>        )<br>    return OpenInferenceTraceCallbackHandler(**kwargs)<br>``` |\n\nBack to top\n\nHi, how can I help you?\n\n\ud83e\udd99",
      "metadata": {
        "title": "Arize phoenix - LlamaIndex",
        "language": "en",
        "sourceURL": "https://docs.llamaindex.ai/en/stable/api_reference/callbacks/arize_phoenix/",
        "ogLocaleAlternate": [],
        "statusCode": 200
      }
    }
  ]
}