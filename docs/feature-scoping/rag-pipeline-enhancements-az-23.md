# Description
I am at a state that RAG pipeline works end to end, however there are several important limitations in the current
implementation that I'd want to address before calling it a v0.1 release.

**Current state for context:**
- user can parse multiple URLs using FireCrawl
- user can chunk the results, only in markdown format
- chunking is brittle and not ideal or close to sufficient IMO - probably requires the most work
- user can create a local persistent chroma db
- user can directly query db
- user can NOT chat with LLM and it automatically decides when to query db
- llm client doesn't have tools to automatically call the vector db when needed
- llm client doesn't recognize user commands to load more documents if necessary

# Problem statements
1. Improve chunking quality and stability further for markdown docs only - DONE
    - Define a clear list of problems based on parsing top 3 libraries I work with - DONE
    - Define a MECE, flexible chunking approach - DONE
2. No validation of chunking result - DONE
   - No basic checks for preservation of h1 and h2 and h3 headers - DONE
   - No basic checks for content volume preservation - DONE
3. No (basic) evals setup for the RAG app with at least F1, precision, recall metrics
4. No support for LLM chat with RAG tools



# Scope
1. Fix identified chunking issues- DONE
2. Setup basic RAG evaluation suite - POSTPONED
3. Implement flow orchestrator - DONE
   - Sonnet 3.5
   - Tool definition
      - RAG
   - Intelligent dynamic system prompt
4. Re-factor and simplify implementation - DONE
5. Implement chainlit for basic UI - POSTPONED
6. Create a live demo using Supabase or Anthropic docs
7. Create a user guide (md) on how to use key modules and limitations
8. Create an in-depth video on how to use RAGDocs over your own data

## Implementation details
- Dynamic system prompt
  - Whenever user successfully crawls a set of doc -> update the system prompt with a short description of the data
    based on URL, number of pages, a sample content. This can be generated by the LLM itself. Update the system
    prompt with the information about the updated docs. This method should be called by the vector database when the
    documents are successfully loaded.
- Adapt the main LLM prompt to be knowledgable about when to use RAG.

## Simplified flow
- User starts the application (Chainlit interface)
  - Claude Assistant initializes all components (VectorDB, RAGTool, ClaudeAssistant)
  - User inputs a query
  - Orchestrator passes the query to ClaudeAssistant
  - ClaudeAssistant processes the query and decides whether to use RAG
    - If using RAG, retrieves and ranks relevant documents
  - ClaudeAssistant generates a response, incorporating RAG results if applicable
  - Response is displayed to the user
  - Process repeats for subsequent queries, maintaining conversation history in ClaudeAssistant


# Success Metrics
1. Chunking
   - % of headers preserved -> 100%
   - % of tokens preserved -> 100% or close enough, but it will never be 1:1 due to pre-processing
2. RAG evals
   - Avg Precision, Recall, F1, MRR



# Implementation
`Claude` class:
- handles the main conversation flow
**Methods**:
- get_response -> streams (or not) the response to the user query
- get_augmented_response -> provides the response with documents returned from vector db
- rag_search -> orchestrates end-to-end rag search to retrieve relevant documents
